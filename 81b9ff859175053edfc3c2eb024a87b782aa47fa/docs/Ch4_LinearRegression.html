<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>4&nbsp; Linear Regression – Computer-Aided Statistical Analysis (B.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch5_Classification.html" rel="next">
<link href="./Ch3_MatrixAlgebra.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch4_LinearRegression.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Intro2R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_StatLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_MatrixAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_LinearRegression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_ResamplingMethods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#sec-LinModAssumptions" id="toc-sec-LinModAssumptions" class="nav-link active" data-scroll-target="#sec-LinModAssumptions"><span class="header-section-number">4.1</span> Assumptions</a></li>
  <li><a href="#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator" class="nav-link" data-scroll-target="#deriving-the-expression-of-the-ols-estimator"><span class="header-section-number">4.2</span> Deriving the Expression of the OLS Estimator</a></li>
  <li><a href="#assessing-the-accuracy-of-the-model-fit-hatf" id="toc-assessing-the-accuracy-of-the-model-fit-hatf" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-model-fit-hatf"><span class="header-section-number">4.3</span> Assessing the Accuracy of the Model fit <span class="math inline">\(\hat{f}\)</span></a></li>
  <li>
<a href="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" id="toc-assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta"><span class="header-section-number">4.4</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span></a>
  <ul class="collapse">
<li><a href="#bias-of-hatbeta" id="toc-bias-of-hatbeta" class="nav-link" data-scroll-target="#bias-of-hatbeta"><span class="header-section-number">4.4.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span></a></li>
  <li><a href="#standard-error-of-hatbeta_j" id="toc-standard-error-of-hatbeta_j" class="nav-link" data-scroll-target="#standard-error-of-hatbeta_j"><span class="header-section-number">4.4.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span></a></li>
  </ul>
</li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">4.5</span> Exercises</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><blockquote class="blockquote">
<p>Additional Reading: Chapter 3 of our course textbook <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a></p>
</blockquote>
<section id="sec-LinModAssumptions" class="level2" data-number="4.1"><h2 data-number="4.1" class="anchored" data-anchor-id="sec-LinModAssumptions">
<span class="header-section-number">4.1</span> Assumptions</h2>
<p>The (multiple) linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:</p>
<p><strong>Assumption 1: Model and Sampling</strong></p>
<p><strong>Part (a): Linear Model</strong></p>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i= \underbrace{\sum_{k=0}^p\beta_k X_{ik}}_{=f(X_i)}+\epsilon_i, \quad i=1,\dots,n,
\end{align}
\qquad(4.1)\]</span></span> where <span class="math display">\[
X_{i0}=1
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> is called “dependent variable” or “outcome variable” or “regressand” or</li>
<li>
<span class="math inline">\(X_{ik}\)</span> is called the <span class="math inline">\(k\)</span>th “predictor variable” or “regressor” or “explanatory variable” or “control variable.” Each of these names emphasizes a slightly different perspective on <span class="math inline">\(X_{ik}.\)</span>
</li>
</ul>
<p>It is convenient to write <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>4.1</span></a> using matrix notation <span class="math display">\[
\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times (p+1))}{X_i'}\underset{((p+1)\times 1)}{\beta} +\epsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
\]</span> where <span class="math display">\[
  X_i=\left(\begin{matrix}X_{i0}\\ \vdots\\  X_{ip}\end{matrix}\right)
  \quad\text{and}\quad
\beta=\left(\begin{matrix}\beta_0\\ \vdots\\ \beta_p\end{matrix}\right).
\]</span> Stacking all individual rows <span class="math inline">\(i=1,\dots,n\)</span> leads to <span class="math display">\[
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times (p+1))}{X}\underset{((p+1)\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon},
\end{eqnarray*}
\]</span> where <span class="math display">\[
\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{10}&amp;\dots&amp;X_{1(p+1)}\\\vdots&amp;\ddots&amp;\vdots\\ X_{n0}&amp;\dots&amp;X_{n(p+1)}\\\end{matrix}\right),\quad\text{and}\quad \epsilon=\left(\begin{matrix}\epsilon_1\\ \vdots\\ \epsilon_n\end{matrix}\right).
\end{equation*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simple Linear Regression and Polynomial Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The special case of <span class="math inline">\(p=1\)</span> <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i
\]</span> is called the <strong><em>simple</em> linear regression model</strong>. With the simple linear regression model, only straight line fits are possible.</p>
<p>By contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define <span class="math display">\[
X_{i2} := X_{i1}^2
\]</span> which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor <code>Age</code><span class="math inline">\(_i=X_{i1}\)</span>) <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i1}^2 + \epsilon_i.
\]</span> Of course, further predictor variables <span class="math inline">\(X_{i2},\dots,X_{ip}\)</span> can (and should) be added to this model.</p>
<p>The same logic applies to polynomials with higher polynomial degrees <span class="math inline">\((\geq 2).\)</span> Large polynomial degrees, however, can lead to unstable estimation results.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The assumption <span class="math inline">\(f(X_i) = X_i'\beta\)</span> may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.</p>
</div>
</div>
<p><strong>Part (b): Random Sample</strong></p>
<p>We assume that the observed data points <span class="math display">\[
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{n(p+1)}))
\]</span> are a realization of the <strong>training data random sample</strong> <span class="math display">\[
((Y_{1},X_{10},\dots,X_{1(p+1)}),\dots,(Y_{n},X_{n0},\dots,X_{n(p+1)})).
\]</span></p>
<p>That is, the <span class="math inline">\(i\)</span>th observed <span class="math inline">\(p+2\)</span> dimensional data point <span class="math display">\[
(y_{i},x_{i0},\dots,x_{i(p+1)})\in\mathbb{R}^{p+2}
\]</span> is a realization of a <span class="math inline">\(p+2\)</span> dimensional random variable <span class="math display">\[
(Y_{i},X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2},
\]</span> where</p>
<ol type="1">
<li>
<span class="math inline">\((Y_{i},X_{i0},\dots,X_{i(p+1)})\)</span> has the identical <span class="math inline">\(p+2\)</span> dimensional distribution for all <span class="math inline">\(i=1,\dots,n.\)</span>
</li>
<li>
<span class="math inline">\((Y_{i},X_{i0},\dots,X_{i(p+1)})\)</span> is independent of <span class="math inline">\((Y_{j},X_{j0},\dots,X_{j(p+1)})\)</span> for all <span class="math inline">\(i\neq j=1,\dots,n.\)</span>
</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Due to <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>4.1</span></a>, this i.i.d. assumption is equivalent to assuming that the multivariate random variables <span class="math display">\[
(\epsilon_i,X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2}
\]</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Remark:</strong> Often, we do not use a different notation for observed realizations <span class="math inline">\((y_{i},x_{i0},\dots,x_{i(p+1)})\in\mathbb{R}^{p+2}\)</span> and for the corresponding random variable <span class="math inline">\((Y_{i},X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2}\)</span> since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.</p>
</div>
</div>
<p><strong>Assumption 2: Exogeneity</strong> <span id="eq-assExogen"><span class="math display">\[
E(\epsilon_i|X_i)=0,\quad i=1,\dots,n
\qquad(4.2)\]</span></span></p>
<p>This assumption demands that the mean of the random error term <span class="math inline">\(\epsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\)</span>. This exogeneity assumption is also called</p>
<ul>
<li>“orthogonality assumption” or</li>
<li>“mean independence assumption.”</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Together with the random sample assumption (Assumption 1, Part (b)) <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>4.2</span></a> even implies <strong>strict exogeneity</strong> <span class="math display">\[
E(\epsilon|X) = \underset{(n\times 1)}{0},
\]</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>. Under strict exogeneity, the mean of the random error <strong>vector</strong> <span class="math inline">\(\epsilon\in\mathbb{R}^n\)</span> is zero irrespective of the realizations of the <span class="math inline">\((n\times (p+1))\)</span>-dimensional random predictor matrix <span class="math inline">\(X.\)</span></p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Independence between error term and predictors
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let</p>
<ul>
<li>
<span class="math inline">\(E(\epsilon_i)=0\)</span> and</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> be independent of <span class="math inline">\(X_i\)</span>
</li>
</ul>
<p>Here the assumption of exogeneity is fulfilled since by the independence between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> we have that <span class="math display">\[
E(\epsilon_i|X_i) = E(\epsilon_i)
\]</span> and by assumption <span class="math inline">\(E(\epsilon_i)=0\)</span> such that <span class="math display">\[
E(\epsilon_i|X_i) = 0.
\]</span></p>
<p>Note: The assumption <span class="math inline">\(E(\epsilon_i)=0\)</span> is not critical (i.e.&nbsp;not restrictive) due to the intercept term in <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>4.2</span></a>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Heteroskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let</p>
<ul>
<li>
<span class="math inline">\(\epsilon_i\sim\mathcal{N}(0,\sigma_i^2),\)</span> where</li>
<li><span class="math inline">\(\sigma_i = |X_{i1}|\)</span></li>
</ul>
<p>Here the assumption of exogeneity is fulfilled since realizations of <span class="math inline">\(X_i\)</span> do not affect the mean of <span class="math inline">\(\epsilon_i,\)</span> thus <span class="math display">\[
E(\epsilon_i|X_i) = 0.
\]</span></p>
<p>However, <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> are not independent of each other, since the conditional variance of <span class="math inline">\(\epsilon_i\)</span> is a function of <span class="math inline">\(X_{i1}\)</span> <span class="math display">\[
Var(\epsilon_i|X_i) = |X_{i1}|^2.
\]</span></p>
</div>
</div>
<p><strong>Assumption 3: Rank Condition (no perfect multicollinearity)</strong></p>
<p><span class="math display">\[
\begin{align*}
\operatorname{rank}(X)&amp;=(p+1)\quad\text{a.s.}\\
\Leftrightarrow P\big(\operatorname{rank}(X)&amp;=(p+1)\big)=1
\end{align*}
\]</span> This assumption demands that, with probability one, no predictor variable <span class="math inline">\(X_{k}\in\mathbb{R}^n\)</span> is linearly dependent of the others. (This is the literal translation of the “almost surely (a.s.)” concept.)</p>
<p><strong>Note:</strong> The assumption implies that <span class="math inline">\(n\geq (p+1),\)</span> since <span class="math display">\[
\operatorname{rank}(X)\leq \min\{n,(p+1)\}\quad(a.s.)
\]</span></p>
<p>This rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables’ individual effects on <span class="math inline">\(Y\)</span>. Therefore, this assumption is also often called an <strong>identification assumption</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Under Assumption 3, we have that <span class="math display">\[
\operatorname{rank}(X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>This implies that the <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\(X'X\)</span> has full rank, i.e.&nbsp;that <span class="math display">\[
\operatorname{rank}(X'X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>Thus <span class="math inline">\((X'X)\)</span> is invertible; i.e.&nbsp;there exists a <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\((X'X)^{-1}\)</span> such that <span class="math display">\[
(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_{(p+1)}.
\]</span></p></li>
</ul>
</div>
</div>
<p><strong>Assumption 4: Error distribution</strong></p>
<p>There are different more or less restrictive assumptions. Some of the most common ones are the following:</p>
<ul>
<li><p><strong>Conditional distribution with sufficiently many moments:</strong> <span class="math display">\[
\epsilon_i|X_i \sim f_{\epsilon|X}
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_{\epsilon|X}\)</span> with two (or more) finite moments.</p></li>
<li><p><strong>Conditional normal distribution:</strong> <span class="math display">\[
\epsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</p></li>
<li><p><strong>Independence between error and predictors:</strong> <span class="math inline">\(\epsilon_i\sim f_\epsilon\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> such that <span class="math inline">\(f_\epsilon=f_{\epsilon|X}\)</span> and such that <span class="math inline">\(f_\epsilon\)</span> has two (or more) finite moments.</p></li>
<li><p><strong>Independence between error and predictors and normality:</strong> As above, but with <span class="math inline">\(f_\epsilon=\mathcal{N}(0,\sigma^2)\)</span>.</p></li>
<li>
<p><strong>Spherical errors:</strong> The conditional distributions of <span class="math inline">\(\epsilon_i|X_i\)</span> may generally depend on <span class="math inline">\(X_i\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> but only such that <span class="math display">\[
E(\epsilon|X)=\underset{(n\times 1)}{0}
\]</span> and <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\dots&amp;Var(\epsilon_n|X)
\end{matrix}\right)\\[2ex]
&amp; = \left(\begin{matrix}
\sigma^2&amp;0&amp;\dots&amp;0\\
0&amp;\sigma^2&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
\]</span> where <span class="math inline">\(I_n\)</span> denotes the <span class="math inline">\((n\times n)\)</span> identity matrix with ones on the diagonal and zeros else. Thus, under the spherical errors assumption, one has, for all possible realizations of <span class="math inline">\(X\)</span>, that:</p>
<ul>
<li>
<strong>uncorrelated:</strong> <span class="math inline">\(Cov(\epsilon_i,\epsilon_j|X)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(j=1,\dots,n\)</span> such that <span class="math inline">\(i\neq j\)</span>
</li>
<li>
<strong>homoskedastic:</strong> <span class="math inline">\(Var(\epsilon_i|X)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
</ul>
</li>
</ul>
<p>All four Assumptions 1-4 must hold for doing inference using the (multiple) linear regression model.</p>
<section id="homoskedastic-versus-heteroskedastic-error-terms" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="homoskedastic-versus-heteroskedastic-error-terms">Homoskedastic versus Heteroskedastic Error Terms</h4>
<p>The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\((X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+1}\)</span>. That is, the error term <span class="math inline">\(\epsilon_i\)</span> can have a conditional distribution which depends on <span class="math inline">\((X_{i0},\dots,X_{i(p+1)}).\)</span></p>
<p>The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of <span class="math inline">\(\epsilon_i\)</span> is independent of <span class="math inline">\(X_i\)</span>. Besides this, dependencies between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_{i0},\dots,X_{i(p+1)}\)</span> are allowed. For instance, the variance of <span class="math inline">\(\epsilon_i\)</span> can be a function of <span class="math inline">\(X_{i0},\dots,X_{i(p+1)}.\)</span> If this is the case, <span class="math inline">\(\epsilon_i\)</span> is said to be <strong>“heteroskedastic.”</strong></p>
<ul>
<li><p><strong>Heteroskedastic error terms:</strong> The conditional variances <span class="math display">\[
Var(\epsilon_i|X_i=x_i)=\sigma^2(x_i)
\]</span> are a non-constant function <span class="math inline">\(\sigma^2(x_i)&gt;0\)</span> of the realizations <span class="math inline">\(X_i=x_i.\)</span></p></li>
<li><p><strong>Homoskedastic error terms:</strong> The conditional variances <span class="math display">\[
Var(\epsilon_i|X_i=x_i)=\sigma^2
\]</span> are constant <span class="math inline">\(\sigma^2&gt;0\)</span> for every possible realization <span class="math inline">\(X_i=x_i.\)</span></p></li>
</ul>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Heteroskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\epsilon_i|X_i\sim \mathcal{U}[-0.5|X_{i2}|, 0.5|X_{i2}|],
\]</span> with <span class="math display">\[
X_{i2}\sim \mathcal{U}[-4,4]
\]</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> where <span class="math inline">\(\mathcal{U}[a,b]\)</span> denotes the uniform distribution over <span class="math inline">\([a,b].\)</span></p>
<p>This error term is mean independent of <span class="math inline">\(X_i\)</span> since <span class="math inline">\(E(\epsilon_i|X_i)=0\)</span>, but it has a heteroskedastic conditional variance since <span class="math display">\[
Var(\epsilon_i|X_i)=\frac{1}{12}X_{i2}^2
\]</span> depends on <span class="math inline">\(X_{i2}.\)</span></p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Homoskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\epsilon_i\sim{\mathcal N} (0, \sigma^2)
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> Here, the conditional variance of the error terms <span class="math inline">\(\epsilon_i\)</span> given <span class="math inline">\(X_i\)</span> <span class="math display">\[
Var(\epsilon_i|X_i)=Var(\epsilon_i)=\sigma^2
\]</span> are equal to the constant <span class="math inline">\(\sigma^2&gt;0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for every possible realization of <span class="math inline">\(X_i.\)</span></p>
</div>
</div>
</section></section><section id="deriving-the-expression-of-the-ols-estimator" class="level2" data-number="4.2"><h2 data-number="4.2" class="anchored" data-anchor-id="deriving-the-expression-of-the-ols-estimator">
<span class="header-section-number">4.2</span> Deriving the Expression of the OLS Estimator</h2>
<p>We derive the expression for the OLS estimator <span class="math display">\[
\hat\beta=(\hat\beta_0,\dots,\hat\beta_p)'\in\mathbb{R}^{p+1}
\]</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math display">\[
\operatorname{RSS}(b)=\sum_{i=1}^n\big(\underbrace{Y_i-X_i'b}_{\text{$i$th residual}}\big)^2
\]</span> with <span class="math inline">\(b\in\mathbb{R}^K\)</span>, for a given sample <span class="math display">\[
((Y_1,X_1),\dots,(Y_n,X_n)).
\]</span></p>
<p>Using matrix/vector notation we can write <span class="math inline">\(S_n(b)\)</span> as <span class="math display">\[
\begin{align*}
\operatorname{RSS}(b)
&amp;=\sum_{i=1}^n(Y_i-X_i'b)^2\\[2ex]
&amp;=(Y-X b)^{\prime}(Y-X b)\\[2ex]
&amp;=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
\]</span> To find the minimizing argument <span class="math display">\[
\hat\beta = \arg\min_{b\in\mathbb{R}^{p+1}}\operatorname{RSS}(b)
\]</span> we compute the vector containing all partial derivatives <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\frac{\partial \operatorname{RSS}(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{align*}
\]</span> Setting each partial derivative to zero leads to <span class="math inline">\((p+1)\)</span> linear equations (“normal equations”) in <span class="math inline">\((p+1)\)</span> unknowns. This linear system of equations defines the OLS estimates, <span class="math inline">\(\hat{\beta}\)</span>, for a given dataset: <span class="math display">\[
\begin{align*}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)
&amp;=\underset{((p+1)\times 1)}{0}\\[2ex]
X^{\prime} X\hat{\beta}
&amp;=\underset{((p+1)\times 1)}{X^{\prime}Y}.
\end{align*}
\]</span> From our full rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is an invertible <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix which allows us to solve the equation system by <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{align*}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given dataset with <span class="math inline">\(X_i\in\mathbb{R}^{p+1}\)</span>, <span class="math inline">\(p=2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Some given data</span></span>
<span><span class="va">X_1</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span><span class="op">)</span></span>
<span><span class="va">X_2</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">66</span>, <span class="fl">62</span>, <span class="fl">64</span>, <span class="fl">61</span>, <span class="fl">63</span>, <span class="fl">70</span>, <span class="fl">68</span>, <span class="fl">62</span>, <span class="fl">68</span>, <span class="fl">66</span><span class="op">)</span></span>
<span><span class="va">Y</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.7</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="op">-</span><span class="fl">0.2</span>,<span class="op">-</span><span class="fl">1.2</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span><span class="op">)</span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"X_1"</span> <span class="op">=</span> <span class="va">X_1</span>, <span class="st">"X_2"</span> <span class="op">=</span> <span class="va">X_2</span>, <span class="st">"Y"</span> <span class="op">=</span> <span class="va">Y</span><span class="op">)</span></span>
<span><span class="co">## Compute the OLS estimation</span></span>
<span><span class="va">lmobj</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span>, data <span class="op">=</span> <span class="va">dataset</span><span class="op">)</span></span>
<span><span class="co">## Plot sample regression surface</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"scatterplot3d"</span><span class="op">)</span> <span class="co"># library for 3d plots</span></span>
<span><span class="va">plot3d</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scatterplot3d/man/scatterplot3d.html">scatterplot3d</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_1</span>, y <span class="op">=</span> <span class="va">X_2</span>, z <span class="op">=</span> <span class="va">Y</span>,</span>
<span>            angle <span class="op">=</span> <span class="fl">33</span>, scale.y <span class="op">=</span> <span class="fl">0.8</span>, pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>            color <span class="op">=</span><span class="st">"red"</span>, </span>
<span>            xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            main <span class="op">=</span><span class="st">"OLS Regression Surface"</span><span class="op">)</span></span>
<span><span class="va">plot3d</span><span class="op">$</span><span class="fu">plane3d</span><span class="op">(</span><span class="va">lmobj</span>, lty.box <span class="op">=</span> <span class="st">"solid"</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.5</span><span class="op">)</span>, draw_polygon<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="special-case-simple-linear-regression-model" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="special-case-simple-linear-regression-model">Special Case: Simple Linear Regression Model</h4>
<p>For a given observed realization of the training data random sample <span class="math display">\[
(x_1,y_1),\dots,(x_n,y_n)
\]</span> we choose <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> such that the <strong>R</strong>esidual <strong>S</strong>um of <strong>S</strong>quares criterion is minimized: <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
&amp; = e_1^2 + \dots + e_n^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2
\end{align*}
\]</span> The minimizers are <span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]</span> and <span class="math display">\[
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
\]</span> where <span class="math inline">\(\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i\)</span> and <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\)</span>.</p>
<p><img src="images/Fig_3_1.png" class="img-fluid"></p>
<!-- 
![](images/Fig_3_2.png) 
-->
</section><section id="some-quantities-of-interest" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="some-quantities-of-interest">Some Quantities of Interest</h4>
<p><strong>Predicted values and residuals.</strong></p>
<ul>
<li><p>The (OLS) <strong>predicted values</strong>: <span class="math display">\[
\hat{Y}_i=X_i'\hat\beta, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of predicted values <span class="math display">\[
\begin{align*}
\hat{Y} = \left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)
&amp;=X\hat{\beta}\\[-2ex]
&amp;=\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=P_X Y
\end{align*}
\]</span></p></li>
<li><p>The (OLS) <strong>residuals</strong>: <span class="math display">\[
e_i=Y_i-\hat{Y}_i, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of residuals <span class="math display">\[
\begin{align*}
e =
\left(\begin{matrix}e_1\\e_2\\ \vdots\\ e_n\end{matrix}\right)
&amp;=
\left(\begin{matrix}Y_1\\[.5ex]Y_2\\[.5ex] \vdots\\[.5ex] Y_n\end{matrix}\right)-
\left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)\\[2ex]
&amp;=Y - \hat{Y}\\[2ex]
%&amp;=Y - X\hat{\beta}\\[-2ex]
%&amp;=Y - \underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=Y - P_X Y\\[2ex]
&amp;=\underbrace{(I_n - P_X)}_{=M_X} Y\\[2ex]
&amp;=M_XY
\end{align*}
\]</span></p></li>
</ul>
<p><strong>Projection matrices.</strong></p>
<p>The matrix <span class="math display">\[
P_X=X(X'X)^{-1}X'
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by the column vectors of <span class="math inline">\(X.\)</span></p>
</section></section><section id="assessing-the-accuracy-of-the-model-fit-hatf" class="level2" data-number="4.3"><h2 data-number="4.3" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-model-fit-hatf">
<span class="header-section-number">4.3</span> Assessing the Accuracy of the Model fit <span class="math inline">\(\hat{f}\)</span>
</h2>
<p>The larger the proportion of the explained variance, the better is the fit of the estimated model <span class="math inline">\(\hat{f}\)</span> to the training data. This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[
\begin{eqnarray*}
R^2
%&amp;=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\operatorname{RSS}}{\operatorname{TSS}}
\end{eqnarray*}
\]</span> with <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat\beta)=\sum_{i=1}^n\left(y_i-x_i'\hat\beta\right)^2=\sum_{i=1}^ne_i^2
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
\operatorname{TSS}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.
\end{align*}
\]</span></p>
<p><span class="math inline">\(\operatorname{TSS}\)</span> “Total Sum of Squares”</p>
<p><span class="math inline">\(\operatorname{RSS}\)</span> “Residual Sum of Squares”</p>
<ul>
<li><p>Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>.</p></li>
<li><p>The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed training data.</p></li>
</ul>
<p>In tendency an accurate model has …</p>
<ul>
<li><p>a low residual standard error <span class="math inline">\(\operatorname{RSE}\)</span> <span class="math display">\[
\operatorname{RSE}=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}
\]</span></p></li>
<li><p>a high <span class="math inline">\(R^2\)</span></p></li>
</ul>
<p><span class="math display">\[
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
\]</span> where <span class="math inline">\(0\leq R^2\leq 1.\)</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Cautionary Note Nr 1:</strong> Do not forget that there is a <strong>irreducible error</strong> <span class="math inline">\(Var(\epsilon)=\sigma^2&gt;0\)</span>. Thus</p>
<ul>
<li>very low <span class="math inline">\(\operatorname{RSE}\)</span> values <span class="math inline">\((\operatorname{RSE}\approx 0)\)</span> and</li>
<li>very high <span class="math inline">\(R^2\)</span> values <span class="math inline">\((R^2\approx 1)\)</span>
</li>
</ul>
<p>can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with a multiple linear regression model.</p>
<p><strong>Cautionary Note Nr 2:</strong> The <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\operatorname{RSE}\)</span> are only based on <strong>training data</strong>. In <a href="Ch2_StatLearning.html" class="quarto-xref"><span>Chapter 2</span></a>, we have seen that a proper assessment of the model accuracy needs to take into account <strong>test data</strong>.</p>
</div>
</div>
<section id="r2-and-correlation-coefficient" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="r2-and-correlation-coefficient">
<span class="math inline">\(R^2\)</span> and correlation coefficient</h4>
<p>In the case of the simple linear regression model, <span class="math inline">\(R^2\)</span> equals the squared sample correlation coefficient between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, <span class="math display">\[
R^2 = r_{yx_1}^2,
\]</span> where <span class="math display">\[
r_{yx_1}=\frac{\sum_{i=1}^n(x_{i1}-\bar{x}_1)(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_{i1}-\bar{x}_1)^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}},
\]</span> where <span class="math inline">\(\bar{x}_1=n^{-1}\sum_{i=1}^nx_{i1}.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the multiple linear regression model <span class="math inline">\(Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i,\)</span> the <span class="math inline">\(R^2\)</span> equals the squared correlation between response and the fitted values: <span class="math display">\[
R^2=r^2_{y\hat{y}}
\]</span> with <span class="math display">\[
r_{y\hat{y}}=\frac{\sum_{i=1}^n(y_i-\bar{y})(\hat{y}_i-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}\sqrt{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}},
\]</span> where <span class="math inline">\(\bar{y}=n^{-1}\sum_{i=1}^ny_{i}.\)</span></p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>A high/low <span class="math inline">\(R^2\)</span> value only means that the predictors have high/low <em>predictive power</em> with respect to the training data.</p></li>
<li><p>A high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.<br></p></li>
</ul>
</div>
</div>
<p>The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will increase the <span class="math inline">\(R^2\)</span>. The below <code>R</code>-codes demonstrates this problem.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>     <span class="op">&lt;-</span> <span class="fl">100</span>                  <span class="co"># Sample size</span></span>
<span><span class="va">X</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>      <span class="co"># Relevant X variable</span></span>
<span><span class="va">X_ir</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">20</span><span class="op">)</span>      <span class="co"># Irrelevant X variable</span></span>
<span><span class="va">error</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>    <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>     <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">error</span>    <span class="co"># Y variable</span></span>
<span><span class="va">lm1</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>     <span class="co"># Correct OLS regression </span></span>
<span><span class="va">lm2</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">+</span><span class="va">X_ir</span><span class="op">)</span><span class="op">)</span><span class="co"># OLS regression with X_ir </span></span>
<span><span class="va">lm1</span><span class="op">$</span><span class="va">r.squared</span> <span class="op">&lt;</span> <span class="va">lm2</span><span class="op">$</span><span class="va">r.squared</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable.</p>
<p>Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[
\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-(p+1)}\sum_{i=1}^ne^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}\leq R^2%\\
\end{eqnarray*}
\]</span> The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-(p+1)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm1</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model without X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.569</code></pre>
</div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm2</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model with X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.564</code></pre>
</div>
</div>
</section></section><section id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="level2" data-number="4.4"><h2 data-number="4.4" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta">
<span class="header-section-number">4.4</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span>
</h2>
<section id="bias-of-hatbeta" class="level3" data-number="4.4.1"><h3 data-number="4.4.1" class="anchored" data-anchor-id="bias-of-hatbeta">
<span class="header-section-number">4.4.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span>
</h3>
<p>Under the Assumptions 1-4, once can show that the OLS estimator <span class="math display">\[
\hat\beta = (X'X)^{-1}X'Y
\]</span> is unbiased, i.e. <span class="math display">\[
\operatorname{Bias}(\hat\beta) = E(\hat\beta) - \beta = \underset{((p+1)\times 1)}{0}.
\]</span> That is, on average <span class="math inline">\(\hat\beta\)</span> equals <span class="math inline">\(\beta.\)</span></p>
<p>This can be shown as following:</p>
<p>Observe that <span class="math display">\[
\hat\beta=(X'X)^{-1}X'Y
\]</span> consists of two multivariate random variables <span class="math inline">\(X\in\mathbb{R}^{n\times(p+1)}\)</span> and <span class="math inline">\(Y\in\mathbb{R}^n.\)</span> Thus one needs to show first the conditional unbiasedness of <span class="math inline">\(\hat\beta\)</span> given <span class="math inline">\(X\)</span> which effectively allows us to focus on randomness due to <span class="math inline">\(\epsilon,\)</span><br><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta|X)
&amp;= E(\hat\beta|X)                                        - \beta \\[2ex]
&amp;= E((X'X)^{-1}X'\underbrace{Y}_{=X\beta+\epsilon}|X) - \beta \\[2ex]
&amp;= E((X'X)^{-1}X'(X\beta+\epsilon)|X)                 - \beta \\[2ex]
&amp;= E(\underbrace{(X'X)^{-1}X'X}_{=I_K}\beta|X) + E((X'X)^{-1}X'\epsilon|X) - \beta \\[2ex]
&amp;= \underbrace{E(\beta|X)}_{=\beta} + \underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=(X'X)^{-1}X'E(\epsilon|X)} - \beta \\[2ex]
&amp;=  (X'X)^{-1}X'\underbrace{E(\epsilon|X)}_{=0} =\underset{(K\times 1)}{0}  
\end{align*}
\]</span> Thus <span class="math inline">\(\hat\beta\)</span> is unbiased conditionally on <span class="math inline">\(X\)</span> <span class="math display">\[
\operatorname{Bias}(\hat\beta|X) = 0.
\]</span></p>
<p>From this if follows, by the iterated law of expectations, that the OLS estimator is also unconditionally unbiased, i.e.<br><span class="math display">\[
\operatorname{Bias}(\hat\beta) = E\left(\operatorname{Bias}(\hat\beta|X)\right) = E(0) = 0.
\]</span></p>
</section><section id="standard-error-of-hatbeta_j" class="level3" data-number="4.4.2"><h3 data-number="4.4.2" class="anchored" data-anchor-id="standard-error-of-hatbeta_j">
<span class="header-section-number">4.4.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span>
</h3>
<p>The standard error of <span class="math inline">\(\hat{\beta}_j,\)</span> for each <span class="math inline">\(j=0,\dots,p,\)</span> is given by <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X)=\sqrt{Var(\hat\beta_j|X)},
\]</span> where <span class="math display">\[
Var(\hat\beta_j|X) = \left[Var(\hat\beta|X)\right]_{(j,j)}
\]</span> denotes the <span class="math inline">\(j\)</span>th diagonal element of the symmetric <span class="math inline">\((p+1)\times (p+1)\)</span> variance-covariance matrix <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=\begin{pmatrix}
Var(\hat\beta_0|X)&amp;Cov(\hat\beta_0,\hat\beta_1|X)&amp;\cdots&amp;Cov(\hat\beta_0,\hat\beta_{p}|X)\\
Cov(\hat\beta_1,\hat\beta_0|X)&amp;Var(\hat\beta_1|X)&amp;  &amp;Cov(\hat\beta_1,\hat\beta_{p}|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\hat\beta_p,\hat\beta_0|X)&amp;Cov(\hat\beta_p,\hat\beta_1|X)&amp;\cdots&amp;Var(\hat\beta_{p}|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>Thus, to compute a useful explicit expression for <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X)=?,
\]</span> we need to compute an explicit expression for the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X).\)</span></p>
<p>Let us derive the general explicit expression for <span class="math inline">\(Var(\hat\beta|X).\)</span></p>
<p>Note that <span class="math display">\[
\begin{align*}
\hat{\beta}
&amp;=(X'X)^{-1}X'Y\\[2ex]
&amp;\text{Using that $Y=X\beta + \epsilon$:}\\[2ex]
&amp;=(X'X)^{-1}X'(X\beta + \epsilon)\\[2ex]
&amp;=\underbrace{(X'X)^{-1}X'X\beta}_{=\beta} + (X'X)^{-1}X'\epsilon
\end{align*}
\]</span> This leads to the so-called sampling error expression <span class="math display">\[
\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon.
\]</span> With this, we can derive the general explicit expression for <span class="math inline">\(Var(\hat\beta|X).\)</span> <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=Var(\hat\beta - \beta|X)\\[2ex]
&amp;\text{Using the sampling error expression:}\\[2ex]
&amp;=Var((X'X)^{-1}X'\epsilon|X)\\[2ex]
&amp;=E\Big[\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)\times\\[2ex]
&amp;\phantom{=\Big(}\,\times\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)'|X\Big]\\[2ex]
&amp;=E\left[((X'X)^{-1}X'\epsilon)((X'X)^{-1}X'\epsilon)'|X\right]\\[2ex]
&amp;=E\left[(X'X)^{-1}X'\epsilon\epsilon' X(X'X)^{-1}|X\right]\\[2ex]
&amp;=\;\;\;(X'X)^{-1}X'\underbrace{E\left(\epsilon\epsilon'|X\right)}_{=Var(\epsilon|X)}X(X'X)^{-1}
\end{align*}
\]</span> That is, the explicit expression for <span class="math inline">\(Var(\hat\beta|X)\)</span> depends on the explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> <span class="math display">\[
\begin{align*}
&amp;Var(\epsilon|X)=\\[2ex]
&amp;=\begin{pmatrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\cdots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;  &amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\cdots&amp;Var(\epsilon_n|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>The explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> depends on our (hopefully correct) assumption on the error-term distribution (Assumption 4).</p>
<section id="case-of-spherical-homoskedastic-uncorrelated-errors" class="level4" data-number="4.4.2.1"><h4 data-number="4.4.2.1" class="anchored" data-anchor-id="case-of-spherical-homoskedastic-uncorrelated-errors">
<span class="header-section-number">4.4.2.1</span> Case of Spherical (Homoskedastic, Uncorrelated) Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma^2 &amp; 0        &amp; \cdots &amp; 0\\
0        &amp; \sigma^2 &amp; \cdots &amp; 0\\
\vdots   &amp; \vdots   &amp; \ddots &amp; 0\\
0        &amp; 0        &amp; \cdots &amp; \sigma^2\\
\end{pmatrix}
&amp;=\sigma^2 I_n,
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1}X' \left(\sigma^2 I_n \right) X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X'X)^{-1}X' \left( I_n \right) X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;\underbrace{(X'X)^{-1}X'X}_{I_{p+1}}\;(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X'X)^{-1},
\end{align*}
\]</span> where the only unknown component is <span class="math inline">\(\sigma^2=Var(\epsilon_i).\)</span></p>
<p>We can estimate the homoskedastik error term variance <span class="math inline">\(\sigma^2\)</span> using the <strong>R</strong>esidual <strong>S</strong>tandard <strong>E</strong>rror: <span class="math display">\[
\begin{align*}
\hat\sigma = \operatorname{RSE}
&amp;=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}\\[2ex]
&amp;=\sqrt{ \frac{1}{n-(p+1)} \sum_{i=1}^n e_i^2}.
\end{align*}
\]</span></p>
<p><strong>Summing up:</strong></p>
<p>In the case of spherical (homoskedastic and uncorrelated) error terms the standard error of <span class="math inline">\(\beta_j\)</span> is <span class="math display">\[
\operatorname{SE}(\beta_j|X) = \sqrt{\left[\sigma^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
\]</span> The above expression is the infeasible (since <span class="math inline">\(\sigma^2\)</span> is typically unknown) population version of the standard error. We can estimate this population version using the empirical standard error <span class="math display">\[
\widehat{\operatorname{SE}}(\beta_j|X) = \sqrt{\left[\hat{\sigma}^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
\]</span></p>
<p>This is the default version for computing the standard error in statistical software packages such as <code>R</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>             <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span>             <span class="co"># OLS regression </span></span>
<span></span>
<span><span class="co">## Standard OLS output table:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span>                         </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Y ~ X_1 + X_2)

Residuals:
    Min      1Q  Median      3Q     Max 
-39.071  -7.138  -0.575   9.570  33.368 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.0112     4.2440   0.238    0.812    
X_1           5.1954     0.4529  11.472  &lt; 2e-16 ***
X_2          -4.7001     0.6690  -7.026 2.95e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 12.84 on 97 degrees of freedom
Multiple R-squared:  0.6565,    Adjusted R-squared:  0.6494 
F-statistic: 92.68 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="case-of-heteroskedastic-but-uncorrelated-errors" class="level4" data-number="4.4.2.2"><h4 data-number="4.4.2.2" class="anchored" data-anchor-id="case-of-heteroskedastic-but-uncorrelated-errors">
<span class="header-section-number">4.4.2.2</span> Case of Heteroskedastic, but Uncorrelated Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma_1^2 &amp; 0          &amp; \cdots &amp; 0\\
0          &amp; \sigma_2^2 &amp; \cdots &amp; 0\\
\vdots     &amp; \vdots     &amp; \ddots &amp; 0\\
0          &amp; 0          &amp; \cdots &amp; \sigma_n^2\\
\end{pmatrix}
&amp;=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2),
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1} \left(X'\left(\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2) \right) X\right) (X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right) (X'X)^{-1}\\[2ex]
\end{align*}
\]</span> Thus, the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X)\)</span> keeps its “sandwich form”, where the inner part of the sandwich <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right)
\]</span> is typically unknown, since <span class="math inline">\(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2\)</span> are typically unknown.</p>
<p>There are different, so-called <strong>Heteroskedasticity Consistent (HC)</strong> estimators to estimate the unknown expression <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right).
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead><tr class="header">
<th>HC-Type</th>
<th>Formular</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>HC0</td>
<td><span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC1</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{n}{n-K}\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC2</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC3</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC4</td>
<td><span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'\)</span></td>
</tr>
</tbody>
</table>
<p>HC3 is the most often used HC-estimator.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The statistic <span class="math inline">\(h_i:=[P_X]_{ii}\)</span> is called the <strong>leverage statistic</strong> of <span class="math inline">\(X_i,\)</span> where</p>
<ul>
<li>
<span class="math inline">\(1/n\leq h_i\leq 1\)</span> and</li>
<li>
<span class="math inline">\(\bar{h}=n^{-1}\sum_{i=1}^nh_i=K/n\)</span>.</li>
</ul>
<p>Observations <span class="math inline">\(X_i\)</span> with leverage statistics <span class="math inline">\(h_i\)</span> that greatly exceed the average leverage value <span class="math inline">\(K/n\)</span> are referred to as “high leverage” observations. High leverage observations <span class="math inline">\(X_i\)</span> are observations that are far away from all other observations <span class="math inline">\(X_j\)</span>, <span class="math inline">\(i\neq j=1,\dots,n.\)</span></p>
<p>High leverage observations <span class="math inline">\(X_i\)</span> have the potential to distort the estimation results, <span class="math inline">\(\hat\beta_n\)</span>. Indeed, a high leverage observation <span class="math inline">\(X_i\)</span> will have an distorting effect on the estimation results if the absolute value of the corresponding residual <span class="math inline">\(|\hat{\varepsilon}_i|\)</span> is unusually large—such observations are called <strong>influential outliers</strong>. Such observations increase the estimation uncertainty.</p>
<p>General idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals <span class="math inline">\(\hat\varepsilon_i\)</span> belonging to <span class="math inline">\(X_i\)</span> values that have a large leverage <span class="math inline">\(h_i\)</span> receive a higher weight and thus increase the value of <span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i').\)</span> This strategy takes into account increased estimation uncertainties due to single influential outliers.</p>
</div>
</div>
<!-- The estimator HC0 was suggested in the econometrics literature by @White1980 and is justified by asymptotic ($n\to\infty$) arguments. The estimators HC1, HC2 and HC3 were suggested by @MacKinnon_White_1985 to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, @Long_Ervin_2000 concludes that HC3 provides the best overall performance in finite samples. @Cribari_2004 suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large $h_i$ values). 
-->
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span>     <span class="co"># True (usually unknown) heteroskedastic error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span></span>
<span></span>
<span><span class="co">## Package for computing robust variance estimations</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://sandwich.R-Forge.R-project.org/">"sandwich"</a></span><span class="op">)</span> <span class="co"># vcovHC(), </span></span>
<span></span>
<span><span class="co">## Package for producing an OLS output table (etc.)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"lmtest"</span><span class="op">)</span><span class="op">)</span> <span class="co"># coeftest</span></span>
<span></span>
<span><span class="co">## Estimate the linear regression model parameters</span></span>
<span><span class="va">lm_obj</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">vcovHC3_mat</span> <span class="op">&lt;-</span> <span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type<span class="op">=</span><span class="st">"HC3"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_obj</span>, vcov <span class="op">=</span> <span class="va">vcovHC3_mat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
t test of coefficients:

            Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  3.16955    2.17658   1.4562   0.1486    
X_1          4.92390    0.22302  22.0784   &lt;2e-16 ***
X_2         -4.57382    0.34716 -13.1748   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Note: The HC3-Robust SE estimates are: </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcovHC3_mat</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         X_1         X_2 
    2.17658     0.22302     0.34716 </code></pre>
</div>
</div>
</section></section></section><section id="exercises" class="level2" data-number="4.5"><h2 data-number="4.5" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">4.5</span> Exercises</h2>
<p>Prepare the following exercises of Chapter 3 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 2</li>
<li>Exercise 3</li>
<li>Exercise 8</li>
<li>Exercise 9</li>
</ul>
<!-- {{< include Ch4_LinearRegression_Solutions.qmd >}} -->

</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./Ch3_MatrixAlgebra.html" class="pagination-link" aria-label="Matrix Algebra">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch5_Classification.html" class="pagination-link" aria-label="Classification">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>