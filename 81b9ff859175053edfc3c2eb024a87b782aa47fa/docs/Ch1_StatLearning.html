<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>2&nbsp; Statistical Learning – Computer-Aided Statistical Analysis (B.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch2_LinearRegression.html" rel="next">
<link href="./Ch0_Intro2R.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch1_StatLearning.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch0_Intro2R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_StatLearning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_LinearRegression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_ResamplingMethods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#what-is-statistical-learning" id="toc-what-is-statistical-learning" class="nav-link active" data-scroll-target="#what-is-statistical-learning"><span class="header-section-number">2.1</span> What is Statistical Learning?</a>
  <ul class="collapse">
<li><a href="#why-estimate-f" id="toc-why-estimate-f" class="nav-link" data-scroll-target="#why-estimate-f"><span class="header-section-number">2.1.1</span> Why Estimate <span class="math inline">\(f\)</span>?</a></li>
  </ul>
</li>
  <li>
<a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a>
  <ul class="collapse">
<li><a href="#accuracy-of-a-prediction" id="toc-accuracy-of-a-prediction" class="nav-link" data-scroll-target="#accuracy-of-a-prediction">Accuracy of a Prediction</a></li>
  </ul>
</li>
  <li><a href="#parameter-inference" id="toc-parameter-inference" class="nav-link" data-scroll-target="#parameter-inference">(Parameter) Inference</a></li>
  <li>
<a href="#how-do-we-estimate-f" id="toc-how-do-we-estimate-f" class="nav-link" data-scroll-target="#how-do-we-estimate-f"><span class="header-section-number">2.2</span> How Do We Estimate <span class="math inline">\(f\)</span>?</a>
  <ul class="collapse">
<li><a href="#parametric-methods" id="toc-parametric-methods" class="nav-link" data-scroll-target="#parametric-methods">Parametric Methods</a></li>
  <li><a href="#non-parametric-methods" id="toc-non-parametric-methods" class="nav-link" data-scroll-target="#non-parametric-methods">Non-Parametric Methods</a></li>
  <li><a href="#the-trade-off-between-prediction-accuracy-and-model-interpretability" id="toc-the-trade-off-between-prediction-accuracy-and-model-interpretability" class="nav-link" data-scroll-target="#the-trade-off-between-prediction-accuracy-and-model-interpretability"><span class="header-section-number">2.2.1</span> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
  <li><a href="#supervised-versus-unsupervised-learning" id="toc-supervised-versus-unsupervised-learning" class="nav-link" data-scroll-target="#supervised-versus-unsupervised-learning"><span class="header-section-number">2.2.2</span> Supervised versus Unsupervised Learning</a></li>
  <li><a href="#regression-versus-classification-problems" id="toc-regression-versus-classification-problems" class="nav-link" data-scroll-target="#regression-versus-classification-problems"><span class="header-section-number">2.2.3</span> Regression Versus Classification Problems</a></li>
  </ul>
</li>
  <li>
<a href="#assessing-model-accuracy" id="toc-assessing-model-accuracy" class="nav-link" data-scroll-target="#assessing-model-accuracy"><span class="header-section-number">2.3</span> Assessing Model Accuracy</a>
  <ul class="collapse">
<li><a href="#measuring-the-quality-of-fit" id="toc-measuring-the-quality-of-fit" class="nav-link" data-scroll-target="#measuring-the-quality-of-fit"><span class="header-section-number">2.3.1</span> Measuring the Quality of Fit</a></li>
  <li><a href="#local-i.e.-point-wise-test-mse" id="toc-local-i.e.-point-wise-test-mse" class="nav-link" data-scroll-target="#local-i.e.-point-wise-test-mse">Local (i.e.&nbsp;Point-Wise) Test MSE</a></li>
  <li><a href="#global-test-mse" id="toc-global-test-mse" class="nav-link" data-scroll-target="#global-test-mse">Global Test MSE</a></li>
  <li><a href="#the-bias-variance-trade-off" id="toc-the-bias-variance-trade-off" class="nav-link" data-scroll-target="#the-bias-variance-trade-off"><span class="header-section-number">2.3.2</span> The Bias-Variance Trade-Off</a></li>
  <li><a href="#the-classification-setting" id="toc-the-classification-setting" class="nav-link" data-scroll-target="#the-classification-setting"><span class="header-section-number">2.3.3</span> The Classification Setting</a></li>
  <li><a href="#the-bayes-classifier" id="toc-the-bayes-classifier" class="nav-link" data-scroll-target="#the-bayes-classifier">The Bayes Classifier</a></li>
  <li><a href="#k-nearest-neighbors-classification" id="toc-k-nearest-neighbors-classification" class="nav-link" data-scroll-target="#k-nearest-neighbors-classification"><span class="math inline">\(K\)</span>-Nearest Neighbors Classification</a></li>
  </ul>
</li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">2.4</span> Exercises</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-SL" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><blockquote class="blockquote">
<p>Reading: Chapter 2 of our course textbook <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a></p>
</blockquote>
<section id="r-codes-for-this-chapter" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="r-codes-for-this-chapter">
<code>R</code>-Codes for this Chapter</h4>
<ul>
<li>Introduction to <code>R</code>: <a href="https://www.dropbox.com/scl/fi/blo6pct6sxbb70jv5iss0/Ch1_0_Rcodes.R?rlkey=ipkll1fy8tyuubhb646mmy6r5&amp;dl=0">Ch1_0_Rcodes.R</a>
</li>
<li>
<code>R</code>-codes for Coding Challenge Nr 1 (replicating Fig. 2.9): <a href="https://www.dropbox.com/scl/fi/8dtu5kje5am70l2dh6w9b/Ch1_1_Rcodes.R?rlkey=o6ucjdql0f88d9wcopj0q0esa&amp;dl=0">Ch1_1_Rcodes.R</a>
</li>
<li>
<code>R</code>-codes for Coding Challenge Nr 2 (KNN-classification): <a href="https://www.dropbox.com/scl/fi/p7rm2ziytfl68s2h6i4gf/Ch1_2_Rcodes.R?rlkey=5rgl4y9xbxejafi32oe1baoza&amp;dl=0">Ch1_2_Rcodes.R</a>
</li>
</ul></section><section id="what-is-statistical-learning" class="level2" data-number="2.1"><h2 data-number="2.1" class="anchored" data-anchor-id="what-is-statistical-learning">
<span class="header-section-number">2.1</span> What is Statistical Learning?</h2>
<p>Suppose that we observe a quantitative response <span class="math inline">\(Y\)</span> and <span class="math inline">\(p\)</span> different predictors, <span class="math inline">\(X_1, X_2, \dots, X_p.\)</span></p>
<p>We assume that there is some relationship between <span class="math inline">\(Y\)</span> and <span class="math display">\[
X = (X_1, X_2, \dots, X_p),
\]</span> which can be written in the very general form <span class="math display">\[
Y = f(X) + \epsilon.
\]</span></p>
<ul>
<li>
<span class="math inline">\(f\)</span> is some fixed but unknown function of <span class="math inline">\(X = (X_1, X_2, \dots, X_p):\)</span>
</li>
<li>
<span class="math inline">\(\epsilon\)</span> a random error term fulfilling the following two assumptions:
<ul>
<li>
<span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(X\)</span> are independent of each other</li>
<li>
<span class="math inline">\(\epsilon\)</span> has mean zero <span class="math inline">\(E(\epsilon)=0.\)</span>
</li>
</ul>
</li>
</ul>
<p>In this formulation, <span class="math inline">\(f\)</span> represents the <em>systematic</em> information that <span class="math inline">\(X\)</span> provides about <span class="math inline">\(Y.\)</span></p>
<p>In essence, <strong>statistical learning</strong> refers to a set of approaches for estimating <span class="math inline">\(f.\)</span> In this chapter we outline some of the key theoretical concepts that arise in estimating <span class="math inline">\(f,\)</span> as well as tools for evaluating the estimates obtained.</p>
<section id="why-estimate-f" class="level3" data-number="2.1.1"><h3 data-number="2.1.1" class="anchored" data-anchor-id="why-estimate-f">
<span class="header-section-number">2.1.1</span> Why Estimate <span class="math inline">\(f\)</span>?</h3>
<p>There are <strong>two main reasons</strong> that we may wish to estimate <span class="math inline">\(f\)</span>:</p>
<ul>
<li>prediction and</li>
<li>inference.</li>
</ul>
<p>We discuss each in turn.</p>
</section></section><section id="prediction" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="prediction">Prediction</h2>
<p>In many situations, a set of inputs <span class="math inline">\(X\)</span> are readily available, but the output <span class="math inline">\(Y\)</span> cannot be easily obtained. In this setting, since the error term averages to zero, we can predict <span class="math inline">\(Y\)</span> using <span class="math display">\[
\hat{Y} = \hat{f}(X),
\]</span></p>
<ul>
<li>
<span class="math inline">\(\hat{f}\)</span> represents our estimate for <span class="math inline">\(f\)</span>
</li>
<li>
<span class="math inline">\(\hat{Y}\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>
</li>
</ul>
<p>In this setting, <span class="math inline">\(\hat{f}\)</span> is often treated as a <strong>black box</strong>, in the sense that one is not typically concerned with the exact form of <span class="math inline">\(\hat{f},\)</span> provided that it yields accurate predictions for <span class="math inline">\(Y.\)</span></p>
<p><strong>Example:</strong> As an example, suppose that <span class="math inline">\((X_1, X_2, \dots, X_p)\)</span> are characteristics of a patient’s blood sample that can be easily measured in a lab, and <span class="math inline">\(Y\)</span> is a variable encoding the patient’s risk for a severe adverse reaction to a particular drug. It is natural to seek to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(X,\)</span> since we can then avoid giving the drug in question to patients who are at high risk of an adverse reaction–that is, patients for whom the estimate of <span class="math inline">\(Y\)</span> is high.</p>
<section id="accuracy-of-a-prediction" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="accuracy-of-a-prediction">Accuracy of a Prediction</h3>
<p>The accuracy of <span class="math inline">\(\hat{Y}\)</span> as a prediction for <span class="math inline">\(Y\)</span> depends on two quantities:</p>
<ul>
<li>the <strong>reducible error</strong> and</li>
<li>the <strong>irreducible error.</strong>
</li>
</ul>
<p>In general, <span class="math inline">\(\hat{f}\)</span> will not be a perfect estimate for <span class="math inline">\(f,\)</span> and this inaccuracy will introduce some error. This error is <strong>reducible</strong>, because we can potentially improve the accuracy of <span class="math inline">\(\hat{f}\)</span> by using the most appropriate statistical learning technique to estimate <span class="math inline">\(f.\)</span></p>
<p>However, even if it were possible to form a perfect estimate for <span class="math inline">\(f,\)</span> so that our estimated response took the form <span class="math display">\[
\hat{Y} = f (X),
\]</span> our prediction would still have some error in it! This is because <span class="math inline">\(Y\)</span> is also a function of <span class="math inline">\(\epsilon\)</span> which, by definition, cannot be predicted using <span class="math inline">\(X.\)</span> Therefore, variability associated with <span class="math inline">\(\epsilon\)</span> also affects the accuracy of our predictions. This is known as the <strong>irreducible error</strong>, because no matter how well we estimate <span class="math inline">\(f,\)</span> we cannot reduce the error introduced by <span class="math inline">\(\epsilon.\)</span></p>
<p>Consider a <em>given</em> estimate <span class="math inline">\(\hat{f}\)</span> and a <em>given</em> set of predictors <span class="math inline">\(X,\)</span> which yields the prediction <span class="math inline">\(\hat{Y} = \hat{f}(X).\)</span> Assume for a moment that both <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(X\)</span> are fixed, so that the only variability comes from <span class="math inline">\(\epsilon.\)</span> Then, it is easy to show that <span class="math display">\[\begin{align*}
\overbrace{E\left[(Y - \hat{Y})^2\right]}^{\text{Mean Squared (Prediction) Error}}
=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}},
\end{align*}\]</span> where</p>
<ul>
<li>
<span class="math inline">\(E\left[(Y - \hat{Y})^2\right]\)</span> represents the expected value, of the squared difference between the predicted <span class="math inline">\(\hat{Y}=\hat{f}(X)\)</span> and actual value of <span class="math inline">\(Y,\)</span>
</li>
<li>and <span class="math inline">\(Var(\epsilon)\)</span> represents the variance associated with the error term <span class="math inline">\(\epsilon.\)</span>
</li>
</ul>
<p>Derivation (for a <em>given</em> <span class="math inline">\(\hat{f}\)</span> and a <em>given</em> <span class="math inline">\(X;\)</span> i.e.&nbsp;only <span class="math inline">\(\epsilon\)</span> is random):</p>
<p><span class="math display">\[\begin{align*}
&amp;E\left[(Y - \hat{Y})^2\right]\\
&amp;=E\left[(f(X) + \epsilon - \hat{f}(X))^2\right] \\
&amp;=E\left[\left(f(X) -\hat{f}(X)\right)^2 - 2\left(f(X) -\hat{f}(X)\right)\epsilon + \epsilon^2\right] \\
% &amp;=E\left[\left(f(X) -\hat{f}(X)\right)^2\right] - 2E\left[\left(f(X) -\hat{f}(X)\right)\epsilon\right] + E\left[\epsilon^2\right] \\
&amp;=\left(f(X) -\hat{f}(X)\right)^2 - 2\left(f(X) -\hat{f}(X)\right) E\left[\epsilon\right] + E\left[\epsilon^2\right] \\
&amp;=\left(f(X) -\hat{f}(X)\right)^2 - 2\left(f(X) -\hat{f}(X)\right) \cdot 0 + Var\left(\epsilon\right) \\
&amp;=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}
\end{align*}\]</span></p>
<p>It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for <span class="math inline">\(Y,\)</span> i.e.&nbsp; <span class="math display">\[
E\left[(Y - \hat{Y})^2\right] \geq Var\left(\epsilon\right)
\]</span> This bound is almost always unknown in practice.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The focus of this course is on techniques for estimating <span class="math inline">\(f\)</span> with the aim of <strong>minimizing the reducible error</strong>.</p>
</div>
</div>
</section></section><section id="parameter-inference" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="parameter-inference">(Parameter) Inference</h2>
<p>We are often interested in <em>understanding</em> the association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1,\dots,X_p.\)</span> In this situation we wish to estimate <span class="math inline">\(f,\)</span> but our goal is not necessarily to make predictions for <span class="math inline">\(Y.\)</span> Now <span class="math inline">\(\hat{f}\)</span> cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:</p>
<ul>
<li>Which predictors are associated with the response?</li>
<li>What is the relationship between the response and each predictor?</li>
<li>Can the relationship between <span class="math inline">\(Y\)</span> and each predictor be adequately summarized using a <em>linear equation</em>, or is the relationship more complicated?</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this course, we will see a number of examples that fall into the <strong>prediction setting</strong>, the <strong>inference setting</strong>, or a <strong>combination</strong> of the two.</p>
</div>
</div>
</section><section id="how-do-we-estimate-f" class="level2" data-number="2.2"><h2 data-number="2.2" class="anchored" data-anchor-id="how-do-we-estimate-f">
<span class="header-section-number">2.2</span> How Do We Estimate <span class="math inline">\(f\)</span>?</h2>
<p><strong>Setup:</strong> Consider the general regression model <span id="eq-GRegMod"><span class="math display">\[
Y=f(X)+\epsilon,
\qquad(2.1)\]</span></span> where <span class="math display">\[
X=(X_{1}, \dots,X_{p})
\]</span> is a multivariate (<span class="math inline">\(p\)</span>-dimensional) predictor.</p>
<p>Let <span id="eq-randomsample"><span class="math display">\[
\{(X_1,Y_1),(X_2,Y_2),\dots,(X_n,Y_n)\},
\qquad(2.2)\]</span></span> be a <strong>random sample</strong> from <a href="#eq-GRegMod" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>, i.e.</p>
<ol type="1">
<li>The multivariate, <span class="math inline">\(p+1\)</span> dimensional, random vectors <span class="math display">\[
(X_i,Y_i)\quad\text{and}\quad (X_j,Y_j)
\]</span> are independent of each other for all <span class="math inline">\(i=1,\dots,n\)</span> and <span class="math inline">\(j=1,\dots,n\)</span> with <span class="math inline">\(i\neq j.\)</span><br><br>
</li>
<li>The multivariate, <span class="math inline">\(p+1\)</span> dimensional, random vectors <span class="math inline">\((X_i,Y_i),\)</span> <span class="math inline">\(i=1,\dots,n,\)</span> have all the same distribution as <span class="math inline">\((X,Y),\)</span> i.e. <span class="math display">\[
(X_i,Y_i)\sim(X,Y)
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> <br><br>
</li>
</ol>
<p>The random sample <a href="#eq-randomsample" class="quarto-xref">Equation&nbsp;<span>2.2</span></a> is thus a set of <span class="math inline">\(n\)</span> many <strong>independent and identically distributed (iid)</strong> multivariate random variables <span class="math display">\[
\{(X_1,Y_1),(X_2,Y_2),\dots,(X_n,Y_n)\}
\]</span> with <span class="math display">\[
(X_i,Y_i)\overset{\text{iid}}{\sim} (X,Y),\quad i=1,\dots,n.
\]</span></p>
<p>An observed <strong>realization</strong> of the random sample will be denoted using lowercase letters <span class="math display">\[
\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}.
\]</span> These <span class="math inline">\(n\)</span> observations are called <strong>training data</strong> because we will use these observations to train/learn <span class="math inline">\(\hat{f}\)</span>, i.e., to compute the estimate <span class="math inline">\(\hat{f}\)</span> of the unknown <span class="math inline">\(f.\)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goal of Statistical Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our goal is to find (i.e.&nbsp;learn from training data) a function <span class="math inline">\(\hat{f}\)</span> such that <span class="math display">\[
Y \approx \hat{f}(X)
\]</span> for <strong><em>any</em></strong> observed realization of <span class="math inline">\((X, Y).\)</span></p>
<p>That is, the estimate <span class="math inline">\(\hat{f}\)</span> needs to provide a good approximation <span class="math display">\[
y_{i} \approx \hat{f}(x_{i})
\]</span> <strong>not only</strong> for the observed training data points <span class="math display">\[
(x_i,y_i),\quad i=1,\dots,n,
\]</span> <strong>but also</strong> for any possible <em>new</em> realization <span class="math inline">\((x_{new},y_{new})\)</span> of <span class="math inline">\((X,Y)\)</span> <span class="math display">\[
y_{new} \approx \hat{f}(x_{new}).
\]</span></p>
<p>Fitting the noise (irreducible component) in the training data will typically lead to bad approximations of new observations of a test data set.</p>
<img src="images/Traintest.png" class="img-fluid"><br><center>
<img src="images/test_vs_train.jpeg" class="img-fluid">
</center>
</div>
</div>
<p>Broadly speaking, most statistical learning methods for this task can be characterized as either</p>
<ul>
<li>
<strong>parametric</strong> or</li>
<li>
<strong>non-parametric</strong>.</li>
</ul>
<p>We discuss each in turn.</p>
<section id="parametric-methods" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="parametric-methods">Parametric Methods</h3>
<p>Parametric methods involve a <strong>two-step model-based estimation approach:</strong></p>
<ol type="1">
<li><p>First, we make an assumption about the functional form, or shape, of <span class="math inline">\(f.\)</span> For example, a very simple, but often used assumption is that <span class="math inline">\(f\)</span> is a linear function, i.e. <span id="eq-linmodeq"><span class="math display">\[
f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p.
\qquad(2.3)\]</span></span></p></li>
<li><p>After a model has been selected, we need a procedure that uses the training data to fit or train the model. For example, in the case of the linear model <a href="#eq-linmodeq" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>, we need to estimate the parameters <span class="math inline">\(\beta_0,\beta_1,\dots,\beta_p\)</span> such that <span class="math display">\[
Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p
\]</span></p></li>
</ol>
<p>Most common estimation technique: <strong>(Ordinary) Least Squares (OLS)</strong></p>
<p>The parametric model-based approach reduces the problem of estimating <span class="math inline">\(f\)</span> down to one of estimating a <em>finite</em> set of parameters <span class="math inline">\(\beta_0,\beta_1,\dots,\beta_p.\)</span></p>
<ul>
<li>
<strong>Pro:</strong> Simple to estimate</li>
<li>
<strong>Con:</strong> Possible model misspecification (Why should we know the true shape/form of <span class="math inline">\(f\)</span>?)</li>
</ul>
<p>We can try to address the problem of model misspecification by choosing flexible models that can fit many different possible functional forms for <span class="math inline">\(f.\)</span></p>
<p><strong>But:</strong> Fitting a more flexible model requires estimating a greater number of parameters (large <span class="math inline">\(p\)</span>). These more complex models can lead to a phenomenon known as <strong>overfitting</strong> the data, which essentially means they follow the errors/noise too closely.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>These issues (model-flexibility and overfitting) are discussed through-out this course.</p>
</div>
</div>
</section><section id="non-parametric-methods" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="non-parametric-methods">Non-Parametric Methods</h3>
<p>Non-parametric methods (e.g.&nbsp;<span class="math inline">\(K\)</span> nearest neighbor regression) do not make explicit assumptions about the functional form of <span class="math inline">\(f.\)</span> Instead, they make qualitative assumptions on <span class="math inline">\(f\)</span> such as, for instance, requiring that <span class="math inline">\(f\)</span> is a <em>smooth</em> (e.g.&nbsp;two times continuously differentiable) function.</p>
<ul>
<li><p><strong>Pro:</strong> By avoiding the assumption of a particular parametric functional form for <span class="math inline">\(f,\)</span> non-parametric methods have the potential to accurately fit a wider range of possible shapes for <span class="math inline">\(f.\)</span></p></li>
<li><p><strong>Con:</strong> Non-parametric methods require a large number of observations to obtain an accurate estimate for <span class="math inline">\(f;\)</span> far more observations than is typically needed for a parametric approach if the parametric model assumption is correct. (Non-parametric methods are “data-hungry”.)</p></li>
</ul></section><section id="the-trade-off-between-prediction-accuracy-and-model-interpretability" class="level3" data-number="2.2.1"><h3 data-number="2.2.1" class="anchored" data-anchor-id="the-trade-off-between-prediction-accuracy-and-model-interpretability">
<span class="header-section-number">2.2.1</span> The Trade-Off Between Prediction Accuracy and Model Interpretability</h3>
<p>One might reasonably ask the following question:</p>
<blockquote class="blockquote">
<p>Why would we ever choose to use a more restrictive method instead of a very flexible approach?</p>
</blockquote>
<p>If we are mainly <strong>interested in inference</strong>, then <strong>restrictive models</strong> are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1,\dots,X_p.\)</span> <br> By contrast, very flexible approaches, such as purely non-parametric methods, can lead to such complicated estimates of <span class="math inline">\(f\)</span> that it is difficult to understand how any individual predictor <span class="math inline">\(X_j\)</span> is associated with the response <span class="math inline">\(Y.\)</span></p>
<p>In some settings, we are only <strong>interested in prediction</strong>, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the algorithm is that it predict accurately. In such settings, we might expect that it will be best to use the most flexible model available. Right?<br> Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for <strong>overfitting</strong> in highly flexible methods.</p>
</section><section id="supervised-versus-unsupervised-learning" class="level3" data-number="2.2.2"><h3 data-number="2.2.2" class="anchored" data-anchor-id="supervised-versus-unsupervised-learning">
<span class="header-section-number">2.2.2</span> Supervised versus Unsupervised Learning</h3>
<p>Most statistical learning problems fall into one of two categories:</p>
<ul>
<li>supervised</li>
<li>unsupervised</li>
</ul>
<p>In supervised learning problems, we observe for each predictor <span class="math inline">\(x_i,\)</span> <span class="math inline">\(i=1,\dots,n,\)</span> also a response <span class="math inline">\(y_i.\)</span></p>
<p>In unsupervised learning problems, we only observe the predictor <span class="math inline">\(x_i,\)</span> <span class="math inline">\(i=1,\dots,n,\)</span> but not the associated responses <span class="math inline">\(y_i.\)</span></p>
<p>Supervised learning methods:</p>
<ul>
<li>regression analysis</li>
<li>logistic regression</li>
<li>lasso</li>
<li>ridge regression</li>
</ul>
<p>Unsupervised learning methods:</p>
<ul>
<li>cluster analysis (clustering)</li>
<li>
<span class="math inline">\(K\)</span>-means</li>
</ul></section><section id="regression-versus-classification-problems" class="level3" data-number="2.2.3"><h3 data-number="2.2.3" class="anchored" data-anchor-id="regression-versus-classification-problems">
<span class="header-section-number">2.2.3</span> Regression Versus Classification Problems</h3>
<p>Variables can be characterized as either <em>quantitative</em> or <em>qualitative</em> (also known as categorical).</p>
<p><strong>Quantitative:</strong> Quantitative variables take on numerical values. Examples include a person’s age, height, or income, the value of a house, and categorical the price of a stock.</p>
<p><strong>Qualitative/Categorial:</strong> Examples of qualitative variables include a person’s marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia).</p>
<p>We tend to refer to problems with a <em>quantitative response</em> as <strong>regression problems</strong>, while those involving a <em>qualitative response</em> are often referred to as <strong>classification problems</strong>.</p>
<p>However, the distinction (regression vs.&nbsp;classification) is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or binary). Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K-nearest neighbors can be used in the case of either quantitative or qualitative responses.</p>
</section></section><section id="assessing-model-accuracy" class="level2" data-number="2.3"><h2 data-number="2.3" class="anchored" data-anchor-id="assessing-model-accuracy">
<span class="header-section-number">2.3</span> Assessing Model Accuracy</h2>
<p>It is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.</p>
<p>Let <span id="eq-trainingsample"><span class="math display">\[
\{(X_{01},Y_{01}),(X_{02},Y_{02}),\dots,(X_{0m},Y_{0m})\},
\qquad(2.4)\]</span></span> denote the <strong>test data random sample</strong>, where <span class="math display">\[
(X_{0i},Y_{0i})\overset{\text{iid}}{\sim}(X,Y),\quad i=1,\dots,m.
\]</span> with <span class="math inline">\((X,Y)\)</span> being defined by the general regression model <span class="math inline">\(Y=f(X)+\epsilon\)</span> in <a href="#eq-GRegMod" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>.</p>
<p>That is, the new test data random sample <a href="#eq-trainingsample" class="quarto-xref">Equation&nbsp;<span>2.4</span></a></p>
<ol type="1">
<li>is independent of the training data random sample <a href="#eq-randomsample" class="quarto-xref">Equation&nbsp;<span>2.2</span></a>
</li>
<li>has the same distribution as the training data random sample <a href="#eq-randomsample" class="quarto-xref">Equation&nbsp;<span>2.2</span></a>
</li>
</ol>
<p>The observed realization <span class="math display">\[
\{(x_{01},y_{01}),(x_{02},y_{02}),\dots,(x_{0m},y_{0m})\},
\]</span> of the test data random sample is used to check the accuracy of the estimate <span class="math inline">\(\hat{f}.\)</span></p>
<section id="measuring-the-quality-of-fit" class="level3" data-number="2.3.1"><h3 data-number="2.3.1" class="anchored" data-anchor-id="measuring-the-quality-of-fit">
<span class="header-section-number">2.3.1</span> Measuring the Quality of Fit</h3>
<p>In the regression setting, the most commonly-used measure is the mean squared (prediction) error (MSE).</p>
<p>The global training (data) MSE is given by <span class="math display">\[\begin{align*}
\operatorname{MSE}_{\text{train}}=\frac{1}{n}\sum_{i=1}^n\left(y_i - \hat{f}(x_i)\right)^2,
\end{align*}\]</span> where</p>
<ul>
<li>
<span class="math inline">\(\hat{f}\)</span> is computed from the training data</li>
<li>
<span class="math inline">\(\hat{f}(x_i)\)</span> is the prediction that <span class="math inline">\(\hat{f}\)</span> gives for the <span class="math inline">\(i\)</span>th training data observation.</li>
</ul>
<p>In general, however, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to <strong>previously unseen test data</strong>.</p>
<!-- In fact, a very flexible (e.g. non-parametric) estimation method will tend to overfit the training data such that $y_i\approx \hat{f}(x_i)$ for all $i=1,\dots,n$ resulting in a training MSE that is close to zero since $\hat{f}(x_i)$ fits also the errors $\epsilon_i.$ -->
<!-- **Example:** Suppose that we are interested in developing an algorithm to predict a stock’s price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price
or next month's price.  -->
<!-- **Example:** Suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements.  -->
<p>Thus, we want to choose the method that gives the <strong>lowest <em>test</em> MSE</strong>, as opposed to the lowest <em>training</em> MSE.</p>
</section><section id="local-i.e.-point-wise-test-mse" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="local-i.e.-point-wise-test-mse">Local (i.e.&nbsp;Point-Wise) Test MSE</h3>
<p>Let <span class="math inline">\(\hat{f}\)</span> be computed from the training data <span class="math inline">\(\{(x_1,y_1),\dots,(x_n,y_n)\}.\)</span> And let <span class="math display">\[
\{(x_{0},y_{01}),(x_{0},y_{02})\dots,(x_{0},y_{0m})\}
\]</span> denote the set of <span class="math inline">\(m\)</span> <strong>test data points</strong> <span class="math inline">\(y_{01},\dots,y_{0m}\)</span> for one given predictor value <span class="math inline">\(x_0\)</span>.</p>
<p>This type of <span class="math inline">\(x_0\)</span>-specific test data is a realization of a <strong>conditional random sample</strong> given <span class="math inline">\(X=x_0,\)</span> <span class="math display">\[
(x_{0},Y_{0i})\overset{\text{iid}}{\sim}(X,Y)|X=x_0,\quad i=1,\dots,m.
\]</span> This test data random sample is independent of the training data random sample whose realization was used to compute <span class="math inline">\(\hat{f}.\)</span></p>
<p>Then, the <strong>point-wise test MSE</strong> at <span class="math inline">\(X=x_0\)</span> is given by, <span class="math display">\[\begin{align*}
\operatorname{MSE}_{\text{test}}(x_0)= \frac{1}{m}\sum_{i=1}^m\left(y_{0i} - \hat{f}(x_{0})\right)^2.
\end{align*}\]</span></p>
</section><section id="global-test-mse" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="global-test-mse">Global Test MSE</h3>
<p>Typically, however, we want that a method has <strong>globally</strong>, i.e.&nbsp;for all predictor values in the range of <span class="math inline">\(X\)</span>, a low test MSE (not only at a certain given value <span class="math inline">\(x_0\)</span>). Let <span class="math display">\[
\{(x_{01},y_{01}),(x_{02},y_{02})\dots,(x_{0m},y_{0m})\}
\]</span> denote the set of <span class="math inline">\(m\)</span> test data points with different predictor values <span class="math inline">\(x_{01},\dots,x_{0m}\)</span> in the range of <span class="math inline">\(X\)</span>. This type of test data is a realization of a random sample <span class="math display">\[
(X_{0i},Y_{0i})\overset{\text{iid}}{\sim}(X,Y),\quad i=1,\dots,m.
\]</span> This test data random sample is independent of the training data random sample whose realization was used to compute <span class="math inline">\(\hat{f}.\)</span></p>
<p>Then, the <strong>global test MSE</strong> is given by, <span class="math display">\[\begin{align*}
\operatorname{MSE}_{\text{test}}=\frac{1}{m}\sum_{i=1}^m\left(y_{0i} - \hat{f}(x_{0i})\right)^2.
\end{align*}\]</span></p>
<p>Note that if <span class="math inline">\(\hat{f}\)</span> is a really good estimate of <span class="math inline">\(f,\)</span> i.e.&nbsp;if <span class="math inline">\(\hat{f}\approx f,\)</span> then <span class="math display">\[
\operatorname{MSE}_{\text{test}}\approx \frac{1}{m}\sum_{i=1}^m\epsilon_{0i}^2
\]</span> estimates the variance of the error term <span class="math inline">\(Var(\epsilon)\)</span>, i.e., the <strong>irreducible error component</strong>.</p>
<p><br></p>
<p>Figure 2.9 shows training and test MSEs for smoothing spline (<code>R</code> command <code><a href="https://rdrr.io/r/stats/smooth.spline.html">smooth.spline()</a></code>) estimates <span class="math inline">\(\hat{f}\)</span> in the case of</p>
<ul>
<li>a moderately complex <span class="math inline">\(f\)</span>
</li>
<li>a moderate signal-to-noise ratio <span class="math inline">\(\frac{Var(f(X))}{Var(\epsilon)}\)</span>
</li>
</ul>
<p><img src="images/Fig_2_9.png" class="img-fluid"></p>
<p><br></p>
<p>Figure 2.10 shows training and test MSEs for smoothing spline estimates <span class="math inline">\(\hat{f}\)</span> in the case of</p>
<ul>
<li>a very simple <span class="math inline">\(f\)</span>
</li>
<li>a moderate signal-to-noise ratio <span class="math inline">\(\frac{Var(f(X))}{Var(\epsilon)}\)</span>
</li>
</ul>
<p><img src="images/Fig_2_10.png" class="img-fluid"></p>
<p><br></p>
<p>Figure 2.11 shows training and test MSEs for smoothing spline estimates <span class="math inline">\(\hat{f}\)</span> in the case of</p>
<ul>
<li>a moderately complex <span class="math inline">\(f\)</span>
</li>
<li>a very large signal-to-noise ratio <span class="math inline">\(\frac{Var(f(X))}{Var(\epsilon)}\)</span>
</li>
</ul>
<p><img src="images/Fig_2_11.png" class="img-fluid"></p>
<p><br></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Coding Challenge (Nr 1):
</div>
</div>
<div class="callout-body-container callout-body">
<p>Generate MSE-results similar to those shown in Figure 2.9.</p>
</div>
</div>
<p>In practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available.</p>
<p>As the three examples in Figures 2.9, 2.10, and 2.11 of our textbook illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably.</p>
<p>Throughout this book, we discuss a variety of approaches that can be used in practice to estimate the minimum point of the test MSE.</p>
<p>One important method is <strong>cross-validation</strong>, which is a method for estimating the test MSE using the training data.</p>
</section><section id="the-bias-variance-trade-off" class="level3" data-number="2.3.2"><h3 data-number="2.3.2" class="anchored" data-anchor-id="the-bias-variance-trade-off">
<span class="header-section-number">2.3.2</span> The Bias-Variance Trade-Off</h3>
<p>The U-shape observed in the test MSE curves (Figures 2.9–2.11) turns out to be the result of two competing properties of statistical learning methods.</p>
<ul>
<li><p>Let <span class="math inline">\(\hat{f}\)</span> be estimated from the training data random sample <span class="math display">\[
\{(X_1,Y_1),\dots,(X_n,Y_n)\},
\]</span> <span class="math display">\[
(X_i,Y_i)\overset{\text{iid}}{\sim}(X,Y),\quad i=1,\dots,n.
\]</span> I.e., since <span class="math inline">\(\hat{f}\)</span> is based on the random variables in the random sample, <span class="math inline">\(\hat{f}\)</span> is it self a random variable. (A realized observation of the training data random sample yields a realized observation of <span class="math inline">\(\hat{f}\)</span>.)</p></li>
<li><p>Let <span class="math inline">\(x_0\)</span> denote a given value of the predictor <span class="math inline">\(X\)</span></p></li>
<li><p>Let <span class="math display">\[
\{(x_{0},Y_{01}),\dots,(x_0,Y_{0m})\}
\]</span> <span class="math display">\[
(x_{0},Y_{0i})\overset{\text{iid}}{\sim}(X,Y)|X=x_0,\quad i=1,\dots,m.
\]</span> be the conditional <strong>test data</strong> random sample given <span class="math inline">\(X=x_0.\)</span></p></li>
</ul>
<!-- * Let 
$$
Y_0=f(x_0)+\epsilon
$$ 
be the response random variable defined for the given predictor value $x_0.$ 
* Let $Y_{01},\dots,Y_{0m}$ be all independently and identically distributed as $Y_0;$ shortly
$$
Y_{01},\dots,Y_{0m}\overset{\text{iid}}{\sim}Y_0.
$$  
* Realizations of $Y_{01},\dots,Y_{0m}$ generate the test data at the predictor value $x_0.$ --><p>One can show that the expected test MSE at a given predictor value <span class="math inline">\(x_0\)</span> can be decomposed as following: <span class="math display">\[\begin{align*}
E\left[\operatorname{MSE}_{test}(x_0)\right]
&amp; =E\left[\frac{1}{m}\sum_{i=1}^m\left(Y_{0i}- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; =\frac{1}{m}\sum_{i=1}^mE\left[\left(Y_{0i}- \hat{f}(x_0)\right)^2\right]\quad(\text{linearity of $E$})\\[2ex]
&amp; =\frac{1}{m}\,m\,E\left[\left(Y_{0}- \hat{f}(x_0)\right)^2\right]\quad(\text{since $Y_{0i}$ are iid})\\[2ex]
&amp; =E\left[\left(Y_0- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; =E\left[\left(f(x_0) + \epsilon_0 - \hat{f}(x_0)\right)^2\right]\quad(Y_0=f(x_0)+\epsilon_0)\\[2ex]
&amp; =E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2 +2\left(f(x_0)- \hat{f}(x_0)\right)\epsilon_0 + \epsilon_0^2 \right]\\[2ex]
&amp; =E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp;+ \underbrace{2E\left[\left(f(x_0)- \hat{f}(x_0)\right)\right]\overbrace{E\left[\epsilon_0\right]}^{=0}}_{\text{using independence between training (in $\hat{f}$) and testing data}}\\[2ex]
&amp;+ E\left[\epsilon_0^2 \right]\\[2ex]
&amp; =\underbrace{E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]}_{\text{MSE of $\hat{f}(x_0)$}}+0+Var(\epsilon_0)\\[2ex]
%&amp; =E\left[\left(Y_0- \hat{f}(x_0) \underbrace{+f(x_0)-f(x_0)}_{=0}\right)^2\right]\\[2ex]
%&amp; =E\left[\left(\left(f(x_0)-\hat{f}(x_0)\right)+\epsilon\right)^2\right]\\[2ex]
%&amp; =E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2+2\left(f(x_0)-\hat{f}(x_0)\right)\epsilon+\epsilon^2\right]\\[2ex]
%&amp;\quad \text{Since $\epsilon$ (train) and $\hat{f}$ (test) are independent:}\\[2ex]
%&amp; =E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2\right]+0+Var(\epsilon_0)\\[2ex]
%&amp; =E\left[\left(f(x_0)-\hat{f}(x_0)\underbrace{+E(\hat{f}(x_0))-E(\hat{f}(x_0))}_{=0}\right)^2\right]+Var(\epsilon_0)\\[2ex]
%&amp; =E\left[\left(-\left\{E(\hat{f}(x_0)) - f(x_0)\right\} - \left\{\hat{f}(x_0)-E(\hat{f}(x_0))\right\}\right)^2\right]+Var(\epsilon_0)\\[2ex]
%&amp;\quad\text{(steps skipped since beyond scope)}\\[2ex]
&amp; = Var\left(\hat{f}(x_0)\right) + \left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2 + Var\left(\epsilon_0\right)
\end{align*}\]</span></p>
<p>The <strong>expected MSE</strong> at <span class="math inline">\(x_0,\)</span> <span class="math inline">\(E\left[\operatorname{MSE}_{test}(x_0)\right],\)</span> refers to the average test MSE that we would obtain if we repeatedly estimated <span class="math inline">\(f\)</span> using training data set, and evaluated each at <span class="math inline">\(x_0.\)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A computed value of <span class="math inline">\(\operatorname{MSE}_{test}(x_0)\)</span> (as done in the coding challenge) is not able to consistently approximate <span class="math inline">\(E\left[\operatorname{MSE}_{test}(x_0)\right].\)</span></p>
<p>However, to get information about Bias and Variance of a method, we need to approximate <span class="math inline">\(E\left[\operatorname{MSE}_{test}(x_0)\right].\)</span> This will be (among others) the topic of <span class="quarto-unresolved-ref">?sec-resamplingmethods</span>.</p>
</div>
</div>
<p>To minimize the expected test MSE, we need to select a statistical learning method that <em>simultaneously</em> achieves <strong>low variance</strong> and <strong>low bias</strong>.</p>
<p>Note that <span class="math display">\[
Var\left(\hat{f}(x_0)\right)\geq 0
\]</span> and that <span class="math display">\[
\left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2\geq 0.
\]</span> Thus, the expected test MSE can never lie below of <span class="math inline">\(Var(\epsilon),\)</span> i.e. <span class="math display">\[
\begin{align*}
E\left[\operatorname{MSE}_{test}(x_0)\right]
&amp; =E\left[\left(Y_0- \hat{f}(x_0)\right)^2\right]
\geq Var\left(\epsilon\right).
\end{align*}
\]</span></p>
<p>➡️ The overall, i.e., <strong>global</strong> expected test MSE can be computed by averaging <span class="math inline">\(E[(Y_0- \hat{f}(x_0))^2]\)</span> over all possible values of <span class="math inline">\(x_0\)</span> in the test set. <!-- $$
E\left[E[(Y_0- \hat{f}(X))^2|X]\right]
$$ --></p>
<section id="variance-of-hatf-at-x_0" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="variance-of-hatf-at-x_0">Variance of <span class="math inline">\(\hat{f}\)</span> at <span class="math inline">\(x_0\)</span>
</h4>
<p><span class="math display">\[
Var(\hat{f}(x_0))=E\left[\left(\hat{f}(x_0) - E\left[\hat{f}(x_0)\right]\right)^2\right]
\]</span> <strong>Variance</strong> refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different <span class="math inline">\(\hat{f}.\)</span> But ideally the estimate for <span class="math inline">\(f\)</span> should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}.\)</span> In general, more flexible statistical methods have higher variance.</p>
<p>➡️ The overall, i.e., <strong>global</strong> variance can be computed by averaging <span class="math inline">\(Var(\hat{f}(x_0))\)</span> over all possible values of <span class="math inline">\(x_0\)</span> in the test set.</p>
</section><section id="bias-of-hatf-at-x_0" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="bias-of-hatf-at-x_0">Bias of <span class="math inline">\(\hat{f}\)</span> at <span class="math inline">\(x_0\)</span>
</h4>
<p><span class="math display">\[
\operatorname{Bias}(\hat{f}(x_0))=E\left[\hat{f}(x_0)\right] - f(x_0)
\]</span> <strong>Bias</strong> refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease—and vice versa.</p>
<p>➡️ The overall, i.e., <em>global</em> bias can be computed by averaging <span class="math inline">\(\operatorname{Bias}(\hat{f}(x_0))\)</span> over all possible values of <span class="math inline">\(x_0\)</span> in the test set.</p>
<p><img src="images/Fig_2_12.png" class="img-fluid"></p>
</section></section><section id="the-classification-setting" class="level3" data-number="2.3.3"><h3 data-number="2.3.3" class="anchored" data-anchor-id="the-classification-setting">
<span class="header-section-number">2.3.3</span> The Classification Setting</h3>
<p>Setup:</p>
<ul>
<li>Observed <strong>training data</strong> <span class="math display">\[
\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}
\]</span>
</li>
<li>Qualitative response <span class="math inline">\(y_i\)</span> with categorical class labels. E.g.
<ul>
<li><span class="math inline">\(y_i\in\{\text{red},\text{blue}\}\)</span></li>
<li><span class="math inline">\(y_i\in\{\text{positive returns},\text{negative returns}\}\)</span></li>
</ul>
</li>
<li>The <strong>classifier</strong> <span class="math inline">\(\hat{f}\)</span> is computed from the training data.</li>
<li>Predicted training data class labels: <span class="math display">\[
\hat{y}_i = \hat{f}(x_i)
\]</span>
</li>
</ul>
<p>The alternative to the training MSE is here the <strong>training error rate</strong> <span class="math display">\[
\frac{1}{n}\sum_{i=1}^nI(y_i\neq \hat{y}_i)
\]</span> which gives the relative frequency of false categorical predictions.</p>
<p>Here, <span class="math display">\[
I(\cdot)
\]</span> is an indicator function with <span class="math inline">\(I(\text{true})=1\)</span> and <span class="math inline">\(I(\text{false})=0.\)</span></p>
<p>Let <span class="math display">\[
\{(y_{01},x_{01}), (y_{02},x_{02}),\dots, (y_{0m},x_{0m})\}
\]</span> denote <span class="math inline">\(m\)</span> <strong>test data</strong> observations.</p>
<p>The alternative to the test MSE is here the <strong>test error rate</strong> <span class="math display">\[
\frac{1}{m}\sum_{i=1}^mI(y_{0i}\neq \hat{y}_{0i}),
\]</span> where <span class="math inline">\(\hat{y}_{0i}\)</span> is the predicted class label that results from applying the classifier <span class="math inline">\(\hat{f}\)</span> (computed from the training data) to the test observation with predictor value <span class="math inline">\(x_{i0}.\)</span></p>
<p>A good classifier is one for which the test error rate is smallest.</p>
</section><section id="the-bayes-classifier" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="the-bayes-classifier">The Bayes Classifier</h3>
<p>It is possible to show (proof is outside of the scope of this course) that the test error rate is minimized, on average, by the classifier that assigns an observation to the most likely class, given its predictor value <span class="math inline">\(x_{0}.\)</span> This classifier is called the <strong>Bayes classifier</strong>.</p>
<p>In other words, the Bayes classifier assigns a test observation with predictor vector <span class="math inline">\(x_{0}\)</span> to the class <span class="math inline">\(j\)</span> for which <span class="math display">\[
P(Y = j | X = x_{0})
\]</span> is largest among all possible class labels <span class="math inline">\(j\)</span> (e.g.&nbsp;<span class="math inline">\(j\in\{1,2\}\)</span>).</p>
<p>In a <strong>two-class problem</strong> where there are only two possible response values, say class <span class="math inline">\(1\)</span> or class <span class="math inline">\(2,\)</span> the Bayes classifier corresponds to predicting class <span class="math inline">\(1\)</span> if <span class="math display">\[
P(Y = 1| X = x_0 ) \geq 0.5,
\]</span> and class <span class="math inline">\(2\)</span> if <span class="math display">\[\begin{align*}
&amp; P(Y = 1| X = x_0 ) &lt; 0.5 \\[2ex]
\Leftrightarrow\; &amp; P(Y = 2| X = x_0 ) &gt; 0.5  
\end{align*}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classification threshold <span class="math inline">\(0.5\)</span>?
</div>
</div>
<div class="callout-body-container callout-body">
<p>If no further information is given, one uses usually a threshold of <span class="math inline">\(0.5\)</span> in a two-class classification problem; or, more generally, a threshold of <span class="math inline">\(\frac{1}{G}\)</span> in a <span class="math inline">\(G\)</span>-classes classification problem.</p>
<p>However, in certain applications, different thresholds are used. If, for instance, a certain classification error is very costly, we want to take this into account when choosing the classification threshold in order to the reduce the costs due to miss-classifications.</p>
<p>Example: <span class="math display">\[
y\in\{\text{Person pays back}, \text{Person does not pay back}\}
\]</span> The classification error <span class="math display">\[
\hat{y}=\text{Person pays back} \neq y = \text{Person does not pay back}
\]</span> can be very costly for a bank. So, it makes sense to classify a person with a certain predictor value <span class="math inline">\(x_0\)</span> to the “Person pays back”-class only if, for instance, <span class="math display">\[
\hat{P}(Y = \text{Person pays back}|X=x_0) \geq 0.9,
\]</span> and otherwise classify this person to the “Person does not pay back”-class. This will reduce the frequency of miss-classifications when classifying into the “Person pays back”-class, and thus reduce the costs.</p>
</div>
</div>
<p>Those values of <span class="math inline">\(x_0\)</span> for which <span class="math display">\[\begin{align*}
P(Y = 1| X = x_0 ) = 0.5 = P(Y = 2| X = x_0 )
\end{align*}\]</span> are called the <strong>Bayes decision boundary</strong>. An example of a Bayes decision boundary is shown as the purple dashed line in Fig. 2.13.</p>
<p><img src="images/Fig_2_13.png" class="img-fluid"></p>
<ul>
<li>Note to Fig. 2.13: Here, a <em>perfect</em> classification (i.e.&nbsp;zero error rate) is impossible, since the Bayes decision boundary does not partition the two groups (yellow, blue) in to complete separate groups.</li>
</ul>
<p>The Bayes classifier produces the lowest possible test error rate, called the <strong>Bayes error rate</strong>. The point-wise Bayes error rate at <span class="math inline">\(x_0\)</span> is given by <span class="math display">\[
1 - \max_{j}P(Y = j| X = x_0 ),
\]</span> where the maximization is over all class labels <span class="math inline">\(j\)</span> (e.g.&nbsp;<span class="math inline">\(j\in\{1,2\}\)</span>).</p>
<p>The <em>global</em> overall Bayes error rate is given by <span class="math display">\[
1 - E\left(\max_{j}P(Y = 1| X )\right),
\]</span> where the expectation averages the probability over all possible values of <span class="math inline">\(X.\)</span></p>
</section><section id="k-nearest-neighbors-classification" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="k-nearest-neighbors-classification">
<span class="math inline">\(K\)</span>-Nearest Neighbors Classification</h3>
<p>In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X,\)</span> and so computing the Bayes classifier is impossible.</p>
<p>Many approaches attempt to estimate the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X,\)</span> and then classify a given observation to the class with highest estimated probability. One such method is the <strong><span class="math inline">\(K\)</span>-nearest neighbors</strong> (KNN) classifier.</p>
<p>Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0,\)</span> the KNN classifier first identifies the <span class="math inline">\(K\)</span> points in the training data <span class="math display">\[
\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\},
\]</span> that are closest to <span class="math inline">\(x_0.\)</span></p>
<p>This set of <span class="math inline">\(K\)</span> nearest points (near to <span class="math inline">\(x_0\)</span>) can be represented by the <span class="math inline">\(x_0\)</span>-specfic index set <span class="math display">\[
\mathcal{N}_0=\{i=1,\dots,n \;|\; x_i \text{ is the $K$th closest point to }x_0 \text{ or closter}\}.
\]</span> I.e. <span class="math inline">\(\mathcal{N}_0\)</span> is an index set that allows to select the <span class="math inline">\(K\)</span> nearest neighbors in the training data.</p>
<p>From the definition of <span class="math inline">\(\mathcal{N}_0\)</span> is follows that:</p>
<ul>
<li><span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n\}\)</span></li>
<li><span class="math inline">\(|\mathcal{N}_0|=K\)</span></li>
</ul>
<p>KNN estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of the <span class="math inline">\(K\)</span> points <span class="math inline">\((x_i,y_i)\)</span> selected by the index-set <span class="math inline">\(\mathcal{N}_0\)</span> whose response value <span class="math inline">\(y_i\)</span> equals <span class="math inline">\(j:\)</span> <span id="eq-DefKNN"><span class="math display">\[
\begin{align}
P(Y = j | X = x_{0})
&amp;\approx \hat{P}(Y = j | X = x_{0})\\[2ex]
&amp;= \frac{1}{K}\sum_{i\in\mathcal{N}_0}I(y_i = j),
\end{align}
\qquad(2.5)\]</span></span> where <span class="math inline">\(I(\texttt{TRUE})=1\)</span> and <span class="math inline">\(I(\texttt{FALSE})=0.\)</span></p>
<p>Finally, KNN classifies the test observation <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> with the largest probability from <a href="#eq-DefKNN" class="quarto-xref">Equation&nbsp;<span>2.5</span></a>.</p>
<p>Figure 2.14 provides an illustrative example of the KNN approach.</p>
<ul>
<li>Two-dimensional predictor <span class="math inline">\(X=(X_1,X_2),\)</span> where <span class="math inline">\(X_1\)</span> is shown on the x-axis and <span class="math inline">\(X_2\)</span> on the y-axis.</li>
<li>Two class labels <span class="math inline">\(Y\in\{\text{yellow}, \text{blue}\}.\)</span>
</li>
<li>Training data consists of six data points <span class="math display">\[
\{(y_1,x_{11},x_{12}),\dots,(y_6,x_{61},x_{62})\}
\]</span> (See the left panel of Figure 2.14.)</li>
<li>Class-label prediction (“classification”) are computed for a regular grid of predictor values <span class="math inline">\(x_{0}=(x_{01},x_{02}).\)</span> (See the regular grid of points in the right panel of Figure 2.14.)</li>
<li>
<span class="math inline">\(K=3\)</span> nearest neighbors are used to compute the class-label predictions.</li>
</ul>
<p><br></p>
<p><img src="images/Fig_2_14.png" class="img-fluid"></p>
<ul>
<li>In the <strong>left-hand panel</strong> of Figure 2.14, a small training data set is shown consisting of six blue and six orange observations. Our goal is to make a prediction for the point labeled by the black cross.</li>
<li>In the <strong>right-hand panel</strong> of Figure 2.14, we have applied the KNN approach with <span class="math inline">\(K = 3\)</span> at all of the possible values for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2,\)</span> and have drawn in the corresponding KNN decision boundary.</li>
</ul>
<p><br></p>
<p><img src="images/Fig_2_15.png" class="img-fluid"></p>
<p><br></p>
<p><img src="images/Fig_2_16.png" class="img-fluid"></p>
<p><br></p>
<p><img src="images/Fig_2_17.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Coding Challenge (Nr 2):
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following <code>R</code>-code generates training data for the two-class classification problem with</p>
<ul>
<li>Class 1: Circle with center <span class="math inline">\((x_1,x_2)=(0.5,0.5)\)</span> and radius <span class="math inline">\(r=0.2,\)</span> i.e. <span class="math display">\[
\text{Cl1} = \{(x_1,x_2)\in[0,1]^2:(x_1-0.5)^2+(x_2-0.5)^2\leq 0.2^2\}
\]</span>
</li>
<li>Class 2: All data points in the square <span class="math inline">\([0,1]^2\)</span> that do not belong to Class 1, i.e. <span class="math display">\[
\text{Cl2} = \{(x_1,x_2)\in[0,1]^2: (x_1,x_2)\not\in\text{Cl1}\}
\]</span>
</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># install.packages("plotrix") # Install plotrix package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/dmurdoch/plotrix">"plotrix"</a></span><span class="op">)</span>   <span class="co"># Load plotrix package (draw.circle())</span></span>
<span></span>
<span><span class="co">## Class 1: Circle  </span></span>
<span><span class="va">radius</span> <span class="op">&lt;-</span> <span class="fl">0.2</span></span>
<span><span class="va">center</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>,<span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="co">## Class 2: [0,1] x [0,1] \ Class 1</span></span>
<span></span>
<span><span class="co">## function to map data points to class labels "Cl1" and "Cl2"</span></span>
<span><span class="va">my_class_fun</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">cent</span> <span class="op">=</span> <span class="va">center</span>, <span class="va">rad</span> <span class="op">=</span> <span class="va">radius</span>, <span class="va">error</span> <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">tmp</span>    <span class="op">&lt;-</span> <span class="op">(</span><span class="va">x1</span> <span class="op">-</span> <span class="va">cent</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="va">x2</span> <span class="op">-</span> <span class="va">cent</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="va">rad</span>    <span class="op">&lt;-</span> <span class="va">rad</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>, <span class="fl">1</span><span class="op">)</span> <span class="op">*</span> <span class="va">error</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">tmp</span> <span class="op">&lt;=</span> <span class="va">rad</span><span class="op">^</span><span class="fl">2</span>, <span class="st">"Cl1"</span>, <span class="st">"Cl2"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">my_class_fun</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Vectorize.html">Vectorize</a></span><span class="op">(</span><span class="va">my_class_fun</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x1"</span>, <span class="st">"x2"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## ##################################</span></span>
<span><span class="co">## Generate training data (with error)</span></span>
<span><span class="co">## ##################################</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">321</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## number of training data points</span></span>
<span><span class="va">n_train</span>    <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span></span>
<span><span class="co">## error</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fl">0.05</span></span>
<span></span>
<span><span class="va">train_X1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n_train</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">train_X2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n_train</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">train_Cl</span> <span class="op">&lt;-</span> <span class="fu">my_class_fun</span><span class="op">(</span>x1 <span class="op">=</span> <span class="va">train_X1</span>, </span>
<span>                         x2 <span class="op">=</span> <span class="va">train_X2</span>, error <span class="op">=</span> <span class="va">error</span><span class="op">)</span></span>
<span><span class="va">train_Cl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">train_Cl</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">train_Cl</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cl1 Cl2 
 46 454 </code></pre>
</div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  <span class="st">"Cl"</span> <span class="op">=</span> <span class="va">train_Cl</span>,</span>
<span>  <span class="st">"X1"</span> <span class="op">=</span> <span class="va">train_X1</span>, </span>
<span>  <span class="st">"X2"</span> <span class="op">=</span> <span class="va">train_X2</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data_train</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Cl        X1        X2
1 Cl2 0.9558938 0.5858416
2 Cl2 0.9372855 0.3106846
3 Cl2 0.2382205 0.7639562
4 Cl2 0.2550736 0.1329140
5 Cl2 0.3905120 0.2018027
6 Cl1 0.3411799 0.6347841</code></pre>
</div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="fl">0</span>, </span>
<span>     type <span class="op">=</span> <span class="st">"n"</span>, </span>
<span>     xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>,</span>
<span>     ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>, </span>
<span>     xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>     ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, asp<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/plotrix/man/draw.circle.html">draw.circle</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">center</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, y <span class="op">=</span> <span class="va">center</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, radius <span class="op">=</span> <span class="va">radius</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0.5</span>, y <span class="op">=</span> <span class="fl">0.5</span>, label <span class="op">=</span> <span class="st">"Class 1"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0.1</span>, y <span class="op">=</span> <span class="fl">0.9</span>, label <span class="op">=</span> <span class="st">"Class 2"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="fl">0</span>, </span>
<span>     type <span class="op">=</span> <span class="st">"n"</span>, </span>
<span>     xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>,</span>
<span>     ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>, </span>
<span>     xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>     ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, asp<span class="op">=</span><span class="fl">1</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Training data with errors"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/plotrix/man/draw.circle.html">draw.circle</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">center</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, y <span class="op">=</span> <span class="va">center</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, radius <span class="op">=</span> <span class="va">radius</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span>x   <span class="op">=</span> <span class="va">data_train</span><span class="op">$</span><span class="va">X1</span>, </span>
<span>       y   <span class="op">=</span> <span class="va">data_train</span><span class="op">$</span><span class="va">X2</span>, </span>
<span>       col <span class="op">=</span> <span class="va">data_train</span><span class="op">$</span><span class="va">Cl</span>,  pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch1_StatLearning_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Challenge:</strong></p>
<ul>
<li>Write an KNN-function that uses the training data to classify a grid of test data in <span class="math inline">\([0,1]^2\)</span> into the two classes.</li>
</ul>
</div>
</div>
</section></section><section id="exercises" class="level2" data-number="2.4"><h2 data-number="2.4" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">2.4</span> Exercises</h2>
<p>Prepare the following exercises of Chapter 2 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 7</li>
<li>Exercise 8</li>
<li>Exercise 9</li>
</ul>
<!-- {{< include Ch1_Solutions.qmd >}} -->

</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./Ch0_Intro2R.html" class="pagination-link" aria-label="`R`-Lab: Introduction to `R`">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch2_LinearRegression.html" class="pagination-link" aria-label="Linear Regression">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>