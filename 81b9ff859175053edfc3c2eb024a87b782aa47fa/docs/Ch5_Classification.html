<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>5&nbsp; Classification – Computer-Aided Statistical Analysis (B.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch6_ResamplingMethods.html" rel="next">
<link href="./Ch4_LinearRegression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch5_Classification.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Intro2R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_StatLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_MatrixAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_LinearRegression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_ResamplingMethods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">5.1</span> Overview</a></li>
  <li><a href="#why-not-linear-regression" id="toc-why-not-linear-regression" class="nav-link" data-scroll-target="#why-not-linear-regression"><span class="header-section-number">5.2</span> Why Not Linear Regression?</a></li>
  <li><a href="#assessing-model-accuracy-the-classification-setting" id="toc-assessing-model-accuracy-the-classification-setting" class="nav-link" data-scroll-target="#assessing-model-accuracy-the-classification-setting"><span class="header-section-number">5.3</span> Assessing Model Accuracy: The Classification Setting</a></li>
  <li><a href="#the-bayes-classifier" id="toc-the-bayes-classifier" class="nav-link" data-scroll-target="#the-bayes-classifier"><span class="header-section-number">5.4</span> The Bayes Classifier</a></li>
  <li><a href="#k-nearest-neighbors-classification" id="toc-k-nearest-neighbors-classification" class="nav-link" data-scroll-target="#k-nearest-neighbors-classification"><span class="header-section-number">5.5</span> <span class="math inline">\(K\)</span>-Nearest Neighbors Classification</a></li>
  <li>
<a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">5.6</span> Logistic Regression</a>
  <ul class="collapse">
<li><a href="#the-simple-p1-logistic-model" id="toc-the-simple-p1-logistic-model" class="nav-link" data-scroll-target="#the-simple-p1-logistic-model"><span class="header-section-number">5.6.1</span> The Simple <span class="math inline">\((p=1)\)</span> Logistic Model</a></li>
  <li><a href="#estimating-the-regression-coefficients" id="toc-estimating-the-regression-coefficients" class="nav-link" data-scroll-target="#estimating-the-regression-coefficients"><span class="header-section-number">5.6.2</span> Estimating the Regression Coefficients</a></li>
  <li><a href="#making-predictions" id="toc-making-predictions" class="nav-link" data-scroll-target="#making-predictions"><span class="header-section-number">5.6.3</span> Making Predictions</a></li>
  <li><a href="#multiple-logistic-regression" id="toc-multiple-logistic-regression" class="nav-link" data-scroll-target="#multiple-logistic-regression"><span class="header-section-number">5.6.4</span> Multiple Logistic Regression</a></li>
  <li><a href="#sec-multinomialLogistic" id="toc-sec-multinomialLogistic" class="nav-link" data-scroll-target="#sec-multinomialLogistic"><span class="header-section-number">5.6.5</span> Multinomial Logistic Regression</a></li>
  </ul>
</li>
  <li>
<a href="#discriminant-analysis-generative-models-for-classification" id="toc-discriminant-analysis-generative-models-for-classification" class="nav-link" data-scroll-target="#discriminant-analysis-generative-models-for-classification"><span class="header-section-number">5.7</span> Discriminant Analysis: Generative Models for Classification</a>
  <ul class="collapse">
<li><a href="#linear-discriminant-analysis-for-p-1" id="toc-linear-discriminant-analysis-for-p-1" class="nav-link" data-scroll-target="#linear-discriminant-analysis-for-p-1"><span class="header-section-number">5.7.1</span> Linear Discriminant Analysis for <span class="math inline">\(p = 1\)</span></a></li>
  <li><a href="#linear-discriminant-analysis-for-p-1-1" id="toc-linear-discriminant-analysis-for-p-1-1" class="nav-link" data-scroll-target="#linear-discriminant-analysis-for-p-1-1"><span class="header-section-number">5.7.2</span> Linear Discriminant Analysis for <span class="math inline">\(p &gt; 1\)</span></a></li>
  <li><a href="#classification-errors-of-binary-classifiers" id="toc-classification-errors-of-binary-classifiers" class="nav-link" data-scroll-target="#classification-errors-of-binary-classifiers"><span class="header-section-number">5.7.3</span> Classification Errors of Binary Classifiers</a></li>
  <li><a href="#the-roc-curve" id="toc-the-roc-curve" class="nav-link" data-scroll-target="#the-roc-curve"><span class="header-section-number">5.7.4</span> The ROC curve</a></li>
  <li><a href="#quadratic-discriminant-analysis-qda" id="toc-quadratic-discriminant-analysis-qda" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis-qda"><span class="header-section-number">5.7.5</span> Quadratic Discriminant Analysis (QDA)</a></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes"><span class="header-section-number">5.7.6</span> Naive Bayes</a></li>
  </ul>
</li>
  <li>
<a href="#self-study-r-lab-classification" id="toc-self-study-r-lab-classification" class="nav-link" data-scroll-target="#self-study-r-lab-classification"><span class="header-section-number">5.8</span> Self-Study <code>R</code>-Lab: Classification</a>
  <ul class="collapse">
<li><a href="#the-stock-market-data" id="toc-the-stock-market-data" class="nav-link" data-scroll-target="#the-stock-market-data"><span class="header-section-number">5.8.1</span> The Stock Market Data</a></li>
  <li><a href="#logistic-regression-1" id="toc-logistic-regression-1" class="nav-link" data-scroll-target="#logistic-regression-1"><span class="header-section-number">5.8.2</span> Logistic Regression</a></li>
  <li><a href="#linear-discriminant-analysis" id="toc-linear-discriminant-analysis" class="nav-link" data-scroll-target="#linear-discriminant-analysis"><span class="header-section-number">5.8.3</span> Linear Discriminant Analysis</a></li>
  <li><a href="#quadratic-discriminant-analysis" id="toc-quadratic-discriminant-analysis" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis"><span class="header-section-number">5.8.4</span> Quadratic Discriminant Analysis</a></li>
  <li><a href="#naive-bayes-1" id="toc-naive-bayes-1" class="nav-link" data-scroll-target="#naive-bayes-1"><span class="header-section-number">5.8.5</span> Naive Bayes</a></li>
  <li><a href="#k-nearest-neighbors" id="toc-k-nearest-neighbors" class="nav-link" data-scroll-target="#k-nearest-neighbors"><span class="header-section-number">5.8.6</span> <span class="math inline">\(K\)</span>-Nearest Neighbors</a></li>
  </ul>
</li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.9</span> Exercises</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><blockquote class="blockquote">
<p>Reading:</p>
</blockquote>
<ul>
<li>Chapter 2 (the classification parts) of our course textbook <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>
</li>
<li>Chapter 4 of our course textbook <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>
</li>
</ul>
<section id="overview" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="overview">
<span class="header-section-number">5.1</span> Overview</h2>
<p>Classification problems occur often, perhaps even more so than regression problems.</p>
<p>Some examples include:</p>
<ol type="1">
<li>A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?</li>
<li>An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.</li>
<li>On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.</li>
</ol>
<p>One of the running example for this chapter: The (simulated) <code>Default</code> data set which is part of the <a href="https://www.statlearning.com/resources-second-edition">online resources</a> of our textbook <code>ISLR</code>, and which is also contained in the <code>R</code> package <code>ISLR2</code>. <img src="images/Fig_4_1.png" class="img-fluid"></p>
<p>Let’s take a first look at the a priori default rate in this dataset:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Default</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## sample size</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Default</span><span class="op">)</span>       </span>
<span></span>
<span><span class="co">## "no-default"-rate and "default"-rate</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span><span class="op">)</span><span class="op">/</span><span class="va">n</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    No    Yes 
0.9667 0.0333 </code></pre>
</div>
</div>
<ul>
<li>Overall “no-default”-rate: approx. <span class="math inline">\(97\%.\)</span>
</li>
<li>Overall “default”-rate: approx. <span class="math inline">\(3\%.\)</span>
</li>
</ul>
<p>Note: Figure 4.1 shows only a small fraction of the individuals who did not default to avoid an overcrowded plot.</p>
</section><section id="why-not-linear-regression" class="level2" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="why-not-linear-regression">
<span class="header-section-number">5.2</span> Why Not Linear Regression?</h2>
<p>Linear regression is often not appropriate in the case of a qualitative response <span class="math inline">\(Y.\)</span></p>
<p>Suppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses:</p>
<ul>
<li><code>stroke</code></li>
<li>
<code>drug overdose</code>, and</li>
<li><code>epileptic seizure</code></li>
</ul>
<p>We can encoding these values as a quantitative response variable, <span class="math display">\[
Y=\left\{
    \begin{array}{ll}
    1&amp;\quad\text{if }\texttt{stroke}\\
    2&amp;\quad\text{if }\texttt{drug overdose}\\
    3&amp;\quad\text{if }\texttt{epileptic seizure}\\
    \end{array}
\right.
\]</span> Using this coding, least squares could be used to fit a linear regression model to predict <span class="math inline">\(Y,\)</span> but</p>
<ol type="1">
<li>the results would then depend on the numeric ordering <span class="math inline">\(1&lt;2&lt;3,\)</span> even though the ordering was completely arbitrary and could have been made differently.</li>
<li>the results would then depend on the assumption that the gap <span class="math inline">\((2-1=1)\)</span> between <code>stroke</code> and <code>drug overdose</code> is comparable to the gap <span class="math inline">\((3-2=1)\)</span> between <code>drug overdose</code> and <code>epileptic seizure</code> which would be a very unrealistic.</li>
</ol>
<p>Generally, both points are quite problematic for most applications.</p>
<p>Only if the response variable’s values did take on a natural ordering, such as <em>“mild”</em>, <em>“moderate”</em>, and <em>“severe”</em>, and we felt the gap between <em>mild</em> and <em>moderate</em> was similar to the gap between <em>moderate</em> and <em>severe</em>, then a 1, 2, 3 coding would be reasonable.</p>
<p>For a <strong>binary (two level) qualitative response</strong>, the situation is better. For instance, if there are only two conditions that we need to predict (e.g.&nbsp;either <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> or <code>default</code><span class="math inline">\(=\)</span><code>No</code>), we can use a dummy variable coding <span id="eq-binaryY"><span class="math display">\[
Y=\left\{
\begin{array}{ll}
    1&amp;\quad\text{if }\texttt{default}=\texttt{Yes}\\
    0&amp;\quad\text{if }\texttt{default}=\texttt{No}\\
\end{array}
\right.
\qquad(5.1)\]</span></span> We could then fit a linear regression to this binary response, and predict drug overdose if <span class="math inline">\(\hat{Y}&gt; 0.5\)</span> and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.</p>
<p>For a binary response, <span class="math inline">\(Y\in\{0,1\},\)</span> as in <a href="#eq-binaryY" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>, linear regression is not completely unreasonable. For the multiple linear regression model, we have that <span class="math display">\[\begin{align*}
Y
&amp;= f(X) + \epsilon \\[2ex]
&amp;= \beta_0+ \beta_1 X_1+\dots +\beta_p X_p + \epsilon,
\end{align*}\]</span> where, under the assumptions of <span class="quarto-unresolved-ref">?sec-linearRegCh</span>, <span class="math display">\[
E(\epsilon|X)=0.
\]</span> Therefore, <span id="eq-LinProb1"><span class="math display">\[
\begin{align*}
E(Y|X)
&amp;=E\left(\beta_0+ \beta_1 X_1+\dots +\beta_p X_p + \epsilon|X\right)\\[2ex]
&amp;=\beta_0+ \beta_1 X_1+\dots +\beta_p X_p.
\end{align*}
\qquad(5.2)\]</span></span> Moreover, since <span class="math inline">\(Y\in\{0,1\}\)</span> is a binary random variable, the conditional mean <span class="math inline">\(E(Y|X)\)</span> equals the conditional probability of <span class="math inline">\(Y=1\)</span> given <span class="math inline">\(X\)</span> <span id="eq-LinProb2"><span class="math display">\[
\begin{align*}
E(Y|X)
&amp; =  Pr(Y=1|X)\cdot 1 + Pr(Y=0|X)\cdot 0\\[2ex]
&amp; =  Pr(Y=1|X).  
\end{align*}
\qquad(5.3)\]</span></span> Combining <a href="#eq-LinProb1" class="quarto-xref">Equation&nbsp;<span>5.2</span></a> and <a href="#eq-LinProb2" class="quarto-xref">Equation&nbsp;<span>5.3</span></a> yields <span class="math display">\[\begin{align*}
Pr(Y=1|X) &amp;=\beta_0+ \beta_1 X_1+\dots +\beta_p X_p
\end{align*}\]</span> Thus the estimated model approximates the conditional probability of <span class="math inline">\(Y=1\)</span> (e.g.&nbsp;<span class="math inline">\(\texttt{default}=\texttt{Yes}\)</span>) given <span class="math inline">\(X:\)</span> <span class="math display">\[
Pr(Y=1|X) \approx \hat{Y}=\hat\beta_0+ \hat\beta_1 X_1+\dots + \hat\beta_p X_p.
\]</span></p>
<p>However, if we use linear regression, some of our estimates might be outside the <span class="math inline">\([0, 1]\)</span> interval (see left panel in Figure 4.2), which doesn’t make sense when predicting probabilities. <img src="images/Fig_4_2.png" class="img-fluid"></p>
<p><strong>Summary:</strong></p>
<ol type="a">
<li>A classic regression method cannot well accommodate a qualitative response with more than two classes</li>
<li>A classic regression method may not provide meaningful estimates of <span class="math inline">\(Pr(Y |X),\)</span> even with just two classes.</li>
</ol>
<p>Thus, it is often preferable to use a classification method that is truly suited for qualitative response values such as, for instance, logistic regression.</p>
</section><section id="assessing-model-accuracy-the-classification-setting" class="level2" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="assessing-model-accuracy-the-classification-setting">
<span class="header-section-number">5.3</span> Assessing Model Accuracy: The Classification Setting</h2>
<p>Setup:</p>
<ul>
<li>Observed <strong>training data</strong> <span class="math display">\[
\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}
\]</span>
</li>
<li>Qualitative response <span class="math inline">\(y_i\)</span> with categorical class labels. E.g.
<ul>
<li><span class="math inline">\(y_i\in\{\text{red},\text{blue}\}\)</span></li>
<li><span class="math inline">\(y_i\in\{\text{positive returns},\text{negative returns}\}\)</span></li>
</ul>
</li>
<li>The <strong>classifier</strong> <span class="math inline">\(\hat{f}\)</span> is computed from the training data.</li>
<li>Predicted training data class labels: <span class="math display">\[
\hat{y}_i = \hat{f}(x_i)
\]</span>
</li>
</ul>
<p>The alternative to the training MSE (introduced in <a href="Ch2_StatLearning.html#sec-mqfit" class="quarto-xref"><span>Section 2.3.1</span></a>) is here the <strong>training error rate</strong> <span class="math display">\[
\frac{1}{n}\sum_{i=1}^nI(y_i\neq \hat{y}_i)
\]</span> which gives the relative frequency of false categorical predictions.</p>
<p>Here, <span class="math display">\[
I(\cdot)
\]</span> is an indicator function with <span class="math inline">\(I(\text{true})=1\)</span> and <span class="math inline">\(I(\text{false})=0.\)</span></p>
<p>Let <span class="math display">\[
\{(y_{01},x_{01}), (y_{02},x_{02}),\dots, (y_{0m},x_{0m})\}
\]</span> denote <span class="math inline">\(m\)</span> <strong>test data</strong> observations.</p>
<p>The alternative to the test MSE is here the <strong>test error rate</strong> <span class="math display">\[
\frac{1}{m}\sum_{i=1}^mI(y_{0i}\neq \hat{y}_{0i}),
\]</span> where <span class="math inline">\(\hat{y}_{0i}\)</span> is the predicted class label that results from applying the classifier <span class="math inline">\(\hat{f}\)</span> (computed from the training data) to the test observation with predictor value <span class="math inline">\(x_{i0}.\)</span></p>
<p>A good classifier is one for which the test error rate is smallest.</p>
</section><section id="the-bayes-classifier" class="level2" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="the-bayes-classifier">
<span class="header-section-number">5.4</span> The Bayes Classifier</h2>
<p>It is possible to show (proof is outside of the scope of this course) that the test error rate is minimized, on average, by the classifier that assigns an observation to the most likely class, given its predictor value <span class="math inline">\(x_{0}.\)</span> This classifier is called the <strong>Bayes classifier</strong>.</p>
<p>In other words, the Bayes classifier assigns a test observation with predictor vector <span class="math inline">\(x_{0}\)</span> to the class <span class="math inline">\(j\)</span> for which <span class="math display">\[
P(Y = j | X = x_{0})
\]</span> is largest among all possible class labels <span class="math inline">\(j\)</span> (e.g.&nbsp;<span class="math inline">\(j\in\{1,2\}\)</span>).</p>
<p>In a <strong>two-class problem</strong> where there are only two possible response values, say class <span class="math inline">\(1\)</span> or class <span class="math inline">\(2,\)</span> the Bayes classifier corresponds to predicting class <span class="math inline">\(1\)</span> if <span class="math display">\[
P(Y = 1| X = x_0 ) \geq 0.5,
\]</span> and class <span class="math inline">\(2\)</span> if <span class="math display">\[
\begin{align*}
&amp; P(Y = 1| X = x_0 ) &lt; 0.5 \\[2ex]
\Leftrightarrow\; &amp; P(Y = 2| X = x_0 ) &gt; 0.5  
\end{align*}
\]</span></p>
<p>Those values of <span class="math inline">\(x_0\)</span> for which <span class="math display">\[
\begin{align*}
P(Y = 1| X = x_0 ) = 0.5 = P(Y = 2| X = x_0 )
\end{align*}
\]</span> are called the <strong>Bayes decision boundary</strong>. An example of a Bayes decision boundary is shown as the purple dashed line in Fig. 2.13.</p>
<p><img src="images/Fig_2_13.png" class="img-fluid"></p>
<p><strong>Note to Fig. 2.13:</strong> Here, a <em>perfect</em> classification (i.e.&nbsp;zero error rate) is impossible, since the Bayes decision boundary does not partition the two groups (yellow, blue) in to complete separate groups.</p>
<p>The Bayes classifier produces the lowest possible test error rate, called the <strong>Bayes error rate</strong>. The point-wise Bayes error rate at <span class="math inline">\(x_0\)</span> is given by <span class="math display">\[
1 - \max_{j}P(Y = j| X = x_0 ),
\]</span> where the maximization is over all class labels <span class="math inline">\(j\)</span> (e.g.&nbsp;<span class="math inline">\(j\in\{1,2\}\)</span>).</p>
<p>The <em>global</em> overall Bayes error rate is given by <span class="math display">\[
1 - E\left(\max_{j}P(Y = j| X )\right),
\]</span> where the expectation averages the probability over all possible values of <span class="math inline">\(X.\)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classification threshold <span class="math inline">\(0.5\)</span>?
</div>
</div>
<div class="callout-body-container callout-body">
<p>If no further information is given, one uses usually a threshold of <span class="math inline">\(0.5\)</span> in a two-class classification problem; or, more generally, a threshold of <span class="math inline">\(\frac{1}{G}\)</span> in a <span class="math inline">\(G\)</span>-classes classification problem.</p>
<p>However, in certain applications, different thresholds are used. If, for instance, a certain classification error is very costly, we want to take this into account when choosing the classification threshold in order to the reduce the costs due to miss-classifications.</p>
<p>Example: <span class="math display">\[
y\in\{\text{Person pays back}, \text{Person does not pay back}\}
\]</span> The classification error <span class="math display">\[
\hat{y}=\text{Person pays back} \neq y = \text{Person does not pay back}
\]</span> can be very costly for a bank. So, it makes sense to classify a person with a certain predictor value <span class="math inline">\(x_0\)</span> to the “Person pays back”-class only if, for instance, <span class="math display">\[
\hat{P}(Y = \text{Person pays back}|X=x_0) \geq 0.9,
\]</span> and otherwise classify this person to the “Person does not pay back”-class. This will reduce the frequency of miss-classifications when classifying into the “Person pays back”-class, and thus reduce the costs.</p>
</div>
</div>
</section><section id="k-nearest-neighbors-classification" class="level2" data-number="5.5"><h2 data-number="5.5" class="anchored" data-anchor-id="k-nearest-neighbors-classification">
<span class="header-section-number">5.5</span> <span class="math inline">\(K\)</span>-Nearest Neighbors Classification</h2>
<p>In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X,\)</span> and so computing the Bayes classifier is impossible.</p>
<p>Many approaches attempt to estimate the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X,\)</span> and then classify a given observation to the class with highest estimated probability. One such method is the <strong><span class="math inline">\(K\)</span>-nearest neighbors</strong> (KNN) classifier.</p>
<p>Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0,\)</span> the KNN classifier first identifies the <span class="math inline">\(K\)</span> points in the training data <span class="math display">\[
\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\},
\]</span> that are closest to <span class="math inline">\(x_0.\)</span></p>
<p>This set of <span class="math inline">\(K\)</span> nearest points (near to <span class="math inline">\(x_0\)</span>) can be represented by the <span class="math inline">\(x_0\)</span>-specfic index set <span class="math display">\[
\mathcal{N}_0=\{i=1,\dots,n \;|\; x_i \text{ is the $K$th closest point to }x_0 \text{ or closter}\}.
\]</span> I.e. <span class="math inline">\(\mathcal{N}_0\)</span> is an index set that allows to select the <span class="math inline">\(K\)</span> nearest neighbors in the training data.</p>
<p>From the definition of <span class="math inline">\(\mathcal{N}_0\)</span> is follows that:</p>
<ul>
<li><span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n\}\)</span></li>
<li><span class="math inline">\(|\mathcal{N}_0|=K\)</span></li>
</ul>
<p>KNN estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of the <span class="math inline">\(K\)</span> points <span class="math inline">\((x_i,y_i)\)</span> selected by the index-set <span class="math inline">\(\mathcal{N}_0\)</span> whose response value <span class="math inline">\(y_i\)</span> equals <span class="math inline">\(j:\)</span> <span id="eq-DefKNN"><span class="math display">\[
\begin{align}
P(Y = j | X = x_{0})
&amp;\approx \hat{P}(Y = j | X = x_{0})\\[2ex]
&amp;= \frac{1}{K}\sum_{i\in\mathcal{N}_0}I(y_i = j),
\end{align}
\qquad(5.4)\]</span></span> where <span class="math inline">\(I(\texttt{TRUE})=1\)</span> and <span class="math inline">\(I(\texttt{FALSE})=0.\)</span></p>
<p>Finally, KNN classifies the test observation <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> with the largest probability from <a href="#eq-DefKNN" class="quarto-xref">Equation&nbsp;<span>5.4</span></a>.</p>
<p>Figure 2.14 provides an illustrative example of the KNN approach.</p>
<ul>
<li>Two-dimensional predictor <span class="math inline">\(X=(X_1,X_2),\)</span> where <span class="math inline">\(X_1\)</span> is shown on the x-axis and <span class="math inline">\(X_2\)</span> on the y-axis.</li>
<li>Two class labels <span class="math inline">\(Y\in\{\text{yellow}, \text{blue}\}.\)</span>
</li>
<li>Training data consists of six data points <span class="math display">\[
\{(y_1,x_{11},x_{12}),\dots,(y_6,x_{61},x_{62})\}
\]</span> (See the left panel of Figure 2.14.)</li>
<li>Class-label prediction (“classification”) are computed for a regular grid of predictor values <span class="math inline">\(x_{0}=(x_{01},x_{02}).\)</span> (See the regular grid of points in the right panel of Figure 2.14.)</li>
<li>
<span class="math inline">\(K=3\)</span> nearest neighbors are used to compute the class-label predictions.</li>
</ul>
<p><br></p>
<p><img src="images/Fig_2_14.png" class="img-fluid"></p>
<ul>
<li>In the <strong>left-hand panel</strong> of Figure 2.14, a small training data set is shown consisting of six blue and six orange observations. Our goal is to make a prediction for the point labeled by the black cross.</li>
<li>In the <strong>right-hand panel</strong> of Figure 2.14, we have applied the KNN approach with <span class="math inline">\(K = 3\)</span> at all of the possible values for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2,\)</span> and have drawn in the corresponding KNN decision boundary.</li>
</ul>
<p><br></p>
<p><img src="images/Fig_2_15.png" class="img-fluid"></p>
<p><br></p>
<p><img src="images/Fig_2_16.png" class="img-fluid"></p>
<p><br></p>
<p><img src="images/Fig_2_17.png" class="img-fluid"></p>
</section><section id="logistic-regression" class="level2" data-number="5.6"><h2 data-number="5.6" class="anchored" data-anchor-id="logistic-regression">
<span class="header-section-number">5.6</span> Logistic Regression</h2>
<p>Logistic regression models the probability that <span class="math inline">\(Y\)</span> belongs to a particular category. We begin with <span class="math inline">\(K=2\)</span> category problems, and consider <span class="math inline">\(K&gt;2\)</span> category problems below in <a href="#sec-multinomialLogistic" class="quarto-xref"><span>Section 5.6.5</span></a>.</p>
<p>For the <code>Default</code> data, logistic regression models the conditional probability of <code>default</code><span class="math inline">\(\in\{\texttt{Yes},\texttt{No}\}\)</span> given values for the predictor(s). For example, the probability of the even <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> given <code>balance</code>: <span class="math display">\[
Pr(\texttt{default}=\texttt{Yes}|\texttt{balance}) = p(\texttt{balance}),
\]</span> where <span class="math inline">\(p(\texttt{balance})\in[0,1]\)</span> is used as a short hand notation.</p>
<p>Then <span class="math display">\[
Pr(\texttt{default}=\texttt{No}|\texttt{balance}) = 1 - p(\texttt{balance}),
\]</span></p>
<p>One might predict the event <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> for any individual with a <code>balance</code>-value such that<br><span class="math display">\[
p(\texttt{balance}) &gt; 0.5.
\]</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>However, <span class="math inline">\(0.5\)</span> this is not the only reasonable classification threshold!</p>
<p>For instance, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as <span class="math display">\[
Pr(\texttt{default}=\texttt{Yes}|\texttt{balance})=p(\texttt{balance}) &gt; 0.1.
\]</span></p>
</div>
</div>
<section id="the-simple-p1-logistic-model" class="level3" data-number="5.6.1"><h3 data-number="5.6.1" class="anchored" data-anchor-id="the-simple-p1-logistic-model">
<span class="header-section-number">5.6.1</span> The Simple <span class="math inline">\((p=1)\)</span> Logistic Model</h3>
<p>For a binary coded dependen variable <span class="math inline">\(Y\in\{0,1\}\)</span> we aim to model the relationship between <span class="math display">\[
p(X)=Pr(Y=1|X)\quad\text{and}\quad X.
\]</span></p>
<p>As discussed above, a linear regression model, e.g., <span class="math display">\[
p(X)=\beta_0+\beta_1 X
\]</span> can produce nonsense predictions <span class="math inline">\(p(X)\not\in[0,1]0;\)</span> see the left panel of Figure 4.2.</p>
<p>Logistic regression avoids this problem <strong>by assuming</strong> that <span class="math inline">\(p(X)\)</span> can be modeled using the <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a>, <span class="math display">\[
p(X)=\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}.
\]</span> To fit the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> we use an estimation method called <strong>maximum likelihood</strong>.</p>
<p>The right panel of Figure 4.2 illustrates the fit of the logistic regression model to the <code>Default</code> data.</p>
<p>Note that <span class="math display">\[
\begin{align*}
p(X) &amp; = \frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}
%\frac{p(X)}{1-p(X)} &amp; = \frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{1-p(X)} \\
%\frac{p(X)}{1-p(X)} &amp; = \frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{1-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
%\frac{p(X)}{1-p(X)} &amp; = \frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{\frac{1+e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
\quad \Leftrightarrow\quad  \frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1 X}
\end{align*}
\]</span></p>
<p>The quantity <span class="math display">\[
0\leq \frac{p(X)}{1 − p(X)} \leq \infty
\]</span> is called the <strong>odds</strong>, and can take any value between 0 and plus infinity.</p>
<ul>
<li>A small odds value (close to zero) indicates a low probability of default.</li>
<li>A large odds value indicates a high probability of default.</li>
</ul>
<p>For instance:</p>
<ul>
<li><p>An odds value of <span class="math inline">\(\frac{1}{4}\)</span> means that <span class="math inline">\(0.2=20\%\)</span> of the people will default <span class="math display">\[
\frac{0.2}{1-0.2}=\frac{1}{4}
\]</span></p></li>
<li><p>An odds value of <span class="math inline">\(9\)</span> means that <span class="math inline">\(0.9=90\%\)</span> of the people will default <span class="math display">\[
\frac{0.9}{1-0.9}=9
\]</span></p></li>
</ul>
<p>By taking the logarithm, we arrive at <strong>log odds</strong> or <strong>logit</strong> <span class="math display">\[
\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0+\beta_1 X
\]</span></p>
<p>Thus increasing <span class="math inline">\(X\)</span> by one unit …</p>
<ul>
<li>… changes the <em>log odds</em> by <span class="math inline">\(\beta_1\)</span>
</li>
<li>… multiplies the odds by <span class="math inline">\(e^{\beta_1}\)</span>
</li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>The amount that <span class="math inline">\(p(X)\)</span> changes due to a one-unit change in <span class="math inline">\(X\)</span> depends on the current value of <span class="math inline">\(X.\)</span> That is, the simple logistic regression model is not linear in the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1.\)</span></p>
<p>But regardless of the value of <span class="math inline">\(X\)</span>, if <span class="math inline">\(\beta_1\)</span> is positive then increasing <span class="math inline">\(X\)</span> will be associated with increasing <span class="math inline">\(p(X)\)</span>, and if <span class="math inline">\(\beta_1\)</span> is negative then increasing <span class="math inline">\(X\)</span> will be associated with decreasing <span class="math inline">\(p(X).\)</span></p>
</div>
</div>
</section><section id="estimating-the-regression-coefficients" class="level3" data-number="5.6.2"><h3 data-number="5.6.2" class="anchored" data-anchor-id="estimating-the-regression-coefficients">
<span class="header-section-number">5.6.2</span> Estimating the Regression Coefficients</h3>
<p>In logistic regression analysis, the unknown model parameters are estimated using <em>maximum likelihood</em>.</p>
<p><strong>Basic intuition:</strong> Find estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> for each person <span class="math inline">\(i\)</span> corresponds as close as possible to its observed <code>default</code> status. I.e., choose <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> such that</p>
<ul>
<li>
<span class="math inline">\(\hat{p}(x_i)\approx 1\)</span> if <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> for person <span class="math inline">\(i\)</span>
</li>
<li>
<span class="math inline">\(\hat{p}(x_i)\approx 0\)</span> if <code>default</code><span class="math inline">\(=\)</span><code>No</code> for person <span class="math inline">\(i\)</span>
</li>
</ul>
<p>simultaneously for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p>This intuition can be formalized using the so-called <strong>likelihood function</strong>: <span class="math display">\[\begin{align*}
\ell(b_0,b_1)
&amp;=\prod_{i:y_{i}=1} p(x_i)\prod_{i:y_{i}=0} (1-p(x_i))\\[2ex]
&amp;=
\prod_{i:y_{i}=1} \frac{e^{b_0+b_1 x_i}}{1+e^{b_0+b_1 x_i}}
\prod_{i:y_{i}=0}\left(1-\frac{e^{b_0+b_1 x_i}}{1+e^{b_0+b_1 x_i}}\right)\\[2ex]
&amp;=
\prod_{i=1}^n
\left(\frac{e^{b_0+b_1 x_i}}{1+e^{b_0+b_1 x_i}}\right)^{y_i}
\left(1-\frac{e^{b_0+b_1 x_i}}{1+e^{b_0+b_1 x_i}}\right)^{1-y_i}\\[2ex]
\end{align*}\]</span> The estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are chosen to <em>maximize</em> this likelihood function, i.e. <span class="math display">\[
(\hat\beta_0,\hat\beta_1)=\arg\max_{(b_0,b_1)\in\mathbb{R}^2}\ell(b_0,b_1)
\]</span></p>
<p>Maximum likelihood is a very general estimation method that allows to estimate also non-linear models (like the logistic regression model).</p>
<p>Table 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the <code>Default</code> data in order to predict the probability of <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> using <code>balance</code> as the only predictor. <img src="images/Tab_4_1.png" class="img-fluid"></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>We see that <span class="math inline">\(\hat\beta_1=0.0055\)</span>; this indicates that an increase in <code>balance</code> is associated with an increase in the probability of default.
<ul>
<li>To be precise, a one-unit increase in <code>balance</code> is associated with an increase in the log odds of default by 0.0055 units.</li>
</ul>
</li>
<li>The <span class="math inline">\(z\)</span>-statistic in Table 4.1 plays the same role as the <span class="math inline">\(t\)</span>-statistic in the linear regression output: a large (absolute) value of the <span class="math inline">\(z\)</span>-statistic indicates evidence against the null hypothesis <span class="math inline">\(H_0: \beta_1= 0.\)</span>
</li>
</ul></section><section id="making-predictions" class="level3" data-number="5.6.3"><h3 data-number="5.6.3" class="anchored" data-anchor-id="making-predictions">
<span class="header-section-number">5.6.3</span> Making Predictions</h3>
<p>Once the coefficients have been estimated, we can compute the probability of <code>default</code><span class="math inline">\(=1\)</span> for any given value of <code>balance</code>.</p>
<p>For example, using the coefficient estimates given in Table 4.1, we predict that the default probability for an individual with a <code>balance</code>-value of 1,000 [USD] is <span class="math display">\[
\begin{align*}
\hat{p}(\texttt{balance}=1000)
&amp;=Pr(\texttt{default}=\texttt{Yes}|\texttt{balance}=1000)\\[2ex]
%&amp;=\frac{e^{\hat\beta_0+\hat\beta_1 \texttt{balance}}}{1+e^{\hat\beta_0+\hat\beta_1 \texttt{balance}}}\\[2ex]
&amp;=\frac{e^{-10.6513+ 0.0055\times 1000}}{1+e^{-10.6513+ 0.0055\times 1000}}\\[2ex]
&amp;= 0.00576 &lt; 1\%
\end{align*}
\]</span></p>
<p>By contrast, the default probability for an individual with a <code>balance</code>-value of 2,000 [USD] is <span class="math display">\[
\begin{align*}
\hat{p}(\texttt{balance}=2000)
&amp;=Pr(\texttt{default}=\texttt{Yes}|\texttt{balance}=2000)\\[2ex]
%&amp;=\frac{e^{\hat\beta_0+\hat\beta_1 \texttt{balance}}}{1+e^{\hat\beta_0+\hat\beta_1 \texttt{balance}}}\\[2ex]
&amp;=\frac{e^{-10.6513+ 0.0055\times 2000}}{1+e^{-10.6513+ 0.0055\times 2000}}\\[2ex]
&amp; = 0.586
\end{align*}
\]</span> and is thus much higher.</p>
<section id="qualitative-predictors" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="qualitative-predictors">Qualitative Predictors:</h4>
<p><img src="images/Tab_4_2.png" class="img-fluid"></p>
<p><span class="math display">\[\begin{align*}
&amp;Pr(\texttt{default}=\texttt{Yes}|\texttt{student}=\texttt{Yes})\\[2ex]
&amp;=\frac{e^{-3.5041+0.4049\times 1}}{1+e^{-3.5041+0.4049\times 1}}\\[2ex]
&amp;= 0.0431\\[4ex]
&amp;Pr(\texttt{default}=\texttt{Yes}|\texttt{student}=\texttt{No})\\[2ex]
&amp;=\frac{e^{-3.5041+0.4049\times 0}}{1+e^{-3.5041+0.4049\times 0}}\\[2ex]
&amp;= 0.0292\\
\end{align*}\]</span></p>
<p>This may indicate that students tend to have higher default probabilities than non-students. But we may <strong>missed further factors</strong> here since we only use one predictor variable. Thus, this result may be driven by an <strong>omitted-variable bias</strong>.</p>
<p>To control for other confounding predictors, we need to use multiple logistic regression.</p>
</section></section><section id="multiple-logistic-regression" class="level3" data-number="5.6.4"><h3 data-number="5.6.4" class="anchored" data-anchor-id="multiple-logistic-regression">
<span class="header-section-number">5.6.4</span> Multiple Logistic Regression</h3>
<p>By analogy with the extension from simple to multiple linear, we can generalize the (simple) logistic regression model as following <span class="math display">\[
\begin{align*}
p(X)
&amp;=\frac{e^{\beta_0+\beta_1 X_1+\dots+\beta_p X_p}}{1+e^{\beta_0+\beta_1 X_1+\dots+\beta_p X_p}}\\[2ex]
\log\left(\frac{p(X)}{1-p(X)}\right)
&amp;= \beta_0+\beta_1 X_1+\dots+\beta_p X_p,\\
\end{align*}
\]</span> where <span class="math inline">\(X=(X_1,\dots,X_p)\)</span>, and where the unknown parameters <span class="math inline">\(\beta_0,\dots,\beta_p\)</span> are estimated by maximum likelihood.</p>
<p><img src="images/Tab_4_3.png" class="img-fluid"></p>
<p>Interestingly, the effect of the dummy variable <code>student[Yes]</code> is now <em>negative</em>, in contrast to the estimation results of the (simple) logistic regression in Table 4.2 where it was <em>positive</em>.</p>
<p>The negative coefficient for student in the multiple logistic regression indicates that for fixed values of balance and income, a student is less likely to default than a non-student.</p>
<!-- *  The variables `student` and `balance` are correlated. -->
<p>The left panel of Figure 4.3 provides a graphical illustration of this apparent paradox:</p>
<ul>
<li>Without considering <code>balance</code>, the (overall) default rates of students are <em>higher</em> than those of non-students (horizontal broken lines). This overall effect was shown in Table 4.2.</li>
<li>However, for given <code>balance</code>-values, the default rate for students is <em>lower</em> than for non-students (solid lines).</li>
</ul>
<p><img src="images/Fig_4_3.png" class="img-fluid"></p>
<p>The right panel of Figure 4.3 provides an explanation for this discrepancy: Students tend to hold higher levels of debt, which is in turn associated with higher probability of default.</p>
<p><strong>Summary:</strong></p>
<ul>
<li>An individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance</li>
<li>However, overall, students tend to default at a higher rate than non-students since, overall, they tend to have higher credit card balances.</li>
</ul>
<p>In other words: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, a student is less risky than a non-student if both have the same credit card balance.</p>
</section><section id="sec-multinomialLogistic" class="level3" data-number="5.6.5"><h3 data-number="5.6.5" class="anchored" data-anchor-id="sec-multinomialLogistic">
<span class="header-section-number">5.6.5</span> Multinomial Logistic Regression</h3>
<p>It is possible to extend the two-class logistic regression approach to the setting of <span class="math inline">\(K &gt; 2\)</span> classes. This extension is sometimes known as <em>multinomial logistic regression</em>.</p>
<p>To do this, we first select a single class to serve as the baseline; without loss of generality, we select the <span class="math inline">\(K\)</span>th class for this role.</p>
<p>We model the probabilities that <span class="math inline">\(Y=k\)</span>, for <span class="math inline">\(k=1,\dots,K\)</span>, using <span class="math inline">\(k\)</span>-specific parameters <span class="math inline">\(\beta_{k0},\dots,\beta_{kp}\)</span> with <span class="math display">\[
p_k(x)=Pr(Y=k|X=x)=\frac{e^{\beta_{k0} + \beta_{k1} x_1 + \dots +  \beta_{kp} x_p}}{1+\sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} x_1 + \dots +  \beta_{lp} x_l}}
\]</span> for <span class="math inline">\(k=1,\dots,K-1\)</span>, and <span class="math display">\[
p_K(x)=Pr(Y=K|X=x)=\frac{1}{1+\sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} x_1 + \dots +  \beta_{lp} x_l}}
\]</span></p>
<p>For <span class="math inline">\(k=1,\dots,K-1\)</span> it holds that <span class="math display">\[
\log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)=\beta_{k0} + \beta_{k1} x_1 + \dots +  \beta_{kp} x_p
\]</span> which is the counterpart to the log odds equation for <span class="math inline">\(K=2.\)</span></p>
<p>Note that:</p>
<ul>
<li><p>The predictions <span class="math inline">\(\hat{p}_k(x)\)</span>, for <span class="math inline">\(k=1,\dots,K\)</span> do not depend on the choice of the baseline class.</p></li>
<li><p>However, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.</p></li>
</ul></section></section><section id="discriminant-analysis-generative-models-for-classification" class="level2" data-number="5.7"><h2 data-number="5.7" class="anchored" data-anchor-id="discriminant-analysis-generative-models-for-classification">
<span class="header-section-number">5.7</span> Discriminant Analysis: Generative Models for Classification</h2>
<p>We now consider an alternative and less direct approach to estimating the probabilities <span class="math display">\[
Pr(Y=K|X=x).
\]</span></p>
<p>Suppose that we wish to classify an observation into one of <span class="math inline">\(K\geq 2\)</span> classes.</p>
<p><strong>Prior probability:</strong> Let <span class="math display">\[
\pi_k=Pr(Y=k)
\]</span> represent the overall <em>prior probability</em> that a randomly chosen observation comes from class <span class="math inline">\(k.\)</span> We have that <span class="math display">\[
\pi_1+\dots+\pi_K=1.
\]</span></p>
<p><strong>Conditional Density function of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=k:\)</span></strong> Let <span class="math display">\[
f_k(x)=Pr(X=x|Y=k)
\]</span> denote the (conditional) density of <span class="math inline">\(X\)</span> for an observation that comes from class <span class="math inline">\(k.\)</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Technically, this definition is only correct if <span class="math inline">\(X\)</span> is a <em>qualitative</em> random variable. If <span class="math inline">\(X\)</span> is quantitative, then <span class="math inline">\(f_k(x)dx\)</span> corresponds to the probability of <span class="math inline">\(X\)</span> falling in a small region <span class="math inline">\(dx\)</span> around <span class="math inline">\(x.\)</span></p>
</div>
</div>
<p><strong>Posterior probability:</strong> Then <em>Bayes’ theorem</em> states that the probability that a observation with predictor values <span class="math inline">\(X=x\)</span> comes from class <span class="math inline">\(k\)</span> (i.e.&nbsp;the posterior probability), is given by <span id="eq-postprob1"><span class="math display">\[
\begin{align*}
p_k(x)
&amp; = Pr(Y=k|X=x)\\
&amp; = \frac{Pr(Y=k\;\text{ and }\;X=x)}{Pr(X=x)}\\
&amp; = \frac{Pr(Y=k)\,Pr(X=x|Y=k)}{Pr(X=x)}\\
&amp; =\frac{\pi_k f_k(x)}{\sum_{l=1}^K\pi_l f_l(x)},
\end{align*}
\qquad(5.5)\]</span></span> where <span class="math display">\[
\begin{align*}
Pr(X = x)
&amp;= \sum_{l=1}^K \underbrace{Pr(Y = l)}_{\pi_l} \;\underbrace{Pr(X_x|Y=l)}_{f_l(x)} \\
&amp;= \sum_{l=1}^K \pi_l f_l(x)
\end{align*}
\]</span></p>
<p>While logistic regression aims at estimating the posterior probability <span class="math inline">\(p_k(x)\)</span> directly, Bayes’s theorem gives us a way to estimate <span class="math inline">\(p_k(x)\)</span> <em>indirectly</em> simply by plugging in estimates of <span class="math inline">\(\pi_k\)</span> and <span class="math inline">\(f_k(x)\)</span>.</p>
<p>We know from <a href="Ch2_StatLearning.html" class="quarto-xref"><span>Chapter 2</span></a> that the <strong>Bayes classifier</strong>, which classifies an observation <span class="math inline">\(x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math inline">\(p_k(x)\)</span> is largest, has the lowest possible error rate out of all classifiers. Therefore, <a href="#eq-postprob1" class="quarto-xref">Equation&nbsp;<span>5.5</span></a> will lead to the best classifier.</p>
<p>However, estimating the densities <span class="math inline">\(f_k(x)\)</span> can be very challenging and we therefore have to make some simplifying assumptions; namely, assuming that the data is normal distributed.</p>
<p>In the following sections, we discuss three classifiers that use different estimates of <span class="math inline">\(f_k(x)\)</span> to approximate the Bayes classifier:</p>
<ul>
<li>linear discriminant analysis</li>
<li>quadratic discriminant analysis</li>
<li>Naive Bayes</li>
</ul>
<section id="linear-discriminant-analysis-for-p-1" class="level3" data-number="5.7.1"><h3 data-number="5.7.1" class="anchored" data-anchor-id="linear-discriminant-analysis-for-p-1">
<span class="header-section-number">5.7.1</span> Linear Discriminant Analysis for <span class="math inline">\(p = 1\)</span>
</h3>
<p>For the beginning, let us assume that we have only one predictor, i.e., <span class="math inline">\(p=1.\)</span></p>
<p>To estimate <span class="math inline">\(f_k(x)\)</span>, we will assume that <span class="math inline">\(f_k\)</span> is <em>normal</em> (or <em>Gaussian</em>). In the simple <span class="math inline">\(p=1\)</span> dimensional setting, the normal distribution is <span class="math display">\[
f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2\right),
\]</span> where</p>
<ul>
<li>
<span class="math inline">\(\mu_k\)</span> is the mean of the <span class="math inline">\(k\)</span>th class and</li>
<li>
<span class="math inline">\(\sigma_k^2\)</span> is the variance of the <span class="math inline">\(k\)</span>th class</li>
<li>
<span class="math inline">\(\pi\approx 3.14159\)</span> is the mathematical constant <span class="math inline">\(\pi\)</span>. (Do not confuse it with the prior probabilities <span class="math inline">\(\pi_k.\)</span>)</li>
</ul>
<p>For now, let us further assume the simplifying case of equal variances across all classes, i.e.&nbsp; <span class="math display">\[
\sigma_1^2=\dots = \sigma_K^2\equiv \sigma^2.
\]</span> Plugging this assumed version of <span class="math inline">\(f_k\)</span> into <a href="#eq-postprob1" class="quarto-xref">Equation&nbsp;<span>5.5</span></a>, leads to <span id="eq-LDp1"><span class="math display">\[
p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(x-\mu_k)^2\right)}{\sum_{l=1}^K\pi_l \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(x-\mu_l)^2\right)}.
\qquad(5.6)\]</span></span></p>
<p>The Bayes classifier involves assigning an observation <span class="math inline">\(X = x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math inline">\(p_k(x)\)</span> is largest. Taking the <span class="math inline">\(\log\)</span> of <a href="#eq-LDp1" class="quarto-xref">Equation&nbsp;<span>5.6</span></a> (i.e.&nbsp;a monotonic transformation) and rearranging terms shows that this is equivalent to assigning an observation <span class="math inline">\(X=x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math display">\[
\delta_k(x)=x\cdot\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
\]</span> is largest. The function <span class="math inline">\(\delta_k(x)\)</span> is called the <strong>discriminant function</strong>.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Derivation of the discriminant function
</div>
</div>
<div class="callout-body-container callout-body">
<p>To see this, observe that the largest <span class="math inline">\(p_k(x)\)</span>-value across all <span class="math inline">\(k=1,\dots,K\)</span> is determined by the larges numerator in <a href="#eq-LDp1" class="quarto-xref">Equation&nbsp;<span>5.6</span></a>, since the denominator does not change with <span class="math inline">\(k.\)</span> Thus, we only need to look at the numerator.</p>
<p>Moreover, if a certain value <span class="math inline">\(k=1,\dots,K\)</span> maximizes <span class="math inline">\(p_k(x),\)</span> then the same <span class="math inline">\(k\)</span> maximizes also <span class="math inline">\(\log(p_k(x))\)</span> since the <span class="math inline">\(\log\)</span>-function is a strictly monotonic increasing formation.</p>
<p>Thus we can simply look at the logarithm of the numerator: <span class="math display">\[\begin{align*}
&amp;\log\left(\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(x-\mu_k)^2\right)\right)\\[2ex]
=&amp;\log(\pi_k) + \log(\frac{1}{\sqrt{2\pi}\sigma}) + \left(-\frac{1}{2\sigma^2}(x-\mu_k)^2\right)\\[2ex]
=&amp;\log(\pi_k) + \log(\frac{1}{\sqrt{2\pi}\sigma}) + \left(-\frac{1}{2\sigma^2}(x^2 - 2x\mu_k +\mu_k^2)\right)\\[2ex]
=&amp;\log(\pi_k) + \underbrace{\log(\frac{1}{\sqrt{2\pi}\sigma}) -\frac{x^2}{2\sigma^2}}_{\text{not depend on $k$}} + \frac{2x\mu_k}{2\sigma^2} -\frac{\mu_k^2}{2\sigma^2}
\end{align*}\]</span></p>
<p>Removing all irrelevant terms that anyways do not depend on <span class="math inline">\(k\)</span> leads to the discriminant function <span class="math inline">\(\delta_k(x).\)</span></p>
</div>
</div>
<p><strong>Example:</strong></p>
<p>In the case of only two classes, <span class="math inline">\(K=2\)</span>, with equal a priori probabilities <span class="math display">\[
\pi_1=\pi_2\equiv \pi^*,
\]</span> the Bayes classifier assigns an observation <span class="math inline">\(X=x\)</span> to class <span class="math inline">\(1\)</span> if <span class="math display">\[
\begin{align*}
\delta_1(x) &amp; &gt; \delta_2(x)\\
%%%
x\cdot\frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2\sigma^2} + \log(\pi^*)
&amp; &gt;
x\cdot\frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2} + \log(\pi^*)\\
%%%
x\cdot\frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2\sigma^2}  
&amp; &gt;
x\cdot\frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2}\\
%%%
x\cdot\mu_1 - \frac{\mu_1^2}{2}  
&amp; &gt;
x\cdot\mu_2 - \frac{\mu_2^2}{2}\\
%%%
2x\cdot(\mu_1-\mu_2)   
&amp; &gt;
\mu_1^2 - \mu_2^2\\
%%%
x   
&amp; &gt; \frac{\mu_1^2 - \mu_2^2}{2(\mu_1-\mu_2)}
\end{align*}
\]</span> The Bayes <strong>decision boundary</strong> is the point for which <span class="math inline">\(\delta_1(x)=\delta_2(x);\)</span> i.e., <span class="math display">\[
\begin{align*}
x   
=\frac{\mu_1^2 - \mu_2^2}{2(\mu_1-\mu_2)}
&amp;=\frac{(\mu_1 - \mu_2)(\mu_1 + \mu_2)}{2(\mu_1-\mu_2)}\\
&amp;=\frac{(\mu_1 + \mu_2)}{2}
\end{align*}
\]</span> Figure 4.4 shows a specific example with</p>
<ul>
<li><span class="math inline">\(\pi_1=\pi_2\)</span></li>
<li>
<span class="math inline">\(\mu_1=-1.25\)</span> and <span class="math inline">\(\mu_2=1.25\)</span> and</li>
<li><span class="math inline">\(\sigma_1=\sigma_2\equiv\sigma =1.\)</span></li>
</ul>
<p>The two densities <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> overlap such that for given <span class="math inline">\(X=x\)</span> there is some uncertainty about the class to which the observation belongs to. In this example, the Bayes classifier assigns an observation <span class="math inline">\(X=x\)</span> …</p>
<ul>
<li>… to class 1 if <span class="math inline">\(x&lt;0\)</span>
</li>
<li>… to class 2 if <span class="math inline">\(x&gt;0\)</span>
</li>
</ul>
<p><img src="images/Fig_4_4.png" class="img-fluid"></p>
<p>In the above example (Figure 4.4) we know all parameters <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(k=1,\dots,K\)</span>, and <span class="math inline">\(\sigma\)</span>. In practice, however, these parameters are usually unknown, and thus need to be estimated from the data.</p>
<p><strong>Summary of LDA for <span class="math inline">\(p=1\)</span>:</strong></p>
<p>The <strong>Linear Discriminant Analysis (LDA)</strong> method approximates the Bayes classifier by using the normality assumption and by plugging in estimates for the unknown parameters <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(k=1,\dots,K\)</span>, and <span class="math inline">\(\sigma\)</span>. The following estimates are used: <span class="math display">\[
\begin{align*}
\hat\pi_k    &amp; = \frac{n_k}{n}\\
\hat\mu_k    &amp; = \frac{1}{n_k}\sum_{i:y_i=k}x_i\\
\hat{\sigma}^2 &amp; = \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat\mu_k)^2,
\end{align*}
\]</span> where the variance estimate <span class="math inline">\(\hat\sigma\)</span> pools the predictor <span class="math inline">\(x_i\)</span> values across all groups <span class="math inline">\(k=1,\dots,K.\)</span></p>
<p>The LDA classifier assigns an observation <span class="math inline">\(X=x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math display">\[
\hat\delta_k(x)=x\cdot\frac{\hat\mu_k}{\hat\sigma^2} - \frac{\hat\mu_k^2}{2\hat\sigma^2} + \log(\hat\pi_k)
\]</span> is largest.</p>
<p>LDA is named <strong>linear</strong> since the discriminant functions <span class="math inline">\(\hat\delta_k(x)\)</span>, <span class="math inline">\(k=1,\dots,K\)</span> are linear functions of <span class="math inline">\(x.\)</span></p>
<p>Assumptions made by the LDA method:</p>
<ul>
<li>Observation for each class come from a normal distribution</li>
<li>The class-specific normal distributions have class-specific means, <span class="math inline">\(\mu_k\)</span>, but <strong>equal variances</strong> <span class="math inline">\(\sigma\)</span>
</li>
</ul></section><section id="linear-discriminant-analysis-for-p-1-1" class="level3" data-number="5.7.2"><h3 data-number="5.7.2" class="anchored" data-anchor-id="linear-discriminant-analysis-for-p-1-1">
<span class="header-section-number">5.7.2</span> Linear Discriminant Analysis for <span class="math inline">\(p &gt; 1\)</span>
</h3>
<p>In the case of multiple predictors <span class="math display">\[
X=(X_1,\dots,X_p)\quad \text{with}\quad p&gt;1
\]</span> we need to assume that the observations for each class <span class="math inline">\(k\)</span> come from a <strong>multivariate Gaussian distribution</strong> with</p>
<ul>
<li>Class-specific <span class="math inline">\((p\times 1)\)</span> mean vectors <span class="math inline">\(\mu_k\)</span>
</li>
<li>Equal <span class="math inline">\((p\times p)\)</span> covariance matrix <span class="math inline">\(\Sigma\)</span> for all classes <span class="math inline">\(k=1,\dots,K\)</span>
</li>
</ul>
<p>That is, a <span class="math inline">\((p\times 1)\)</span> random vector <span class="math inline">\(X\)</span> of class <span class="math inline">\(k\)</span> is distributed as <span class="math display">\[
\begin{align*}
X
&amp; \sim \mathcal{N}_p\left(\mu_k,\Sigma\right)%\\
%&amp; \sim \mathcal{N}_p\left(
%  \left(\begin{matrix}
%  \mu_{k1}\\
%  \mu_{k2}\\
%  \vdots\\
%  \mu_{kp}
%  \end{matrix}\right),
%  \left(\begin{matrix}
%  Var(X_1)    &amp;Cov(X_1,X_2)&amp;\dots  &amp; Cov(X_1,X_p)\\
%  Cov(X_2,X_1)&amp;Var(X_2)    &amp;\dots  &amp; Cov(X_2,X_p)\\
%  \vdots      &amp;\ddots      &amp;\ddots &amp; \vdots      \\
%  Cov(X_p,X_1)&amp;Cov(X_p,X_2)&amp;\dots  &amp; Var(X_p)
%  \end{matrix}\right)
%  \right)
\end{align*}
\]</span> with class-specific <span class="math inline">\((p\times 1)\)</span> mean vector <span class="math inline">\(\mu_k\)</span>, <span class="math display">\[
\mu_k=E(X|Y=k)=\left(\begin{matrix}
  \mu_{k1}\\
  \mu_{k2}\\
  \vdots\\
  \mu_{kp}
  \end{matrix}\right),
\]</span> and <span class="math inline">\((p\times p)\)</span> covariance matrix, <span class="math inline">\(\Sigma,\)</span> that is common for all classes <span class="math inline">\(k=1,\dots,K,\)</span> i.e. <span class="math display">\[
\Sigma=Cov(X|Y=1)=\dots=Cov(X|Y=K)=Cov(X),
\]</span> where <span class="math display">\[
Cov(X)=
\left(
  \begin{matrix}
  Var(X_1)    &amp;Cov(X_1,X_2)&amp;\dots  &amp; Cov(X_1,X_p)\\
  Cov(X_2,X_1)&amp;Var(X_2)    &amp;\dots  &amp; Cov(X_2,X_p)\\
  \vdots      &amp;\vdots      &amp;\ddots &amp; \vdots      \\
  Cov(X_p,X_1)&amp;Cov(X_p,X_2)&amp;\dots  &amp; Var(X_p)
  \end{matrix}
\right)
\]</span></p>
<p>The multivariate Gaussian density for class <span class="math inline">\(k=1,\dots,K\)</span> is defined as <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)\right),
\]</span> where <span class="math inline">\(|\Sigma|\)</span> denotes the determinant of the <span class="math inline">\((p\times p)\)</span> matrix <span class="math inline">\(\Sigma.\)</span></p>
<p>Figure 4.5 shows the case of bivariate (<span class="math inline">\(p=2\)</span>) Gaussian distribution predictors.</p>
<p><img src="images/Fig_4_5.png" class="img-fluid"></p>
<p>Under the multivariate normality assumption (with common covariance matrix and group-specific mean vectors) the Bayes classifier assigns an multivariate observation <span class="math inline">\(X=x\)</span> to the class for which the discriminant function <span id="eq-LDA_mult"><span class="math display">\[
\delta_k(x)=x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log(\pi_k)
\qquad(5.7)\]</span></span> is largest.</p>
<p>An example for <span class="math inline">\(p=2\)</span> and <span class="math inline">\(K=3\)</span> with three equally sized (i.e.&nbsp;<span class="math inline">\(\pi_1=\pi_2=\pi_3\)</span>) Gaussian classes is shown in the left-hand panel of Figure 4.6. The three ellipses represent regions that contain <span class="math inline">\(95\%\)</span> of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which <span class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, <span class="math inline">\(k\neq l\)</span>.</p>
<p><img src="images/Fig_4_6.png" class="img-fluid"></p>
<p>Again, the unknown parameters <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\pi_k\)</span>, and <span class="math inline">\(\Sigma\)</span> must be estimated from the data with formulas similar to those for the <span class="math inline">\(p=1\)</span> case.</p>
<section id="the-default-data-example" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="the-default-data-example">The <code>Default</code> data example</h4>
<p>We can perform LDA on the <code>Default</code> data in order to predict whether or not an individual will default on the basis of</p>
<ul>
<li>credit card balance (real variable) and</li>
<li>student status (qualitative variable).</li>
</ul>
<p><strong>Note:</strong> Student status is qualitative, and thus the normality assumption made by LDA is clearly violated in this example! However, LDA is often remarkably robust to model violations, as this example (apparently) shows.</p>
<p>The LDA model fit to the <span class="math inline">\(10,000\)</span> training samples results in an overall training error rate of <span class="math inline">\(2.75\%:\)</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># MASS package contains the lda() function</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span><span class="op">)</span> </span>
<span></span>
<span><span class="co">## Sample size </span></span>
<span><span class="va">n</span>              <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Default</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lda_obj</span>        <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">default</span> <span class="op">~</span> <span class="va">balance</span> <span class="op">+</span> <span class="va">student</span>, </span>
<span>                      data <span class="op">=</span> <span class="va">Default</span><span class="op">)</span></span>
<span><span class="va">ldaPredict_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lda_obj</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Training error rate:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span> <span class="op">!=</span> <span class="va">ldaPredict_obj</span><span class="op">$</span><span class="va">class</span><span class="op">)</span><span class="op">/</span><span class="va">n</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0275</code></pre>
</div>
</div>
<p>This sounds like a low error rate, but two caveats must be noted:</p>
<ul>
<li>Training error rates are usually smaller than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a <strong>new set of individuals</strong> will default.
<ul>
<li>The reason is that we specifically adjust the parameters of our model to do well on the training data. Thus, we may <strong>overfit</strong> the training data.</li>
<li>Generally, the higher the ratio of parameters <span class="math inline">\(p\)</span> to the number of samples <span class="math inline">\(n\)</span>, the more we expect <em>overfitting</em> to play a role. However, for these data we don’t expect this to be a problem, since <span class="math inline">\(p = 2\)</span> and <span class="math inline">\(n = 10,000.\)</span>
</li>
</ul>
</li>
<li>Since only <span class="math inline">\(3.33\%\)</span> of the individuals in the training sample defaulted, a simple naive classifier that always predicts “not default” for every person, regardless of his or her credit card balance and student status, will result in an error rate of <span class="math inline">\(3.33\%.\)</span> In other words, the trivial <strong>null classifier</strong> will achieve an error rate that is only a bit higher than the LDA training set error rate.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Training error rate of the "null classifier":</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span> <span class="op">!=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"No"</span>, <span class="va">n</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="va">n</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0333</code></pre>
</div>
</div>
</section></section><section id="classification-errors-of-binary-classifiers" class="level3" data-number="5.7.3"><h3 data-number="5.7.3" class="anchored" data-anchor-id="classification-errors-of-binary-classifiers">
<span class="header-section-number">5.7.3</span> Classification Errors of Binary Classifiers</h3>
<p>Any binary classifier can make two types of errors:</p>
<ul>
<li>False Negatives (FN): Predicting “no default” for a person who actually defaults</li>
<li>False Positives (FP): Predicing “default” for a person who actually does not default</li>
</ul>
<p>Depending on the context, both errors can have different application/context specific costs.</p>
<p>A <strong>confusion matrix</strong> shows both types of errors. The upper off-diagonal entry shows the FNs; the lower off-diagonal entry shows the FPs. A perfect confusion matrix has zeros in the off-diagonal entries.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Confusion matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">ldaPredict_obj</span><span class="op">$</span><span class="va">class</span>, <span class="va">Default</span><span class="op">$</span><span class="va">default</span>, </span>
<span>      dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Predicted default status"</span>, </span>
<span>              <span class="st">"True default status"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        True default status
Predicted default status   No  Yes
                     No  9644  252
                     Yes   23   81</code></pre>
</div>
</div>
<p><img src="images/Tab_4_4.png" class="img-fluid"></p>
<p>Numbers of True/False Positive Predictions:</p>
<ul>
<li>LDA predicts that <span class="math inline">\(P^*=104\)</span> people default, where <span class="math inline">\(P^*\)</span> denotes the number of <strong>Predicted Positives</strong>.
<ul>
<li>
<span class="math inline">\(TP=81\)</span> of <span class="math inline">\(P^*=104\)</span> actually default(<span class="math inline">\(TP=\)</span><strong>Number of True Positives</strong>).</li>
<li>
<span class="math inline">\(FP=23\)</span> of <span class="math inline">\(P^*=104\)</span> actually do not default(<span class="math inline">\(FP=\)</span><strong>Number of False Positives</strong>).</li>
</ul>
</li>
</ul>
<p>Numbers of True/False Negative Predictions:</p>
<ul>
<li>LDA predicts that <span class="math inline">\(N^*=9896\)</span> people do not default, where <span class="math inline">\(N^*\)</span> denotes the number of <strong>Predicted Negatives</strong>.
<ul>
<li>
<span class="math inline">\(TN=9644\)</span> of <span class="math inline">\(N^*=9896\)</span> actually do not default (<span class="math inline">\(TN=\)</span><strong>Number of True Negatives</strong>)</li>
<li>
<span class="math inline">\(FN=252\)</span> of <span class="math inline">\(N^*=9896\)</span> actually default (<span class="math inline">\(FN=\)</span><strong>Number of FALSE Negatives</strong>)</li>
</ul>
</li>
</ul>
<p>True Positive/Negative Rates:</p>
<ul>
<li>True Positive Rate: <span class="math inline">\(TP/P=(81/333)\cdot 100\%\approx 24.3\%\)</span> <!-- True Positive Rate is also called *sensitivity* --> <!-- $81$ of the $333$ default individuals are correctly labeled by LDA.<br> -->
</li>
<li>True Negative Rate: <span class="math inline">\(TN/N=(9644/9667)\cdot 100\%\approx 99.8\%\)</span> <!-- $9644$ of the $9667$ no default individuals are correctly labeled by LDA.<br> -->
</li>
</ul>
<p>False Positive/Negative Rates:</p>
<ul>
<li>False Positive Rate: <span class="math inline">\(FP/N=(23/9667)\cdot 100\%\approx 0.2\%\)</span><br><!-- $23$ of the $9667$ no default individuals are incorrectly labeled.<br> --><!-- False Positive Rage equals one minus *specificity* -->
</li>
<li>False Negative Rate: <span class="math inline">\(FN/P=(252/333)\cdot 100\%\approx 75.7\%\)</span><br><!-- $252$ of the $333$ default individuals are in correctly labeled.<br>  -->
</li>
</ul>
<p>The small False Positive Rate of <span class="math inline">\(0.2\%\)</span> is good since this means that trustworthy “no default” people are rarely labeled as non-trustworthy “default” people.</p>
<p>However, the high False Negative Rate of <span class="math inline">\(75.7\%\)</span> is very problematic for a bank since this means that the bank often does not detect the non-trustworthy “default” people.</p>
<section id="choosing-the-classification-threshold" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="choosing-the-classification-threshold">Choosing the Classification Threshold</h4>
<p>If a “False Negative Event” costs the bank more than a “False Positive Event” (or vice versa), then the bank will want to adjust the classifying threshold of the classifier.</p>
<!-- In the two class cases, e.g. $\texttt{default}\in\{\texttt{No}, \texttt{Yes}\},$  -->
<p>The Bayes classifier, and thus also all classifiers which try to approximate the Bayes classifier, use a threshold of <span class="math inline">\(50\%\)</span>. I.e., a person <span class="math inline">\(i\)</span> with predictor value <span class="math inline">\(X=x_i\)</span> is assigned to the <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> class if <span class="math display">\[
Pr(\texttt{default}_i=\texttt{Yes}|X=x_i)&gt;0.5.
\]</span></p>
<p>However, if the costs of False Negatives (FN) are higher than the costs of False Positives (FP), one can consider lowering the classifying threshold. This decreases the predicted negatives (N<span class="math inline">\(^*\)</span>) and thus also decreases the False Negatives (FN) since N<span class="math inline">\(^*=\)</span>TN<span class="math inline">\(+\)</span>FN.</p>
<p>For instance, we might classify any person <span class="math inline">\(i\)</span> with a posterior probability of default above <span class="math inline">\(20\%,\)</span> <span class="math display">\[
Pr(\texttt{default}=\texttt{Yes}|X=x_i)&gt;0.2,
\]</span> into the <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> class.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Predicted posterior probabilities </span></span>
<span><span class="co">## head(ldaPredict_obj$posterior)</span></span>
<span></span>
<span><span class="co">## Container for the predicted classes</span></span>
<span><span class="va">PredictedDefault</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"No"</span>, <span class="va">n</span><span class="op">)</span></span>
<span><span class="co">## Fill in the "Yes" classifications </span></span>
<span><span class="va">PredictedDefault</span><span class="op">[</span><span class="va">ldaPredict_obj</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0.2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"Yes"</span></span>
<span></span>
<span><span class="co">## Confusion matrix for 0.2 threshold:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">PredictedDefault</span>, <span class="va">Default</span><span class="op">$</span><span class="va">default</span>, </span>
<span>      dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Predicted default status"</span>, </span>
<span>              <span class="st">"True default status"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        True default status
Predicted default status   No  Yes
                     No  9432  138
                     Yes  235  195</code></pre>
</div>
</div>
<p><img src="images/Tab_4_5.png" class="img-fluid"></p>
<p>Changing the threshold from <span class="math inline">\(0.5\)</span> to <span class="math inline">\(0.2\)</span> …</p>
<ul>
<li>… <strong>lowered</strong> the False Negative Rate (<span class="math inline">\(FN/P\)</span>) 😎
<ul>
<li>from <span class="math inline">\(252/333\approx 75.7\%\)</span>
</li>
<li>to <span class="math inline">\(138/333\approx 41.4\%\)</span><br>
</li>
</ul>
</li>
<li>… <strong>increased</strong> the False Positive Rate (<span class="math inline">\(FP/N\)</span>) 😕
<ul>
<li>from <span class="math inline">\(23/9667\approx 0.2\%\)</span>
</li>
<li>to <span class="math inline">\(235/9667\approx 2.4\%\)</span>
</li>
</ul>
</li>
<li>… <strong>increased</strong> the Overall Error Rate (<span class="math inline">\((FN+FP)/n\)</span>) 😕
<ul>
<li>from <span class="math inline">\((23 + 252)/10000\approx 2.75\%\)</span>
</li>
<li>to <span class="math inline">\((235 + 138)/10000\approx 3.73\%\)</span>
</li>
</ul>
</li>
</ul>
<p>For a credit card company, this slight increase in the overall error rate can be a small price to pay for a more accurate identification of people who indeed default.</p>
<!-- Such an adjustment of the threshold increases the overall error rate, and changes the False Positives and False Negatives. The latter can be advantageous in a cost-benefit analysis - even though the overall error rate is increased.   -->
</section><section id="classification-performance-measures" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="classification-performance-measures">Classification Performance Measures</h4>
<p>Table 4.6 and 4.7 summarize the different classification performance measures.</p>
<p><img src="images/Tab_4_6_and_4_7.png" class="img-fluid"></p>
</section></section><section id="the-roc-curve" class="level3" data-number="5.7.4"><h3 data-number="5.7.4" class="anchored" data-anchor-id="the-roc-curve">
<span class="header-section-number">5.7.4</span> The ROC curve</h3>
<p>The (Receiver Operating Characteristics) <strong>ROC curve</strong> is a popular graphic for displaying the <span class="math display">\[
\text{TP Rate}=\text{TP/P}\quad \text{(``sensitivity'' or ``power'')}
\]</span> against the <span class="math display">\[
\text{FP Rate}=\text{FP/N} \quad \text{(``}1-\text{specificity'' or ``type I error'')}
\]</span> for all possible threshold values (from zero to one).</p>
<p>The following <code>R</code>-codes provides a “by hand” implementation of a plot of the ROC curve for LDA applied to the <code>Default</code>-dataset.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Considered thresholds (exclude the trivial 0/1 values)</span></span>
<span><span class="va">thresholds</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">250</span><span class="op">)</span></span>
<span><span class="va">n_thresholds</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">thresholds</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Container for the predicted classes</span></span>
<span><span class="va">PredictedDefault_mat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="st">"No"</span>, <span class="va">n</span>, <span class="va">n_thresholds</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Fill in the "Yes" classifications </span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n_thresholds</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">PredicedYes</span>  <span class="op">&lt;-</span> <span class="va">ldaPredict_obj</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="va">thresholds</span><span class="op">[</span><span class="va">j</span><span class="op">]</span></span>
<span>  <span class="va">PredictedDefault_mat</span><span class="op">[</span><span class="va">PredicedYes</span>, <span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"Yes"</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">## Number of actual positives</span></span>
<span><span class="va">P</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span><span class="op">[</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span> <span class="op">==</span> <span class="st">"Yes"</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">## Number of actual negatives</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span><span class="op">[</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span> <span class="op">==</span> <span class="st">"No"</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="va">TP_Rate</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">n_thresholds</span><span class="op">)</span></span>
<span><span class="va">FP_Rate</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">n_thresholds</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n_thresholds</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="co">## Classification results among the actually positives</span></span>
<span>  <span class="va">Classifications_among_trueYes</span> <span class="op">&lt;-</span> <span class="va">PredictedDefault_mat</span><span class="op">[</span>,<span class="va">j</span><span class="op">]</span><span class="op">[</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span> <span class="op">==</span> <span class="st">"Yes"</span><span class="op">]</span></span>
<span>  <span class="co">## Classification results among the actually negatives</span></span>
<span>  <span class="va">Classifications_among_trueNo</span> <span class="op">&lt;-</span> <span class="va">PredictedDefault_mat</span><span class="op">[</span>,<span class="va">j</span><span class="op">]</span><span class="op">[</span><span class="va">Default</span><span class="op">$</span><span class="va">default</span> <span class="op">==</span> <span class="st">"No"</span><span class="op">]</span></span>
<span>  <span class="co">## Number of True Positives</span></span>
<span>  <span class="va">TP</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Classifications_among_trueYes</span><span class="op">[</span><span class="va">Classifications_among_trueYes</span> <span class="op">==</span> <span class="st">"Yes"</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="co">## Number of False Positives</span></span>
<span>  <span class="va">FP</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Classifications_among_trueNo</span><span class="op">[</span><span class="va">Classifications_among_trueNo</span> <span class="op">==</span> <span class="st">"Yes"</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co">## TP error rate:</span></span>
<span>  <span class="va">TP_Rate</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">TP</span><span class="op">/</span><span class="va">P</span></span>
<span>  <span class="co">## FP error rate:</span></span>
<span>  <span class="va">FP_Rate</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">FP</span><span class="op">/</span><span class="va">N</span> </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">## Layout for the plotting region (two plots side-by-side)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/layout.html">layout</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span>, width <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>,height <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">col_range</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/grDevices/palettes.html">hcl.colors</a></span><span class="op">(</span>n       <span class="op">=</span> <span class="va">n_thresholds</span>, </span>
<span>                       palette <span class="op">=</span> <span class="st">"viridis"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Plot the ROC curve </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x    <span class="op">=</span> <span class="va">FP_Rate</span>, </span>
<span>     y    <span class="op">=</span> <span class="va">TP_Rate</span>, </span>
<span>     col  <span class="op">=</span> <span class="va">col_range</span>, </span>
<span>     xlab <span class="op">=</span> <span class="st">"False Positive Rate"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"True Positive Rate"</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"ROC Curve"</span>, </span>
<span>     type <span class="op">=</span> <span class="st">"o"</span>, lwd<span class="op">=</span><span class="fl">1.5</span>, pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>a <span class="op">=</span> <span class="fl">0</span>, b <span class="op">=</span> <span class="fl">1</span>, lty <span class="op">=</span> <span class="fl">3</span>, col <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Color-Legend for threshold values</span></span>
<span><span class="va">legend_image</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/grDevices/as.raster.html">as.raster</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rev.html">rev</a></span><span class="op">(</span><span class="va">col_range</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">2</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"n"</span>, axes <span class="op">=</span> <span class="cn">FALSE</span>, xlab <span class="op">=</span> <span class="st">""</span>, ylab <span class="op">=</span> <span class="st">""</span>, main <span class="op">=</span> <span class="st">'Threshold-Value'</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span>x      <span class="op">=</span> <span class="fl">1.5</span>, </span>
<span>     y      <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">1</span>, l <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>, </span>
<span>     labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">1</span>, l<span class="op">=</span><span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/rasterImage.html">rasterImage</a></span><span class="op">(</span><span class="va">legend_image</span>, xleft <span class="op">=</span> <span class="fl">0</span>, ybottom <span class="op">=</span> <span class="fl">0</span>, xright <span class="op">=</span> <span class="fl">1</span>, ytop <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-ROC" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ROC-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Ch5_Classification_files/figure-html/fig-ROC-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ROC-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: The ROC curve traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are often not shown. Here, however, we show them using a color code from yellow [threshold = 1] to purple [threshold = 0]. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default.
</figcaption></figure>
</div>
</div>
</div>
<!-- ![](images/Fig_4_8.png) -->
<p>The commonly reported summary statistic for the ROC curve is the “Area Under the (ROC) Curve” <strong>AUC</strong></p>
<ul>
<li>Ideal AUC-value is AUC<span class="math inline">\(=1.\)</span>
</li>
<li>A “no information” classifier has AUC<span class="math inline">\(=0.5.\)</span>
</li>
</ul>
<p>The R package <code>ROCR</code> contains functions for computing and plotting the ROC curve and for computing the AUC value.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## install.packages("ROCR")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://ipa-tys.github.io/ROCR/">"ROCR"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Caution: Choose the posterior probability column carefully, </span></span>
<span><span class="co">## it may be lda.pred$posterior[,1] or lda.pred$posterior[,2], </span></span>
<span><span class="co">## depending on your factor levels</span></span>
<span></span>
<span><span class="co">## Predicted and actual classes:</span></span>
<span><span class="va">predict</span> <span class="op">&lt;-</span> <span class="fu"><a href="http://ipa-tys.github.io/ROCR/reference/prediction.html">prediction</a></span><span class="op">(</span><span class="va">ldaPredict_obj</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span>, <span class="va">Default</span><span class="op">$</span><span class="va">default</span><span class="op">)</span> </span>
<span><span class="co">## compute TP-Ratios and FP-Ratios:</span></span>
<span><span class="va">perform</span> <span class="op">&lt;-</span> <span class="fu"><a href="http://ipa-tys.github.io/ROCR/reference/performance.html">performance</a></span><span class="op">(</span><span class="va">predict</span>, <span class="st">"tpr"</span>, <span class="st">"fpr"</span><span class="op">)</span></span>
<span><span class="co">## ROC curve</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">perform</span>, colorize <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch5_Classification_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## AUC</span></span>
<span><span class="va">AUC</span>     <span class="op">&lt;-</span> <span class="fu"><a href="http://ipa-tys.github.io/ROCR/reference/performance.html">performance</a></span><span class="op">(</span><span class="va">predict</span>, measure <span class="op">=</span> <span class="st">"auc"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">AUC</span><span class="op">@</span><span class="va">y.values</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
[1] 0.9495584</code></pre>
</div>
</div>
</section><section id="quadratic-discriminant-analysis-qda" class="level3" data-number="5.7.5"><h3 data-number="5.7.5" class="anchored" data-anchor-id="quadratic-discriminant-analysis-qda">
<span class="header-section-number">5.7.5</span> Quadratic Discriminant Analysis (QDA)</h3>
<p>Unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form <span class="math display">\[
X|Y=k \sim N(\mu_k,\Sigma_k),
\]</span> where <span class="math inline">\(\Sigma_k\)</span> is a <span class="math inline">\((p\times p)\)</span> covariance matrix for the <span class="math inline">\(k\)</span>th class.</p>
<p>Under this assumption, the Bayes classifier assigns an observation <span class="math inline">\(X=x\)</span> to the class for which <span class="math display">\[
\begin{align*}
&amp;\delta_k(x)=\\
&amp;= - \frac{1}{2}\left(x-\mu_k\right)^T\Sigma_k^{-1}\left(x-\mu_k\right)
   - \frac{1}{2}\log\left(|\Sigma_k|\right)
   + \log(\pi_k)\\
&amp;= - \frac{1}{2}x^T    \Sigma_k^{-1}x
   + \frac{1}{2}x^T    \Sigma_k^{-1}\mu_k
   - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k
   - \frac{1}{2}\log\left(|\Sigma_k|\right)
   + \log(\pi_k)
\end{align*}
\]</span> is largest.</p>
<p>This classifier is called <em>quadratic</em> discriminant analysis since the predictor-value <span class="math inline">\(x\)</span> appears as a quadratic function.</p>
<p>QDA has <strong>much</strong> more parameters than LDA and is thus a much more flexible classifier than LDA:</p>
<ul>
<li>LDA requires
<ul>
<li>
<span class="math inline">\(K\)</span> a-priori probability estimates</li>
<li>
<span class="math inline">\(K\cdot p\)</span> mean estimations</li>
<li>
<span class="math inline">\((p+1)/2\)</span> covariance estimates</li>
</ul>
</li>
<li>QDA requires
<ul>
<li>
<span class="math inline">\(K\)</span> a-priori probability estimates</li>
<li>
<span class="math inline">\(K\cdot p\)</span> mean estimations</li>
<li>
<span class="math inline">\({\color{red}K}\cdot(p+1)/2\)</span> covariance estimates</li>
</ul>
</li>
</ul>
<p>The difference in flexibility needs to be considered under the <strong>bias variance trade-off</strong>:</p>
<ul>
<li>Low flexibility generally means low variance, but large bias</li>
<li>High flexibility generally means high variance, but low bias</li>
</ul>
<p>Roughly:</p>
<ul>
<li>LDA tends to be better than QDA if there are relatively few training observations (small <span class="math inline">\(n_{Train}\)</span>) and so reducing variance is crucial.</li>
<li>QDA is recommended
<ul>
<li>if the training set <span class="math inline">\(n_{Train}\)</span> is large, so that the variance of the classifier is not a major concern, or</li>
<li>if the assumption of a common covariance matrix for the <span class="math inline">\(K\)</span> classes is clearly unrealistic.</li>
</ul>
</li>
</ul>
<p><img src="images/Fig_4_9.png" class="img-fluid"></p>
</section><section id="naive-bayes" class="level3" data-number="5.7.6"><h3 data-number="5.7.6" class="anchored" data-anchor-id="naive-bayes">
<span class="header-section-number">5.7.6</span> Naive Bayes</h3>
<p>Remember, that Bayes’ theorem states that the probability that a observation with predictor values <span class="math inline">\(X=x\)</span> comes from class <span class="math inline">\(k\)</span> (i.e.&nbsp;the posterior probability), is given by <span class="math display">\[
p_k(x) = Pr(Y=k|X=x)=\frac{\pi_k f_k(x)}{\sum_{l=1}^K\pi_l f_l(x)},
\]</span> where</p>
<ul>
<li>
<span class="math inline">\(\pi_k=Pr(Y=k)\)</span> denote the prior probabilities and</li>
<li>
<span class="math inline">\(f_k(x)\)</span> denotes the <span class="math inline">\(p\)</span>-dimensional, conditional density of <span class="math inline">\(X,\)</span> given <span class="math inline">\(Y=k.\)</span>
</li>
</ul>
<p>We have seen above, that estimating the prior probabilities <span class="math inline">\(\pi_1,\dots,\pi_K\)</span> is typically straight forward using the relative frequencies <span class="math display">\[
\hat\pi_k=\frac{n_k}{n},\quad k=1,\dots,K.
\]</span> However, estimating the <span class="math inline">\(p\)</span> dimensional density functions <span class="math display">\[
f_1(x),\dots,f_K(x)
\]</span> was and is substantially more difficult.</p>
<p>To make estimation tractable and useful, LDA and QDA assume parametric families for these density functions; namely, multivariate normals with and without common covariances.</p>
<p>By contrast, <strong>Naive Bayes</strong> only makes the assumption that the <span class="math inline">\(p\)</span> dimensional components of the predictor are <em>independent</em> from each other. I.e. that the joint, <span class="math inline">\(p\)</span>-dimensional density <span class="math inline">\(f_k(x)\)</span> can be written as the product of the <span class="math inline">\(p\)</span> marginal densities <!-- $f_{kj}(x_j)$, $j=1,\dots,p$,  --> <span class="math display">\[
f_k(x) = f_{k1}(x_1)\cdot f_{k2}(x_2)\cdot\;\dots\;\cdot f_{kp}(x_p)
\]</span></p>
<p>This simplifying assumption may not be completely correct, but it often leads to very good classification results - particularly, in classification problems with smallish sample sizes.</p>
<p>Under the naive Bayes assumption, we have that <span class="math display">\[
Pr(Y=k|X=x)=\frac{\pi_k f_{k1}(x_1)\cdot\;\dots\;\cdot f_{kp}(x_p)}{\sum_{l=1}^K\pi_l f_{l1}(x_1)\cdot\;\dots\;\cdot f_{lp}(x_p)}.
\]</span></p>
<p>For estimating the single, one dimensional marginal density functions <span class="math display">\[
f_{kj}(x),\quad j=1,\dots,p\quad\text{and}\quad k=1,\dots,K
\]</span> we can use …</p>
<ul>
<li>a univariate parametric distribution assumption. Examples:
<ul>
<li>A normal distribution <span class="math inline">\(X_j|Y=k \sim N(\mu_{jk},\sigma_{jk}).\)</span>
</li>
<li>Alternative parametric distributions: Exponential, <span class="math inline">\(\chi^2\)</span>-distribution, Gamma distribution, etc.</li>
</ul>
</li>
<li>non-parametric kernel density estimation (for quantitative predictors) using the <code>R</code>-function <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code>
</li>
<li>simple relative frequency (for qualitative predictors)
<ul>
<li>Example: Student status <span class="math inline">\(x\in\{\texttt{Yes},\texttt{No}\}\)</span> <span class="math display">\[
\hat{f}_{kj}(x)=\left\{
  \begin{matrix}
  \frac{\#\text{students in group $k$}}{n_k}&amp;\text{if }\; x=\texttt{Yes}\\
  1-\frac{\#\text{students in group $k$}}{n_k}&amp;\text{if }\; x=\texttt{No}
  \end{matrix}\right.
\]</span>
</li>
</ul>
</li>
</ul>
<p>The different estimation approaches can differ for each <span class="math inline">\(j=1,\dots,p.\)</span></p>
<!-- 
OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? 
Why do we need another method, when we have logistic regression?
There are several reasons:

* When there is substantial separation between the two classes, the
parameter estimates for the logistic regression model are surprisingly
unstable. The methods that we consider in this section do not suffer
from this problem.

* If the distribution of the predictors $X$ is approximately normal in
each of the classes and the sample size is small, then the approaches
in this section may be more accurate than logistic regression.
OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN?  
-->
</section></section><section id="self-study-r-lab-classification" class="level2" data-number="5.8"><h2 data-number="5.8" class="anchored" data-anchor-id="self-study-r-lab-classification">
<span class="header-section-number">5.8</span> Self-Study <code>R</code>-Lab: Classification</h2>
<section id="the-stock-market-data" class="level3" data-number="5.8.1"><h3 data-number="5.8.1" class="anchored" data-anchor-id="the-stock-market-data">
<span class="header-section-number">5.8.1</span> The Stock Market Data</h3>
<p>We will begin by examining some numerical and graphical summaries of the <code>Smarket</code> data, which is part of the <code>ISLR2</code> library. This data set consists of percentage returns for the S&amp;P 500 stock index over <span class="math inline">\(1,250\)</span> days, from the beginning of 2001 until the end of 2005. For each date, we have recorded</p>
<ul>
<li>the percentage returns for each of the five previous trading days: <code>Lag1</code>, <code>Lag2</code>, … <code>Lag5</code>
</li>
<li>
<code>Volume</code> (the number of shares traded on the previous day, in billions)</li>
<li>
<code>Today</code> (the percentage return on the date in question)</li>
<li>
<code>Direction</code> (whether the market was <code>Up</code> or <code>Down</code> on this date).</li>
</ul>
Our goal is to predict the qualitative response
<center>
<code>Direction</code> <span class="math inline">\(\in\{\)</span> <code>Up</code>, <code>Down</code> <span class="math inline">\(\}\)</span>
</center>
<p>using the other predictors/features.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span>   <span class="co"># package contains the data</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">)</span>  <span class="co"># attach(Smarket) allows to use the variables </span></span>
<span>                 <span class="co"># contained Smarket directly </span></span>
<span>                 <span class="co"># (i.e. 'Direction' instead of 'Smarket$Direction')</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">)</span>   <span class="co"># names of the variables in the Smarket data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Year"      "Lag1"      "Lag2"      "Lag3"      "Lag4"      "Lag5"     
[7] "Volume"    "Today"     "Direction"</code></pre>
</div>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">)</span>     <span class="co"># total sample size n, number of variables</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1250    9</code></pre>
</div>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">)</span> <span class="co"># descriptive statistics (mean, median, ...)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Year           Lag1                Lag2                Lag3          
 Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  
 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  
 Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  
 Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  
 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  
 Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  
      Lag4                Lag5              Volume           Today          
 Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  
 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  
 Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  
 Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  
 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  
 Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  
 Direction 
 Down:602  
 Up  :648  
           
           
           
           </code></pre>
</div>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">)</span>   <span class="co"># pairwise scatter plots</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch5_Classification_files/figure-html/chunk1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/cor.html">cor()</a></code> function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the <code>Direction</code> variable is qualitative (<code>Up</code> and <code>Down</code>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in cor(Smarket): 'x' must be numeric</code></pre>
</div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">Smarket</span><span class="op">)</span> <span class="op">!=</span> <span class="st">"Direction"</span><span class="op">]</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Year  Lag1  Lag2  Lag3  Lag4  Lag5 Volume Today
Year   1.00  0.03  0.03  0.03  0.04  0.03   0.54  0.03
Lag1   0.03  1.00 -0.03 -0.01  0.00 -0.01   0.04 -0.03
Lag2   0.03 -0.03  1.00 -0.03 -0.01  0.00  -0.04 -0.01
Lag3   0.03 -0.01 -0.03  1.00 -0.02 -0.02  -0.04  0.00
Lag4   0.04  0.00 -0.01 -0.02  1.00 -0.03  -0.05 -0.01
Lag5   0.03 -0.01  0.00 -0.02 -0.03  1.00  -0.02 -0.03
Volume 0.54  0.04 -0.04 -0.04 -0.05 -0.02   1.00  0.01
Today  0.03 -0.03 -0.01  0.00 -0.01 -0.03   0.01  1.00</code></pre>
</div>
</div>
<p>As one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between <code>Year</code> and <code>Volume</code>.</p>
<p><strong>Note:</strong> The variable <code>Direction</code> contains simply the sign-information extracted from the variable <code>Today</code></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Direction</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] Up   Up   Down Up  
Levels: Down Up</code></pre>
</div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Today</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  0.959  1.032 -0.623  0.614</code></pre>
</div>
</div>
<!-- By plotting the data, which is ordered chronologically, we see that `Volume` is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.





::: {.cell layout-align="center"}

```{.r .cell-code}
attach(Smarket)
```

::: {.cell-output .cell-output-stderr}

```
The following objects are masked from Smarket (pos = 3):

    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year
```


:::

```{.r .cell-code}
plot(Volume)
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/chunk3-1.png){fig-align='center' width=672}
:::
:::




 -->
</section><section id="logistic-regression-1" class="level3" data-number="5.8.2"><h3 data-number="5.8.2" class="anchored" data-anchor-id="logistic-regression-1">
<span class="header-section-number">5.8.2</span> Logistic Regression</h3>
<p>Next, we will fit a logistic regression model in order to predict <code>Direction</code> using <code>Lag1</code> through <code>Lag5</code> and <code>Volume</code>.</p>
<p>The <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function can be used to fit many types of generalized linear models, including logistic regression.</p>
<p>The syntax of the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function is similar to that of <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>, except that we must pass in the argument <code>family = binomial</code> in order to tell <code>R</code> to run a logistic regression rather than some other type of generalized linear model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glm.fits</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>    <span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span> <span class="op">+</span> <span class="va">Lag3</span> <span class="op">+</span> <span class="va">Lag4</span> <span class="op">+</span> <span class="va">Lag5</span> <span class="op">+</span> <span class="va">Volume</span>,</span>
<span>    data <span class="op">=</span> <span class="va">Smarket</span>, family <span class="op">=</span> <span class="va">binomial</span></span>
<span>  <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">glm.fits</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
    Volume, family = binomial, data = Smarket)

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -0.126000   0.240736  -0.523    0.601
Lag1        -0.073074   0.050167  -1.457    0.145
Lag2        -0.042301   0.050086  -0.845    0.398
Lag3         0.011085   0.049939   0.222    0.824
Lag4         0.009359   0.049974   0.187    0.851
Lag5         0.010313   0.049511   0.208    0.835
Volume       0.135441   0.158360   0.855    0.392

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1731.2  on 1249  degrees of freedom
Residual deviance: 1727.6  on 1243  degrees of freedom
AIC: 1741.6

Number of Fisher Scoring iterations: 3</code></pre>
</div>
</div>
<p>The smallest <span class="math inline">\(p\)</span>-value here is associated with <code>Lag1</code>. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of <span class="math inline">\(0.145\)</span>, the <span class="math inline">\(p\)</span>-value is still relatively large, and so there is no clear evidence of a real association between <code>Lag1</code> and <code>Direction</code>.</p>
<p>We use the <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code> function in order to access just the coefficients for this fitted model. We can also use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function to access particular aspects of the fitted model, such as the <span class="math inline">\(p\)</span>-values for the coefficients.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">glm.fits</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 
-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 
      Volume 
 0.135440659 </code></pre>
</div>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">glm.fits</span><span class="op">)</span><span class="op">$</span><span class="va">coef</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983
Lag1        -0.073073746 0.05016739 -1.4565986 0.1452272
Lag2        -0.042301344 0.05008605 -0.8445733 0.3983491
Lag3         0.011085108 0.04993854  0.2219750 0.8243333
Lag4         0.009358938 0.04997413  0.1872757 0.8514445
Lag5         0.010313068 0.04951146  0.2082966 0.8349974
Volume       0.135440659 0.15835970  0.8552723 0.3924004</code></pre>
</div>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">glm.fits</span><span class="op">)</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span>, <span class="fl">4</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 
  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 
     Volume 
  0.3924004 </code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function can be used to predict the probability that the market will go up, given values of the predictors. The <code>type = "response"</code> option tells <code>R</code> to output probabilities of the form <span class="math inline">\(P(Y=1|X)\)</span>, as opposed to other information such as the logit. If no data set is supplied to the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Posterior probabilities</span></span>
<span><span class="va">glm.probs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">glm.fits</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="va">glm.probs</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2         3         4 
0.5070841 0.4814679 0.4811388 0.5152224 </code></pre>
</div>
</div>
<p>We know that these values correspond to the probability of the market going up, rather than down, because the <code><a href="https://rdrr.io/r/stats/contrasts.html">contrasts()</a></code> function indicates that <code>R</code> has created a dummy variable with a 1 for <code>Up</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Up
Down  0
Up    1</code></pre>
</div>
</div>
<p>I.e. <span class="math display">\[\begin{align*}
Y = 0 &amp;\Leftrightarrow \texttt{Direction} = \texttt{Down}\\[2ex]
Y = 1 &amp;\Leftrightarrow \texttt{Direction} = \texttt{Up}
\end{align*}\]</span> Such that, <span class="math display">\[
p(X) = P(Y=1|X) = P(\texttt{Direction} = \texttt{Up} |X).
\]</span></p>
<p>In order to actually make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities <span class="math display">\[
0\leq p(X)\leq 1
\]</span> into class labels, <code>Up</code> or <code>Down</code>. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than <span class="math inline">\(0.5\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glm.pred</span>                 <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Down"</span>, <span class="fl">1250</span><span class="op">)</span> <span class="co"># sample size: 1250</span></span>
<span><span class="va">glm.pred</span><span class="op">[</span><span class="va">glm.probs</span> <span class="op">&gt;</span> <span class="fl">.5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"Up"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first command creates a vector of 1,250 <code>Down</code> elements. The second line transforms to <code>Up</code> all of the elements for which the predicted probability of a market increase exceeds <span class="math inline">\(0.5\)</span>.</p>
<p>Given these predictions, the <code><a href="https://rdrr.io/r/base/table.html">table()</a></code> function can be used to produce a <strong>confusion matrix</strong> in order to determine how many observations were correctly or incorrectly classified. <!-- %By inputting two qualitative vectors R will create a two by two table with counts of the number of times each combination occurred e.g. predicted `Up` and market increased, predicted `Up` and the market decreased etc. --></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">glm.pred</span>, <span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction
glm.pred Down  Up
    Down  145 141
    Up    457 507</code></pre>
</div>
</div>
<p>The <strong>diagonal elements</strong> of the confusion matrix indicate <strong>correct predictions</strong>, while the <strong>off-diagonals</strong> represent <strong>incorrect predictions</strong>.</p>
<p>Hence our model correctly predicted that the market would go up on <span class="math inline">\(507\)</span> days and that it would go down on <span class="math inline">\(145\)</span> days, for a total of <span class="math inline">\(507+145 = 652\)</span> correct predictions.</p>
<p>The <strong>overall training error rate</strong> is</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Overall training error rate </span></span>
<span><span class="op">(</span><span class="fl">141</span> <span class="op">+</span> <span class="fl">457</span><span class="op">)</span> <span class="op">/</span> <span class="fl">1250</span> <span class="co"># Equivalently: 1 - (507+145)/1250</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4784</code></pre>
</div>
</div>
<p>Alternatively, the <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> function can be used to compute the overall training error rate; i.e., the fraction of days for which the prediction was correct (within the training data). In this case, logistic regression correctly predicted the movement of the market <span class="math inline">\(52.2\%\)</span> of the time.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Alternative way to compute the </span></span>
<span><span class="co">## overall training error rate </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">glm.pred</span> <span class="op">!=</span> <span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4784</code></pre>
</div>
</div>
<p>At first glance, it appears that the logistic regression model is working a little better than random guessing, since the training error rate is smaller than <span class="math inline">\(0.5.\)</span></p>
<p>However, this result is misleading because we trained and tested the model on the same set of <span class="math inline">\(1,250\)</span> observations. In other words, <span class="math inline">\(47.8\%\)</span> is the <em>training</em> error rate.</p>
<p>The training error rate is often overly optimistic. It tends to <em>underestimate</em> the true (unknown) test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the <em>held out</em> data.</p>
<p>This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.</p>
<p>To implement this strategy, we will first create a boolean <code>train</code> vector allowing us to select the daily observations from 2001 through 2004. We will then use this vector to create</p>
<ul>
<li>a training subset (days from 2001 through 2004) and</li>
<li>a testing subset (days in 2005)</li>
</ul>
<p>from the total <code>Smarket</code> dataset.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">train</span>          <span class="op">&lt;-</span> <span class="op">(</span><span class="va">Year</span> <span class="op">&lt;</span> <span class="fl">2005</span><span class="op">)</span></span>
<span><span class="co">## Test data </span></span>
<span><span class="va">Smarket_2005</span>   <span class="op">&lt;-</span> <span class="va">Smarket</span><span class="op">[</span><span class="op">!</span><span class="va">train</span>, <span class="op">]</span> </span>
<span><span class="co">## Test sample size </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Smarket_2005</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 252</code></pre>
</div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Dependent variable of the test data </span></span>
<span><span class="va">Direction_2005</span> <span class="op">&lt;-</span> <span class="va">Direction</span><span class="op">[</span><span class="op">!</span><span class="va">train</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The object <code>train</code> is a vector of <span class="math inline">\(1250\)</span> elements, corresponding to the observations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to <code>TRUE</code>, whereas those that correspond to observations in 2005 are set to <code>FALSE</code>.</p>
<p>The object <code>train</code> is a <em>Boolean</em> vector, since its elements are <code>TRUE</code> and <code>FALSE</code>. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command <code>Smarket[train, ]</code> would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of <code>train</code> are <code>TRUE</code>.</p>
<p>The <code>!</code> symbol can be used to reverse all of the elements of a Boolean vector. That is, <code>!train</code> is a vector similar to <code>train</code>, except that the elements that are <code>TRUE</code> in <code>train</code> get swapped to <code>FALSE</code> in <code>!train</code>, and the elements that are <code>FALSE</code> in <code>train</code> get swapped to <code>TRUE</code> in <code>!train</code>. Therefore, <code>Smarket[!train, ]</code> yields a submatrix of the stock market data containing only the observations for which <code>train</code> is <code>FALSE</code>—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.</p>
<p>We now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the <code>subset</code> argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glm.fits</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>    <span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span> <span class="op">+</span> <span class="va">Lag3</span> <span class="op">+</span> <span class="va">Lag4</span> <span class="op">+</span> <span class="va">Lag5</span> <span class="op">+</span> <span class="va">Volume</span>,</span>
<span>    data   <span class="op">=</span> <span class="va">Smarket</span>, </span>
<span>    family <span class="op">=</span> <span class="va">binomial</span>, </span>
<span>    subset <span class="op">=</span> <span class="va">train</span></span>
<span>  <span class="op">)</span></span>
<span><span class="co">## Using the trained model to predict the held out test data  </span></span>
<span><span class="va">glm.probs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">glm.fits</span>, </span>
<span>                     newdata <span class="op">=</span> <span class="va">Smarket_2005</span>,</span>
<span>                     type    <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that we have trained and tested our model on two completely separate data sets:</p>
<ul>
<li>training (estimation) was performed using only the <strong>dates before 2005</strong>
</li>
<li>testing was performed using only the <strong>dates in 2005</strong>.</li>
</ul>
<p>Finally, we compute the <code>Down</code>/<code>Up</code>-predictions for 2005 and compare them to the actual movements of the market over that time period.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glm.pred</span>                 <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Down"</span>, <span class="fl">252</span><span class="op">)</span></span>
<span><span class="va">glm.pred</span><span class="op">[</span><span class="va">glm.probs</span> <span class="op">&gt;</span> <span class="fl">.5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"Up"</span></span>
<span><span class="co">## Test data confusion matrix </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">glm.pred</span>, <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction_2005
glm.pred Down Up
    Down   77 97
    Up     34 44</code></pre>
</div>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Rate of prediction errors in the test data </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">glm.pred</span> <span class="op">!=</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5198413</code></pre>
</div>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Rate of correct predictions in the test data </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">glm.pred</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4801587</code></pre>
</div>
</div>
<p>This result is rather disappointing: the test error rate is <span class="math inline">\(52\%\)</span>, which is worse than random guessing!</p>
<p>Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then we should start trading rather than learning statistics.)</p>
<p>We recall that the logistic regression model had very underwhelming <span class="math inline">\(p\)</span>-values associated with all of the predictors, and that the smallest <span class="math inline">\(p\)</span>-value, though not very small, corresponded to <code>Lag1</code>. Perhaps by removing the variables that appear not to be helpful in predicting <code>Direction</code>, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. Below we have refit the logistic regression using just <code>Lag1</code> and <code>Lag2</code>, which seemed to have the highest predictive power in the original logistic regression model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Train model using the training data</span></span>
<span><span class="va">glm.fits</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, </span>
<span>                 family <span class="op">=</span> <span class="va">binomial</span>, </span>
<span>                 data   <span class="op">=</span> <span class="va">Smarket</span>, </span>
<span>                 subset <span class="op">=</span> <span class="va">train</span><span class="op">)</span></span>
<span><span class="co">## Predict the test data                  </span></span>
<span><span class="va">glm.probs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">glm.fits</span>, <span class="va">Smarket_2005</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="co">##</span></span>
<span><span class="va">glm.pred</span>                 <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Down"</span>, <span class="fl">252</span><span class="op">)</span></span>
<span><span class="va">glm.pred</span><span class="op">[</span><span class="va">glm.probs</span> <span class="op">&gt;</span> <span class="fl">.5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"Up"</span></span>
<span><span class="co">## Test data confusion matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">glm.pred</span>, <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction_2005
glm.pred Down  Up
    Down   35  35
    Up     76 106</code></pre>
</div>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data rate of correct predictions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">glm.pred</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5595238</code></pre>
</div>
</div>
<p>Now the results appear to be a little better: <span class="math inline">\(56\%\)</span> of the daily movements have been correctly predicted.</p>
<p>However, you should compare this with the naive prediction that the market will <strong>increase every day</strong>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="st">"Up"</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.56</code></pre>
</div>
</div>
<p>This naive prediction strategy will be correct <span class="math inline">\(56\%\)</span> of the time in the test data! Hence, in terms of overall error rate, the logistic regression method is no better than this naive approach.</p>
<p>However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a <span class="math inline">\(58\%\)</span> accuracy rate (Positive predictive value TP/P*):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data positive predictive value TP/P*</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span> <span class="fl">106</span> <span class="op">/</span> <span class="op">(</span><span class="fl">106</span> <span class="op">+</span> <span class="fl">76</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.58</code></pre>
</div>
</div>
<p>This suggests a possible trading strategy of buying on days when the logistic regression model predicts an <em>increasing</em> market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.</p>
<p>Suppose that we want to predict the returns associated with particular values of <code>Lag1</code> and <code>Lag2</code>. In particular, we want to predict <code>Direction</code> on a day when <code>Lag1</code> and <code>Lag2</code> equal <span class="math inline">\(1.2\)</span> and <span class="math inline">\(1.1\)</span>, respectively, and on a day when they equal <span class="math inline">\(1.5\)</span> and <span class="math inline">\(-0.8.\)</span> We do this using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Caution: You need to use the same variables names </span></span>
<span><span class="co">## here "Lag1" and "Lag2" as used when computing 'glm.fits'</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">glm.fits</span>,</span>
<span>        newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Lag1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.2</span>,  <span class="fl">1.5</span><span class="op">)</span>,  </span>
<span>                             Lag2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.1</span>, <span class="op">-</span><span class="fl">0.8</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        type    <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2 
0.4791462 0.4960939 </code></pre>
</div>
</div>
</section><section id="linear-discriminant-analysis" class="level3" data-number="5.8.3"><h3 data-number="5.8.3" class="anchored" data-anchor-id="linear-discriminant-analysis">
<span class="header-section-number">5.8.3</span> Linear Discriminant Analysis</h3>
<p>Now we will perform LDA on the <code>Smarket</code> data.</p>
<p>In <code>R</code>, we fit an LDA model using the <code><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda()</a></code> function, which is part of the <code>MASS</code> library.</p>
<p>The syntax for the <code><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda()</a></code> function is identical to that of <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>, and to that of <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> except for the absence of the <code>family</code> option.</p>
<p>We fit the model using only the training observations before 2005.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="va">lda.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, </span>
<span>               data   <span class="op">=</span> <span class="va">Smarket</span>,</span>
<span>               subset <span class="op">=</span> <span class="va">train</span><span class="op">)</span></span>
<span><span class="va">lda.fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)

Prior probabilities of groups:
    Down       Up 
0.491984 0.508016 

Group means:
            Lag1        Lag2
Down  0.04279022  0.03389409
Up   -0.03954635 -0.03132544

Coefficients of linear discriminants:
            LD1
Lag1 -0.6420190
Lag2 -0.5135293</code></pre>
</div>
</div>
<p>The LDA output indicates that</p>
<ul>
<li>
<span class="math inline">\(\hat\pi_1=0.492\)</span> and</li>
<li><span class="math inline">\(\hat\pi_2=0.508\)</span></li>
</ul>
<p>That is, <span class="math inline">\(49.2\%\)</span> of the training observations correspond to days during which the market went down.</p>
<p>The output also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of <span class="math inline">\(\mu_k\)</span>. These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines.</p>
<p>The “coefficients of linear discriminants” output provides the linear combination of <code>Lag1</code> and <code>Lag2</code> that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of <span class="math inline">\(X=x\)</span> in <a href="#eq-LDA_mult" class="quarto-xref">Equation&nbsp;<span>5.7</span></a>. If <span class="math inline">\(-0.642\times\)</span><code>Lag1</code><span class="math inline">\(- 0.514 \times\)</span> <code>Lag2</code> is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function produces plots of the <em>linear discriminants</em>, obtained by computing <span class="math inline">\(-0.642\times\)</span><code>Lag1</code><span class="math inline">\(- 0.514 \times\)</span> <code>Lag2</code> for each of the training observations. The <code>Up</code> and <code>Down</code> observations are displayed separately.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lda.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch5_Classification_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function returns a list with three elements. The first element, <code>class</code>, contains LDA’s predictions about the movement of the market. The second element, <code>posterior</code>, is a matrix whose <span class="math inline">\(k\)</span>th column contains the posterior probability that the corresponding observation belongs to the <span class="math inline">\(k\)</span>th class, computed from (4.15). Finally, <code>x</code> contains the linear discriminants, described earlier.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lda.pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lda.fit</span>, <span class="va">Smarket_2005</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">lda.pred</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "class"     "posterior" "x"        </code></pre>
</div>
</div>
<p>The LDA and logistic regression predictions are almost identical. This observation holds true also general since LDA and logistic regression are effectively very similar classifiers; see Section 4.5 in our textbook <code>ISLR2</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lda.class</span> <span class="op">&lt;-</span> <span class="va">lda.pred</span><span class="op">$</span><span class="va">class</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">lda.class</span>, <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction_2005
lda.class Down  Up
     Down   35  35
     Up     76 106</code></pre>
</div>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">lda.class</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5595238</code></pre>
</div>
</div>
<p>Applying a <span class="math inline">\(50\%\)</span> threshold to the posterior probabilities allows us to recreate the predictions contained in <code>lda.pred$class</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lda.pred</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;=</span> <span class="fl">.5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 70</code></pre>
</div>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lda.pred</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;</span> <span class="fl">.5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 182</code></pre>
</div>
</div>
<p>The first column in <code>lda.pred$posterior</code> contains the posterior probabilities that the market will <em>decrease</em> and the second column contains the posterior probabilities that the market will <em>increase</em>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lda.pred</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="fl">1</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      999      1000      1001      1002      1003      1004      1005      1006 
0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 
     1007      1008      1009      1010      1011      1012      1013      1014 
0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 
     1015      1016      1017      1018 
0.4935775 0.5030894 0.4978806 0.4886331 </code></pre>
</div>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lda.pred</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="fl">2</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      999      1000      1001      1002      1003      1004      1005      1006 
0.5098208 0.5207815 0.5331815 0.5259989 0.5072123 0.5061438 0.5048984 0.5127139 
     1007      1008      1009      1010      1011      1012      1013      1014 
0.5092987 0.5155974 0.5093037 0.4880012 0.5104848 0.5293239 0.5255407 0.5200417 
     1015      1016      1017      1018 
0.5064225 0.4969106 0.5021194 0.5113669 </code></pre>
</div>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lda.class</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  
[16] Up   Up   Down Up   Up  
Levels: Down Up</code></pre>
</div>
</div>
<p>If we wanted to use a posterior probability threshold other than <span class="math inline">\(50\%\)</span> in order to make predictions, then we could easily do so.</p>
<p>For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability for a decrease is at least <span class="math inline">\(90\%.\)</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lda.pred</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">.9</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>
<p>No days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was <span class="math inline">\(52.02\%:\)</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">lda.pred</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.520235</code></pre>
</div>
</div>
</section><section id="quadratic-discriminant-analysis" class="level3" data-number="5.8.4"><h3 data-number="5.8.4" class="anchored" data-anchor-id="quadratic-discriminant-analysis">
<span class="header-section-number">5.8.4</span> Quadratic Discriminant Analysis</h3>
<p>We will now fit a QDA model to the <code>Smarket</code> data. QDA is implemented in <code>R</code> using the <code><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda()</a></code> function, which is also part of the <code>MASS</code> library. The syntax is identical to that of <code><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda()</a></code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">qda.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, data <span class="op">=</span> <span class="va">Smarket</span>,</span>
<span>    subset <span class="op">=</span> <span class="va">train</span><span class="op">)</span></span>
<span><span class="va">qda.fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)

Prior probabilities of groups:
    Down       Up 
0.491984 0.508016 

Group means:
            Lag1        Lag2
Down  0.04279022  0.03389409
Up   -0.03954635 -0.03132544</code></pre>
</div>
</div>
<p>The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function works in exactly the same fashion as for LDA.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">qda.class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">qda.fit</span>, <span class="va">Smarket_2005</span><span class="op">)</span><span class="op">$</span><span class="va">class</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">qda.class</span>, <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction_2005
qda.class Down  Up
     Down   30  20
     Up     81 121</code></pre>
</div>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">qda.class</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5992063</code></pre>
</div>
</div>
<p>Interestingly, the QDA predictions are accurate almost <span class="math inline">\(60\%\)</span> of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!</p>
</section><section id="naive-bayes-1" class="level3" data-number="5.8.5"><h3 data-number="5.8.5" class="anchored" data-anchor-id="naive-bayes-1">
<span class="header-section-number">5.8.5</span> Naive Bayes</h3>
<p>Next, we fit a naive Bayes model to the <code>Smarket</code> data. Naive Bayes is implemented in <code>R</code> using the <code><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html">naiveBayes()</a></code> function, which is part of the <code>e1071</code> library. The syntax is identical to that of <code><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda()</a></code> and <code><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda()</a></code>. This rather simple implementation of the naive Bayes classifier models uses a univariate Gaussian distribution for each quantitative predictor.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## install.packages("e1071") </span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"e1071"</span><span class="op">)</span></span>
<span><span class="va">nb_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html">naiveBayes</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, </span>
<span>                     data   <span class="op">=</span> <span class="va">Smarket</span>,</span>
<span>                     subset <span class="op">=</span> <span class="va">train</span><span class="op">)</span></span>
<span><span class="va">nb_fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = X, y = Y, laplace = laplace)

A-priori probabilities:
Y
    Down       Up 
0.491984 0.508016 

Conditional probabilities:
      Lag1
Y             [,1]     [,2]
  Down  0.04279022 1.227446
  Up   -0.03954635 1.231668

      Lag2
Y             [,1]     [,2]
  Down  0.03389409 1.239191
  Up   -0.03132544 1.220765</code></pre>
</div>
</div>
<p>The output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for <code>Lag1</code> is <span class="math inline">\(0.0428\)</span> for <code>Direction=Down</code>, and the standard deviation is <span class="math inline">\(1.23\)</span>. We can easily verify this:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Lag1</span><span class="op">[</span><span class="va">train</span><span class="op">]</span><span class="op">[</span><span class="va">Direction</span><span class="op">[</span><span class="va">train</span><span class="op">]</span> <span class="op">==</span> <span class="st">"Down"</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.04279022</code></pre>
</div>
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">Lag1</span><span class="op">[</span><span class="va">train</span><span class="op">]</span><span class="op">[</span><span class="va">Direction</span><span class="op">[</span><span class="va">train</span><span class="op">]</span> <span class="op">==</span> <span class="st">"Down"</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.227446</code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function is straightforward to use:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">nb.class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">nb_fit</span>, <span class="va">Smarket_2005</span><span class="op">)</span></span>
<span><span class="co">## Test data confusion matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">nb.class</span>, <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction_2005
nb.class Down  Up
    Down   28  20
    Up     83 121</code></pre>
</div>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data overall rate of correct classifications</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">nb.class</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5912698</code></pre>
</div>
</div>
<p>Naive Bayes performs very well on this data, with accurate predictions over <span class="math inline">\(59\%\)</span> of the time. This is slightly worse than QDA, but much better than LDA.</p>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function can also generate estimates of the probability that each observation belongs to a particular class.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb108"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">nb.preds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">nb_fit</span>, <span class="va">Smarket_2005</span>, type <span class="op">=</span> <span class="st">"raw"</span><span class="op">)</span></span>
<span><span class="va">nb.preds</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Down        Up
[1,] 0.4873164 0.5126836
[2,] 0.4762492 0.5237508
[3,] 0.4653377 0.5346623
[4,] 0.4748652 0.5251348
[5,] 0.4901890 0.5098110</code></pre>
</div>
</div>
</section><section id="k-nearest-neighbors" class="level3" data-number="5.8.6"><h3 data-number="5.8.6" class="anchored" data-anchor-id="k-nearest-neighbors">
<span class="header-section-number">5.8.6</span> <span class="math inline">\(K\)</span>-Nearest Neighbors</h3>
<p>We will now perform KNN using the <code><a href="https://rdrr.io/pkg/class/man/knn.html">knn()</a></code> function, which is part of the <code>class</code> library. This function works rather differently from the other model-fitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, <code><a href="https://rdrr.io/pkg/class/man/knn.html">knn()</a></code> forms predictions using a single command. The function requires four inputs.</p>
<ul>
<li>A matrix containing the predictors associated with the training data, labeled <code>train.X</code> below.</li>
<li>A matrix containing the predictors associated with the data for which we wish to make predictions, labeled <code>test.X</code> below.</li>
<li>A vector containing the class labels for the training observations, labeled <code>train.Direction</code> below.</li>
<li>A value for <span class="math inline">\(K\)</span>, the number of nearest neighbors to be used by the classifier.</li>
</ul>
<p>We use the <code><a href="https://rdrr.io/r/base/cbind.html">cbind()</a></code> function, short for <em>column bind</em>, to bind the <code>Lag1</code> and <code>Lag2</code> variables together into two matrices, one for the training set and the other for the test set.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## install.packages("class")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">"class"</a></span><span class="op">)</span></span>
<span><span class="va">train.X</span>         <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">Lag1</span>, <span class="va">Lag2</span><span class="op">)</span><span class="op">[</span><span class="va">train</span>, <span class="op">]</span></span>
<span><span class="va">test.X</span>          <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">Lag1</span>, <span class="va">Lag2</span><span class="op">)</span><span class="op">[</span><span class="op">!</span><span class="va">train</span>, <span class="op">]</span></span>
<span><span class="va">train.Direction</span> <span class="op">&lt;-</span> <span class="va">Direction</span><span class="op">[</span><span class="va">train</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now the <code><a href="https://rdrr.io/pkg/class/man/knn.html">knn()</a></code> function can be used to predict the market’s movement for the dates in 2005. We set a random seed before we apply <code><a href="https://rdrr.io/pkg/class/man/knn.html">knn()</a></code> because if several observations are tied as nearest neighbors, then <code>R</code> will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">knn.pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">train.X</span>, <span class="va">test.X</span>, <span class="va">train.Direction</span>, k <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">## Test data confusion matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">knn.pred</span>, <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction_2005
knn.pred Down Up
    Down   43 58
    Up     68 83</code></pre>
</div>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data overall rate of correct classifications</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">knn.pred</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5</code></pre>
</div>
</div>
<p>The results using <span class="math inline">\(K=1\)</span> are not very good, since only <span class="math inline">\(50\%\)</span> of the observations are correctly predicted. Of course, it may be that <span class="math inline">\(K=1\)</span> results in an overly flexible fit to the data. Below, we repeat the analysis using <span class="math inline">\(K=3\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">knn.pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">train.X</span>, <span class="va">test.X</span>, <span class="va">train.Direction</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">## Test data confusion matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">knn.pred</span>, <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction_2005
knn.pred Down Up
    Down   48 54
    Up     63 87</code></pre>
</div>
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data overall rate of correct classifications</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">knn.pred</span> <span class="op">==</span> <span class="va">Direction_2005</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5357143</code></pre>
</div>
</div>
<p>The results have improved slightly. But increasing <span class="math inline">\(K\)</span> further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.</p>
<p>KNN does not perform well on the <code>Smarket</code> data, but generally, KNN does often provide impressive results.</p>
<p>As an example we will apply the KNN approach to the <code>Insurance</code> data set, which is part of the <code>ISLR2</code> library. This data set includes <span class="math inline">\(85\)</span> predictors that measure demographic characteristics for <span class="math inline">\(5822\)</span> individuals. The response variable is <code>Purchase</code>, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only <span class="math inline">\(6\%\)</span> of people purchased a caravan insurance.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Caravan</span><span class="op">)</span>     <span class="co"># sample size and number of variables in the Caravan dataset </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5822   86</code></pre>
</div>
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Caravan</span><span class="op">)</span>  <span class="co"># makes the variables in Caravan (e.g. Purchase) directly usable</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Purchase</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  No  Yes 
5474  348 </code></pre>
</div>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Empirical prior probability of 'Purchase = Yes' </span></span>
<span><span class="fl">348</span> <span class="op">/</span> <span class="fl">5822</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05977327</code></pre>
</div>
</div>
<p>Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the <em>distance</em> between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, <code>salary</code> and <code>age</code> (measured in dollars and years, respectively). As far as KNN is concerned, a difference of <span class="math inline">\(1000\)</span> in salary is enormous compared to a difference of <span class="math inline">\(50\)</span> years in age. Consequently, <code>salary</code> will drive the KNN classification results, and <code>age</code> will have almost no effect. This is contrary to our intuition that a salary difference of <span class="math inline">\(1000\)</span> is quite small compared to an age difference of <span class="math inline">\(50\)</span> years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured <code>salary</code> in Japanese yen, or if we measured <code>age</code> in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.</p>
<p>A good way to handle this problem is to the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> function does just this. In standardizing the data, we exclude column <span class="math inline">\(86\)</span>, because that is the qualitative <code>Purchase</code> variable.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">standardized.X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">Caravan</span><span class="op">[</span>, <span class="op">-</span><span class="fl">86</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">Caravan</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 165.0378</code></pre>
</div>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">Caravan</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1647078</code></pre>
</div>
<div class="sourceCode" id="cb129"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">standardized.X</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
<div class="sourceCode" id="cb131"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">standardized.X</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
</div>
<p>Now every column of <code>standardized.X</code> has a standard deviation of one and a mean of zero.</p>
<p>We now split the observations into a test set, containing the first <span class="math inline">\(1000\)</span> observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using <span class="math inline">\(K=1\)</span>, and evaluate its performance on the test data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb133"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">test</span>     <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span></span>
<span><span class="co">## Training data </span></span>
<span><span class="va">train.X</span>  <span class="op">&lt;-</span> <span class="va">standardized.X</span><span class="op">[</span><span class="op">-</span><span class="va">test</span>, <span class="op">]</span></span>
<span><span class="va">train.Y</span>  <span class="op">&lt;-</span> <span class="va">Purchase</span><span class="op">[</span><span class="op">-</span><span class="va">test</span><span class="op">]</span></span>
<span><span class="co">## Testing data </span></span>
<span><span class="va">test.X</span>   <span class="op">&lt;-</span> <span class="va">standardized.X</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span></span>
<span><span class="va">test.Y</span>   <span class="op">&lt;-</span> <span class="va">Purchase</span><span class="op">[</span><span class="va">test</span><span class="op">]</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">## KNN (K=1)</span></span>
<span><span class="va">knn.pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">train.X</span>, <span class="va">test.X</span>, <span class="va">train.Y</span>, k <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">## Test data overall classification error </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">test.Y</span> <span class="op">!=</span> <span class="va">knn.pred</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.118</code></pre>
</div>
<div class="sourceCode" id="cb135"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data overall classification error </span></span>
<span><span class="co">## of the no information classifier ("always No")</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">test.Y</span> <span class="op">!=</span> <span class="st">"No"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.059</code></pre>
</div>
</div>
<p>The vector <code>test</code> is numeric, with values from <span class="math inline">\(1\)</span> through <span class="math inline">\(1000\)</span>. Typing <code>standardized.X[test, ]</code> yields the submatrix of the data containing the observations whose indices range from <span class="math inline">\(1\)</span> to <span class="math inline">\(1000\)</span>, whereas typing <code>standardized.X[-test, ]</code> yields the submatrix containing the observations whose indices do <em>not</em> range from <span class="math inline">\(1\)</span> to <span class="math inline">\(1,000\)</span>.</p>
<p>The KNN overall test error rate on the <span class="math inline">\(1000\)</span> test observations is just under <span class="math inline">\(12\%.\)</span> At first glance, this may appear to be fairly good. However, since only <span class="math inline">\(6\%\)</span> of customers purchased insurance, we could get the error rate down to <span class="math inline">\(6\%\)</span> by always predicting <code>No</code> regardless of the values of the predictors!</p>
<p>Suppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, perhaps a salesperson must visit each potential customer.</p>
<p>If the company tries to sell insurance to a random selection of customers, then the success rate will be only <span class="math inline">\(6\%\)</span> (the empirical prior probability of <code>Purchase = Yes</code>), which may be far too low given the costs involved. Instead, the company would like to try to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest. Instead, the fraction of individuals that are correctly predicted to buy insurance is of interest.</p>
<p>It turns out that KNN with <span class="math inline">\(K=1\)</span> does far better than random guessing among the customers that are predicted to buy insurance. Among the P* <span class="math inline">\(=77\)</span> <code>Purchase=Yes</code> predictions, <span class="math inline">\(9\)</span> (TP), or <span class="math inline">\(11.7\%\)</span> (TP/P*), actually do purchase the insurance. This is double the rate that one would obtain from random guessing.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb137"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data confusion matrix (KNN, K=1)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">knn.pred</span>, <span class="va">test.Y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
knn.pred  No Yes
     No  873  50
     Yes  68   9</code></pre>
</div>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data positive predictive value TP/P*</span></span>
<span><span class="fl">9</span> <span class="op">/</span> <span class="op">(</span><span class="fl">68</span> <span class="op">+</span> <span class="fl">9</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1168831</code></pre>
</div>
</div>
<p>Using <span class="math inline">\(K=3\)</span>, the success rate increases to <span class="math inline">\(19\%,\)</span> and with <span class="math inline">\(K=5\)</span> the rate is <span class="math inline">\(26.7\%.\)</span> This is over four times the rate that results from random guessing. It appears that KNN is finding some real patterns in a difficult data set!</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## K = 3</span></span>
<span><span class="va">knn.pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">train.X</span>, <span class="va">test.X</span>, <span class="va">train.Y</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Test data confusion matrix (KNN, K=3)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">knn.pred</span>, <span class="va">test.Y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
knn.pred  No Yes
     No  920  54
     Yes  21   5</code></pre>
</div>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data positive predictive value TP/P*</span></span>
<span><span class="fl">5</span> <span class="op">/</span> <span class="fl">26</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1923077</code></pre>
</div>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## K = 5</span></span>
<span><span class="va">knn.pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">train.X</span>, <span class="va">test.X</span>, <span class="va">train.Y</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Test data confusion matrix (KNN, K=5)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">knn.pred</span>, <span class="va">test.Y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
knn.pred  No Yes
     No  930  55
     Yes  11   4</code></pre>
</div>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Test data positive predictive value TP/P*</span></span>
<span><span class="fl">4</span> <span class="op">/</span> <span class="fl">15</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2666667</code></pre>
</div>
</div>
<p>However, while this strategy is cost-effective, it is worth noting that only 15 customers are predicted to purchase insurance using KNN with <span class="math inline">\(K=5\)</span>. In practice, the insurance company may wish to expend resources on convincing more than just 15 potential customers to buy insurance.</p>
<p>As a comparison, we can also fit a logistic regression model to the data. If we use <span class="math inline">\(0.5\)</span> as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of <span class="math inline">\(0.5\)</span>. If we instead predict a purchase any time the predicted probability of purchase exceeds <span class="math inline">\(0.25\)</span>, we get much better results: we predict that 33 people will purchase insurance, and we are correct for about <span class="math inline">\(33\%\)</span> of these people. This is over five times better than random guessing!</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb149"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glm.fits</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Purchase</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Caravan</span>,</span>
<span>                 family <span class="op">=</span> <span class="va">binomial</span>, </span>
<span>                 subset <span class="op">=</span> <span class="op">-</span><span class="va">test</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
</div>
<div class="sourceCode" id="cb151"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glm.probs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">glm.fits</span>, <span class="va">Caravan</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Classifications (Bayes threshold)</span></span>
<span><span class="va">glm.pred</span>                 <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"No"</span>, <span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">glm.pred</span><span class="op">[</span><span class="va">glm.probs</span> <span class="op">&gt;</span> <span class="fl">.5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"Yes"</span></span>
<span></span>
<span><span class="co">## confusion matrix (test set)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">glm.pred</span>, <span class="va">test.Y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
glm.pred  No Yes
     No  934  59
     Yes   7   0</code></pre>
</div>
<div class="sourceCode" id="cb153"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Classifications (adjusted threshold)</span></span>
<span><span class="va">glm.pred</span>                  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"No"</span>, <span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">glm.pred</span><span class="op">[</span><span class="va">glm.probs</span> <span class="op">&gt;</span> <span class="fl">.25</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"Yes"</span></span>
<span></span>
<span><span class="co">## confusion matrix (test set)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">glm.pred</span>, <span class="va">test.Y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
glm.pred  No Yes
     No  919  48
     Yes  22  11</code></pre>
</div>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fl">11</span> <span class="op">/</span> <span class="op">(</span><span class="fl">22</span> <span class="op">+</span> <span class="fl">11</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3333333</code></pre>
</div>
</div>
<!-- ### Poisson Regression

Finally, we fit a  Poisson regression model to the `Bikeshare` data set, which measures the number of bike rentals (`bikers`) per hour in Washington, DC. The data can be found in the `ISLR2` library.






::: {.cell layout-align="center"}

```{.r .cell-code}
attach(Bikeshare)
dim(Bikeshare)
```

::: {.cell-output .cell-output-stdout}

```
[1] 8645   15
```


:::

```{.r .cell-code}
names(Bikeshare)
```

::: {.cell-output .cell-output-stdout}

```
 [1] "season"     "mnth"       "day"        "hr"         "holiday"   
 [6] "weekday"    "workingday" "weathersit" "temp"       "atemp"     
[11] "hum"        "windspeed"  "casual"     "registered" "bikers"    
```


:::
:::






We begin by fitting a least squares linear regression model to the data.





::: {.cell layout-align="center"}

```{.r .cell-code}
mod.lm <- lm(
    bikers ~ mnth + hr + workingday + temp + weathersit,
    data = Bikeshare
  )
summary(mod.lm)
```

::: {.cell-output .cell-output-stdout}

```

Call:
lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    data = Bikeshare)

Residuals:
    Min      1Q  Median      3Q     Max 
-299.00  -45.70   -6.23   41.08  425.29 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)                -68.632      5.307 -12.932  < 2e-16 ***
mnthFeb                      6.845      4.287   1.597 0.110398    
mnthMarch                   16.551      4.301   3.848 0.000120 ***
mnthApril                   41.425      4.972   8.331  < 2e-16 ***
mnthMay                     72.557      5.641  12.862  < 2e-16 ***
mnthJune                    67.819      6.544  10.364  < 2e-16 ***
mnthJuly                    45.324      7.081   6.401 1.63e-10 ***
mnthAug                     53.243      6.640   8.019 1.21e-15 ***
mnthSept                    66.678      5.925  11.254  < 2e-16 ***
mnthOct                     75.834      4.950  15.319  < 2e-16 ***
mnthNov                     60.310      4.610  13.083  < 2e-16 ***
mnthDec                     46.458      4.271  10.878  < 2e-16 ***
hr1                        -14.579      5.699  -2.558 0.010536 *  
hr2                        -21.579      5.733  -3.764 0.000168 ***
hr3                        -31.141      5.778  -5.389 7.26e-08 ***
hr4                        -36.908      5.802  -6.361 2.11e-10 ***
hr5                        -24.135      5.737  -4.207 2.61e-05 ***
hr6                         20.600      5.704   3.612 0.000306 ***
hr7                        120.093      5.693  21.095  < 2e-16 ***
hr8                        223.662      5.690  39.310  < 2e-16 ***
hr9                        120.582      5.693  21.182  < 2e-16 ***
hr10                        83.801      5.705  14.689  < 2e-16 ***
hr11                       105.423      5.722  18.424  < 2e-16 ***
hr12                       137.284      5.740  23.916  < 2e-16 ***
hr13                       136.036      5.760  23.617  < 2e-16 ***
hr14                       126.636      5.776  21.923  < 2e-16 ***
hr15                       132.087      5.780  22.852  < 2e-16 ***
hr16                       178.521      5.772  30.927  < 2e-16 ***
hr17                       296.267      5.749  51.537  < 2e-16 ***
hr18                       269.441      5.736  46.976  < 2e-16 ***
hr19                       186.256      5.714  32.596  < 2e-16 ***
hr20                       125.549      5.704  22.012  < 2e-16 ***
hr21                        87.554      5.693  15.378  < 2e-16 ***
hr22                        59.123      5.689  10.392  < 2e-16 ***
hr23                        26.838      5.688   4.719 2.41e-06 ***
workingday                   1.270      1.784   0.711 0.476810    
temp                       157.209     10.261  15.321  < 2e-16 ***
weathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***
weathersitlight rain/snow  -66.494      2.965 -22.425  < 2e-16 ***
weathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 76.5 on 8605 degrees of freedom
Multiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 
F-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16
```


:::
:::





Due to space constraints, we truncate the output of `summary(mod.lm)`.
In `mod.lm`, the first level of `hr` (0) and `mnth` (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines. For example, the Feb coefficient of $6.845$ signifies that, holding all other variables constant, there are on average about 7 more riders in February than in January. Similarly there are about 16.5 more riders in March than in January.

The results seen in Section 4.6.1 used a slightly different coding of the variables `hr` and `mnth`, as follows:





::: {.cell layout-align="center"}

```{.r .cell-code}
contrasts(Bikeshare$hr) = contr.sum(24)
contrasts(Bikeshare$mnth) = contr.sum(12)
mod.lm2 <- lm(
    bikers ~ mnth + hr + workingday + temp + weathersit,
    data = Bikeshare
  )
summary(mod.lm2)
```

::: {.cell-output .cell-output-stdout}

```

Call:
lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    data = Bikeshare)

Residuals:
    Min      1Q  Median      3Q     Max 
-299.00  -45.70   -6.23   41.08  425.29 

Coefficients:
                           Estimate Std. Error t value Pr(>|t|)    
(Intercept)                 73.5974     5.1322  14.340  < 2e-16 ***
mnth1                      -46.0871     4.0855 -11.281  < 2e-16 ***
mnth2                      -39.2419     3.5391 -11.088  < 2e-16 ***
mnth3                      -29.5357     3.1552  -9.361  < 2e-16 ***
mnth4                       -4.6622     2.7406  -1.701  0.08895 .  
mnth5                       26.4700     2.8508   9.285  < 2e-16 ***
mnth6                       21.7317     3.4651   6.272 3.75e-10 ***
mnth7                       -0.7626     3.9084  -0.195  0.84530    
mnth8                        7.1560     3.5347   2.024  0.04295 *  
mnth9                       20.5912     3.0456   6.761 1.46e-11 ***
mnth10                      29.7472     2.6995  11.019  < 2e-16 ***
mnth11                      14.2229     2.8604   4.972 6.74e-07 ***
hr1                        -96.1420     3.9554 -24.307  < 2e-16 ***
hr2                       -110.7213     3.9662 -27.916  < 2e-16 ***
hr3                       -117.7212     4.0165 -29.310  < 2e-16 ***
hr4                       -127.2828     4.0808 -31.191  < 2e-16 ***
hr5                       -133.0495     4.1168 -32.319  < 2e-16 ***
hr6                       -120.2775     4.0370 -29.794  < 2e-16 ***
hr7                        -75.5424     3.9916 -18.925  < 2e-16 ***
hr8                         23.9511     3.9686   6.035 1.65e-09 ***
hr9                        127.5199     3.9500  32.284  < 2e-16 ***
hr10                        24.4399     3.9360   6.209 5.57e-10 ***
hr11                       -12.3407     3.9361  -3.135  0.00172 ** 
hr12                         9.2814     3.9447   2.353  0.01865 *  
hr13                        41.1417     3.9571  10.397  < 2e-16 ***
hr14                        39.8939     3.9750  10.036  < 2e-16 ***
hr15                        30.4940     3.9910   7.641 2.39e-14 ***
hr16                        35.9445     3.9949   8.998  < 2e-16 ***
hr17                        82.3786     3.9883  20.655  < 2e-16 ***
hr18                       200.1249     3.9638  50.488  < 2e-16 ***
hr19                       173.2989     3.9561  43.806  < 2e-16 ***
hr20                        90.1138     3.9400  22.872  < 2e-16 ***
hr21                        29.4071     3.9362   7.471 8.74e-14 ***
hr22                        -8.5883     3.9332  -2.184  0.02902 *  
hr23                       -37.0194     3.9344  -9.409  < 2e-16 ***
workingday                   1.2696     1.7845   0.711  0.47681    
temp                       157.2094    10.2612  15.321  < 2e-16 ***
weathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***
weathersitlight rain/snow  -66.4944     2.9652 -22.425  < 2e-16 ***
weathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 76.5 on 8605 degrees of freedom
Multiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 
F-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16
```


:::
:::





What is the difference between the two codings?  In `mod.lm2`,  a coefficient estimate is reported for all but the last level of `hr` and `mnth`. Importantly, in `mod.lm2`, the coefficient estimate for the last level of `mnth` is not zero: instead, it equals the *negative of the sum of the coefficient estimates for all of the other levels*. Similarly, in `mod.lm2`, the coefficient estimate for the last level of `hr` is the negative of the sum of the coefficient estimates for all of the other levels. This means that the coefficients of `hr` and `mnth` in `mod.lm2` will always sum to zero, and can be interpreted as the difference from the mean level. For example, the coefficient for January of $-46.087$ indicates that, holding all other variables constant, there are typically 46 fewer riders in January relative to the yearly average.

It is important to realize that the choice of coding really does not matter, provided that we interpret the model output correctly in light of the coding used. For example, we see   that the predictions from the linear model are the same regardless of coding:





::: {.cell layout-align="center"}

```{.r .cell-code}
sum((predict(mod.lm) - predict(mod.lm2))^2)
```

::: {.cell-output .cell-output-stdout}

```
[1] 1.586608e-18
```


:::
:::





The sum of squared differences is zero. We can also see this using the `all.equal()` function:





::: {.cell layout-align="center"}

```{.r .cell-code}
all.equal(predict(mod.lm), predict(mod.lm2))
```

::: {.cell-output .cell-output-stdout}

```
[1] TRUE
```


:::
:::






To reproduce the left-hand side of Figure 4.13, we must first obtain the coefficient estimates associated with `mnth`. The coefficients for January through November can be obtained directly from the `mod.lm2` object. The coefficient for December must be explicitly computed as the negative sum of all the other months.





::: {.cell layout-align="center"}

```{.r .cell-code}
coef.months <- c(coef(mod.lm2)[2:12],
    -sum(coef(mod.lm2)[2:12]))
```
:::





To make the plot, we manually label the $x$-axis with the names of the months.





::: {.cell layout-align="center"}

```{.r .cell-code}
plot(coef.months, xlab = "Month", ylab = "Coefficient",
    xaxt = "n", col = "blue", pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A",
    "M", "J", "J", "A", "S", "O", "N", "D"))
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/chunk41-1.png){fig-align='center' width=672}
:::
:::





Reproducing the right-hand side of Figure 4.13 follows a similar process.





::: {.cell layout-align="center"}

```{.r .cell-code}
coef.hours <- c(coef(mod.lm2)[13:35],
    -sum(coef(mod.lm2)[13:35]))
plot(coef.hours, xlab = "Hour", ylab = "Coefficient",
    col = "blue", pch = 19, type = "o")
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/chunk42-1.png){fig-align='center' width=672}
:::
:::







Now, we consider instead fitting a Poisson regression model to the `Bikeshare` data. Very little changes, except that we now use the function `glm()` with the argument `family = poisson` to specify that we wish to  fit a Poisson regression model:





::: {.cell layout-align="center"}

```{.r .cell-code}
mod.pois <- glm(
    bikers ~ mnth + hr + workingday + temp + weathersit,
    data = Bikeshare, family = poisson
  )
summary(mod.pois)
```

::: {.cell-output .cell-output-stdout}

```

Call:
glm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    family = poisson, data = Bikeshare)

Coefficients:
                           Estimate Std. Error  z value Pr(>|z|)    
(Intercept)                4.118245   0.006021  683.964  < 2e-16 ***
mnth1                     -0.670170   0.005907 -113.445  < 2e-16 ***
mnth2                     -0.444124   0.004860  -91.379  < 2e-16 ***
mnth3                     -0.293733   0.004144  -70.886  < 2e-16 ***
mnth4                      0.021523   0.003125    6.888 5.66e-12 ***
mnth5                      0.240471   0.002916   82.462  < 2e-16 ***
mnth6                      0.223235   0.003554   62.818  < 2e-16 ***
mnth7                      0.103617   0.004125   25.121  < 2e-16 ***
mnth8                      0.151171   0.003662   41.281  < 2e-16 ***
mnth9                      0.233493   0.003102   75.281  < 2e-16 ***
mnth10                     0.267573   0.002785   96.091  < 2e-16 ***
mnth11                     0.150264   0.003180   47.248  < 2e-16 ***
hr1                       -0.754386   0.007879  -95.744  < 2e-16 ***
hr2                       -1.225979   0.009953 -123.173  < 2e-16 ***
hr3                       -1.563147   0.011869 -131.702  < 2e-16 ***
hr4                       -2.198304   0.016424 -133.846  < 2e-16 ***
hr5                       -2.830484   0.022538 -125.586  < 2e-16 ***
hr6                       -1.814657   0.013464 -134.775  < 2e-16 ***
hr7                       -0.429888   0.006896  -62.341  < 2e-16 ***
hr8                        0.575181   0.004406  130.544  < 2e-16 ***
hr9                        1.076927   0.003563  302.220  < 2e-16 ***
hr10                       0.581769   0.004286  135.727  < 2e-16 ***
hr11                       0.336852   0.004720   71.372  < 2e-16 ***
hr12                       0.494121   0.004392  112.494  < 2e-16 ***
hr13                       0.679642   0.004069  167.040  < 2e-16 ***
hr14                       0.673565   0.004089  164.722  < 2e-16 ***
hr15                       0.624910   0.004178  149.570  < 2e-16 ***
hr16                       0.653763   0.004132  158.205  < 2e-16 ***
hr17                       0.874301   0.003784  231.040  < 2e-16 ***
hr18                       1.294635   0.003254  397.848  < 2e-16 ***
hr19                       1.212281   0.003321  365.084  < 2e-16 ***
hr20                       0.914022   0.003700  247.065  < 2e-16 ***
hr21                       0.616201   0.004191  147.045  < 2e-16 ***
hr22                       0.364181   0.004659   78.173  < 2e-16 ***
hr23                       0.117493   0.005225   22.488  < 2e-16 ***
workingday                 0.014665   0.001955    7.502 6.27e-14 ***
temp                       0.785292   0.011475   68.434  < 2e-16 ***
weathersitcloudy/misty    -0.075231   0.002179  -34.528  < 2e-16 ***
weathersitlight rain/snow -0.575800   0.004058 -141.905  < 2e-16 ***
weathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 1052921  on 8644  degrees of freedom
Residual deviance:  228041  on 8605  degrees of freedom
AIC: 281159

Number of Fisher Scoring iterations: 5
```


:::
:::





We can plot the coefficients associated with `mnth` and `hr`, in order to reproduce Figure 4.15:





::: {.cell layout-align="center"}

```{.r .cell-code}
coef.mnth <- c(coef(mod.pois)[2:12],
    -sum(coef(mod.pois)[2:12]))
plot(coef.mnth, xlab = "Month", ylab = "Coefficient",
     xaxt = "n", col = "blue", pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D"))
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/chunk44-1.png){fig-align='center' width=672}
:::

```{.r .cell-code}
coef.hours <- c(coef(mod.pois)[13:35],
     -sum(coef(mod.pois)[13:35]))
plot(coef.hours, xlab = "Hour", ylab = "Coefficient",
    col = "blue", pch = 19, type = "o")
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/chunk44-2.png){fig-align='center' width=672}
:::
:::





We can once again use the `predict()` function to obtain the fitted values (predictions) from this Poisson regression model. However, we must use the argument `type = "response"` to specify that we want `R` to output $\exp(\hat\beta_0 + \hat\beta_1 X_1 + \ldots +\hat\beta_p X_p)$ rather than $\hat\beta_0 + \hat\beta_1 X_1 + \ldots + \hat\beta_p X_p$, which it will output by default.





::: {.cell layout-align="center"}

```{.r .cell-code}
plot(predict(mod.lm2), predict(mod.pois, type = "response"))
abline(0, 1, col = 2, lwd = 3)
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/chunk45-1.png){fig-align='center' width=672}
:::
:::





The predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative. As a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership.

In this section, we used the `glm()` function with the argument `family = poisson` in order to perform Poisson regression. Earlier in this lab we used the `glm()` function with `family = binomial` to perform logistic regression. Other choices for the `family` argument can be used to fit other types of GLMs. For instance, `family = Gamma` fits a gamma regression model. -->
</section></section><section id="exercises" class="level2" data-number="5.9"><h2 data-number="5.9" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">5.9</span> Exercises</h2>
<p>Prepare the following exercises of <strong>Chapter 2 and 4</strong> in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 7 (Chapter 2)</li>
<li>Exercise 1 (Chapter 4)</li>
<li>Exercise 6 (Chapter 4)</li>
<li>Exercise 13 (Chapter 4)</li>
<li>Exercise 15 (Chapter 4)</li>
<li>Exercise 16 (Chapter 4)</li>
</ul>
<!--
### Solutions

#### Exercise 7 (Chapter 2) {-} 

The table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Suppose we wish to use this data set to make a prediction for $Y$ when $X_1 = X_2 = X_3 = 0$ using K-nearest neighbors.

| Obs. |$X_1$|$X_2$|$X_3$| $Y$   |
|:----:|:---:|:---:|:---:|:-----:|
|  1   |  0  |  3  |  0  |  Red  |
|  2   |  2  |  0  |  0  |  Red  |
|  3   |  0  |  1  |  3  |  Red  |
|  4   |  0  |  1  |  2  | Green |
|  5   | −1  |  0  |  1  | Green |
|  6   |  1  |  1  |  1  |  Red  |


**7. a)** Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$.


**Answer:**





::: {.cell layout-align="center"}

```{.r .cell-code}
# Outcome
Y    <- c("red", "red", "red", "green", "green", "red")
# Predictor values
obs1 <- c( 0, 3, 0)
obs2 <- c( 2, 0, 0)
obs3 <- c( 0, 1, 3)
obs4 <- c( 0, 1, 2)
obs5 <- c(-1, 0, 1)
obs6 <- c( 1, 1, 1)

# Test Point
obs0 <- c(0, 0, 0)

# Create a Vector Dist_vec to store the results
Dist <- numeric(length = 6)

# Compute and store the Euclidean distances
Dist[1] <- sqrt(sum((obs1-obs0)^2)) 
Dist[2] <- sqrt(sum((obs2-obs0)^2)) 
Dist[3] <- sqrt(sum((obs3-obs0)^2)) 
Dist[4] <- sqrt(sum((obs4-obs0)^2)) 
Dist[5] <- sqrt(sum((obs5-obs0)^2)) 
Dist[6] <- sqrt(sum((obs6-obs0)^2))  

# Print the results
Dist
```

::: {.cell-output .cell-output-stdout}

```
[1] 3.000000 2.000000 3.162278 2.236068 1.414214 1.732051
```


:::
:::





**7. b)** What is your prediction with $K = 1$? Why?

**Answer:**





::: {.cell layout-align="center"}

```{.r .cell-code}
which.min(Dist)
```

::: {.cell-output .cell-output-stdout}

```
[1] 5
```


:::

```{.r .cell-code}
Y[which.min(Dist)]
```

::: {.cell-output .cell-output-stdout}

```
[1] "green"
```


:::
:::





Closest $K=1$ neighbor is `obs5` and thus, our prediction is `Green` because `Green` is the $Y$ value associated to `obs5`.

**7. c)** What is your prediction with $K = 3$? Why?

**Answer:**





::: {.cell layout-align="center"}

```{.r .cell-code}
order(Dist)[1:3]
```

::: {.cell-output .cell-output-stdout}

```
[1] 5 6 2
```


:::

```{.r .cell-code}
Y[order(Dist)[1:3]]
```

::: {.cell-output .cell-output-stdout}

```
[1] "green" "red"   "red"  
```


:::
:::





Closest $K=3$ neighbors are:

*  `obs5` with $Y=$`green`
*  `obs6` with $Y=$`red`
*  `obs2` with $Y=$`red`

This leads to the following local (at $X_1=X_2=X_3=0)$ relative frequencies: 

* for `green`: $1/K=1/3$
* for `red`: $2/K=2/3$

Thus, our prediction for $X_1=X_2=X_3=0$ is `Red`.

**7. d)** If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for $K$ to be large or small? Why?

**Answer:**

In the case of a highly nonlinear decision boundary, the neighborhoods of similar $Y$-values close to the boundary become generally small. Therefore, also $K$ must be chosen relatively small so that we can capture more of the non-linear decision boundary. 




### Exercise 1  (Chapter 4) {-}

Using a little bit of algebra, use (4.2) to achieve (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.

**Answer:** 

Equations (4.2) and (4.3) are the following:

$$ 
(4.2) \quad p(X) = \frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}\qquad\qquad
(4.3)  \quad  \frac {p(X)} {1 - p(X)} =e^{\beta_0 + \beta_1 X}
$$


Derivations:

$$
\begin{align*}
\frac {p(X)} {1 - p(X)} 
&= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}{1 - \frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}\\[2ex] 
&= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}{\frac {1 + e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}} - \frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}\\[2ex]  
&= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}{\frac {1} {1 + e^{\beta_0 + \beta_1 X}}} 
= e^{\beta_0 + \beta_1 X}
\end{align*}
$$

### Exercise 6  (Chapter 4) {-}

Suppose we collect data for a group of students in a statistics class with variables 

* $X_1=$`hours studied`,
* $X_2 =$`undergrad GPA` (GPA: Grade Point Average), and 
* $Y =$`receive an A`. 

We fit a logistic regression and produce estimated coefficients:

$$\hat{\beta}_0 = -6, \quad \hat{\beta}_1 = 0.05, \quad \hat{\beta}_2 = 1.$$

**6 a)** Estimate the probability that a student who studies for $40$ h and has an `undergrad GPA` of $3.5$ gets an `A` in the class.

**Answer:**

Remember from the previous exercise that:

$$ 
p(X) = \frac {\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}
             {1 + \exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}
\\
$$

Thus, the probability of $Y=1$ given $X=(x_1,x_2)$ with $x_1 =40$ (hours) and $x_2=3.5$ (GPA) yields:

$$ 
p(X) =  \frac {\exp(-6 + 0.05\cdot 40 + 3.5)} {1 + \exp(-6 + 0.05\cdot 40 + 3.5)} = \frac {\exp(-0.5)} {1 + \exp(-0.5)} = 37.75\%
$$

Calculations in `R`:




::: {.cell layout-align="center"}

```{.r .cell-code}
( exp(-0.5) )/( 1 + exp(-0.5) )
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.3775407
```


:::
:::






**6 b)** How many hours would the student in part (a) need to study to have a $50\%$ chance of getting an `A` in the class?

**Answer:**

Finding $x_1$, where $X = (x_1, 3.5)$, such that $p(X) = 0.5$ yields:

$$
\begin{align*}
0.50 &= \frac {\exp(-6 + 0.05 x_1 + 3.5)} {1 + \exp(-6 + 0.05 x_1 + 3.5)} \\[2ex]
\Leftrightarrow 0.50 (1 + \exp(-2.5 + 0.05\,x_1)) &= \exp(-2.5 + 0.05\,x_1)\\[2ex]
\Leftrightarrow 0.50 + 0.50 \exp(-2.5 + 0.05\,x_1)) &= \exp(-2.5 + 0.05\,x_1)\\[2ex]
\Leftrightarrow 0.50 &= 0.50 \exp(-2.5 + 0.05\,x_1)\\[2ex]
\Leftrightarrow \log(1) &= -2.5 + 0.05\,x_1 \\[2ex]
\Leftrightarrow x_1 &= 2.5 / 0.05 = 50 
\end{align*}
$$

Thus, on average, a student with an `undergrad GPA` of $3.5$ needs to study $50$ hours to have a $50\%$ chance of getting an `A`.

### Exercise 13 (Chapter 4) {-}

This question should be answered using the `Weekly` data set, which is part of the `ISLR2` package. This data is similar in nature to the `Smarket` data from this chapter’s lab, except that it contains $1,089$ weekly returns for $21$ years, from the beginning of $1990$ to the end of $2010$.

**13 a)** Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?

**Answer:**






::: {.cell layout-align="center"}

```{.r .cell-code}
# You may need to install first the following R-packages:
# install.packages("ISLR2")
# install.packages("MASS")
# install.packages("class")
# install.packages("e1071") # naiveBayes()

# Load the packages you need
library("ISLR2")

# Eliminates the need of referring to a variable 
# like 'Weekly$Year', and thus allows direct use of 'Year'
attach(Weekly) 
```

::: {.cell-output .cell-output-stderr}

```
The following objects are masked from Smarket (pos = 7):

    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year
```


:::

::: {.cell-output .cell-output-stderr}

```
The following objects are masked from Smarket (pos = 8):

    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year
```


:::

```{.r .cell-code}
# Use summary function to produce a numerical summary for each variable
summary(Weekly)
```

::: {.cell-output .cell-output-stdout}

```
      Year           Lag1               Lag2               Lag3         
 Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950  
 1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580  
 Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410  
 Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472  
 3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090  
 Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260  
      Lag4               Lag5              Volume            Today         
 Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747   Min.   :-18.1950  
 1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202   1st Qu.: -1.1540  
 Median :  0.2380   Median :  0.2340   Median :1.00268   Median :  0.2410  
 Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462   Mean   :  0.1499  
 3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373   3rd Qu.:  1.4050  
 Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821   Max.   : 12.0260  
 Direction 
 Down:484  
 Up  :605  
           
           
           
           
```


:::
:::

::: {.cell layout-align="center"}

```{.r .cell-code}
# Use cor function to produce a table of correlations for all variables 
# (excluding the non-numerical variable 'Direction')
round(cor(Weekly[,-9]), 2)
```

::: {.cell-output .cell-output-stdout}

```
        Year  Lag1  Lag2  Lag3  Lag4  Lag5 Volume Today
Year    1.00 -0.03 -0.03 -0.03 -0.03 -0.03   0.84 -0.03
Lag1   -0.03  1.00 -0.07  0.06 -0.07 -0.01  -0.06 -0.08
Lag2   -0.03 -0.07  1.00 -0.08  0.06 -0.07  -0.09  0.06
Lag3   -0.03  0.06 -0.08  1.00 -0.08  0.06  -0.07 -0.07
Lag4   -0.03 -0.07  0.06 -0.08  1.00 -0.08  -0.06 -0.01
Lag5   -0.03 -0.01 -0.07  0.06 -0.08  1.00  -0.06  0.01
Volume  0.84 -0.06 -0.09 -0.07 -0.06 -0.06   1.00 -0.03
Today  -0.03 -0.08  0.06 -0.07 -0.01  0.01  -0.03  1.00
```


:::
:::

::: {.cell layout-align="center"}

```{.r .cell-code}
# Use pairs function to produce pairwise scatter plots
pairs(Weekly)
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=672}
:::
:::





Yes, it appears that `Year` and `Volume` have a strong positive, but non-linear relationship.

**13 b)** Use the full data set to perform a logistic regression with `Direction` as the response and the five `Lag` variables plus `Volume` as predictors. Use the `summary()` function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
# Estimate a generalized linear regression model where the third input family is a description of the error distribution 
# and link function to be used in the model, supplied as the result of a call to a family function - here use binomial.
# Why binomial? Because our independent variable Direction takes two values.

glm_fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
              data   = Weekly,
              family = binomial)

# Use summary function to print the results
summary(glm_fit)
```

::: {.cell-output .cell-output-stdout}

```

Call:
glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
    Volume, family = binomial, data = Weekly)

Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept)  0.26686    0.08593   3.106   0.0019 **
Lag1        -0.04127    0.02641  -1.563   0.1181   
Lag2         0.05844    0.02686   2.175   0.0296 * 
Lag3        -0.01606    0.02666  -0.602   0.5469   
Lag4        -0.02779    0.02646  -1.050   0.2937   
Lag5        -0.01447    0.02638  -0.549   0.5833   
Volume      -0.02274    0.03690  -0.616   0.5377   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1496.2  on 1088  degrees of freedom
Residual deviance: 1486.4  on 1082  degrees of freedom
AIC: 1500.4

Number of Fisher Scoring iterations: 4
```


:::
:::





**Conclusion:** The predictor `Lag2` appears to have some statistical significance with a $p$-value smaller than $3\%$. (Ignoring issues due to multiple testing.)

**13 c)** Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
# Use predict function on results of previous regression in 10 b)
glm_probs <-  predict(glm_fit, type="response")

# Create a vector containing the string "Down" 
glm_pred <- rep("Down", times = length(glm_probs))

# Substitute "Down" for "Up", whenever the estimated probability is above 0.5
glm_pred[glm_probs > 0.5] <- "Up"

# Construct a summary table with the predictions against 
# the actual 'Direction'-values
table(glm_pred, Direction)
```

::: {.cell-output .cell-output-stdout}

```
        Direction
glm_pred Down  Up
    Down   54  48
    Up    430 557
```


:::

```{.r .cell-code}
contrasts(Weekly$Direction)
```

::: {.cell-output .cell-output-stdout}

```
     Up
Down  0
Up    1
```


:::
:::





Counts of the classification errors are found at the off-diagonal entries of the confusion matrix 

* Upper off-diagonal entry: Number of False Negatives 
* Lower off-diagonal entry: Number of False Positives

Counts of the correct classifications are found at the diagonal entries of the confusion matrix 

* Upper diagonal entry: Number of True Negatives
* Lower diagonal entry: Number of True Positives


**Possible Conclusions:**

- Percentage of correct predictions (TP+TN)/n: $(54+557)/(54+557+48+430) = 56.1\%$ 

- Percentage of false predictions (FP+FN)/n: $(48+430)/(54+557+48+430) = 43.9\%$ 

- During weeks when the market goes `Up`, the logistic regression is right about (True Pos. Rate TP/P) $557/(557+48) = 92.1\%$ of the time.

- During weeks when the market goes `Down`, the logistic regression is right about (True Neg. Rate TN/N) $54/(430+54) = 11.2\%$ of the time.


**Caution:** All these answers are with respect to the training errors - not the test errors. 

**13 d)** Now fit the logistic regression model using a training data period from $1990$ to $2008$, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from $2009$ and $2010$).

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
# generate condition for our training data
train = (Year < 2009)

# create data frame for the Weekly data from 2009 and 2010 
# (usage of ! to define the "opposite")
Weekly_0910 <- Weekly[!train,]

# run regression on the training data subset
glm_fit <- glm(Direction ~ Lag2,
               data   = Weekly,
               family = binomial,
               subset = train)

# create data frame
glm_probs <- predict(glm_fit, Weekly_0910, type="response")

# fill with our predictions
glm_pred                  <- rep("Down", length(glm_probs))
glm_pred[glm_probs > 0.5] <- "Up"

# construct confusion table using only the test data
Direction_0910 <- Direction[!train]
table(glm_pred, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
        Direction_0910
glm_pred Down Up
    Down    9  5
    Up     34 56
```


:::

```{.r .cell-code}
# compute the overall rate of correct predictions 
# in the test set 
mean(glm_pred == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.625
```


:::
:::





**13 e)** Repeat (d) using LDA.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
#call the packages you need
suppressPackageStartupMessages(library("MASS"))

# same approach as before but now using LDA method
lda_fit  <- lda(Direction ~ Lag2, data=Weekly, subset=train)
lda_pred <- predict(lda_fit, Weekly_0910)

# confusion table using only the test data
table(lda_pred$class, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
      Direction_0910
       Down Up
  Down    9  5
  Up     34 56
```


:::

```{.r .cell-code}
# compute the overall rate of correct predictions 
# in the test set 
mean(lda_pred$class == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.625
```


:::
:::





**13 f)** Repeat (d) using QDA.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
# same approach as before but now using QDA method
qda_fit   <- qda(Direction~Lag2, data=Weekly, subset=train)
qda_class <- predict(qda_fit, Weekly_0910)$class

# confusion table using only the test data
table(qda_class, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
         Direction_0910
qda_class Down Up
     Down    0  0
     Up     43 61
```


:::

```{.r .cell-code}
# compute the overall rate of correct predictions 
# in the test set 
mean(qda_class == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5865385
```


:::
:::





**13 g)** Repeat (d) using KNN with $K = 1.$

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
# call the package you need
library("class")

# same approach as before but now using KNN method with K=1
train_X         <- as.matrix(Lag2[train])
test_X          <- as.matrix(Lag2[!train])
train_Direction <- Direction[train]

# Note: If several observations are tied as nearest neighbors, 
# then R will randomly break the tie. 
# Setting a common seed guarantees that we get the same results 
set.seed(1)

# Caution: KNN prediction uses a different function
knn_pred <- knn(train_X, test_X, train_Direction, k=1)

# confusion table using only the test data
table(knn_pred, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
        Direction_0910
knn_pred Down Up
    Down   21 30
    Up     22 31
```


:::

```{.r .cell-code}
# compute the overall rate of correct predictions 
# in the test set 
mean(knn_pred == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5
```


:::
:::






**13 h)** Repeat (d) using naive Bayes. 

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
library("e1071")
nb_fit  <- naiveBayes(Direction ~ Lag2, 
                      data   = Weekly, 
                      subset = train)
nb_pred <- predict(nb_fit, Weekly_0910)

# confusion table using only the test data
table(nb_pred, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
       Direction_0910
nb_pred Down Up
   Down    0  0
   Up     43 61
```


:::

```{.r .cell-code}
# compute the overall rate of correct predictions 
# in the test set 
mean(nb_pred == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5865385
```


:::
:::






**13 i)** Which of these methods appears to provide the best results on this data?

**Answer:** 

Logistic regression and LDA have the largest rates of (overall) correct predictions on the held out test set.

**13 j)** Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for $K$ in the KNN classifier.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
# Logistic regression with Lag2:Lag1
glm_fit   <- glm(Direction~Lag2:Lag1, data=Weekly, family=binomial, subset=train)
glm_probs <- predict(glm_fit, Weekly_0910, type="response")
glm_pred  <- rep("Down", length(glm_probs))
glm_pred[glm_probs>.5] <- "Up"
Direction_0910 <- Direction[!train]
table(glm_pred, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
        Direction_0910
glm_pred Down Up
    Down    1  1
    Up     42 60
```


:::

```{.r .cell-code}
mean(glm_pred == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5865385
```


:::
:::

::: {.cell layout-align="center"}

```{.r .cell-code}
# LDA with Lag2 interaction with Lag1
lda_fit  <- lda(Direction ~ Lag2:Lag1, data=Weekly, subset=train)
lda_pred <- predict(lda_fit, Weekly_0910)
mean(lda_pred$class == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5769231
```


:::
:::

::: {.cell layout-align="center"}

```{.r .cell-code}
# QDA with sqrt(abs(Lag2))
qda_fit   <- qda(Direction~Lag2+sqrt(abs(Lag2)), data=Weekly, subset=train)
qda_class <- predict(qda_fit, Weekly_0910)$class
table(qda_class, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
         Direction_0910
qda_class Down Up
     Down   12 13
     Up     31 48
```


:::

```{.r .cell-code}
mean(qda_class == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5769231
```


:::
:::

::: {.cell layout-align="center"}

```{.r .cell-code}
# KNN k =10, as before KNN uses a different command
set.seed(1)
knn_pred <- knn(train_X, test_X, train_Direction, k=10)
table(knn_pred, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
        Direction_0910
knn_pred Down Up
    Down   17 21
    Up     26 40
```


:::

```{.r .cell-code}
mean(knn_pred == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5480769
```


:::
:::

::: {.cell layout-align="center"}

```{.r .cell-code}
# KNN k = 100
set.seed(1)
knn_pred <- knn(train_X, test_X, train_Direction, k=100)
table(knn_pred, Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
        Direction_0910
knn_pred Down Up
    Down   10 11
    Up     33 50
```


:::

```{.r .cell-code}
mean(knn_pred == Direction_0910)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.5769231
```


:::
:::





**Conclusion:** Out of these experiments, the original LDA and logistic regression have better performances in terms of overall correct prediction rates on the held out test set.


### Exercise 15 (Chapter 4) {-}

This problem involves writing functions.

**15 a)** Write a function, `Power()`, that prints out the result of raising 2 to the 3rd power. In other words, your function should compute $2^3$ and print out the results. Hint: Recall that `x^a` raises `x` to the power `a`. Use the `print()` function to output the result.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
Power <- function() {
  2^3
}
Power()
```

::: {.cell-output .cell-output-stdout}

```
[1] 8
```


:::
:::





**15 b)** Create a new function, `Power2()`, that allows you to pass any two numbers, `x` and `a`, and prints out the value of `x^a`. You can do this by beginning your function with the line `Power2 <- function (x,a){`. You should be able to call your function by entering, for instance, `Power2 (3,8)` on the command line. This should output the value of $38$, namely, $6,561$.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
Power2 <- function(x, a) {
  x^a
}
Power2(3, 8)
```

::: {.cell-output .cell-output-stdout}

```
[1] 6561
```


:::
:::





**15 c)** Using the `Power2()` function that you just wrote, compute $10^3$, $8^{17}$, and $131^3$.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
Power2(10, 3)
```

::: {.cell-output .cell-output-stdout}

```
[1] 1000
```


:::

```{.r .cell-code}
Power2(8, 17)
```

::: {.cell-output .cell-output-stdout}

```
[1] 2.2518e+15
```


:::

```{.r .cell-code}
Power2(131, 3)
```

::: {.cell-output .cell-output-stdout}

```
[1] 2248091
```


:::
:::





**15 d)** Now create a new function, `Power3()`, that actually returns the result `x^a` as an R object, rather than simply printing it to the screen. That is, if you store the value `x^a` in an object called result within your function, then you can simply `return()` this result, using the following line: `return(result)`. This should be the last line in your function, before the `}` symbol.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
Power3 <- function(x, a) {
  result <- x^a
  return(result)
}
```
:::





**15 e)** Now using the `Power3()` function, create a plot of f(x) = $x^2$. The x-axis should display a range of integers from 1 to 10, and the y-axis should display $x^2$. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using `log="x"`, `log="y"`, or `log="xy"` as arguments to the `plot()` function.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
x <- 1:10
par(mfrow=c(1,3))# graph parameters (three plots in one column)
plot(x = x, y = Power3(x, 2), type="b", log="x", 
     ylab = expression(x^2), 
     xlab = expression(x),
     main = "log-transformed\nx-axis")
plot(x = x, y = Power3(x, 2), type="b", log="y", 
     ylab = expression(x^2), 
     xlab = expression(x),
     main = "log-transformed\ny-axis")
plot(x = x, y = Power3(x, 2), type="b", log="xy", 
     ylab = expression(x^2), 
     xlab = expression(x),
     main = "log-transformed\nx and y-axis")
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=672}
:::

```{.r .cell-code}
par(mfrow=c(1,1))# reset graphic parameters
```
:::





**15 f)** Create a function, `PlotPower()`, that allows you to create a plot of `x` against `x^a` for a fixed `a` and for a range of values of `x`. For instance, if you call `PlotPower (1:10 ,3)` then a plot should be created with an x-axis taking on values $1, 2, \dots , 10$, and a y-axis taking on values $1^3$, $2^3$, . . . , $10^3$.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
PlotPower = function(x, a) {
  ylab_text <- bquote('x'^.(a)) # write y-axis label
  plot(x = x, y = Power3(x, a), type = "b",
       ylab = ylab_text)
}
PlotPower(1:10, 3)
```

::: {.cell-output-display}
![](Ch5_Classification_files/figure-html/unnamed-chunk-40-1.png){fig-align='center' width=672}
:::
:::





### Exercise 16 (Chapter 4) {-}

Using the `Boston` data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.

**Answer:** 





::: {.cell layout-align="center"}

```{.r .cell-code}
attach(Boston)

crime01                      <- rep(0, length(crim))
crime01[crim > median(crim)] <- 1

Boston <- data.frame(Boston, crime01)

train <- 1:(dim(Boston)[1]/2)
test  <- (dim(Boston)[1]/2+1):dim(Boston)[1]

Boston.train <- Boston[train,]
Boston.test  <- Boston[test,]
crime01.test <- crime01[test]
```
:::

::: {.cell layout-align="center"}

```{.r .cell-code}
# logistic regression of crime01 on all predictors 
# except 'crime01' and 'crim'
glm_fit <- glm(crime01 ~ . -crime01 - crim, 
              data   = Boston, 
              family = binomial, 
              subset = train)
```

::: {.cell-output .cell-output-stderr}

```
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```


:::

```{.r .cell-code}
glm_probs <- predict(glm_fit, Boston.test, type="response")

glm_pred                  <- rep(0, length(glm_probs))
glm_pred[glm_probs > 0.5] <- 1

mean(glm_pred != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.1818182
```


:::
:::





**Conclusion:** This logistic regression has a test error rate of $18.2\%$.





::: {.cell layout-align="center"}

```{.r .cell-code}
glm_fit   <- glm(crime01 ~ . -crime01 -crim -chas -tax, 
              data=Boston, family=binomial, subset=train)
```

::: {.cell-output .cell-output-stderr}

```
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```


:::

```{.r .cell-code}
glm_probs <- predict(glm_fit, Boston.test, type="response")
##
glm_pred                  <- rep(0, length(glm_probs))
glm_pred[glm_probs > 0.5] <- 1
mean(glm_pred != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.1857708
```


:::
:::





**Conclusion:** This logistic regression has a test error rate of $18.6\%$.






::: {.cell layout-align="center"}

```{.r .cell-code}
# LDA
lda_fit  <- lda(crime01~.-crime01-crim, data=Boston, subset=train)
lda_pred <- predict(lda_fit, Boston.test)
mean(lda_pred$class != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.1343874
```


:::
:::





**Conclusion:** This LDA has a test error rate of $13.4\%$.






::: {.cell layout-align="center"}

```{.r .cell-code}
lda_fit  <- lda(crime01~.-crime01-crim-chas-tax, data=Boston, subset=train)
lda_pred <- predict(lda_fit, Boston.test)
mean(lda_pred$class != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.1225296
```


:::
:::





**Conclusion:** This LDA has a test error rate of $12.3\%$.






::: {.cell layout-align="center"}

```{.r .cell-code}
lda_fit  <- lda(crime01~.-crime01-crim-chas-tax-lstat-indus-age,
              data=Boston, subset=train)
lda_pred <- predict(lda_fit, Boston.test)
mean(lda_pred$class != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.1185771
```


:::
:::





**Conclusion:** This LDA has a test error rate of $11.9\%$.





::: {.cell layout-align="center"}

```{.r .cell-code}
# KNN
library(class)
train_X <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black,
                lstat, medv)[train,]
test_X  <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black,
                lstat, medv)[test,]

train.crime01 <- crime01[train]

set.seed(1)
# KNN(k=1)
knn_pred  <- knn(train_X, test_X, train.crime01, k=1)
##
mean(knn_pred != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.458498
```


:::
:::





**Conclusion:** This KNN prediction has a test error rate of $45.8\%$.





::: {.cell layout-align="center"}

```{.r .cell-code}
# KNN(k=10)
set.seed(1)
knn_pred <- knn(train_X, test_X, train.crime01, k=10)
mean(knn_pred != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.1106719
```


:::
:::





**Conclusion:** This KNN prediction has a test error rate of $11.1\%$.





::: {.cell layout-align="center"}

```{.r .cell-code}
# KNN(k=100)
set.seed(1)
knn_pred = knn(train_X, test_X, train.crime01, k=100)
mean(knn_pred != crime01.test)
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.486166
```


:::
:::





**Conclusion:** This KNN prediction has a test error rate of $48.6\%$.


**Overall conclusion:** The best models are the ones with the smaller test error rates. In our case, this means that the smallest (fewest predictors) LDA-model and the KNN prediction with `K=10` are the best prediction models.


-->

</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./Ch4_LinearRegression.html" class="pagination-link" aria-label="Linear Regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch6_ResamplingMethods.html" class="pagination-link" aria-label="Resampling Methods">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>