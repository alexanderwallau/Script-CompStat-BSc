# Linear Regression 


> Additional Reading: Chapter 3 of our course textbook [An Introduction to Statistical Learning](https://www.statlearning.com/) 



## Assumptions {#sec-LinModAssumptions}

The (multiple) linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:

**Assumption 1: Model and Sampling**

**Part (a): Linear Model**

$$
\begin{align}
  Y_i= \underbrace{\sum_{k=0}^p\beta_k X_{ik}}_{=f(X_i)}+\epsilon_i, \quad i=1,\dots,n,
\end{align}
$${#eq-LinMod}
where 
$$
X_{i0}=1
$$ 
for all $i=1,\dots,n.$ 

* $Y_i$ is called "dependent variable" or "outcome variable" or "regressand" or 
* $X_{ik}$ is called the $k$th "predictor variable" or "regressor" or "explanatory variable" or "control variable." Each of these names emphasizes a slightly different perspective on $X_{ik}.$

It is convenient to write @eq-LinMod using matrix notation
$$
\begin{eqnarray*}
  Y_i&=&\underset{(1\times (p+1))}{X_i'}\underset{((p+1)\times 1)}{\beta} +\epsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
$$
where 
$$
  X_i=\left(\begin{matrix}X_{i0}\\ \vdots\\  X_{ip}\end{matrix}\right)
  \quad\text{and}\quad 
\beta=\left(\begin{matrix}\beta_0\\ \vdots\\ \beta_p\end{matrix}\right).
$$ 
Stacking all individual rows $i=1,\dots,n$ leads to
$$
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&=&\underset{(n\times (p+1))}{X}\underset{((p+1)\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon},
\end{eqnarray*}
$$
where 
$$
\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{10}&\dots&X_{1(p+1)}\\\vdots&\ddots&\vdots\\ X_{n0}&\dots&X_{n(p+1)}\\\end{matrix}\right),\quad\text{and}\quad \epsilon=\left(\begin{matrix}\epsilon_1\\ \vdots\\ \epsilon_n\end{matrix}\right).
\end{equation*}
$$

::: {.callout-tip}

# Simple Linear Regression and Polynomial Regression Model 

The special case of $p=1$
$$
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i
$$
is called the ***simple* linear regression model**. With the simple linear regression model, only straight line fits are possible. 

By contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define
$$
X_{i2} := X_{i1}^2
$$
which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor `Age`$_i=X_{i1}$)
$$
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i1}^2 + \epsilon_i.
$$
Of course, further predictor variables $X_{i2},\dots,X_{ip}$ can (and should) be added to this model. 

The same logic applies to polynomials with higher polynomial degrees $(\geq 2).$ Large polynomial degrees, however, can lead to unstable estimation results. 
:::


::: {.callout-note}
The assumption $f(X_i) = X_i'\beta$ may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.  
:::


**Part (b): Random Sample**

We assume that the observed data points 
$$
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{n(p+1)}))
$$ 
are a realization of the **training data random sample** 
$$
((Y_{1},X_{10},\dots,X_{1(p+1)}),\dots,(Y_{n},X_{n0},\dots,X_{n(p+1)})).
$$ 

That is, the $i$th observed $p+2$ dimensional data point 
$$
(y_{i},x_{i0},\dots,x_{i(p+1)})\in\mathbb{R}^{p+2}
$$ 
is a realization of a $p+2$ dimensional random variable 
$$
(Y_{i},X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2},
$$ 
where 

1. $(Y_{i},X_{i0},\dots,X_{i(p+1)})$ has the identical $p+2$ dimensional distribution for all $i=1,\dots,n.$ 
2. $(Y_{i},X_{i0},\dots,X_{i(p+1)})$ is independent of 
$(Y_{j},X_{j0},\dots,X_{j(p+1)})$ for all $i\neq j=1,\dots,n.$

::: {.callout-note}
Due to @eq-LinMod, this i.i.d. assumption is equivalent to assuming that the multivariate random variables 
$$
(\epsilon_i,X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2}
$$ 
are i.i.d. across $i=1,\dots,n$. 
:::

::: {.callout-caution}
**Remark:** Often, we do not use a different notation for observed realizations 
$(y_{i},x_{i0},\dots,x_{i(p+1)})\in\mathbb{R}^{p+2}$ 
and for the corresponding random variable 
$(Y_{i},X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2}$ 
since often both interpretations (random variable and its realizations) can make sense 
in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.
:::



**Assumption 2: Exogeneity**
$$
E(\epsilon_i|X_i)=0,\quad i=1,\dots,n
$${#eq-assExogen}

This assumption demands that the mean of the random error term $\epsilon_i$ is zero irrespective of the realizations of $X_i$. This exogeneity assumption is also called

* "orthogonality assumption" or 
* "mean independence assumption."

::: {.callout-note}
Together with the random sample assumption (Assumption 1, Part (b)) @eq-assExogen even implies **strict exogeneity** 
$$
E(\epsilon|X) = \underset{(n\times 1)}{0},
$$ 
since we have independence across $i=1,\dots,n$.  Under strict exogeneity, the mean of the random error **vector** $\epsilon\in\mathbb{R}^n$ is zero irrespective of the realizations of the $(n\times (p+1))$-dimensional random predictor matrix $X.$ 
:::


::: {.callout icon="false"}

# Example: Independence between error term and predictors

Let

* $E(\epsilon_i)=0$ and
* $\epsilon_i$ be independent of $X_i$

Here the assumption of exogeneity is fulfilled since by the independence between $\epsilon_i$ and $X_i$ we have that
$$
E(\epsilon_i|X_i) = E(\epsilon_i) 
$$
and by assumption $E(\epsilon_i)=0$ such that 
$$
E(\epsilon_i|X_i) = 0.
$$

Note: The assumption $E(\epsilon_i)=0$ is not critical (i.e. not restrictive) due to the intercept term in @eq-assExogen. 
:::


::: {.callout icon="false"}

# Example: Heteroskedastic Error

Let

* $\epsilon_i\sim\mathcal{N}(0,\sigma_i^2),$ where 
* $\sigma_i = |X_{i1}|$ 

Here the assumption of exogeneity is fulfilled since realizations of $X_i$ do not affect the mean of $\epsilon_i,$ thus
$$
E(\epsilon_i|X_i) = 0. 
$$

However, $\epsilon_i$ and $X_i$ are not independent of each other, since the conditional variance of $\epsilon_i$ is a function of $X_{i1}$
$$
Var(\epsilon_i|X_i) = |X_{i1}|^2.
$$
:::



**Assumption 3: Rank Condition (no perfect multicollinearity)**

$$
\begin{align*}
\operatorname{rank}(X)&=(p+1)\quad\text{a.s.}\\
\Leftrightarrow P\big(\operatorname{rank}(X)&=(p+1)\big)=1
\end{align*}
$$
This assumption demands that, with probability one, no predictor variable $X_{k}\in\mathbb{R}^n$ is linearly dependent of the others. (This is the literal translation of the "almost surely (a.s.)" concept.) 


**Note:** The assumption implies that $n\geq (p+1),$ since 
$$
\operatorname{rank}(X)\leq \min\{n,(p+1)\}\quad(a.s.)
$$

This rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation).  The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables' individual effects on $Y$. Therefore, this assumption is also often called an **identification assumption**.


::: {.callout-tip}

* Under Assumption 3, we have that 
$$
\operatorname{rank}(X)=(p+1)\quad\text{(a.s.)}
$$ 

* This implies that the $((p+1)\times (p+1))$-dimensional matrix $X'X$ has full rank, i.e. that 
$$
\operatorname{rank}(X'X)=(p+1)\quad\text{(a.s.)}
$$

* Thus $(X'X)$ is invertible; i.e. there exists a $((p+1)\times (p+1))$-dimensional matrix $(X'X)^{-1}$ such that 
$$
(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_{(p+1)}.
$$ 
:::



**Assumption 4: Error distribution** 

There are different more or less restrictive assumptions.  Some of the most common ones are the following:

- **Conditional distribution with sufficiently many moments:** 
$$
\epsilon_i|X_i \sim f_{\epsilon|X}
$$ 
for all $i=1,\dots,n$ and for any distribution $f_{\epsilon|X}$ with two (or more) finite moments.

- **Conditional normal distribution:** 
$$
\epsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))
$$ 
for all $i=1,\dots,n$.

- **Independence between error and predictors:** $\epsilon_i\sim f_\epsilon$ for all $i=1,\dots,n$ such that $f_\epsilon=f_{\epsilon|X}$ and such that $f_\epsilon$ has two (or more) finite moments.  

- **Independence between error and predictors and normality:** As above, but with $f_\epsilon=\mathcal{N}(0,\sigma^2)$. 

- **Spherical errors:** The conditional distributions of  $\epsilon_i|X_i$ may generally depend on $X_i$ for all $i=1,\dots,n,$ but only such that
$$
E(\epsilon|X)=\underset{(n\times 1)}{0}
$$
and 
$$
\begin{align*}
&\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex] 
& = \left(\begin{matrix}
Var(\epsilon_1|X)&Cov(\epsilon_1,\epsilon_2|X)&\dots&Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&Var(\epsilon_2|X)&\dots&Cov(\epsilon_2,\epsilon_n|X)\\
\vdots&\vdots&\ddots&\vdots\\
Cov(\epsilon_n,\epsilon_1|X)&Cov(\epsilon_n,\epsilon_2|X)&\dots&Var(\epsilon_n|X)
\end{matrix}\right)\\[2ex]
& = \left(\begin{matrix}
\sigma^2&0&\dots&0\\
0&\sigma^2&\dots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\dots&\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
$$
where $I_n$ denotes the $(n\times n)$ identity matrix with ones on the diagonal and zeros else.</br> 
Thus, under the spherical errors assumption, one has, for all possible realizations of $X$, that: 
   * **uncorrelated:** $Cov(\epsilon_i,\epsilon_j|X)=0$ for all $i=1,\dots,n$ and all $j=1,\dots,n$ such that $i\neq j$ 
   * **homoskedastic:** $Var(\epsilon_i|X)=\sigma^2$ for all $i=1,\dots,n$




All four Assumptions 1-4 must hold for doing inference using the (multiple) linear regression model.




#### Homoskedastic versus Heteroskedastic Error Terms {-}


The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between $\epsilon_i$ and $(X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+1}$. That is, the error term $\epsilon_i$ can have a conditional distribution which depends on $(X_{i0},\dots,X_{i(p+1)}).$ 


The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of $\epsilon_i$ is independent of $X_i$. Besides this, dependencies between $\epsilon_i$ and $X_{i0},\dots,X_{i(p+1)}$ are allowed. For instance, the variance of $\epsilon_i$ can be a function of $X_{i0},\dots,X_{i(p+1)}.$ If this is the case, $\epsilon_i$ is said to be **"heteroskedastic."** 

- **Heteroskedastic error terms:** The conditional variances 
$$
Var(\epsilon_i|X_i=x_i)=\sigma^2(x_i)
$$ 
are a non-constant function $\sigma^2(x_i)>0$ of the realizations $X_i=x_i.$ 




- **Homoskedastic error terms:** The conditional variances 
$$
Var(\epsilon_i|X_i=x_i)=\sigma^2
$$ 
are constant $\sigma^2>0$ for every possible realization $X_i=x_i.$ 






::: {.callout icon="false"}

# Example: Heteroskedastic Error 

$$
\epsilon_i|X_i\sim \mathcal{U}[-0.5|X_{i2}|, 0.5|X_{i2}|],
$$ 
with 
$$
X_{i2}\sim \mathcal{U}[-4,4]
$$ 
for all $i=1,\dots,n,$ where $\mathcal{U}[a,b]$ denotes the uniform distribution over $[a,b].$ 

This error term is mean independent of $X_i$ since $E(\epsilon_i|X_i)=0$, but it has a heteroskedastic conditional variance since 
$$
Var(\epsilon_i|X_i)=\frac{1}{12}X_{i2}^2
$$ 
depends on $X_{i2}.$
:::






::: {.callout icon="false"}

# Example: Homoskedastic Error

$$
\epsilon_i\sim{\mathcal N} (0, \sigma^2)
$$
for all $i=1,\dots,n.$ Here, the conditional variance of the error terms $\epsilon_i$ given $X_i$
$$
Var(\epsilon_i|X_i)=Var(\epsilon_i)=\sigma^2
$$ 
are equal to the constant $\sigma^2>0$ for all $i=1,\dots,n$ and for every possible realization of $X_i.$ 
::: 






## Deriving the Expression of the OLS Estimator

We derive the expression for the OLS estimator 
$$
\hat\beta=(\hat\beta_0,\dots,\hat\beta_p)'\in\mathbb{R}^{p+1}
$$ 
as the vector-valued minimizing argument of the sum of squared residuals, 
$$
\operatorname{RSS}(b)=\sum_{i=1}^n\big(\underbrace{Y_i-X_i'b}_{\text{$i$th residual}}\big)^2
$$ 
with $b\in\mathbb{R}^K$, for a given sample 
$$
((Y_1,X_1),\dots,(Y_n,X_n)).
$$  

Using matrix/vector notation we can write $S_n(b)$ as 
$$
\begin{align*}
\operatorname{RSS}(b)
&=\sum_{i=1}^n(Y_i-X_i'b)^2\\[2ex]
&=(Y-X b)^{\prime}(Y-X b)\\[2ex]
&=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
$$
To find the minimizing argument 
$$
\hat\beta = \arg\min_{b\in\mathbb{R}^{p+1}}\operatorname{RSS}(b)
$$ 
we compute the vector containing all partial derivatives
$$
\begin{align*}
\underset{((p+1)\times 1)}{\frac{\partial \operatorname{RSS}(b)}{\partial b}} &=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{align*}
$$
Setting each partial derivative to zero leads to $(p+1)$ linear equations ("normal equations") in $(p+1)$ unknowns. This linear system of equations defines the OLS estimates, $\hat{\beta}$, for a given dataset:
$$
\begin{align*}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)
&=\underset{((p+1)\times 1)}{0}\\[2ex]
X^{\prime} X\hat{\beta}
&=\underset{((p+1)\times 1)}{X^{\prime}Y}.
\end{align*}
$$
From our full rank assumption (Assumption 3) it follows that $X^{\prime}X$ is an invertible $((p+1)\times (p+1))$-dimensional matrix which allows us to solve the equation system by 
$$
\begin{align*}
\underset{((p+1)\times 1)}{\hat{\beta}} &=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{align*}
$$


The following codes computes the estimate $\hat{\beta}$ for a given dataset with $X_i\in\mathbb{R}^{p+1}$, $p=2$. 

```{r, fig.align="center"}
# Some given data
X_1     <- c(1.9,0.8,1.1,0.1,-0.1,4.4,4.6,1.6,5.5,3.4)
X_2     <- c(66, 62, 64, 61, 63, 70, 68, 62, 68, 66)
Y       <- c(0.7,-1.0,-0.2,-1.2,-0.1,3.4,0.0,0.8,3.7,2.0)
dataset <-  data.frame("X_1" = X_1, "X_2" = X_2, "Y" = Y)
## Compute the OLS estimation
lmobj   <- lm(Y ~ X_1 + X_2, data = dataset)
## Plot sample regression surface
library("scatterplot3d") # library for 3d plots
plot3d  <- scatterplot3d(x = X_1, y = X_2, z = Y,
            angle = 33, scale.y = 0.8, pch = 16,
            color ="red", 
            xlab = expression(X[1]),
            ylab = expression(X[2]),
            main ="OLS Regression Surface")
plot3d$plane3d(lmobj, lty.box = "solid", col=gray(.5), draw_polygon=TRUE)
```


#### Special Case: Simple Linear Regression Model {-}

For a given observed realization of the training data random sample 
$$
(x_1,y_1),\dots,(x_n,y_n)
$$
we choose $\hat\beta_0$ and $\hat\beta_1$ such that the **R**esidual **S**um of **S**quares criterion is minimized:
$$
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
& = e_1^2 + \dots + e_n^2\\[2ex]
&=\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2\\[2ex]
&=\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2
\end{align*}
$$
The minimizers are 
$$
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$
and
$$
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
$$
where 
$\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i$ 
and 
$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. 

![](images/Fig_3_1.png)

<!-- 
![](images/Fig_3_2.png) 
-->




#### Some Quantities of Interest {-}


**Predicted values and residuals.**

- The (OLS) **predicted values**: 
$$
\hat{Y}_i=X_i'\hat\beta, \quad i=1,\dots,n
$$ 
The $(n\times 1)$ vector of predicted values
$$
\begin{align*}
\hat{Y} = \left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)
&=X\hat{\beta}\\[-2ex]
&=\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&=P_X Y
\end{align*}
$$

- The (OLS) **residuals**: 
$$
e_i=Y_i-\hat{Y}_i, \quad i=1,\dots,n
$$ 
The $(n\times 1)$ vector of residuals
$$
\begin{align*}
e = 
\left(\begin{matrix}e_1\\e_2\\ \vdots\\ e_n\end{matrix}\right)
&=
\left(\begin{matrix}Y_1\\[.5ex]Y_2\\[.5ex] \vdots\\[.5ex] Y_n\end{matrix}\right)-
\left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)\\[2ex]
&=Y - \hat{Y}\\[2ex]
%&=Y - X\hat{\beta}\\[-2ex]
%&=Y - \underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&=Y - P_X Y\\[2ex]
&=\underbrace{(I_n - P_X)}_{=M_X} Y\\[2ex]
&=M_XY
\end{align*}
$$

**Projection matrices.** 

The matrix 
$$
P_X=X(X'X)^{-1}X'
$$ 
is the $(n\times n)$ **projection matrix** that projects any vector from $\mathbb{R}^n$ into the column space spanned by the column vectors of $X$ and 
$$
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
$$ 
is the associated $(n\times n)$ **orthogonal projection matrix** that projects any vector from $\mathbb{R}^n$ into the vector space that is orthogonal to that spanned by the column vectors of $X.$




## Assessing the Accuracy of the Model fit $\hat{f}$


The larger the proportion of the explained variance, the better is the fit of the estimated model $\hat{f}$ to the training data. This motivates the definition of the so-called $R^2$ coefficient of determination:
$$
\begin{eqnarray*}
R^2 
%&=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&=1-\frac{\operatorname{RSS}}{\operatorname{TSS}}
\end{eqnarray*}
$$
with 
$$
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat\beta)=\sum_{i=1}^n\left(y_i-x_i'\hat\beta\right)^2=\sum_{i=1}^ne_i^2
\end{align*}
$$
and
$$
\begin{align*}
\operatorname{TSS}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.
\end{align*}
$$


$\operatorname{TSS}$ "Total Sum of Squares"

$\operatorname{RSS}$ "Residual Sum of Squares"


* Obviously, we have that $0\leq R^2\leq 1$. 

* The closer $R^2$ lies to $1$, the better is the fit of the model to the observed training data. 


In tendency an accurate model has ...

* a low residual standard error $\operatorname{RSE}$
$$
\operatorname{RSE}=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}
$$

* a high $R^2$

$$
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
$$
where $0\leq R^2\leq 1.$



::: {.callout-caution}
**Cautionary Note Nr 1:** Do not forget that there is a **irreducible error** $Var(\epsilon)=\sigma^2>0$. Thus 

* very low $\operatorname{RSE}$ values $(\operatorname{RSE}\approx 0)$ and 
* very high $R^2$ values $(R^2\approx 1)$ 

can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with a multiple linear regression model.

**Cautionary Note Nr 2:** The $R^2$ and $\operatorname{RSE}$ are only based on **training data**. In @sec-SL, we have seen that a proper assessment of the model accuracy needs to take into account **test data**.
:::



#### $R^2$ and correlation coefficient {-}

In the case of the simple linear regression model, $R^2$ equals the squared sample correlation coefficient between $Y$ and $X$,
$$
R^2 = r_{yx_1}^2,
$$
where 
$$
r_{yx_1}=\frac{\sum_{i=1}^n(x_{i1}-\bar{x}_1)(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_{i1}-\bar{x}_1)^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}},
$$
where $\bar{x}_1=n^{-1}\sum_{i=1}^nx_{i1}.$

::: {.callout-tip}
In the multiple linear regression model $Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i,$ the $R^2$ equals the squared correlation between response and the fitted values: 
$$
R^2=r^2_{y\hat{y}}
$$
with 
$$
r_{y\hat{y}}=\frac{\sum_{i=1}^n(y_i-\bar{y})(\hat{y}_i-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}\sqrt{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}},
$$
where $\bar{y}=n^{-1}\sum_{i=1}^ny_{i}.$
:::



::: {.callout-caution}
* A high/low $R^2$ value only means that the predictors have high/low *predictive power* with respect to the training data. 

* A high/low $R^2$ does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.   
:::


The most often criticized disadvantage of the $R^2$ is that additional regressors (relevant or not) will increase the $R^2$. The below `R`-codes demonstrates this problem.

```{r}
set.seed(123)
n     <- 100                  # Sample size
X     <- runif(n, 0, 10)      # Relevant X variable
X_ir  <- runif(n, 5, 20)      # Irrelevant X variable
error <- rt(n, df = 10)*10    # True (usually unknown) error
Y     <- 1 + 5 * X + error    # Y variable
lm1   <- summary(lm(Y~X))     # Correct OLS regression 
lm2   <- summary(lm(Y~X+X_ir))# OLS regression with X_ir 
lm1$r.squared < lm2$r.squared
```
So, $R^2$ increases here even though `X_ir` is a completely irrelevant explanatory variable.  

Because of this, the $R^2$ cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called **adjusted** $R^2$, $\overline{R}^2,$ defined as
$$
\begin{eqnarray*}
  \overline{R}^2&=&1-\frac{\frac{1}{n-(p+1)}\sum_{i=1}^ne^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}\leq R^2%\\
\end{eqnarray*}
$$
The adjustment is in terms of the degrees of freedom $n-(p+1)$.


```{r}
round(lm1$adj.r.squared, digits = 3) # model without X_ir
round(lm2$adj.r.squared, digits = 3) # model with X_ir
```


## Assessing the Accuracy of the Coefficient Estimators $\hat{\beta}$


### Bias of $\hat{\beta}$


Under the Assumptions 1-4, once can show that the OLS estimator 
$$
\hat\beta = (X'X)^{-1}X'Y
$$
is unbiased, i.e.
$$
\operatorname{Bias}(\hat\beta) = E(\hat\beta) - \beta = \underset{((p+1)\times 1)}{0}.
$$
That is, on average $\hat\beta$ equals $\beta.$


This can be shown as following: 

Observe that 
$$
\hat\beta=(X'X)^{-1}X'Y
$$ 
consists of two multivariate random variables $X\in\mathbb{R}^{n\times(p+1)}$ and $Y\in\mathbb{R}^n.$ Thus one needs to show first the conditional unbiasedness of $\hat\beta$ given $X$ which effectively allows us to focus on randomness due to $\epsilon,$  
$$
\begin{align*}
\operatorname{Bias}(\hat\beta|X) 
&= E(\hat\beta|X)                                        - \beta \\[2ex]
&= E((X'X)^{-1}X'\underbrace{Y}_{=X\beta+\epsilon}|X) - \beta \\[2ex]
&= E((X'X)^{-1}X'(X\beta+\epsilon)|X)                 - \beta \\[2ex]
&= E(\underbrace{(X'X)^{-1}X'X}_{=I_K}\beta|X) + E((X'X)^{-1}X'\epsilon|X) - \beta \\[2ex]
&= \underbrace{E(\beta|X)}_{=\beta} + \underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=(X'X)^{-1}X'E(\epsilon|X)} - \beta \\[2ex]
&=  (X'X)^{-1}X'\underbrace{E(\epsilon|X)}_{=0} =\underset{(K\times 1)}{0}  
\end{align*}
$$
Thus $\hat\beta$ is unbiased conditionally on $X$ 
$$
\operatorname{Bias}(\hat\beta|X) = 0.
$$


From this if follows, by the iterated law of expectations, that the OLS estimator is also unconditionally unbiased, i.e.  
$$
\operatorname{Bias}(\hat\beta) = E\left(\operatorname{Bias}(\hat\beta|X)\right) = E(0) = 0.
$$


### Standard Error of $\hat{\beta}_j$

The standard error of $\hat{\beta}_j,$ for each $j=0,\dots,p,$ is given by
$$
\operatorname{SE}(\hat\beta_j|X)=\sqrt{Var(\hat\beta_j|X)},
$$
where 
$$
Var(\hat\beta_j|X) = \left[Var(\hat\beta|X)\right]_{(j,j)}
$$ 
denotes the $j$th diagonal element of the symmetric $(p+1)\times (p+1)$ variance-covariance matrix
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&=\begin{pmatrix}
Var(\hat\beta_0|X)&Cov(\hat\beta_0,\hat\beta_1|X)&\cdots&Cov(\hat\beta_0,\hat\beta_{p}|X)\\
Cov(\hat\beta_1,\hat\beta_0|X)&Var(\hat\beta_1|X)&  &Cov(\hat\beta_1,\hat\beta_{p}|X)\\
\vdots &&\ddots&\\
Cov(\hat\beta_p,\hat\beta_0|X)&Cov(\hat\beta_p,\hat\beta_1|X)&\cdots&Var(\hat\beta_{p}|X)\\
\end{pmatrix}
\end{align*}
$$

Thus, to compute a useful explicit expression for 
$$
\operatorname{SE}(\hat\beta_j|X)=?,
$$ 
we need to compute an explicit expression for the symmetric $(p+1)\times(p+1)$ variance-covariance matrix $Var(\hat\beta|X).$ 


Let us derive the general explicit expression for $Var(\hat\beta|X).$

Note that
$$
\begin{align*}
\hat{\beta}
&=(X'X)^{-1}X'Y\\[2ex]
&\text{Using that $Y=X\beta + \epsilon$:}\\[2ex]
&=(X'X)^{-1}X'(X\beta + \epsilon)\\[2ex]
&=\underbrace{(X'X)^{-1}X'X\beta}_{=\beta} + (X'X)^{-1}X'\epsilon
\end{align*}
$$
This leads to the so-called sampling error expression 
$$
\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon.
$$
With this, we can derive the general explicit expression for $Var(\hat\beta|X).$
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&=Var(\hat\beta - \beta|X)\\[2ex]
&\text{Using the sampling error expression:}\\[2ex]
&=Var((X'X)^{-1}X'\epsilon|X)\\[2ex]
&=E\Big[\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)\times\\[2ex]
&\phantom{=\Big(}\,\times\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)'|X\Big]\\[2ex]
&=E\left[((X'X)^{-1}X'\epsilon)((X'X)^{-1}X'\epsilon)'|X\right]\\[2ex]
&=E\left[(X'X)^{-1}X'\epsilon\epsilon' X(X'X)^{-1}|X\right]\\[2ex]
&=\;\;\;(X'X)^{-1}X'\underbrace{E\left(\epsilon\epsilon'|X\right)}_{=Var(\epsilon|X)}X(X'X)^{-1}
\end{align*}
$$
That is, the explicit expression for $Var(\hat\beta|X)$ depends on the explicit form of the symmetric $(n\times n)$ matrix $Var(\epsilon|X)$ 
$$
\begin{align*}
&Var(\epsilon|X)=\\[2ex]
&=\begin{pmatrix}
Var(\epsilon_1|X)&Cov(\epsilon_1,\epsilon_2|X)&\cdots&Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&Var(\epsilon_2|X)&  &Cov(\epsilon_2,\epsilon_n|X)\\
\vdots &&\ddots&\\
Cov(\epsilon_n,\epsilon_1|X)&Cov(\epsilon_n,\epsilon_2|X)&\cdots&Var(\epsilon_n|X)\\
\end{pmatrix}
\end{align*}
$$

The explicit form of the symmetric $(n\times n)$ matrix $Var(\epsilon|X)$ depends on our (hopefully correct) assumption on the error-term distribution (Assumption 4). 

#### Case of Spherical (Homoskedastic, Uncorrelated) Errors 

If 
$$
\begin{align*}
Var(\epsilon|X)
&=
\begin{pmatrix}
\sigma^2 & 0        & \cdots & 0\\
0        & \sigma^2 & \cdots & 0\\
\vdots   & \vdots   & \ddots & 0\\
0        & 0        & \cdots & \sigma^2\\
\end{pmatrix}
&=\sigma^2 I_n,
\end{align*}
$$
then 
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&=(X'X)^{-1}X' \left(\sigma^2 I_n \right) X(X'X)^{-1}\\[2ex]
&=\sigma^2\;(X'X)^{-1}X' \left( I_n \right) X(X'X)^{-1}\\[2ex]
&=\sigma^2\;\underbrace{(X'X)^{-1}X'X}_{I_{p+1}}\;(X'X)^{-1}\\[2ex]
&=\sigma^2\;(X'X)^{-1},
\end{align*}
$$
where the only unknown component is $\sigma^2=Var(\epsilon_i).$ 

We can estimate the homoskedastik error term variance $\sigma^2$ using the **R**esidual **S**tandard **E**rror:
$$
\begin{align*}
\hat\sigma = \operatorname{RSE}
&=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}\\[2ex]
&=\sqrt{ \frac{1}{n-(p+1)} \sum_{i=1}^n e_i^2}.
\end{align*}
$$

**Summing up:** 

In the case of spherical (homoskedastic and uncorrelated) error terms the standard error of $\beta_j$ is
$$
\operatorname{SE}(\beta_j|X) = \sqrt{\left[\sigma^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
$$
The above expression is the infeasible (since $\sigma^2$ is typically unknown) population version of the standard error. We can estimate this population version using the empirical standard error
$$
\widehat{\operatorname{SE}}(\beta_j|X) = \sqrt{\left[\hat{\sigma}^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
$$


This is the default version for computing the standard error in statistical software packages such as `R`. 

```{r}
set.seed(123)
n      <- 100                           # Sample size
X_1    <- runif(n, 0, 10)               # Predictor variable X_1
X_2    <- rnorm(n, -5, 2)               # Predictor variable X_2
error  <- rt(n, df = 10)*10             # True (usually unknown) error
Y      <- 1 + 5 * X_1 -5 * X_2 + error  # Y variable
lm_obj <- lm(Y ~ X_1 + X_2)             # OLS regression 

## Standard OLS output table:
summary(lm_obj)                         
```


#### Case of Heteroskedastic, but Uncorrelated Errors 


If 
$$
\begin{align*}
Var(\epsilon|X)
&=
\begin{pmatrix}
\sigma_1^2 & 0          & \cdots & 0\\
0          & \sigma_2^2 & \cdots & 0\\
\vdots     & \vdots     & \ddots & 0\\
0          & 0          & \cdots & \sigma_n^2\\
\end{pmatrix}
&=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2),
\end{align*}
$$
then 
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&=(X'X)^{-1} \left(X'\left(\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2) \right) X\right) (X'X)^{-1}\\[2ex]
&=(X'X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right) (X'X)^{-1}\\[2ex]
\end{align*}
$$
Thus, the symmetric $(p+1)\times(p+1)$ variance-covariance matrix $Var(\hat\beta|X)$ keeps its "sandwich form", where the inner part of the sandwich
$$
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right)
$$
is typically unknown, since $\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2$ are typically unknown. 


There are different, so-called **Heteroskedasticity Consistent (HC)** estimators to estimate the unknown expression 
$$
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right).
$$  


HC-Type  | Formular
------|------------
HC0   | $\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i'$
HC1   | $\sum_{i=1}^n\frac{n}{n-K}\hat\varepsilon_i^2X_iX_i'$
HC2   | $\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'$
HC3   | $\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'$
HC4    | $\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'$

HC3 is the most often used HC-estimator. 


::: {.callout-tip}
The statistic $h_i:=[P_X]_{ii}$ is called the **leverage statistic** of $X_i,$ where 

* $1/n\leq h_i\leq 1$ and 
* $\bar{h}=n^{-1}\sum_{i=1}^nh_i=K/n$. 

Observations $X_i$ with leverage statistics $h_i$ that greatly exceed the average leverage value $K/n$ are referred to as "high leverage" observations. High leverage observations $X_i$ are observations that are far away from all other observations $X_j$, $i\neq j=1,\dots,n.$ 

High leverage observations $X_i$ have the potential to distort the estimation results, $\hat\beta_n$. Indeed, a high leverage observation $X_i$ will have an distorting effect on the estimation results if the absolute value of the corresponding residual $|\hat{\varepsilon}_i|$ is unusually large---such observations are called **influential outliers**. Such observations increase the estimation uncertainty. 

General idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals $\hat\varepsilon_i$ belonging to $X_i$ values that have a large leverage $h_i$ receive a higher weight and thus increase the value of $\widehat{E}(\varepsilon^2_iX_iX_i').$ This strategy takes into account increased estimation uncertainties due to single influential outliers. 
:::


<!-- The estimator HC0 was suggested in the econometrics literature by @White1980 and is justified by asymptotic ($n\to\infty$) arguments. The estimators HC1, HC2 and HC3 were suggested by @MacKinnon_White_1985 to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, @Long_Ervin_2000 concludes that HC3 provides the best overall performance in finite samples. @Cribari_2004 suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large $h_i$ values). 
-->


```{r}
set.seed(123)
n      <- 100                           # Sample size
X_1    <- runif(n, 0, 10)               # Predictor variable X_1
X_2    <- rnorm(n, -5, 2)               # Predictor variable X_2
error  <- rt(n, df = 10) * abs(X_2)     # True (usually unknown) heteroskedastic error
Y      <- 1 + 5 * X_1 -5 * X_2 + error  # Y variable


## Package for computing robust variance estimations
library("sandwich") # vcovHC(), 

## Package for producing an OLS output table (etc.)
suppressMessages(library("lmtest")) # coeftest

## Estimate the linear regression model parameters
lm_obj      <- lm(Y ~ X_1 + X_2)

vcovHC3_mat <- sandwich::vcovHC(lm_obj, type="HC3")

lmtest::coeftest(lm_obj, vcov = vcovHC3_mat)

## Note: The HC3-Robust SE estimates are: 
round(sqrt(diag(vcovHC3_mat)), digits = 5)
```




## Exercises

Prepare the following exercises of Chapter 3 in our course textbook `ISLR`: 

- Exercise 1
- Exercise 2
- Exercise 3
- Exercise 8
- Exercise 9

<!-- {{< include Ch4_LinearRegression_Solutions.qmd >}} -->