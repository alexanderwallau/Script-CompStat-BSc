## Solutions

### Exercise 1  {-}

**1 a)** Describe the null hypotheses to which the $p$-values given in Table 3.4 correspond. 

![](DATA/Table.PNG)

**1 b)** Explain what conclusions you can draw based on these $p$-values. Your explanation should be phrased in terms of `sales`, `TV`, `radio`, and `newspaper`, rather than in terms of the coefficients of the linear model.


**Answers:**

**1 a)** In Table 3.4, the null hypothesis for `TV` is that in the presence of `radio` ads and `newspaper` ads, `TV` ads have no effect on sales. Similarly, the null hypothesis for `radio` is that in the presence of `TV` ads and `newspaper` ads, `radio` ads have no effect on sales.

**1 b)** On the one hand, the low p-values of `TV` and `radio` allow us to reject the "no effect" null hypotheses for `TV` and `radio`. Hence, we believe that `TV` (`radio`) ads have an effect on `sales` in the presence of `radio` (`TV`) and `newspaper` ads.<br>
On the other hand, the high p-value of `newspaper` does *not* allow us to reject the "no effect" null-hypothesis. This constitutes an *inconclusive result* and only says that the possible effects of `newspaper` ads are not large enough to stand out from the estimation errors. 

**Remember:** An insignificant hypothesis test result is never informative about whether the tested null hypothesis is true. We do not have an error-control for falsely accepting the null-hypothesis, i.e. for Type-II-errors. We only have an error-control (by the significance level) for falsely rejecting the null-hypothesis, i.e. for Type-I-errors.


### Exercise 2  {-}

Carefully explain the main difference between the KNN classifier and KNN regression methods.

**Answer:**

KNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for $Y$ (qualitative), given a certain predictor $x_0$, where as the output for a KNN regression predicts the quantitative value for $f(x_0)$, given a certain predictor $x_0$.

### Exercise 3 {-}

Suppose we have a data set with five predictors:

$X_1 =GPA$

$X_2 = IQ$

$X_3 = Gender$ ($1$ for Female and $0$ for Male)

$X_4 =$ Interaction between $GPA$ and $IQ$

$X_5 =$ Interaction between $GPA$ and $Gender$

The response variable (in thousands of dollars) is defined as:

$Y =$ starting salary after graduation

Suppose we use least squares to fit the model, and get: 

$\hat{\beta}_0 = 50$, $\hat{\beta}_1 = 20$, $\hat{\beta}_2 = 0.07$, $\hat{\beta}_3 = 35$, $\hat{\beta}_4 = 0.01$, and $\hat{\beta}_5 = −10$.

Thus we have:

$$
\begin{align*}
&E[Y|X] = \\
& 50 + 20\,\overbrace{GPA}^{X_1} + 0.07\,\overbrace{IQ}^{X_2} + 35\,\overbrace{Gender}^{X_3} + 
 0.01\,\overbrace{GPA\cdot IQ}^{X_4=X_1\cdot X_2} - 10\,\overbrace{GPA\cdot Gender}^{X_5=X_1\cdot X_3}
\end{align*}
$$

**3 a)** Which answer is correct, and why?

 i) For a fixed value of $IQ$ and $GPA$, males earn more on average than females.
 ii) For a fixed value of $IQ$ and $GPA$, females earn more on average than males.
 iii) For a fixed value of $IQ$ and $GPA$, males earn more on average than females provided that the $GPA$ is high enough.
 iv) For a fixed value of $IQ$ and $GPA$, females earn more on average than males provided that the $GPA$ is high enough.


**Answer:**
Observe that:
$$
\begin{align*}
\text{Male\; $(X_3 = 0)$:}\quad   & 50 + 20 X_1 + 0.07 X_2 + \phantom{3}0 + 0.01\,(X_1 \cdot X_2) -0     \\[1.5ex]
\text{Female\; $(X_3 = 1)$:}\quad & 50 + 20 X_1 + 0.07 X_2 + 35 + 0.01(X_1 \cdot X_2) - 10\,X_1
\end{align*}
$$

Thus 3 a) iii. is correct, since once the $X_1=$`GPA` is high enough ($35-10\,X_1<0 \Leftrightarrow X_1>3.5$), males earn more on average.

**3 b)** Predict the salary of a female with `IQ` of 110 and a `GPA` of 4.0.

**Answer:**

```{r}
GPA    <-   4
IQ     <- 110
Gender <-   1 # female = 1
## Prediction
Y_hat  <- 50 + 20*GPA + 0.07*IQ + 35*Gender + 0.01*GPA*IQ - 10*GPA
Y_hat
```


**3 c)** True or false: Since the coefficient for the `GPA`$\times$`IQ` interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

**Answer:**

False. We must examine the $p$-value (or the $t$-statistic) of the regression coefficient to determine if the interaction term is statistically significant or not.

### Exercise 8 {-}

This question involves the use of simple linear regression on the `Auto` data set.

**8 a)** Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Use the `summary()` function to print the results. 

```{r}
library("ISLR2")

data("Auto")

# Perform linear regression
lmObj_1 <- lm(mpg ~ horsepower, data=Auto)

# Use summary function to print the results
summary(lmObj_1)
```

Comment on the output. For example:

**i)** Is there a relationship between the predictor and the response?

**Answer:**

Yes, there is. The predictor horsepower has a statistically significant ($p<0.001$) linear relationship with the response.

**ii)** How strong is the relationship between the predictor and the response?

**Answer:**

Statistical significance does not necessarily mean a practically strong or important relationship. 

To quantify the strength of the relationship between the predictor and the response, we can look at the following quantities: 

- Residual Standard Error (RSE) (estimate of the standard deviation of $\epsilon$) in comparison to the RSE of the trivial linear regression model with only an intercept.
- The $R^2$ Statistic (the proportion of variance explained by the model)
- The $F$-Statistic

The Residual Standard Error (RSE) of the regression model with `intercept` and `horsepower` as predictors is given by:

```{r}
## RSE of lm(mpg ~ horsepower):
RSS <- sum(resid(lmObj_1)^2)
n   <- length(resid(lmObj_1))
RSE <- sqrt(RSS/(n-2))
round(RSE, 3)

## Alternatively: 
round(summary(lmObj_1)$sigma, 3)
```

This RSE value is considerable smaller than the RSE of a model with only an intercept:

```{r}
lmObj_onlyIntercept <- lm(mpg ~ +1, data = Auto)
RSS_onlyIntercept   <- sum(resid(lmObj_onlyIntercept)^2)
n                   <- length(resid(lmObj_onlyIntercept))
RSE_onlyIntercept   <- sqrt(RSS_onlyIntercept/(n-1))
round(RSE_onlyIntercept, 3)
```

Thus, the larger model with `horsepower` included explains more of the variances in the response variable `mpg`. Including `horsepower` as a predictor reduces the RSE by `((RSE_onlyIntercept - RSE)/RSE_onlyIntercept)*100` %; i.e. by `r round(((RSE_onlyIntercept - RSE)/RSE_onlyIntercept)*100, 2)`%.

The $R^2$ value:

```{r}
round(summary(lmObj_1)$r.squared, 2)
```
shows that  $60\%$ of variability in $Y$ can be explained using an intercept and `horsepower` as predictors. 

The value of the $F$ statistic
```{r}
round(summary(lmObj_1)$fstatistic, 2)
```
is much larger than $1$ which means that the linear regression model with intercept and `horsepower` fits the data significantly better than the trivial regression model with only an intercept. 


**iii)** Is the relationship between the predictor and the response positive or negative?

**Answer:**

The relationship is negative, as we can see from the parameter estimate for `horsepower`

```{r}
coef(lmObj_1)[2]
```

**iv)** What is the predicted `mpg` associated with a `horsepower` of $98$? What are the associated $95\%$ confidence and prediction intervals?

**Answer:**

The predicted value plus confidence interval:

```{r}
# Horsepower of 98
new_df <- data.frame(horsepower = 98)

# confidence interval 
predict(object = lmObj_1, newdata = new_df, interval = "confidence")
```

The predicted value plus prediction interval:
```{r}
# Horsepower of 98
new_df <- data.frame(horsepower = 98)

# prediction interval
predict(object = lmObj_1, newdata = new_df, interval = "prediction")
```

**8 b)** Plot the response and the predictor. Use the `abline()` function to display the least squares regression line.

**Answer:**

```{r}
plot(x = Auto$horsepower, y = Auto$mpg, ylab = "MPG", xlab = "Horsepower")
abline(lmObj_1, col="blue")
legend("topright", 
       legend = c("(y,x)", expression(paste("(",hat(y),",x)"))), 
       pch=c(1,NA), lty=c(NA,1), col=c("black", "blue"))
```

**8 c)** Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

**Answer:**

```{r}
par(mfrow=c(2,2))
plot(lmObj_1, col='blue')
```

Looking at the smoothing line of the residuals ($e_i=y_i−\hat{y}_i$) vs. the fitted values ($\hat{y}_i$), there is a strong pattern in the residuals, indicating non-linearity. You can see evidence of this also in the scatter plot in the answer for question 8 b).

There also appears to be non-constant variance in the error terms (heteroscedasticity), but this may be corrected to an extent when trying a quadratic fit. If not, transformations such as $log(y)$ or $\sqrt{y}$ can shrink larger responses by a greater amount and reduce this issue.

There are some observations with large standardized residuals & high leverage (hence, high Cook’s Distance) that we need to review.

### Exercise 9 {-}

This question involves the use of multiple linear regression on the `Auto` data set.

**9 a)** Produce a scatterplot matrix which includes all of the variables in the data set.

**Answer:**

```{r}
library("ISLR2")

data("Auto")

# Produce scatterplot matrix
pairs(Auto)
```

**9 b)** Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the `name` variable, which is qualitative.

**Answer:**

```{r}
round(cor(subset(Auto, select = -name)), 1)
```

**9 c)** Use the `lm()` function to perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors. Use the `summary()` function to print the results. Comment on the output by answering the below questions 9 c i) to 9 c iii).

**Answer:**

```{r}
#| scrolled: true
# Perform multiplie linear regression
fit.lm <- lm(mpg ~ . -name, data=Auto)

# Print results
summary(fit.lm)
```

**9 c i)** Is there a relationship between the predictors and the response?

**Answer:**

Yes, there is a relationship between the predictors and the response. By testing the null hypothesis of whether all (except intercept) the regression coefficients are zero (i.e. H$_0$: $\beta_1=\dots=\beta_7=0$), we can see that the $F$-statistic is big and its $p$-value is close to zero, indicating evidence against the null hypothesis.

**9 c ii)** Which predictors appear to have a statistically significant relationship to the response?

**Answer:**

Looking at the $p$-values associated with each predictor's $t$-statistic, we see that `displacement`, `weight`, `year`, and `origin` have a statistically significant relationship, while `cylinders`, `horsepower`, and `acceleration` do not. 

**Caution:** This consideration neglects issues due to multiple testing. When testing at the significance level $\alpha=0.05$, then each single test has a type I error (false H$_0$ rejections) rate of up to $5\%$. These type I error rates accumulate since we consider seven hypothesis tests simultaneously, and thus the probability of seeing one type I error among the seven tests is up to $7\cdot 5\%=35\%$. So is quite likely to see one type I error.  

**Bonferroni correction for multiple testing:** To determine if any of the seven predictors is statistically significant, the corresponding $p$-value must be smaller than $\alpha/7$. For instance, with $\alpha/7=0.05/7\approx 0.007$, only `weight`, `year`, and `origin` have a statistically significant relationships to the response.


**9 c iii)** What does the coefficient for the `year` variable suggest?

**Answer:**

The regression coefficient for `year` suggests that, on average, one `year` later year-of-construction is associated with an  increased `mpg` by $0.75$, when holding every other predictor value constant. 

**9 d)** Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? 

**Answer:**

```{r, fig.height=20}
par(mfrow=c(4,1))
plot(fit.lm)
```

- The "Residuals vs Fitted" plot (1st plot) shows some systematic deviations of the residuals from $0$. 
The reason is that we are imposing a straight "line" (better hyper plane) fit for the conditional mean function $E[Y|X]=f(X)$ which appears non-linear here. This results in a systematic underestimation of the true conditional mean function for large and small fitted values $\hat{y}=\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p$. 

- The "Normal Q-Q" plot (2nd plot) suggests non-normally distributed residuals--particularly the upper tail deviates from that of a normal distribution. 


- The "Residuals vs Leverage" plot (3rd plot) shows that there are some potential outliers that we can see when: standardized residuals are below $-2$  or above $+2$. Moreover, the plot shows also potentially problematic "high-leverage" points with leverage values heavily exceeding the rule-of-thumb threshold $(p+1)/n=8/392=0.02$. All points with simultaneously high-leverages and large absolute standardized residuals should be handled with care since these may distort the estimation. 


- The "Scale-Location" plot (4th plot) shows is rather inconclusive about heteroscedasticity. However the "Residuals vs Fitted" plot (1st plot)shows some clear sign of heteroscedastic residuals.


**9 e)** Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

**Answer:**

Violating the hierarchy principle:

```{r}
fit.lm0 <- lm(mpg ~ horsepower+cylinders+year+weight:displacement, 
              data=Auto)
summary(fit.lm0)
```

Following the hierarchical principle:
```{r}
fit.lm1 <- lm(mpg~horsepower+cylinders+year+weight*displacement, 
              data=Auto)
summary(fit.lm1)
```


Note that there is a difference between using `A:B` and `A*B` when running a regression. While the first includes only the interaction term between the variable `A` and `B`, the second one also includes the stand-alone variables `A` and `B`. 


Generally, you should follow the hierarchical principle for interaction effects: If we include an interaction in a model, we should also include the main effects, even if the $p$-values associated with their coefficients are not significant.


**9 f)**

Try a few different transformations of the variables, such as $\log(X)$, $\sqrt{X}$, $X^2$. Comment on your findings.

**Answer:**

```{r, fig.height=20}
fit.lm2 <- lm(mpg~log(weight)+sqrt(horsepower)+
                acceleration+I(acceleration^2),
              data=Auto)
summary(fit.lm2)
##
par(mfrow=c(4,1))
plot(fit.lm2)
```

This try suffers basically from the same issues as the model considered in 9 d)


Let's consider again the model with all predictors (except `name`), but with transforming the outcome variable `mpg` by a $\log$-transformation.

```{r, fig.height=20}
fit.lm3 <-lm(log(mpg)~ . -name, data=Auto)
summary(fit.lm3)
##
par(mfrow=c(4,1))
plot(fit.lm3)
```

This model specification is much better! 

- No clear issues of systematic under/over estimations for given fitted values. 
- No clear issues of heteroscedastic residuals.
- Normality assumption may be wrong, but this isn't problematic since we have a large dataset, such that a central limit theorem will make the estimators asymptotically normal distributed.
- One large leverage point which, however, has a small residual. 


