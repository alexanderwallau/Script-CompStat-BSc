## Solutions

### Exercise 1  {-}

**1 a)** Describe the null hypotheses to which the $p$-values given in Table 3.4 correspond. 

![](DATA/Table.PNG)

**1 b)** Explain what conclusions you can draw based on these $p$-values. Your explanation should be phrased in terms of `sales`, `TV`, `radio`, and `newspaper`, rather than in terms of the coefficients of the linear model.


**Answers:**

**1 a)** In Table 3.4, the null hypothesis for `TV` is that in the presence of `radio` ads and `newspaper` ads, `TV` ads have no effect on sales. Similarly, the null hypothesis for `radio` is that in the presence of `TV` ads and `newspaper` ads, `radio` ads have no effect on sales.

**1 b)** On the one hand, the low p-values of TV and radio allow us to reject the null hypothesis for `TV` and `radio`. Hence, we believe that `TV` (`radio`) ads have an effect on `sales` in the presence of `radio` (`TV`) and `newspaper` ads.<br>
On the other hand, the high p-value of `newspaper` does *not* allow us to reject the null-hypothesis (of no effect, i.e. a coefficient zero). This is an inconclusive result and only says that possible effects of `newspaper` ads are not large enough to stand out from the estimation errors (quantified by the standard error). 

**Remember:** An insignificant hypothesis test-result is never informative about whether the null is true. We do not have an error-control for falsely accepting the null-hypothesis. We only have an error-control (by the significance level) for falsely rejecting the null-hypothesis.

### Exercise 2  {-}

Carefully explain the main difference between the KNN classifier and KNN regression methods.

**Answer:**

KNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for $Y$ (qualitative), where as the output for a KNN regression predicts the quantitative value for $f(X)$.

### Exercise 3 {-}

Suppose we have a data set with five predictors:

$X_1 =GPA$

$X_2 = IQ$

$X_3 = Gender$ ($1$ for Female and $0$ for Male)

$X_4 =$ Interaction between $GPA$ and $IQ$

$X_5 =$ Interaction between $GPA$ and $Gender$

The response variable (in thousands of dollars) is defined as:

$Y =$ starting salary after graduation

Suppose we use least squares to fit the model, and get: 

$\hat{\beta}_0 = 50$, $\hat{\beta}_1 = 20$, $\hat{\beta}_2 = 0.07$, $\hat{\beta}_3 = 35$, $\hat{\beta}_4 = 0.01$, and $\hat{\beta}_5 = −10$.

Thus we have:

$$
\begin{align*}
&E[Y|X] = \\
& 50 + 20\,\overbrace{GPA}^{X_1} + 0.07\,\overbrace{IQ}^{X_2} + 35\,\overbrace{Gender}^{X_3} + 
 0.01\,\underbrace{GPA\cdot IQ}_{X_4=X_1\cdot X_2} - 10\,\underbrace{GPA\cdot Gender}_{X_5=X_1\cdot X_3}
\end{align*}
$$

**3 a)** Which answer is correct, and why?

 i) For a fixed value of $IQ$ and $GPA$, males earn more on average than females.
 ii) For a fixed value of $IQ$ and $GPA$, females earn more on average than males.
 iii) For a fixed value of $IQ$ and $GPA$, males earn more on average than females provided that the $GPA$ is high enough.
 iv) For a fixed value of $IQ$ and $GPA$, females earn more on average than males provided that the $GPA$ is high enough.


**Answer:**
$$
\begin{align*}
\text{Male\; $(X_3 = 0)$:}\quad   & 50 + 20 X_1 + 0.07 X_2 + \phantom{3}0 + 0.01\,(X_1 \cdot X_2) -0     \\[1.5ex]
\text{Female\; $(X_3 = 1)$:}\quad & 50 + 20 X_1 + 0.07 X_2 + 35 + 0.01(X_1 \cdot X_2) - 10\,X_1
\end{align*}
$$
Thus, once the $X_1=$`GPA` is high enough ($35-10\,X_1<0 \Leftrightarrow X_1>3.5$), males earn more on average.

**3 b)** Predict the salary of a female with `IQ` of 110 and a `GPA` of 4.0.

**Answer:**

```{r}
Y_hat <- 50 + 20*4 + 0.07*110 + 35*1 + 0.01*4*110 - 10*4
Y_hat
```


**3 c)** True or false: Since the coefficient for the $GPA\cdot IQ$ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

**Answer:**

False. We must examine the $p$-value of the regression coefficient to determine if the interaction term is statistically significant or not.

### Exercise 8 {-}

This question involves the use of simple linear regression on the `Auto` data set.

**8 a)** Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Use the `summary()` function to print the results. Comment on the output (see questions i) to iv) below).

```{r}
# Store data into dataframe Auto
Auto <- read.csv("DATA/Auto.csv", header=TRUE, na.strings="?")

# Remove missing values from the data
Auto <- na.omit(Auto)

# Perform linear regression
fit.lm <- lm(mpg ~ horsepower, data=Auto)

# Use summary function to print the results
summary(fit.lm)
```

**i)** Is there a relationship between the predictor and the response?

**Answer:**

Yes, there is. The predictor horsepower has a statistically significant ($p<0.001$) linear relationship with the response.

**ii)** How strong is the relationship between the predictor and the response?

**Answer:**

Statistical significance does not necessarily mean a practically strong or important relationship. 

To quantify the strength of the relationship between the predictor and the response, we can look at the following quantities: 

- Residual Standard Error `RSE` (estimate of the standard deviation of $\epsilon$)
- The $R^2$ Statistic (the proportion of variance explained by the model)
- The $F$-Statistic.

The Residual Standard Error `RSE` of the regression model with `intercept` and `horsepower` as predictors is given by:

```{r}
## RSE of lm(mpg ~ horsepower)
RSS <- sum(resid(fit.lm)^2)
n   <- length(resid(fit.lm))
RSE <- sqrt(RSS/(n-2))
round(RSE, 3)
```

This is considerable smaller than the RSE of a model with only an intercept:

```{r}
RSS_onlyIntercept <- sum(c(Auto$mpg - mean(Auto$mpg))^2)
RSE_onlyIntercept <- sqrt(RSS_onlyIntercept/(n-1))
round(RSE_onlyIntercept, 3)
```

Thus, the larger model with `horsepower` included explains more of the variances in the response variable `mpg`. 

The $R^2$ value

```{r}
round(summary(fit.lm)$r.squared, 2)
```
shows that  $60\%$ of variability in $Y$ can be explained using $X$.


The value of the $F$ statistic
```{r}
round(summary(fit.lm)$fstatistic, 2)
```
is much larger than $1$ which means that we probably can reject the null hypothesis that $\beta_{horsepower}=0$. 


**iii)** Is the relationship between the predictor and the response positive or negative?

**Answer:**

The relationship is negative, as we can see from the parameter estimate for `horsepower`

```{r}
coef(fit.lm)
```

**iv)** What is the predicted `mpg` associated with a `horsepower` of $98$? What are the associated $95\%$ confidence and prediction intervals?

**Answer:**

```{r}
# Horsepower of 98
new_df <- data.frame(horsepower = 98)

# confidence interval 
print(predict(object = fit.lm, newdata = new_df, interval = "confidence"))

# prediction interval
print(predict(object = fit.lm, newdata = new_df, interval = "prediction"))
```

**8 b)** Plot the response and the predictor. Use the `abline()` function to display the least squares regression line.

**Answer:**

```{r}
plot(x = Auto$horsepower, y = Auto$mpg, ylab = "MPG", xlab = "Horsepower")
abline(fit.lm, col="blue")
legend("topright", legend = c("(y,x)", expression(paste("(",hat(y),"x)"))), 
       pch=c(1,NA), lty=c(NA,1), col=c("black", "blue"))
```

**8 c)** Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

**Answer:**

```{r}
par(mfrow=c(2,2))
plot(fit.lm, col='blue')
```

Looking at the smoothing line of the residuals ($e_i=y_i−\hat{y}_i$) vs the fitted values ($\hat{y}_i$), there is a strong pattern in the residuals, indicating non-linearity. You can see evidence of this in the scatter plot in part 8 b) too.

There also appears to be non-constant variance in the error terms (heteroscedasticity), but this could be corrected to an extent when trying a quadratic fit. If not, transformations such as $log(y)$ or $\sqrt{y}$ can shrink larger responses by a greater amount and reduce this issue.

There are some observations with large standardized residuals & high leverage (hence, high Cook’s Distance) that may want to be reviewed.

### Exercise 9 {-}

This question involves the use of multiple linear regression on the Auto data set.

**9 a)** Produce a scatterplot matrix which includes all of the variables in the data set.

**Answer:**

```{r}
# Store data into dataframe Auto
Auto <- read.csv("DATA/Auto.csv", header=T, na.strings="?")

# Remove missing values from the data
Auto <- na.omit(Auto)

# Produce scatterplot matrix
pairs(Auto[,-9])
```

**9 b)** Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the `name` variable, which is qualitative.

**Answer:**

```{r}
round(cor(subset(Auto, select = -name)), 2)
```

**9 c)** Use the `lm()` function to perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors. Use the `summary()` function to print the results. Comment on the output by answering the below questions 9 c i) to 9 c iii).

**Answer:**

```{r}
#| scrolled: true
# Perform multiplie linear regression
fit.lm <- lm(mpg ~ .-name, data=Auto)

# Print results
summary(fit.lm)
```

**9 c i)** Is there a relationship between the predictors and the response?

**Answer:**

Yes, there is a relationship between the predictors and the response. By testing the null hypothesis of whether all the regression coefficients are zero (i.e. H$_0$: $\beta_1=\dots=\beta_7=0$), we can see that the $F$-statistic is big and its $p$-value is close to zero, indicating evidence against the null hypothesis.

**9 c ii)** Which predictors appear to have a statistically significant relationship to the response?

**Answer:**

Looking at the $p$-values associated with each predictor's $t$-statistic, we see that `displacement`, `weight`, `year`, and `origin` have a statistically significant relationship, while `cylinders`, `horsepower`, and `acceleration` do not. 

(**Caution:** This consideration neglects issues due to multiple testing. Bonferroni correction for multiple testing: To determine if any of the $7$ predictors is statistically significant, the corresponding $p$-value must be smaller than $\alpha/7$. For instance, with $\alpha/7=0.05/7\approx 0.007$, only `weight`, `year`, and `origin` have a statistically significant relationships to the response.)

**9 c iii)** What does the coefficient for the `year` variable suggest?

**Answer:**

The regression coefficient for `year` suggests that, on average, one `year` later year-of-construction is associated with an  increased `mpg` by $0.75$, when holding every other predictor value constant. 

**9 d)** Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? 

**Answer:**

```{r}
par(mfrow=c(2,2))
plot(fit.lm, col='blue')
```

- The "Residuals vs Fitted" plot (top left) shows some systematic deviations of the residuals from $0$. 
The reason is that we are imposing a straight line fit for the conditional mean function $E[Y|X]=f(X)$ which appears non-linear here. This results in a systematic underestimation of the true conditional mean function for large and small fitted values $\hat{y}=\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p$. 

- The "Residuals vs Leverage" plot (bottom right) shows that there are some potential outliers that we can see when: standardized residuals are below $-2$  or above $+2$. Moreover, the plot shows also potentially problematic "high-leverage" points with leverage values heavily exceeding the rule-of-thumb threshold $(p+1)/n=8/392=0.02$. All points with simultaneously high-leverages and large absolute standardized residuals should be handled with care since these may distort the estimation. 

- The "Normal Q-Q" plot (top right) suggests non-normally distributed residuals--particularly the upper tail deviates from that of a normal distribution. 

- The "Scale-Location" plot (bottom left) shows heteroscedastic residuals with larger variances for larger fitted values $\hat{y}=\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p$.


**9 e)** Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

**Answer:**

```{r}
fit.lm0 <- lm(mpg ~ horsepower+cylinders+year+weight:displacement, data=Auto)
summary(fit.lm0)
```

```{r}
fit.lm1 <- lm(mpg~horsepower+cylinders+year+weight*displacement, data=Auto)
summary(fit.lm1)
```


- Note that there is a difference between using `A:B` and `A*B` when running a regression. While the first includes only the interaction term between the variable `A` and `B`, the second one also includes the stand-alone variables `A` and `B`. 


- Follow the hierarchical principle for interaction effects: If we include an interaction in a model, we should also include the main effects, even if the $p$-values associated with their coefficients are not significant.


**9 f)**

Try a few different transformations of the variables, such as $\log(X)$, $\sqrt{X}$, $X^2$. Comment on your findings.

**Answer:**

```{r}
fit.lm2 <- lm(mpg~log(weight)+sqrt(horsepower)+
                acceleration+I(acceleration^2),
              data=Auto)
summary(fit.lm2)
par(mfrow=c(2,2))
plot(fit.lm2)
```

This try suffers basically from the same issues as the model considered in 9 d)


Let's consider again the model with all predictors (except `name`), but with transforming the outcome variable `mpg` by a $\log$-transformation.

```{r}
fit.lm3 <-lm(log(mpg)~ . -name, data=Auto)
summary(fit.lm3)
par(mfrow=c(2,2))
plot(fit.lm3)
```

This model specification is much better! 

- No clear issues of systematic under/over estimations for given fitted values. 
- No clear issues of heteroscedastic residuals.



