# Linear Regression

## Lecture Notes

## (Ch. 3.1) Simple Linear Regression {-}

The linear regression model assumes a *linear relationship* between $Y$ and the predictor(s) $X$. 

The simple (only one predictor) linear regression model: 
$$
Y\approx \beta_0 + \beta_1 X
$$

For instance,
<center>
`sales` $\approx \beta_0 + \beta_1$ `TV`
</center>

### (Ch. 3.1.1) Estimating the Coefficients {-}

We choose $\hat\beta_0$ and $\hat\beta_1$ such that the Residual Sum of Squares criterion is minimized:
$$
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
& = e_1^2 + \dots + e_n^2\\
&=)\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2
\end{align*}
$$
The minimizers are 
$$
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$
and
$$
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
$$
where 
$\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i$ 
and 
$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. 

![](images/Fig_3_1.png)


![](images/Fig_3_2.png)


### (Ch. 3.1.2) Assessing the Accuracy of the Coefficient Estimates {-}

True unknown model 
$$
Y=f(X)+\epsilon
$$

In in linear regression analysis, we assume[^1] that 
$$
f(X) = \beta_0 + \beta_1 X
$$

[^1]: The assumption $f(X) = \beta_0 + \beta_1 X$ is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple. 

Ordinary least squares estimators 
$$
\hat\beta_0\quad\text{and}\quad\hat\beta_1
$$
are **unbiased**, that is
$$
\begin{align*}
\operatorname{Bias}(\hat\beta_0)&=E(\hat\beta_0)-\beta_0=0\\
\operatorname{Bias}(\hat\beta_1)&=E(\hat\beta_1)-\beta_1=0
\end{align*}
$$
I.e., on average, the estimation results equal the true (unknown) parameters. However, in an actual data analysis, we only have one realization of the estimators $\hat\beta_0$ and $\hat\beta_1$ computed from one give dataset and thus we cannot compute averages of estimation results. Each single estimation result will have estimation errors, i.e., 
$$
\hat\beta_0\neq \beta_0\quad\text{and}\quad\hat\beta_1\neq \beta_1.
$$ 


The following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook `ISLR`. 

```{r}
## ###############################
## A function to generate data 
## similar to that shown in Fig 3.3
## ##############################

beta_0 <- 0.1                          # intercept parameter
beta_1 <- 5  

## A Function to simulate data
myDataGenerator <- function(){
  n      <- 50                           # sample size
  beta_0 <- 0.1                          # intercept parameter
  beta_1 <- 5                            # slope parameter
  X      <- runif(n, min = -2, max = 2)  # predictor
  error  <- rnorm(n, mean = 0, sd = 8.5) # error term
  Y      <- beta_0 + beta_1 * X + error  # outcome 
  ##
  return(data.frame("Y" = Y, "X" = X))
}

## Generate a first realization of the data
set.seed(123)
data_sim <- myDataGenerator()
head(data_sim)
```


Using repeated samples form the data generating process defined in `myDataGenerator()`, we can generate multiple estimation results of the unknown simple linear regression parameters $\beta_0$ and $\beta_1$ and plot the corresponding empirical regression lines:
```{r}
## Estimation
lm_obj <- lm(Y ~ X, data = data_sim)

## Plotting the results
par(mfrow=c(1,2)) # Two plots side by side

## First Plot (fit for the first realization of the data)
plot(x = data_sim$X, y = data_sim$Y, xlab = "X", ylab = "Y")
abline(a = beta_0, b = beta_1, col = "red")
abline(lm_obj, col = "blue")

## Second Plot (fits for multiple data realizations)
plot(x = data_sim$X, y = data_sim$Y, xlab = "X", ylab = "Y", type = "n") # type = "n": empty plot
##
for(r in 1:10){
  data_sim_new <- myDataGenerator()
  lm_obj_new   <- lm(Y ~ X, data=data_sim_new)
  abline(lm_obj_new, col = "lightskyblue")
}
## Adding the first fit
abline(a = beta_0, b = beta_1, col = "red", lwd = 2)
abline(lm_obj, col = "blue", lwd = 2)
```

*  Coding-Questions: Can you do this animated? https://gganimate.com/articles/gganimate.html



The magnitude of the estimation errors is expressed in unites of **standard errors**:
$$
\operatorname{SE}(\hat\beta_0)=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right]
$$
and
$$
\operatorname{SE}(\hat\beta_1)=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2},
$$
where $Var(\epsilon)=\sigma^2\Leftrightarrow \operatorname{SD}(\epsilon)=\sqrt{Var(\epsilon)}=\sigma$. 

Typically, $\sigma$ is unknown, but can be estimated by 
$$
\sigma\approx\hat{\sigma}=\operatorname{RSE}=\sqrt{\frac{\operatorname(RSS)}{n-2}},
$$
where we subtract $2$ from the sampel size $n$ since $n-2$ are the remaining degrees of freedom in the data after estimating two parameters $\hat\beta_0$ and $\hat\beta_1$. 


Knowing $\operatorname{SE}(\hat\beta_0)$ and $\operatorname{SE}(\hat\beta_1)$ allows us to construct **Confidence Intervals**:
$$
\begin{align*}
\operatorname{CI}_{\beta_1}
&=\left[\hat{\beta_1}-2\operatorname{SE}(\hat\beta_1),\;
        \hat{\beta_1}+2\operatorname{SE}(\hat\beta_1)\right]\\
&=\hat\beta_1\pm 2\operatorname{SE}(\hat\beta_1)
\end{align*}
$$
likewise for $\operatorname{CI}_{\beta_1}$. 

**Interpretation:** There is approximately a 95% change (in infinite resamplings) that the (random) confidence interval $\operatorname{CI}_{\beta_1}$ contains the true (fix) parameter value $\beta_1$.

Thus, a given confidence interval either contains the true parameter value or not and we usually do not know it. To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:

* [Interactive visualization for interpreting confidence intervals](https://rpsychologist.com/d3/ci/)


Standard errors can also be used to do hypothesis testing: 

$$
\begin{align*}
H_0:&\;\text{There is no relationship between $Y$ and $X$; i.e. $\beta_1=0$}\\ 
H_1:&\;\text{There is a relationship between $Y$ and $X$; i.e. $\beta_1\neq 0$}
\end{align*}
$$

$t$-test statistic

$$
t=\frac{\hat\beta_1 - 0}{\operatorname{SE}(\hat\beta_1)}\overset{H_0}{\sim}t_{(n-1)}
$$


$p$-value

$$
\begin{align*}
p
&=P_{H_0}\left(|t|\geq|t_{obs}|\right)
&=2\cdot\min\{P_{H_0}\left(t\geq t_{obs} \right),\; P_{H_0}\left(t\leq t_{obs} \right)\},
\end{align*}
$$
where $t_{obs}$ denotes the observed value of the $t$-test statistic and where $t$ is $t$-distributed with $(n-2)$ degrees of freedom. 

Select a significance level $\alpha$ (e.g. $\alpha=0.01$ or $\alpha=0.05$) and reject $H_0$ if 
$$
p<\alpha
$$

![](images/Tab_3_1.png)


### (Ch. 3.1.3) Assessing the Accuracy of the Model {-}

In tendency an accurate model has ...

* a low $\operatorname{RSE}$
$$
\operatorname{RSE}=\hat\sigma=\sqrt{\frac{\operatorname{RSS}}{n-2}}
$$

* a high $R^2$

$$
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
$$

where $0\leq R^2\leq 1$ and 

$$
\begin{align*}
\operatorname{TSS}&=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2\\ 
\operatorname{RSS}&=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2\\ 
\hat{y}_i&=\hat\beta_0+\hat\beta_1x_i
\end{align*}
$$

**Caution:** Do not forget that there is a **irreducible error** $Var(\epsilon)=\sigma^2>0$. Thus 

* very low $\operatorname{RSE}$ values, $\operatorname{RSE}\approx 0$, and 
* very high $R^2$ values, $R^2\approx 1$, 

can be warning signals indicating overfitting. 


In the case of the simple linear regression model, $R^2$ equals the squared sample correlation coefficient between $Y$ and $X$,
$$
R^2 = r^2,
$$
where 
$$
r=\widehat{cor}(Y,X)=\frac{\sqrt{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}.
$$


## Multiple Linear Regression (Ch. 3.2) {-}


The multiple linear regression model allows for more than only one predictor:  
$$
Y\approx \beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p + \epsilon
$$

For instance,
<center>
`sales` $\approx \beta_0 + \beta_1$ `TV` $+\beta_2$ `radio` $+\beta_3$ `newspaper` $+\epsilon$
</center>



### (Ch. 3.2.1) Estimating the Regression Coefficients {-}

Select 

$$
\hat\beta_0,\dots,\hat\beta_p
$$ 
by minimizing 
$$
\operatorname{RSS}=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2,
$$
where 
$$
\hat{y}_i=\hat\beta_0 + \hat\beta_1 x_{i1} \dots + \hat\beta_p x_{ip}
$$


![](images/Fig_3_4.png)


Multiple linear regression is more than mere composition of single simple linear regression models. 

Take a look at the following two simple linear regression results: 

![](images/Tab_3_3.png)

Thus in separate simple linear regressions, the effects of `radio` and the effect of `newspaper` on `sales` are both (but separately) statistically. 

By contrast, when looking at the multiple linear regression when regressing `sales` onto both `radio` and `newspaper`, only the effect of `radio` remains statistically significant: 

![](images/Tab_3_4.png)


**Reason: Omitted Variable Bias** 

* `radio` has an effect on `sales`
* `newspaper` has actually no effect on `sales`
* But, `newspaper` is "strongly" correlated with `radio` (cor(`newspaper`,`radio`)=0.3541); see Table 3.5

![](images/Tab_3_5.png)

* Thus, when omitting `radio` from the multiple regression model, `newspaper` becomes a surrogate for `radio`. This is called a *Omitted Variable Bias*.

Conclusion: Simple linear regression can be dangerous. We need to control for all possibly relevant variables if we want to interpret the estimation results ("Inference"). 

**Interpretation of the Coefficients in Table 3.5** 

For fixed values of `TV` and `newspaper`, spending additionally 1000 USD for `radio`, increases on average `sales` by approximately 189 units. 


### (Ch. 3.2.2) Some Important Questions {-}

**1. Is There a Relationship Between the Response and Predictors?**

$$
\begin{align*}
H_0:&\;\beta_1=\beta_2=\dots=\beta_p=0\\ 
H_1:&\;\text{at least one $\beta_j\neq 0$; $j=1,\dots,p$}
\end{align*}
$$

$F$-test statistic
$$
F=\frac{(\operatorname{TSS}-\operatorname{RSS})/p}{\operatorname{
  RSS}/(n-p-1)}
$$

If $H_0$ is correct
$$
\begin{align*}
E(\operatorname{RSS}/(n-p-1))&=\sigma^2\\ 
E((\operatorname{TSS}-\operatorname{RSS})/p)&=\sigma^2\\ 
\end{align*}
$$


* Thus, if $H_0$ is correct, we expect values of $F\approx 1$. 
* But if $H_1$ is correct, we expect values of $F\gg 1$.


Caution: Cannot be computed if $p>n$. (Chapter 6 on "high dimensional problems")


## (Ch. 3.3) Other Considerations in the Regression Model {-}

### (Ch. 3.3.1) Qualitative Predictors {-}

Often some predictors are *qualitative* variables (also known as a *factor* variables). For instance, the `Credit` dataset contains the following qualitative predictors: 

* `own` (house ownership)
* `student` (student status)
* `status` (marital status) 
* `region` (East, West or South)


#### Predictors with Only Two Levels {-}

If a qualitative predictor (factor) only has two levels (i.e. possible values), then incorporating it into a regression model is very simple. We simply create an indicator or **dummy variable** that takes on two possible numerical values; for instance,
$$
x_{i} = \left\{
  \begin{array}{ll}
  1&\quad \text{if the $i$th person owns a house}\\
  0&\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
$$
Using this dummy variable as a predictor in the regression equation results in the following regression model:
$$
y_{i}=\beta_0 + \beta_1 x_i + \epsilon_i = \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i &\quad \text{if the $i$th person owns a house}\\
  \beta_0 + \epsilon_i           &\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
$$

**Interpretation:**

* $\beta_0$: The average credit card balance among those who do not own a house
* $\beta_0+\beta_1$: The average credit card balance among those who do own a house
* $\beta_1$: The average difference in credit card balance between owners and non-owners

![](images/Tab_3_7.png)

Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable
$$
x_{i} = \left\{
  \begin{array}{ll}
  1 &\quad \text{if the $i$th person owns a house}\\
 -1 &\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
$$
$$
y_{i}=\beta_0 + \beta_1 x_i + \epsilon_i = \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i&\quad \text{if the $i$th person owns a house}\\
  \beta_0 - \beta_1 + \epsilon_i&\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
$$

**Interpretation:**

* $\beta_0$: The overall average credit card balance (ignoring the house ownership effect)
* $\beta_1$: The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.


#### Qualitative Predictors with More than Two Levels {-}

When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the `region` $\in\{$`South`, `West`, `East`$\}$ variable we create **two** dummy variables. The first could be
$$
x_{i1} = \left\{
  \begin{array}{ll}
  1&\quad \text{if the $i$th person is from the South}\\
  0&\quad \text{if the $i$th person is not from the South,}
  \end{array}\right.
$$
and the second could be
$$
x_{i2} = \left\{
  \begin{array}{ll}
  1&\quad \text{if the $i$th person is from the West}\\
  0&\quad \text{if the $i$th person is not from the West.}
  \end{array}\right.
$$
Using both of these dummy variables results in the following regression model:
order to obtain the model
$$
y_{i}=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i = \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1  + \epsilon_i& \quad \text{if the $i$th person is from the South}\\
  \beta_0 + \beta_2  + \epsilon_i& \quad \text{if the $i$th person is from the West}\\ 
  \beta_0            + \epsilon_i& \quad \text{if the $i$th person is from the East.}\\ 
  \end{array}\right.
$$


**Interpretation:**

* $\beta_0$: The average credit card balance for individuals from the East
* $\beta_1$: The difference in the average balance between people from the South versus the East
* $\beta_2$: The difference in the average balance between people from the West versus the East

![](images/Tab_3_8.png)



There are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular **contrasts**. (A detailed discussion of *contrasts* is beyond the scope of this lecture.)


### (Ch. 3.3.2) Extensions of the Linear Model {-}

#### Interaction Effects: Removing the Additive Assumption using Interaction Effects {-}

Previously, we used the following model 

<center>
`sales` $= \beta_0 + \beta_1$ `TV` $+ \beta_2$ `radio` $+ \beta_3$ `newspaper` $+\epsilon$
</center>

which states that the average increase in sales associated with a one-unit increase in `TV` is always $\beta_1,$ regardless of the amount spent on radio.


However, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on radio advertising actually increases the effectiveness of TV advertising. 


Figure 3.5 suggests that such an effect may be present in the advertising data: 

* When levels of either `TV` or `radio` are low, then the true `sales` are lower than predicted by the linear model. 
* But when advertising is split between the two media, then the model tends to **underestimate** sales.
![](images/Fig_3_5.png)


**Solution: Interaction Effects:**

Consider the standard linear regression model with two variables,
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon.
$$
Here each predictor $X_1$ and $X_2$ has a given effect, $\beta_1$ and $\beta_2$, on $Y$ and this effect does not depend on the value of the other predictor. (Additive Assumption)

One way of extending this model is to include a third predictor, called an **interaction term**, which is constructed by computing the product of $X_1$ and $X_2.$ This results in the model
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2 + \epsilon.
$$
This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as
$$
\begin{align*}
Y &= \beta_0 + \underbrace{(\beta_1 + \beta_3 X_2)}_{=\tilde{\beta}_1} X_1 + \beta_2 X_2 + \epsilon,
\end{align*}
$$
where the new slope parameter $\tilde{\beta}_2$ is a linear function of $X_2$
$$
\tilde{\beta}_1\equiv\tilde{\beta}_1(X_2)=\beta_1 + \beta_3 X_2.
$$

Thus, a change in the value of $X_2$ will change the association between $X_1$ and $Y.$ 

A similar argument shows that a change in the value of $X_1$ changes the association between $X_2$ and $Y.$


Let us return to the `Advertising` example. A linear model that uses `radio`, `TV`, and an interaction, `radio`$\times$`radio`, between the two to predict `sales` takes the
form 

<center>
`sales` $= \beta_0 + \beta_1\times$ `TV` $+ \beta_2\times$ `radio` $+ \beta_3\times($ `radio`$\times$ `TV`$)+\epsilon$
</center>

which can be rewritten as

<center>
`sales` $=\beta_0 + (\beta_1+ \beta_3\times$ `radio` $)\times$ `TV` $+ \beta_2\times$ `radio`  $+\epsilon$
</center>


<br>

**Interpretation:** 

* $\beta_3$ denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising (or vice-versa).

![](images/Tab_3_9.png)


**Interpretation of Table 3.9:**

* Both (separate) main effects, `TV` and `radio`, are statistically significant ($p$-values smaller than 0.01). 
* Additionally, the $p$-value for the interaction term, `TV`$\times$`radio`, is extremely low, indicating that there is strong evidence for $H_1: \beta_3\neq 0.$ In other words, it is clear that the true relationship is not additive. 



**Hierarchical Principle:**

If we include an interaction in a model, we should also include the main effects, even if the $p$-values associated with their coefficients are not significant.


**Interactions with Qualitative Variables:**

An interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation. 


Consider the `Credit` data set and suppose that we wish to predict `balance` using the predictors: 

* `income` (quantitative) and 
* `student` (qualitative) using a dummy variable with $x_{i2}=1$ if $i$th person is a student and $x_{i2}=0$ if not. 

In the absence of an interaction term, the model takes the form
![](images/Eq_3_34.png)

Thus, the regression lines for students and non-students have different intercepts, $\beta_0+\beta_2$ versus $\beta_0$, but the same slope $\beta_1$.

This represents a potentially serious limitation of the model, since in fact a change in `income` may have a very different effect on the credit card balance of a student versus a non-student.

This limitation can be addressed by adding an interaction variable, created by multiplying `income` with the dummy variable for student. Our model now becomes
![](images/Eq_3_35.png)

Now we have different intercepts for students and non-students but also different slopes for these groups. 
![](images/Fig_3_7.png)


#### Polynomial Regression: Non-linear Relationships {-}

Polynomial regression allows to accommodate non-linear relationships between the predictors $X$ and the outcome $Y.$
![](images/Fig_3_8.png)


For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form

<center>
`mpg` $=\beta_0 + \beta_1\times$ `horsepower` $+ \beta_2\times($`horsepower`$)^2+\epsilon$
</center>



This regression model involves predicting `mpg` using a non-linear function of `horsepower`. **But it is still a linear model!** It's simply a multiple linear regression model with $X_1=$`horsepower` and $X_2 =($`horsepower`$)^2.$ 

So we can use standard linear regression software to estimate $\beta_0$, $\beta_1$, and $\beta_2$ in order to produce a non-linear fit. 

![](images/Tab_3_10.png)


### (Ch. 3.3.3) Potential Problems {-}


**1. Non-linearity of the response-predictor relationships.**

Diagnostic residual plots are most useful to detect possible non-linear response-predictor relationships. 

```{r}
library("ISLR2")
data(Auto) 

## Gives the variable names in the Auto dataset
# names(Auto)

## Simple linear regression
lmobj_1 <- lm(mpg ~ horsepower, data = Auto)

## Quadratic regression 
lmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)

## Diagnostic Plot
par(mfrow = c(1,2))
plot(lmobj_1, which = 1)
plot(lmobj_2, which = 1)
```

*Residual plots* are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals, 
$$
e_i = y_i - \hat{y}_i,
$$ 
versus the predictor $x_i.$ 

In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values $\hat{y}_i.$ Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.


If the residual plot indicates that there are non-linear associations in the
data, then a simple approach is to use non-linear transformations of the
predictors, such as 
$$
\log(X),\; \sqrt{X},\; \text{or}\; X^2
$$ 
in the regression model. In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue.


**2. Correlation of Error Terms**


An important assumption of the linear regression model is that the error terms, $\epsilon_1, \epsilon_2, \dots , \epsilon_n$, are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that $\epsilon_i$ is positive provides little or no information about the sign of $\epsilon_{i+1}.$  The **standard errors** that are computed for the estimated regression coefficients or the fitted values are based on the **assumption of uncorrelated error terms**.  If in fact there is correlation among the error terms, then the estimated standard errors will tend to **underestimate** the true standard errors. 


Correlations among the error terms typically occur in time series data (see Fig. 3.10). 

![](images/Fig_3_10.png)

**3. Non-Constant Variance of Error Terms**

Another important assumption of the linear regression model is that the
error terms have a constant variance, 
$$
Var(\epsilon_i) = \sigma^2.
$$ 
The standard formulas for standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.


One can identify **non-constant variances "heteroscedasticity"** in the errors, using diagnostic residual plots. 

Often one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values which indicates. When faced with this problem, one possible solution is to transform the response $Y$ using a concave function such as 
$$
\log(Y)\;\text{ or }\; \sqrt{Y}.
$$
Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. 

```{r}
## Quadratic regression 
lmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)

## Quadratic regression with transformed response log(Y)
lmobj_3 <- lm(I(log(mpg)) ~ horsepower + I(horsepower^2), data = Auto)

## Diagnostic Plot
par(mfrow = c(1,2))
plot(lmobj_2, which = 1)
plot(lmobj_3, which = 1)
```


**4. Outliers**

An outlier is a point for which $y_i$ is far from the value predicted by the outlier
model. Outliers can arise for a variety of reasons, such as incorrect recording
of an observation during data collection.

Outliers typically have a strong effect on the $R^2$ value since they add a **very large residual** to its computation. 

Figure 3.12 in the textbook `ISLR` shows a clear outlier (observation 20) which, however, has a typical predictor value $x_i$. Such outliers have little effect on the regression fit.
![](images/Fig_3_12.png)


Figure 3.13 in the textbook `ISLR` shows again a clear outlier (observation 41) which has a predictor value $x_i$ that is *very atypical*. Such outliers are said to have  large **leverage** giving them power to affect the regression fit considerably. 
![](images/Fig_3_13.png)


Summary: Critical outliers have both, large residuals *and* large leverage.


**5. High Leverage Points**

In order to quantify an observation's leverage, we compute the **leverage statistic** $h_i$ for each observation $i=1,\dots,n.$ A large value of this statistic indicates an observation with high leverage. For a simple linear regression,
$$
h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_{j=1}^n(x_j-\bar{x})^2}
$$
There is a simple extension of $h_i$ to the case of multiple predictors, though
we do not provide the formula here. 


* The leverage statistic $h_i$ is always between $1/n$ and $1$
* The average leverage for all the observations is equal to $\bar{h}=\frac{1}{n}\sum_{i=1}^n h_i=(p + 1)/n.$ 
* If a given observation has a leverage statistic $h_i$ that greatly exceeds $(p+1)/n,$ then we may suspect that the corresponding point has high leverage.


**6. Collinearity**

Collinearity refers to the situation in which two or more predictor variables are closely related to one another. 


```{r}
library("ISLR2")
data(Credit) # names(Credit)

par(mfrow=c(1,2))
plot(y = Credit$Age,    x = Credit$Limit, main = "No Collinearity", ylab = "Age", xlab = "Limit")
plot(y = Credit$Rating, x = Credit$Limit, main = "Strong Collinearity", ylab = "Rating", xlab = "Limit")
```


![](images/Fig_3_15.png)

![](images/Tab_3_11.png)


We call this situation **multicollinearity**. 


To detect multicollinearity issues, one can use the variance inflation factor (VIF) 
$$
\operatorname{VIF}(\hat{\beta}_j)=\frac{1}{1-R^2_{X_j|X_-j}},
$$
where $R^2_{X_j|X_-j}$ is the $R^2$ from a regression of $X_j$ onto all of the other predictors. 

* If $R^2_{X_j|X_-j}$ is close to one, then multicollinearity is present, and $\operatorname{VIF}(\hat{\beta}_j)$ will be large. 


In the `Credit` data, a regression of balance on `age`, `rating`, and `limit` indicates that the predictors have VIF values of 1.01 (`age`), 160.67 (`rating`), and 160.59 (`limit`). Thus, as we suspected, there is considerable collinearity in the data!


Possible solutions:

1. Drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression
fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. <br> **Caution:** In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases. 

2. Combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.

3. Use a different estimation procedure like ridge regression. 

4. Live with it. Sometimes you're not allowed to drop or combine variables (e.g. important control variables) and also no other estimation procedure can be used. Then you have to live with large standard errors due to multicollinearity. But at least you know where the large stand errors are coming from. 



## (Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors {-}


Linear regression is an example of a parametric approach because it assumes a linear model form for $f(X).$

**Advantages of parametric approaches:**

* Typically easy to fit 
* Simple interpretation
* Simple inference


**Disadvantages of parametric approaches:**

* The parametric model assumption can be far from true; i.e.
$$
f(X) \neq \beta_0+\beta_1X_1+\dots+\beta_pX_p 
$$


Alternative: **Non-parametric methods** such as *K-nearest neighbors regression* since non-parametric approaches do not explicitly assume a parametric form for $f(X).$


#### K-nearest neighbors regression (KNN regression) {-}

Given a value for $K$ and a prediction point $x_0,$ KNN regression
regression ...

1. identifies the $K$ training observations that are closest to
$x_0$, represented by the index set $\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}.$ 
2. estimates $f(x_0)$ using the average of all the training responses $y_i$ with $i\in\mathcal{N}_0.$ 

In other words,
$$
\hat{f}(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}y_i.
$$



![](images/Fig_3_16.png)


In general, the optimal value for $K$ will depend on the *bias-variance tradeoff*, which we introduced in Chapter 2. 

* A small value for $K$ provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation of $K=1$.
* A large value of $K$ provide a smoother and less wiggly fit; the
prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in $f(X).$


In Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of $K$ in KNN regression.


Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of $f$ and vice versa. 


Figure 3.17 of our textbook `ISLR` provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent the true $f(X)$, while the blue curves correspond to the KNN fits using $K = 1$ (left plot) and $K = 9$ (right plot). In this case, the $K = 1$ predictions are far too variable, while the smoother $K = 9$ fit is much closer to the true $f(X).$ However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. 
![](images/Fig_3_17.png)


The blue dashed line in the left-hand panel of Figure 3.18 represents the linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. 
![](images/Fig_3_18.png)


Figure 3.19 displays a non-linear situations in which KNN performs much better than linear regression. 
![](images/Fig_3_19.png)


**Curse of dimensionality:**

Unfortunately, in higher dimensions, KNN often performs worse than linear regression, since non-parametric approaches suffer from the **curse of dimensionality**. Figure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise (i.e. redundant) predictors that are not associated with the response. 

* When $p = 1$ or $p = 2$, KNN outperforms linear regression. 
* But for $p = 3$ the results are mixed, and for $p\geq 4$ linear regression is superior to KNN. 
![](images/Fig_3_20.png)


When $p=1$, $50$ data points can provide enough information to estimate $f(X)$ accurately using non-parametric methods since the $K$ nearest neighbors can actually be close to a given test observation $x_0.$ However, when spreading the $50$ data points over a large number of, for instance, $p=20$ dimensions, even the $K$ nearest neighbors tend to become far away from $x_0.$ 


## `R`-Lab: Linear Regression


### Libraries

The `library()` function is used to load *libraries*, or groups of  functions and data sets that are not included in the base `R` distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution,  but more exotic functions require additional libraries. Here we load the `MASS` package, which is a very large collection of data sets and functions. We  also load the `ISLR2` package, which includes the data sets associated with this book.

```{r chunk1}
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(ISLR2))
```


If you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as `MASS`, come with `R` and do not need to be separately installed on your computer. However, other packages, such as `ISLR2`, must be downloaded the first time they are used. This can be done directly from within `R`. For example, on a Windows system,  select the `Install package` option under the `Packages` tab.  After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and `R` will automatically download the package. Alternatively, this can be done at the `R` command line via `install.packages("ISLR2")`. This installation only needs to be done the first time you use a package. However, the `library()` function must be called within each `R` session.

### Simple Linear Regression

The `ISLR2` library contains the `Boston`  data set, which
records `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).

```{r chunk2}
head(Boston)
```

To find out more about the data set, we can type `?Boston`.

We will start by using the `lm()` function to fit a simple  linear regression model, with `medv` as the response and `lstat`  as the predictor. The basic syntax is `lm(y ~ x, data)`, where `y` is the response, `x` is the predictor, and `data` is the data set in which these two variables are kept.

```{r chunk3, error=TRUE}
lm.fit <- lm(medv ~ lstat)
```

The command causes an error because `R` does not know where to find the variables `medv` and `lstat`. 

The next line tells `R` that the variables are in `Boston`: 
```{r chunk4a}
lm.fit <- lm(medv ~ lstat, data = Boston)
```

Alternatively, we can attach the `Boston` object:
```{r chunk4b}
attach(Boston)
lm.fit <- lm(medv ~ lstat)
```

If we type `lm.fit`,  some basic information about the model is output. For more detailed information, we use `summary(lm.fit)`. This gives us $p$-values and standard errors for the coefficients, as well as the $R^2$ statistic and $F$-statistic for the model.


```{r chunk5}
lm.fit
summary(lm.fit)
```

We can use the `names()` function in order to find out what other pieces of information  are stored in `lm.fit`. Although we can extract these quantities by name---e.g. `lm.fit$coefficients`---it is safer to use the extractor functions like `coef()` to access them.

```{r chunk6}
names(lm.fit)
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command. 

Type `confint(lm.fit)` at the command line to obtain the confidence intervals for the linear regression coefficients.

```{r chunk7}
confint(lm.fit)
```
The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`.

```{r chunk8}
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), 
        interval = "confidence")
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), 
        interval = "prediction")
```

For instance, the 95\% confidence interval associated with a `lstat` value of 10 is $(24.47, 25.63)$, and the 95\% prediction interval is $(12.828, 37.28)$. As expected, the confidence and prediction intervals are centered around the same point (a predicted value of $25.05$ for `medv` when `lstat` equals 10), but the latter are substantially wider.

We will now plot `medv` and `lstat` along with the least squares regression line using the `plot()` and `abline()` functions.

```{r chunk9}
plot(lstat, medv)
abline(lm.fit)
```

There is some evidence for non-linearity in the relationship between `lstat` and `medv`. We will explore this issue  later in this lab.

The `abline()` function can be used to draw any line, not just the least squares regression line. To draw a line with intercept `a` and slope `b`, we  type `abline(a, b)`. Below we experiment with some additional settings for plotting lines and points. The `lwd = 3` command causes the width of the regression line to be increased by a factor of 3;  this works for the `plot()` and `lines()` functions also. We can also use the `pch` option to create different plotting symbols.

```{r chunk10}
plot(lstat, medv)
abline(lm.fit, lwd = 3, col = "red")
plot(lstat, medv, col = "red")
plot(lstat, medv, pch = 20)
plot(lstat, medv, pch = "+")
plot(1:20, 1:20, pch = 1:20)
```


Next we examine some diagnostic plots, several of which were discussed
in Section 3.3.3. Four diagnostic plots are automatically
produced by applying the `plot()` function directly to the output
from `lm()`. In general, this command will produce one plot at a
time, and hitting *Enter* will generate the next plot. However,
it is often convenient to view all four plots together. We can achieve
this by using the `par()` and `mfrow()` functions, which tell `R` to split
the display screen into separate panels so that multiple plots can be
viewed simultaneously. For example,  `par(mfrow = c(2, 2))` divides the plotting
region into a $2 \times 2$ grid of panels.

```{r chunk11}
par(mfrow = c(2, 2))
plot(lm.fit)
```

Alternatively, we can compute the residuals from a linear regression
fit using the `residuals()` function. The function
`rstudent()` will return the studentized residuals, and we
can use this function to plot the residuals against the fitted values.

```{r chunk12}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

On the basis of the residual plots, there is some evidence of non-linearity.


Leverage statistics can be computed for any number of predictors using the `hatvalues()` function.

```{r chunk13}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

The `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.

```{r}
sort(hatvalues(lm.fit), decreasing = TRUE)[1:3]
```

The `sort()` function can be used to sort and print values of a vector like `hatvalues(lm.fit)`.

### Multiple Linear Regression

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y ~ x1 + x2 + x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors.

```{r chunk14}
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

The `Boston` data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors.
Instead, we can use the following short-hand:

```{r chunk15}
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by name
(type `?summary.lm` to see what is available). Hence
`summary(lm.fit)$r.sq` gives us the $R^2$, and
`summary(lm.fit)$sigma` gives us the RSE. The `vif()`
function, part of the `car` package, can be used to compute variance
inflation factors.   Most VIF's are
low to moderate for this data. The `car` package is not part of the base `R` installation so it must be downloaded the first time you use it via the `install.packages()` function in `R`.

```{r chunk16}
suppressPackageStartupMessages(library(car)) # contains the vif() function
sort(vif(lm.fit)) # computes the VIF statistics and sorts them
```

What if we would like to perform a regression using all of the variables but one?  For example, in the above regression output,  `age` has a high $p$-value. So we may wish to run a regression excluding this predictor.
 The following syntax results in a regression using all predictors except `age`.

```{r chunk17}
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
```

Alternatively, the `update()` function can be used.

```{r chunk18}
lm.fit1 <- update(lm.fit, ~ . - age)
```


### Interaction Terms

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells `R` to include an interaction term between `lstat` and `black`.
The syntax `lstat * age` simultaneously includes `lstat`, `age`, and the interaction term `lstat`$\times$`age` as predictors; it is a shorthand for `lstat + age + lstat:age`.
  %We can also pass in transformed versions of the predictors.

```{r chunk19}
summary(lm(medv ~ lstat * age, data = Boston))
```


### Non-linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor $X$, we can create a predictor $X^2$ using
 `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula object; wrapping as we do allows the standard usage in `R`, which is to raise `X` to the power `2`. We now
perform a regression of `medv` onto `lstat` and `lstat^2`.

```{r chunk20}
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
```

The near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model.
We use the `anova()` function  to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{r chunk21}
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat^2`.
The `anova()` function performs a hypothesis test
comparing the two models. The   null hypothesis is that the two models fit the data equally well,  and the alternative hypothesis is that the full model is superior. Here the $F$-statistic is $135$
 and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`.
 This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type

```{r chunk22}
par(mfrow = c(2, 2))
plot(lm.fit2)
```

 then we see that when the `lstat^2` term is included in the model, there is little discernible pattern in the residuals.

In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a
fifth-order polynomial fit:

```{r chunk23}
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
```

This suggests that including additional  polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant $p$-values
in a regression fit.

 By default, the `poly()` function orthogonalizes the predictors:
 this means that the features output by this function are not simply a
 sequence of powers of the argument. However, a linear model applied to the output of the `poly()` function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the `poly()` function,  the argument `raw = TRUE` must be used.

Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.

```{r chunk24}
summary(lm(medv ~ log(rm), data = Boston))
```


### Qualitative Predictors

We will now examine the `Carseats` data, which is part of the
`ISLR2` library. We will  attempt to predict `Sales`
(child car seat sales) in $400$ locations based on a number of
predictors.

```{r chunk25}
head(Carseats)
```

The `Carseats` data includes qualitative predictors such as `shelveloc`, an indicator of the quality of the shelving location---that is, the  space within a store in which the car seat is displayed---at each location. The predictor `shelveloc` takes on three possible values:  *Bad*, *Medium*, and *Good*. Given a qualitative variable such as `shelveloc`, `R` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.

```{r chunk26}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, 
    data = Carseats)
summary(lm.fit)
```

The `contrasts()` function returns the coding that `R` uses for the dummy variables.

```{r chunk27}
attach(Carseats)
contrasts(ShelveLoc)
```

Use `?contrasts` to learn about other contrasts, and how to set them.

`R` has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise.
A bad shelving location corresponds to a zero for each of the two dummy variables.
The fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.


### Writing  Functions

As we have seen, `R` comes with many useful functions, and still more functions are available by way of `R` libraries.
However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the `ISLR2` and `MASS` libraries, called
`LoadLibraries()`. Before we have created the function, `R` returns an error if we try to call it.

```{r chunk28, error=TRUE}
LoadLibraries
LoadLibraries()
```

We now create the function. 

```{r chunk29}
LoadLibraries <- function() {
 library(ISLR2)
 library(MASS)
 print("The libraries have been loaded.")
}
```

Now if we type in `LoadLibraries`, `R` will tell us what is in the function.

```{r chunk30}
LoadLibraries
```

If we call the function, the libraries are loaded in and the print statement is output.

```{r chunk31}
LoadLibraries()
```


## Exercises

Prepare the following exercises of Chapter 3 in our course textbook `ISLR`: 

- Exercise 1
- Exercise 2
- Exercise 3
- Exercise 8
- Exercise 9

{{< include Ch3_Solutions.qmd >}}