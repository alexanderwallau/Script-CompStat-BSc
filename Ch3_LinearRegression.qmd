# Linear Regression

## Simple Linear Regression (Ch. 3.1) {-}

The linear regression model assumes a *linear relationship* between $Y$ and the predictor(s) $X$. 

The simple (only one predictor) linear regression model: 
$$
Y\approx \beta_0 + \beta_1 X
$$

For instance,
<center>
`sales` $\approx \beta_0 + \beta_1$ `TV`
</center>

### Estimating the Coefficients (Ch. 3.1.1)  {-}

We choose $\hat\beta_0$ and $\hat\beta_1$ such that the Residual Sum of Squares criterion is minimized:
$$
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
& = e_1^2 + \dots + e_n^2\\
&=)\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2
\end{align*}
$$
The minimizers are 
$$
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$
and
$$
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
$$
where 
$\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i$ 
and 
$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. 

![](images/Fig_3_1.png)


![](images/Fig_3_2.png)


### Assessing the Accuracy of the Coefficient Estimates (Ch. 3.1.2) {-}

True unknown model 
$$
Y=f(X)+\epsilon
$$

In in linear regression analysis, we assume[^1] that 
$$
f(X) = \beta_0 + \beta_1 X
$$

[^1]: The assumption $f(X) = \beta_0 + \beta_1 X$ is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple. 

Ordinary least squares estimators 
$$
\hat\beta_0\quad\text{and}\quad\hat\beta_1
$$
are **unbiased**, that is
$$
\begin{align*}
\operatorname{Bias}(\hat\beta_0)&=E(\hat\beta_0)-\beta_0=0\\
\operatorname{Bias}(\hat\beta_1)&=E(\hat\beta_1)-\beta_1=0
\end{align*}
$$
I.e., on average, the estimation results equal the true (unknown) parameters. However, in an actual data analysis, we only have one realization of the estimators $\hat\beta_0$ and $\hat\beta_1.$ 

And, of course, there are estimation errors, i.e., 
$$
\hat\beta_0\neq \beta_0\quad\text{and}\quad\hat\beta_1\neq \beta_1.
$$ 


The following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook `ISLR`. 

```{r}
## ###############################
## A function to generate data 
## similar to that shown in Fig 3.3
## ##############################

beta_0 <- 0.1                          # intercept parameter
beta_1 <- 5  

## A Function to simulate data
myDataGenerator <- function(){
  n      <- 50                           # sample size
  beta_0 <- 0.1                          # intercept parameter
  beta_1 <- 5                            # slope parameter
  X      <- runif(n, min = -2, max = 2)  # predictor
  error  <- rnorm(n, mean = 0, sd = 8.5) # error term
  Y      <- beta_0 + beta_1 * X + error  # outcome 
  ##
  return(data.frame("Y" = Y, "X" = X))
}

## Generate a first realization of the data
set.seed(123)
data_sim <- myDataGenerator()
head(data_sim)
```


Using repeated samples form the data generating process defined in `myDataGenerator()`, we can generate multiple estimation results of the unknown simple linear regression parameters $\beta_0$ and $\beta_1$ and plot the corresponding empirical regression lines:
```{r}
## Estimation
lm_obj <- lm(Y ~ X, data = data_sim)

## Plotting the results
par(mfrow=c(1,2)) # Two plots side by side

## First Plot (fit for the first realization of the data)
plot(x = data_sim$X, y = data_sim$Y, xlab = "X", ylab = "Y")
abline(a = beta_0, b = beta_1, col = "red")
abline(lm_obj, col = "blue")

## Second Plot (fits for multiple data realizations)
plot(x = data_sim$X, y = data_sim$Y, xlab = "X", ylab = "Y", type = "n") # type = "n": empty plot
##
for(r in 1:10){
  data_sim_new <- myDataGenerator()
  lm_obj_new   <- lm(Y ~ X, data=data_sim_new)
  abline(lm_obj_new, col = "lightskyblue")
}
## Adding the first fit
abline(a = beta_0, b = beta_1, col = "red", lwd = 2)
abline(lm_obj, col = "blue", lwd = 2)
```

*  Coding-Questions: Can you do this animated? https://gganimate.com/articles/gganimate.html



The magnitude of the estimation errors is expressed in unites of **standard errors**:
$$
\operatorname{SE}(\hat\beta_0)=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right]
$$
and
$$
\operatorname{SE}(\hat\beta_1)=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2},
$$
where $Var(\epsilon)=\sigma^2\Leftrightarrow \operatorname{SD}(\epsilon)=\sqrt{Var(\epsilon)}=\sigma$. 

Typically, $\sigma$ is unknown, but can be estimated by 
$$
\sigma\approx\hat{\sigma}=\operatorname{RSE}=\sqrt{\frac{\operatorname(RSS)}{n-2}},
$$
where we subtract $2$ from the sampel size $n$ since $n-2$ are the remaining degrees of freedom in the data after estimating two parameters $\hat\beta_0$ and $\hat\beta_1$. 


Knowing $\operatorname{SE}(\hat\beta_0)$ and $\operatorname{SE}(\hat\beta_1)$ allows us to construct **Confidence Intervals**:
$$
\begin{align*}
\operatorname{CI}_{\beta_1}
&=\left[\hat{\beta_1}-2\operatorname{SE}(\hat\beta_1),\;
        \hat{\beta_1}+2\operatorname{SE}(\hat\beta_1)\right]\\
&=\hat\beta_1\pm 2\operatorname{SE}(\hat\beta_1)
\end{align*}
$$
likewise for $\operatorname{CI}_{\beta_1}$. 

**Interpretation:** There is approximately a 95% change (in infinite resamplings) that the (random) confidence interval $\operatorname{CI}_{\beta_1}$ contains the true (fix) parameter value $\beta_1$.

Thus, a given confidence interval either contains the true parameter value or not and we usually do not know it. To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:

* [Interactive visualization for interpreting confidence intervals](https://rpsychologist.com/d3/ci/)


Standard errors can also be used to do hypothesis testing: 

$$
\begin{align*}
H_0:&\;\text{There is no relationship between $Y$ and $X$; i.e. $\beta_1=0$}\\ 
H_1:&\;\text{There is a relationship between $Y$ and $X$; i.e. $\beta_1\neq 0$}
\end{align*}
$$

$t$-test statistic

$$
t=\frac{\hat\beta_1 - 0}{\operatorname{SE}(\hat\beta_1)}\overset{H_0}{\sim}t_{(n-1)}
$$


$p$-value

$$
\begin{align*}
p
&=P_{H_0}\left(|t|\geq|t_{obs}|\right)
&=2\cdot\min\{P_{H_0}\left(t\geq t_{obs} \right),\; P_{H_0}\left(t\leq t_{obs} \right)\},
\end{align*}
$$
where $t_{obs}$ denotes the observed value of the $t$-test statistic and where $t$ is $t$-distributed with $(n-2)$ degrees of freedom. 

Select a significance level $\alpha$ (e.g. $\alpha=0.01$ or $\alpha=0.05$) and reject $H_0$ if 
$$
p<\alpha
$$

![](images/Tab_3_1.png)


### Assessing the Accuracy of the Model (Ch. 3.1.3)

In *tendency* an accurate model has ...

* a low $\operatorname{RSE}$
$$
\operatorname{RSE}=\hat\sigma=\sqrt{\frac{\operatorname{RSS}}{n-2}}$
$$

* a high $R^2$

$$
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
$$

where $0\leq R^2\leq 1$ and 

$$
\begin{align*}
\operatorname{TSS}&=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2\\ 
\operatorname{RSS}&=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2\\ 
\hat{y}_i&=\hat\beta_0+\hat\beta_1x_i
\end{align*}
$$

**Caution:** Do not forget that there is a **irreducible error** $Var(\epsilon)=\sigma^2>0$. Thus $\operatorname{RSE}\approx 0$ and $R^2\approx 1$ can be warning signals indicating overfitting. 

In the case of the simple linear regression model it hold that

$$
R^2 = r^2,
$$
where 
$$
r=\widehat{cor}(Y,X)=\frac{\sqrt{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}
$$


## Multiple Linear Regression (Ch. 3.2) {-}


The multiple linear regression model allows for more than only one predictor:  
$$
Y\approx \beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p + \epsilon
$$

For instance,
<center>
`sales` $\approx \beta_0 + \beta_1$ `TV` $+\beta_2$ `radio` $+\beta_3$ `newspaper` $+\epsilon$
</center>

### Estimating the Regression Coefficients (Ch. 3.2.1) {-}

Select 

$$
\hat\beta_0,\dots,\hat\beta_p
$$ 
by minimizing 
$$
\operatorname{RSS}=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2,
$$
where 
$$
\hat{y}_i=\hat\beta_0 + \hat\beta_1 x_{i1} \dots + \hat\beta_p x_{ip}
$$


![](images/Fig_3_4.png)


Multiple linear regression is more than mere composition of single simple linear regression models. 

Take a look at the following two simple linear regression results: 

![](images/Tab_3_3.png)

Here both the effects of `radio` and of `newspaper` are statistically significant on `sales`. 

By contrast, when looking at the multiple linear regression when regressing `sales` onto both `radio` and `newspaper` only `radio` remains statistically significant: 

![](images/Tab_3_4.png)


**Reasoning:** 

* `newspaper` has actually no effect on `sales`
* `radio` has an effect on `sales`
* But, `newspaper` is "strongly" correlated with `radio` (cor(`newspaper`,`radio`)=0.3541)

![](images/Tab_3_5.png)

Thus, wen omitting `radio` from the multiple regression model, `newspaper` becomes a surrogate for `radio`. This is called a *Omitted Variable Bias*.

Conclusion: Simple linear regression can be dangerous. We need to control for all possibly relevant variables if we want to interpret the esitmation results ("Inference"). 

**Interpretation of Coefficients:** For fixed values of `TV` and `newspaper`, spending additionally 1000 USD for `radio`, increases on average `sales` by approximately 189 units. 


### Some Important Questions (Ch. 3.2.2) {-}

**1. Is There a Relationship Between the Response and Predictors?**


$$
\begin{align*}
H_0:&\;\beta_1=\beta_2=\dots=\beta_p=0\\ 
H_1:&\;\text{at least one $\beta_j\neq 0$; $j=1,\dots,p$}
\end{align*}
$$

$F$-test statistic
$$
F=\frac{(\operatorname{TSS}-\operatorname{RSS})/p}{\operatorname{
  RSS}/(n-p-1)}
$$

If $H_0$ is correct
$$
\begin{align*}
E(\operatorname{RSS}/(n-p-1))&=\sigma^2\\ 
E((\operatorname{TSS}-\operatorname{RSS})/p)&=\sigma^2\\ 
\end{align*}
$$


* Thus, if $H_0$ is correct, we expect values of $F\approx 1$. 
* But if $H_1$ is correct, we expect values of $F\gg 1$.


Caution: Cannot be computed if $p>n$. (Chapter 6 on "high dimensional problems")


### Potential Problems (Ch. 3.3.3) {-}

**1. Non-linearity of the response-predictor relationships.**

Diagnostic residual plots are most useful to detect possible non-linear response-predictor relationships. 

```{r}
library("ISLR2")
data(Auto) 

## Gives the variable names in the Auto dataset
# names(Auto)

## Simple linear regression
lmobj_1 <- lm(mpg ~ horsepower, data = Auto)

## Quadratic regression 
lmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)

## Diagnostic Plot
par(mfrow = c(1,2))
plot(lmobj_1, which = 1)
plot(lmobj_2, which = 1)
```

*Residual plots* are a useful graphical tool for identifying non-linearity. residual plot
Given a simple linear regression model, we can plot the residuals, 
$$
e_i = y_i - \hat{y}_i,
$$ 
versus the predictor $x_i.$ 

In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values $\hat{y}_i.$ Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.


If the residual plot indicates that there are non-linear associations in the
data, then a simple approach is to use non-linear transformations of the
predictors, such as 
$$
\log(X),\; \sqrt{X},\; \text{or}\; X^2
$$ 
in the regression model. In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue.


**2. Correlation of Error Terms**


An important assumption of the linear regression model is that the error
terms, $\epsilon_1, \epsilon_2, \dots , \epsilon_n$, are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that $\epsilon_i$ is positive provides little or no information about the sign of $\epsilon_{i+1}.$  The **standard errors** that are computed for the estimated regression coefficients or the fitted values are based on the **assumption of uncorrelated error terms**.  If in fact there is correlation among the error terms, then the estimated standard errors will tend to **underestimate** the true standard errors. 


Correlations among the error terms typically occur in time series data. 


**3. Non-constant Variance of Error Terms**

Another important assumption of the linear regression model is that the
error terms have a constant variance, 
$$
Var(\epsilon_i) = \sigma^2.
$$ 
The standard formulas for standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.


One can identify non-constant variances in the errors ("heteroscedasticity"), from the using diagnostic residual plots. Often one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values. 

When faced with this problem, one possible solution is to transform the response $Y$ using a concave function such as 
$$
\log(Y)\;\text{ or }\; \sqrt{Y}.
$$
Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. 

```{r}
## Quadratic regression 
lmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)

## Quadratic regression with transformed response log(Y)
lmobj_3 <- lm(I(log(mpg)) ~ horsepower + I(horsepower^2), data = Auto)

## Diagnostic Plot
par(mfrow = c(1,2))
plot(lmobj_2, which = 1)
plot(lmobj_3, which = 1)
```


**4. Outliers**

An outlier is a point for which $y_i$ is far from the value predicted by the outlier
model. Outliers can arise for a variety of reasons, such as incorrect recording
of an observation during data collection.

Outliers typically have a strong effect on the $R^2$ value since they add a very large residual to its computation. 

Figure 3.12 in the textbook `ISLR` shows an outlier (observation 20) which has a typical predictor value $x_i$. Such outliers have little effect on the regression fit. 
![](images/Fig_3_12.png)


Figure 3.13 in the textbook `ISLR` shows an outlier (observation 41) which a predictor value $x_i$ that is very atypical. Such outliers have large **leverage** giving them power to considerable affect the regression fit. 

![](images/Fig_3_13.png)


Summary: Critical outliers have both, large residuals *and* large leverage.


**5. High Leverage Points**

In order to quantify an observation's leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage. For a simple linear regression,
$$
h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_{j=1}^n(x_j-\bar{x})^2}
$$
There is a simple extension of $h_i$ to the case of multiple predictors, though
we do not provide the formula here. 


* The leverage statistic $h_i$ is always between $1/n$ and $1$
* The average leverage for all the observations is equal to $\bar{h}=\frac{1}{n}\sum_{i=1}^n h_i=(p + 1)/n.$ 
* If a given observation has a leverage statistic $h_i$ that greatly exceeds $(p+1)/n,$ then we may suspect that the corresponding point has high leverage.


**6. Collinearity**

Collinearity refers to the situation in which two or more predictor variables are closely related to one another. 


```{r}
library("ISLR2")
data(Credit) # names(Credit)

par(mfrow=c(1,2))
plot(y = Credit$Age,    x = Credit$Limit, main = "Not Collinear", ylab = "Age", xlab = "Limit")
plot(y = Credit$Rating, x = Credit$Limit, main = "High Collinearity", ylab = "Rating", xlab = "Limit")
```


![](images/Fig_3_15.png)

![](images/Tab_3_11.png)


We call this situation *multicollinearity*. 


To detect multicollinearity issues, one can use the variance inflation factor (VIF) 
$$
\operatorname{VIF}(\hat{\beta}_j)=\frac{1}{1-R^2_{X_j|X_-j}},
$$
where $R^2_{X_j|X_-j}$ is the $R^2$ from a regression of $X_j$ onto all of the other predictors. 

* If $R^2_{X_j|X_-j}$ is close to one, then multicollinearity is present, ans so $\operatorname{VIF}(\hat{\beta}_j)$ will be large. 


In the Credit data, a regression of balance on age, rating, and limit indicates that the predictors have VIF values of 1.01, 160.67, and 160.59. As we suspected, there is considerable collinearity in the data!


Possible solutions:

1. Drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression
fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. <br> **Caution:** In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases. 

2. Combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.

3. Use a different estimation procedure like ridge regression. 

4. Live with it. Sometimes you're not allowed to drop or combine variables (e.g. important control variables) and also no other estimation procedure can be used. Then you have to live with large standard errors due to multicollinearity. But at least you know where the large stand errors are coming from. 



### Comparison of Linear Regression with K-Nearest Neighbors (Ch. 3.5) {-}


Linear regression is an example of a parametric approach because it assumes a linear functional form for $f(X).$

Advantages of parametric approaches:

* Often easy to fit 
* Simple interpretation
* Simple inference

... since there are only a few parameters that need to be computed, interpreted, and tested. 


Disadvantages of parametric approaches: 

* The parametric model assumption can be far from true; i.e.
$$
f(X) \neq \beta_0+\beta_1X_1+\dots+\beta_pX_p 
$$


Solution: **Non-parametric methods** such as *K-nearest neighbors regression* since they do not explicitly assume a parametric form for $f(X).$


#### K-nearest neighbors regression (KNN regression) {-}

Given a value for $K$ and a prediction point $x_0,$ KNN regression
regression ...

1. identifies the $K$ training observations that are closest to
$x_0$, represented by the index set $\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}.$ 
2. estimates $f(x_0)$ using the average of all the training responses $y_i$ with $i\in\mathcal{N}_0.$ 

In other words,
$$
\hat{f}(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}y_i.
$$



![](images/Fig_3_16.png)


In general, the optimal value for $K$ will depend on the *bias-variance tradeoff*, which we introduced in Chapter 2. 

* A small value for $K$ provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation of $K=1$.
* A large value of $K$ provide a smoother and less wiggly fit; the
prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in $f(X).$


In Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of $K$ in KNN regression.



Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of $f.$




## `R`-Lab: Linear Regression


### Libraries

The `library()` function is used to load *libraries*, or groups of  functions and data sets that are not included in the base `R` distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution,  but more exotic functions require additional libraries. Here we load the `MASS` package, which is a very large collection of data sets and functions. We  also load the `ISLR2` package, which includes the data sets associated with this book.

```{r chunk1}
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(ISLR2))
```


If you receive an error message when loading any of these libraries, it
likely indicates that the corresponding library has not yet been
installed on your system. Some libraries, such as `MASS`, come with `R`
and do not need to be separately installed on your computer. However,
other packages, such as `ISLR2`, must be downloaded the first time they
are used. This can be done directly from within `R`. For example, on a
Windows system,  select the `Install package` option
under the `Packages` tab.  After you select any mirror site, a
list of available packages will appear. Simply select the package you
wish to install and `R` will automatically download the
package. Alternatively, this can be done at the `R` command line
via `install.packages("ISLR2")`. This installation only needs
to be done the first time you use a package. However, the
`library()` function must be called within each `R` session.

### Simple Linear Regression

The `ISLR2` library contains the `Boston`  data set, which
records `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).

```{r chunk2}
head(Boston)
```

To find out more about the data set, we can type `?Boston`.

We will start by using the `lm()` function to fit a simple  linear regression model, with `medv` as the response and `lstat`  as the predictor. The basic syntax is `lm(y ~ x, data)`, where `y` is the response, `x` is the predictor, and `data` is the data set in which these two variables are kept.

```{r chunk3, error=TRUE}
lm.fit <- lm(medv ~ lstat)
```

The command causes an error because `R` does not know where to find the variables `medv` and `lstat`. The next line tells `R` that the variables are in `Boston`. If we attach `Boston`, the first line works fine because `R` now recognizes the variables.

```{r chunk4}
lm.fit <- lm(medv ~ lstat, data = Boston)
attach(Boston)
lm.fit <- lm(medv ~ lstat)
```

If we type `lm.fit`,  some basic information about the model is output. For more detailed information, we use `summary(lm.fit)`. This gives us $p$-values and standard errors for the coefficients, as well as the $R^2$ statistic and $F$-statistic for the model.


```{r chunk5}
lm.fit
summary(lm.fit)
```

We can use the `names()` function in order to find out what other pieces of information  are stored in `lm.fit`. Although we can extract these quantities by name---e.g. `lm.fit$coefficients`---it is safer to use the extractor functions like `coef()` to access them.

```{r chunk6}
names(lm.fit)
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command. %Type `confint(lm.fit)` at the command line to obtain the confidence intervals.

```{r chunk7}
confint(lm.fit)
```
The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`.

```{r chunk8}
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))),
    interval = "confidence")
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))),
    interval = "prediction")
```

For instance, the 95\,\% confidence interval associated with a `lstat` value of 10 is $(24.47, 25.63)$, and the 95\,\% prediction interval is $(12.828, 37.28)$. As expected, the confidence and prediction intervals are centered around the same point (a predicted value of $25.05$ for `medv` when `lstat` equals 10), but the latter are substantially wider.

We will now plot `medv` and `lstat` along with the least squares regression line using the `plot()` and `abline()` functions.

```{r chunk9}
plot(lstat, medv)
abline(lm.fit)
```

There is some evidence for non-linearity in the relationship between `lstat` and `medv`. We will explore this issue  later in this lab.

The `abline()` function can be used to draw any line, not just the least squares regression line. To draw a line with intercept `a` and slope `b`, we  type `abline(a, b)`. Below we experiment with some additional settings for plotting lines and points. The `lwd = 3` command causes the width of the regression line to be increased by a factor of 3;  this works for the `plot()` and `lines()` functions also. We can also use the `pch` option to create different plotting symbols.

```{r chunk10}
plot(lstat, medv)
abline(lm.fit, lwd = 3)
abline(lm.fit, lwd = 3, col = "red")
plot(lstat, medv, col = "red")
plot(lstat, medv, pch = 20)
plot(lstat, medv, pch = "+")
plot(1:20, 1:20, pch = 1:20)
```


Next we examine some diagnostic plots, several of which were discussed
in Section 3.3.3. Four diagnostic plots are automatically
produced by applying the `plot()` function directly to the output
from `lm()`. In general, this command will produce one plot at a
time, and hitting *Enter* will generate the next plot. However,
it is often convenient to view all four plots together. We can achieve
this by using the `par()` and `mfrow()` functions, which tell `R` to split
the display screen into separate panels so that multiple plots can be
viewed simultaneously. For example,  `par(mfrow = c(2, 2))` divides the plotting
region into a $2 \times 2$ grid of panels.

```{r chunk11}
par(mfrow = c(2, 2))
plot(lm.fit)
```

Alternatively, we can compute the residuals from a linear regression
fit using the `residuals()` function. The function
`rstudent()` will return the studentized residuals, and we
can use this function to plot the residuals against the fitted values.

```{r chunk12}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

On the basis of the residual plots, there is some evidence of non-linearity.
Leverage statistics can be computed for any number of predictors using the `hatvalues()` function.

```{r chunk13}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

The `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.

### Multiple Linear Regression

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y ~ x1 + x2 + x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors.

```{r chunk14}
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

The `Boston` data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors.
Instead, we can use the following short-hand:

```{r chunk15}
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by name
(type `?summary.lm` to see what is available). Hence
`summary(lm.fit)$r.sq` gives us the $R^2$, and
`summary(lm.fit)$sigma` gives us the RSE. The `vif()`
function, part of the `car` package, can be used to compute variance
inflation factors.   Most VIF's are
low to moderate for this data. The `car` package is not part of the base `R` installation so it must be downloaded the first time you use it via the `install.packages()` function in `R`.

```{r chunk16}
library(car)
vif(lm.fit)
```

What if we would like to perform a regression using all of the variables but one?  For example, in the above regression output,  `age` has a high $p$-value. So we may wish to run a regression excluding this predictor.
 The following syntax results in a regression using all predictors except `age`.

```{r chunk17}
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
```

Alternatively, the `update()` function can be used.

```{r chunk18}
lm.fit1 <- update(lm.fit, ~ . - age)
```


### Interaction Terms

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells `R` to include an interaction term between `lstat` and `black`.
The syntax `lstat * age` simultaneously includes `lstat`, `age`, and the interaction term `lstat`$\times$`age` as predictors; it is a shorthand for `lstat + age + lstat:age`.
  %We can also pass in transformed versions of the predictors.

```{r chunk19}
summary(lm(medv ~ lstat * age, data = Boston))
```


### Non-linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor $X$, we can create a predictor $X^2$ using
 `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula object; wrapping as we do allows the standard usage in `R`, which is to raise `X` to the power `2`. We now
perform a regression of `medv` onto `lstat` and `lstat^2`.

```{r chunk20}
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
```

The near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model.
We use the `anova()` function  to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{r chunk21}
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat^2`.
The `anova()` function performs a hypothesis test
comparing the two models. The   null hypothesis is that the two models fit the data equally well,  and the alternative hypothesis is that the full model is superior. Here the $F$-statistic is $135$
 and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`.
 This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type

```{r chunk22}
par(mfrow = c(2, 2))
plot(lm.fit2)
```

 then we see that when the `lstat^2` term is included in the model, there is little discernible pattern in the residuals.

In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a
fifth-order polynomial fit:

```{r chunk23}
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
```

This suggests that including additional  polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant $p$-values
in a regression fit.

 By default, the `poly()` function orthogonalizes the predictors:
 this means that the features output by this function are not simply a
 sequence of powers of the argument. However, a linear model applied to the output of the `poly()` function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the `poly()` function,  the argument `raw = TRUE` must be used.

Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.

```{r chunk24}
summary(lm(medv ~ log(rm), data = Boston))
```


### Qualitative Predictors

We will now examine the `Carseats` data, which is part of the
`ISLR2` library. We will  attempt to predict `Sales`
(child car seat sales) in $400$ locations based on a number of
predictors.

```{r chunk25}
head(Carseats)
```

The `Carseats` data includes qualitative predictors such as `shelveloc`, an indicator of the quality of the shelving location---that is, the  space within a store in which the car seat is displayed---at each location. The predictor `shelveloc` takes on three possible values:  *Bad*, *Medium*, and *Good*. Given a qualitative variable such as `shelveloc`, `R` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.

```{r chunk26}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, 
    data = Carseats)
summary(lm.fit)
```

The `contrasts()` function returns the coding that `R` uses for the dummy variables.

```{r chunk27}
attach(Carseats)
contrasts(ShelveLoc)
```

Use `?contrasts` to learn about other contrasts, and how to set them.

`R` has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise.
A bad shelving location corresponds to a zero for each of the two dummy variables.
The fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.


### Writing  Functions

As we have seen, `R` comes with many useful functions, and still more functions are available by way of `R` libraries.
However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the `ISLR2` and `MASS` libraries, called
`LoadLibraries()`. Before we have created the function, `R` returns an error if we try to call it.

```{r chunk28, error=TRUE}
LoadLibraries
LoadLibraries()
```

We now create the function. 

```{r chunk29}
LoadLibraries <- function() {
 library(ISLR2)
 library(MASS)
 print("The libraries have been loaded.")
}
```

Now if we type in `LoadLibraries`, `R` will tell us what is in the function.

```{r chunk30}
LoadLibraries
```

If we call the function, the libraries are loaded in and the print statement is output.

```{r chunk31}
LoadLibraries()
```


## Exercises

Prepare the following exercises of Chapter 3 in our course textbook `ISLR`: 

- Exercise 1
- Exercise 2
- Exercise 3
- Exercise 8
- Exercise 9

<!-- {{< include Ch3_Solutions.qmd >}} -->