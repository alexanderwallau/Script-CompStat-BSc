[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer-Aided Statistical Analysis (B.Sc.)",
    "section": "",
    "text": "Day \n    Time \n    Lecture Hall \n  \n \n\n  \n    Monday \n    12:15-13:45 \n    Jur / Hörsaal K \n  \n  \n    Wednesday \n    16:15-17:45 \n    Jur / Hörsaal K \n  \n\n\n\n\n\n\n\n\n\nCourse Textbook (ISLR):\n\nAn Introduction to Statistical Learning (2nd Edition), by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\nThe pdf-Version of the textbook ISLR can be downloaded for free: Free Book\n\nOnline resources (datasets, etc.) for the book can be found HERE.\neWhiteboard for the lecture notes.\nThis online script\n\nThe above links to the lecture materials can also be found at eCampus\n\n\n\n\nYou can use the Zulip-Chat CompStat (B.Sc.) to post questions, share codes, etc. Happy sharing and discussing!"
  },
  {
    "objectID": "Ch2_StatLearning.html",
    "href": "Ch2_StatLearning.html",
    "title": "2  Statistical Learning",
    "section": "",
    "text": "This tutorial aims to serve as an introduction to the software package R. Other very good and much more exhaustive tutorials and useful reference-cards can be found at the following links:\n\nReference card for R commands (always useful)\nThe official Introduction to R (very detailed)\nAnd many more at www.r-project.org (see “Documents”) \nAn R-package for learning R: www.swirl.com\nAn excellent book project which covers also advanced issues such as “writing performant code” and “package development”: adv-r.had.co.nz\n\nAnother excellent book: R for Data Science\n\nSome other tutorials:\n\nIntroduction to data science\nCreating dynamic graphics\n\nWhy R?\n\nR is free of charge from: www.r-project.org\nThe celebrated IDE RStudio for R is also free of charge: www.rstudio.com\nR is equipped with one of the most flexible and powerful graphics routines available anywhere. For instance, check out one of the following repositories:\n\nClean Graphs\nPublication Ready Plots\n\nToday, R is the de-facto standard for statistical science.\n\n\n\nLets start the tutorial with a (very) short glossary:\n\nConsole: The thing with the > sign at the beginning.\nScript file: An ordinary text file with suffix .R. For instance, yourFavoritFileName.R.\nWorking directory: The file-directory you are working in. Useful commands: with getwd() you get the location of your current working directory and setwd() allows you to set a new location for it.\nWorkspace: This is a hidden file (stored in the working directory), where all objects you use (e.g., data, matrices, vectors, variables, functions, etc.) are stored. Useful commands: ls() shows all elements in our current workspace and rm(list=ls()) deletes all elements in our current workspace.\n\n\n\n\nA good idea is to use a script file such as yourFavoritFileName.R in order to store your R commands. You can send single lines or marked regions of your R-code to the console by pressing the keys STRG+ENTER.\nTo begin with baby steps, do some simple computations:\n\n2+2 # and all the others: *,/,-,^2,^3,... \n\n[1] 4\n\n\nNote: Everything that is written after the #-sign is ignored by R, which is very useful to comment your code.\nThe assignment operator <- or = will be your most often used tool. Here an example to create a scalar variable:\n\nx <- 4 \nx\n\n[1] 4\n\n4 -> x # possible but unusual\nx\n\n[1] 4\n\n\nNote: The R community loves the <- assignment operator, which is a very unusual syntax. Alternatively, you can use the more common = operator which is also used in languages like python or matlab.\nAnd now a more interesting object - a vector:\n\ny <- c(2,7,4,1)\ny\n\n[1] 2 7 4 1\n\n\nThe command ls() shows the total content of your current workspace, and the command rm(list=ls()) deletes all elements of your current workspace:\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nNote: RStudio’s Environment pane also lists all the elements in your current workspace. That is, the command ls() becomes a bit obsolete when working with RStudio.\nLet’s try how we can compute with vectors and scalars in R.\n\nx <- 4\ny <- c(2,7,4,1)\n\nx*y # each element in the vector, y, is multiplied by the scalar, x.\n\n[1]  8 28 16  4\n\ny*y # this is a term by term product of the elements in y\n\n[1]  4 49 16  1\n\n\nPerforming vector multiplications as you might expect from your last math-course, e.g., an outer product: \\(y\\,y^\\top\\):\n\ny %*% t(y)\n\n     [,1] [,2] [,3] [,4]\n[1,]    4   14    8    2\n[2,]   14   49   28    7\n[3,]    8   28   16    4\n[4,]    2    7    4    1\n\n\nOr an inner product \\(y^\\top y\\):\n\nt(y) %*% y\n\n     [,1]\n[1,]   70\n\n\nNote: Sometimes, R’s treatment of vectors can be annoying. The product y %*% y is treated as the product t(y) %*% y.\nThe term-by-term execution as in the above example, y*y, is actually a central strength of R. We can conduct many operations vector-wisely:\n\ny^2\n\n[1]  4 49 16  1\n\nlog(y)\n\n[1] 0.6931472 1.9459101 1.3862944 0.0000000\n\nexp(y)\n\n[1]    7.389056 1096.633158   54.598150    2.718282\n\ny-mean(y)\n\n[1] -1.5  3.5  0.5 -2.5\n\n(y-mean(y))/sd(y) # standardization \n\n[1] -0.5669467  1.3228757  0.1889822 -0.9449112\n\n\nThis is a central characteristic of so called matrix based languages like R (or Matlab). Other programming languages often have to use loops instead:\n\nN <- length(y)\n1:N\n\ny.sq <- numeric(N)\ny.sq\n\nfor(i in 1:N){\n  y.sq[i] <- y[i]^2\n  if(i == N){\n    print(y.sq)\n  }\n}\n\nThe for()-loop is the most common loop. But there is also a while()-loop and a repeat()-loop. However, loops in R can be rather slow, therefore, try to avoid them!\n\nUseful commands to produce sequences of numbers:\n\n1:10\n-10:10\n?seq # Help for the seq()-function\nseq(from=1, to=100, by=7)\n\nUsing the sequence command 1:16, we can go for our first matrix:\n\n?matrix\nA <- matrix(data=1:16, nrow=4, ncol=4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA <- matrix(1:16, 4, 4)\n\nNote that a matrix has always two dimensions, but a vector has only one dimension:\n\ndim(A)    # Dimension of matrix A?\n\n[1] 4 4\n\ndim(y)    # dim() does not operate on vectors.\n\nNULL\n\nlength(y) # Length of vector y?\n\n[1] 4\n\n\nLets play a bit with the matrix A and the vector y. As we have seen in the loop above, the []-operator selects elements of vectors and matrices:\n\nA[,1]\nA[4,4]\ny[c(1,4)]\n\nThis can be done on a more logical basis, too. For example, if you want to know which elements in the first column of matrix A are strictly greater than 2:\n\nA[,1][A[,1]>2]\n\n[1] 3 4\n\n# Note that this give you a boolean vector:\nA[,1]>2\n\n[1] FALSE FALSE  TRUE  TRUE\n\n# And you can use it in a non-sense relation, too:\ny[A[,1]>2]\n\n[1] 4 1\n\n\nNote: Logical operations return so-called boolean objects, i.e., either a TRUE or a FALSE. For instance, if we ask R whether 1>2 we get the answer FALSE.\n\n\n\nBesides classical data objects such as scalars, vectors, and matrices there are three further data objects in R:\n\nThe array: As a matrix but with more dimensions. Here is an example of a \\(2\\times 2\\times 2\\)-dimensional array:\n\n\nmyFirst.Array <- array(c(1:8), dim=c(2,2,2)) # Take a look at it!\n\n\nThe list: In lists you can organize different kinds of data. E.g., consider the following example:\n\n\nmyFirst.List <- list(\"Some_Numbers\" = c(66, 76, 55, 12, 4, 66, 8, 99), \n                     \"Animals\"      = c(\"Rabbit\", \"Cat\", \"Elefant\"),\n                     \"My_Series\"    = c(30:1)) \n\nA very useful function to find specific values and entries within lists is the str()-function:\n\nstr(myFirst.List)\n\nList of 3\n $ Some_Numbers: num [1:8] 66 76 55 12 4 66 8 99\n $ Animals     : chr [1:3] \"Rabbit\" \"Cat\" \"Elefant\"\n $ My_Series   : int [1:30] 30 29 28 27 26 25 24 23 22 21 ...\n\n\n\nThe data frame: A data.frame is a list-object but with some more formal restrictions (e.g., equal number of rows for all columns). As indicated by its name, a data.frame-object is designed to store data:\n\n\nmyFirst.Dataframe <- data.frame(\"Credit_Default\"   = c( 0, 0, 1, 0, 1, 1), \n                                \"Age\"              = c(35,41,55,36,44,26), \n                                \"Loan_in_1000_EUR\" = c(55,65,23,12,98,76)) \n# Take a look at it!\n\n\n\n\nAlright, let’s do some statistics with real data. You can download the data HERE. Save it on your computer, at a place where you can find it, and give the path (e.g. \"C:\\textbackslash path\\textbackslash auto.data.csv\", which references to the data, to the file-argument of the function read.csv():\n\n# ATTENTION! YOU HAVE TO CHANGE \"\\\" TO \"/\":\nauto.data <- read.csv(file=\"C:/your_path/autodata.txt\", header=TRUE)\nhead(auto.data)\n\nIf you have problems to read the data into R, go on with these commands. (For this you need a working internet connection!):\n\n# install.packages(\"readr\")\nlibrary(\"readr\")\nauto.data <- suppressMessages(read_csv(file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\",col_names = TRUE))\n# head(auto.data)\n\nYou can select specific variables of the auto.data using the $-operator:\n\ngasolin.consumption      <- auto.data$MPG.city\ncar.weight               <- auto.data$Weight\n## Take a look at the first elements of these vectors:\nhead(cbind(gasolin.consumption,car.weight))\n\n     gasolin.consumption car.weight\n[1,]                  25       2705\n[2,]                  18       3560\n[3,]                  20       3375\n[4,]                  19       3405\n[5,]                  22       3640\n[6,]                  22       2880\n\n\nThis is how you can produce your first plot:\n\n## Plot the data:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy Light-Weight Cars!\")\n\n\n\n\nFigure 2.1: Scatterplot of Gasoline consumption (mpg) vs. car weight.\n\n\n\n\nAs a first step, we might assume a simple kind of linear relationship between the variables gasolin.consumption and car.weight. Let us assume that the data was generated by the following simple regression model: \\[\ny_i=\\alpha+\\beta_1 x_i+\\varepsilon_i,\\quad i=1,\\dots,n\n\\] where \\(y_i\\) denotes the gasoline-consumption, \\(x_i\\) the weight of car \\(i\\), and \\(\\varepsilon_i\\) is a mean zero constant variance noise term. (This is clearly a non-sense model!)\nThe command lm() computes the estimates of this linear regression model. The command (in fact it’s a method) summary() computes further quantities of general interest from the object that was returned from the lm() function.\n\nlm.result   <- lm(gasolin.consumption~car.weight)\nlm.summary  <- summary(lm.result)\nlm.summary\n\n\nCall:\nlm(formula = gasolin.consumption ~ car.weight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7946 -1.9711  0.0249  1.1855 13.8278 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 47.048353   1.679912   28.01   <2e-16 ***\ncar.weight  -0.008032   0.000537  -14.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.038 on 91 degrees of freedom\nMultiple R-squared:  0.7109,    Adjusted R-squared:  0.7077 \nF-statistic: 223.8 on 1 and 91 DF,  p-value: < 2.2e-16\n\n\nOf course, we want to have a possibility to access all the quantities computed so far, e.g., in order to plot the results. This can be done as following:\n\n## Accessing the computed quantities\nnames(lm.summary) ## Alternatively: str(lm.summary)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nalpha <- lm.summary$coefficients[1]\nbeta  <- lm.summary$coefficients[2]\n\n## Plot all:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy light-weight Cars!\")\nabline(a=alpha, \n       b=beta, col=\"red\")\n\n\n\n\nScatterplot of Gasoline consumption (mpg) vs. car weight plus linear regression fit.\n\n\n\n\n\n\n\nLet’s write, i.e., program our own R-function for estimating linear regression models. In order to be able to validate our function, we start with simulating data for which we then know all true parameters.\nSimulating data is like being the “Data-God”: For instance, we generate realizations of the error term \\(\\varepsilon_i\\), i.e., something which we never observe in real data.\nLet us consider the following multiple regression model:\n\\[y_i=\\beta_1 +\\beta_2 x_{2i}+\\beta_3 x_{3i}+\\varepsilon_{i},\\quad i=1,\\dots,n,\\] where \\(\\varepsilon_{i}\\) is a heteroscedastic error term \\[\\varepsilon_{i}\\sim N(0,\\sigma_i^2),\\quad \\sigma_i=|x_{3i}|,\\]\nand where for all \\(i=1,\\dots,n=50\\):\n\n\\(x_{2i}\\sim N(10,1.5^2)\\)\n\\(x_{3i}\\) comes from a t-distribution with 5 degrees of freedom and non-centrality parameter 2\n\n\nset.seed(109) # Sets the \"seed\" of the random number generators:\nn   <- 50     # Number of observations\n\n## Generate two explanatory variables plus an intercept-variable:\nX.1 <- rep(1, n)                 # Intercept\nX.2 <- rnorm(n, mean=10, sd=1.5) # Draw realizations form a normal distr.\nX.3 <- rt(n, df=5, ncp=2)        # Draw realizations form a t-distr.\nX   <- cbind(X.1, X.2, X.3)      # Save as a Nx3-dimensional data matrix.\n\nOK, we have regressors, i.e., data that we also have in real data sets.\nNow we define the elements of the \\(\\beta\\)-vector. Be aware of the difference: In real data sets we do not know the true \\(\\beta\\)-vector, but try to estimate it. However, when simulating data, we determine (as “Data-Gods”) the true \\(\\beta\\)-vector and can compare our estimate \\(\\hat{\\beta}\\) with the true \\(\\beta\\):\n\n## Define the slope-coefficients\nbeta.vec  <- c(1,-5,5)\n\nWe still need to simulate realizations of the dependent variable \\(y_i\\). Remember that \\(y_i=\\beta_1 x_{1i}+\\beta_1 x_{2i}+\\beta_3 x_{3i}+\\varepsilon_{i}\\). That is, we only need realizations from the error terms \\(\\varepsilon_i\\) in order to compute the realizations from \\(y_i\\). This is how you can simulate realizations from the heteroscedastic error terms \\(\\varepsilon_i\\):\n\n## Generate realizations from the heteroscadastic error term\neps       <- rnorm(n, mean=0, sd=abs(X.3))\n\nTake a look at the heteroscedasticity in the error term:\n\nplot(y=eps, x=X.3, \n     main=\"Realizations of the \\nHeteroscedastic Error Term\")\n\n\n\n\nScatterplot of error term realizations (usually unknown) versus the predictor values of X.3.\n\n\n\n\nWith the (pseudo-random) realizations from \\(\\varepsilon_i\\), we can finally generate realizations from the dependent variable \\(y_i\\):\n\n## Dependent variable:\ny   <- X %*% beta.vec + eps\n\nLet’s take a look at the data:\n\nmydata    <- data.frame(\"Y\"=y, \"X.1\"=X.1, \"X.2\"=X.2, \"X.3\"=X.3)\npairs(mydata[,-2]) # The '-2' removes the intercept variable \"X.1\"\n\n\n\n\nOnce we have data, we can compute the OLS estimate of the true \\(\\beta\\) vector. Remember the formula: \\[\\hat{\\beta}=(X^\\top X)^{-1}X^\\top y\\] In R-Code this is: \\((X^\\top X)^{-1}=\\)solve(t(X) %*% X), i.e.:\n\n## Computation of the beta-Vector:\nbeta.hat <- solve(t(X) %*% X) %*% t(X) %*% y\nbeta.hat\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\nWell done. Using the above lines of code we can easily program our own myOLSFun() function!\n\nmyOLSFun <- function(y, x, add.intercept=FALSE){\n  \n  ## Number of Observations:\n  n         <- length(y)\n  \n  ## Add an intercept to x:\n  if(add.intercept){\n    Intercept <- rep(1, n)\n    x         <- cbind(Intercept, x)\n  }\n  \n  ## Estimation of the slope-parameters:\n  beta.hat.vec <- solve(t(x) %*% x) %*% t(x) %*% y\n  \n  ## Return the result:\n  return(beta.hat.vec)\n}\n\n## Run the function:\nmyOLSFun(y=y, x=X)\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\nCan you extend the function for the computation of the covariance matrix of the slope-estimates, several measures of fits (R\\(^2\\), adj.-R\\(^2\\), etc.), t-tests, …?\n\n\n\nOne of the best features in R are its contributed packages. The list of all packages on CRAN is impressive! Take a look at it HERE\nFor instance, nice plots can be produced using the R-package is ggplot2. You can find an intro do this package HERE.\n\n# install.packages(\"ggplot2\")\nlibrary(\"ggplot2\")\n\nqplot(Sepal.Length, Petal.Length, data = iris, color = Species)\n\n\n\n\n\n\n\n\nOf course, ggplot2 concerns “only” plotting, but you’ll find R-packages for almost any statistical method out there.\n\n\n\nThe tidyverse package is a collection of packages that lets you import, manipulate, explore, visualize and model data in a harmonized and consistent way which helps you to be more productive.\nInstalling the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nTo use the tidyverse package load it using the library() function:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.7      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.0 \n✔ purrr   0.3.4      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nChick Weight Data\nR comes with many datasets installed. We will use the ChickWeight dataset to learn (a little) about the tidyverse. The help system gives a basic summary of the experiment from which the data was collect:\n\n“The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups of chicks on different protein diets.”\n\nYou can get more information, including references by typing:\n\nhelp(\"ChickWeight\")\n\nThe Data:  There are 578 observations (rows) and 4 variables:\n\nChick – unique ID for each chick.\nDiet – one of four protein diets.\nTime – number of days since birth.\nweight – body weight of chick in grams.\n\nNote: weight has a lower case w (recall R is case sensitive).\nStore the data locally:\n\nChickWeight %>%\n  dplyr::select(Chick, Diet, Time, weight) %>% \n  dplyr::arrange(Chick, Diet, Time) %>% \n  write_csv(\"DATA/ChickWeight.csv\")\n\nFirst we will import the data from a file called ChickWeight.csv using the read_csv() function from the readr package (part of the tidyverse). The first thing to do, outside of R, is to open the file ChickWeight.csv to check what it contains and that it makes sense. Now we can import the data as follows:\n\nCW <- readr::read_csv(\"DATA/ChickWeight.csv\")\n\nRows: 578 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Chick, Diet, Time, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf all goes well then the data is now stored in an R object called CW. If you get the following error message then you need to change the working directory to where the data is stored:\n\nError: ‘ChickWeight.csv’ does not exist in current working directory …\n\nChanging the working directory: In RStudio you can use the menu bar (“Session - Set Working Directory - Choose Directory…”). Alternatively, you can use the function setwd(). Last but not least, to avoid issues with brocken paths to files and data sets, use RStudios’ “Project” tools.\nLooking at the Dataset: To look at the data type just type the object (dataset) name:\n\nCW\n\n# A tibble: 578 × 4\n   Chick  Diet  Time weight\n   <dbl> <dbl> <dbl>  <dbl>\n 1    18     1     0     39\n 2    18     1     2     35\n 3    16     1     0     41\n 4    16     1     2     45\n 5    16     1     4     49\n 6    16     1     6     51\n 7    16     1     8     57\n 8    16     1    10     51\n 9    16     1    12     54\n10    15     1     0     41\n# … with 568 more rows\n\n\nIf there are too many variables then not all them may be printed. To overcome this issue we can use the glimpse() function which makes it possible to see every column in your dataset (called a “data frame” in R speak).\n\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  <dbl> 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   <dbl> 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight <dbl> 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe function View() allows for a spread-sheet type of view on the data:\n\nView(CW)\n\n\n\nTo visualize the chick weight data, we will use the ggplot2 package (part of the tidyverse). Our interest is in seeing how the weight changes over time for the chicks by diet. For the moment don’t worry too much about the details just try to build your own understanding and logic. To learn more try different things even if you get an error messages.\nLet’s plot the weight data (vertical axis) over time (horizontal axis). Generally, ggplot2 works in layers. The following codes generates an empty plot:\n\n# An empty plot\nggplot(CW, aes(Time, weight))  \n\n\n\n\nEmpty ggplot layer.\n\n\n\n\nTo the empty plot, one can add fuhrer layers:\n\n# Adding a scatter plot \nggplot(CW, aes(Time, weight)) + geom_point() \n\n\n\n\nAdding a scatter plot layer to the empty ggplot layer.\n\n\n\n\nAdd color for Diet. The graph above does not differentiate between the diets. Let’s use a different color for each diet.\n\n# Adding colour for diet\nggplot(CW,aes(Time,weight,colour=factor(Diet))) +\n  geom_point() \n\n\n\n\nAdding a further layer for shown the effect of the Diet.\n\n\n\n\nIt is difficult to conclude anything from this graph as the points are printed on top of one another (with diet 1 underneath and diet 4 at the top).\nTo improve the plot, it will be handy to store Diet and Time as a factor variables.\nFactor Variables: Before we continue, we have to make an important change to the CW dataset by making Diet and Time factor variables. This means that R will treat them as categorical variables (see the <fct> variables below) instead of continuous variables. It will simplify our coding. The next section will explain the mutate() function.\n\nCW <- mutate(CW, Diet = factor(Diet))\nCW <- mutate(CW, Time = factor(Time))\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  <dbl> 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   <fct> 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight <dbl> 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe facet_wrap() function: To plot each diet separately in a grid using facet_wrap():\n\n# Adding jitter to the points\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  geom_point() +\n  facet_wrap(~Diet) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Diet 4 has the least variability but we can’t really say anything about the mean effect of each diet although diet 3 seems to have the highest.\nNext we will plot the mean changes over time for each diet using the stat_summary() function:\n\nggplot(CW, aes(Time, weight, \n               group=Diet, colour=Diet)) +\n  stat_summary(fun=\"mean\", geom=\"line\") \n\n\n\n\nInterpretation: We can see that diet 3 has the highest mean weight gains by the end of the experiment. However, we don’t have any information about the variation (uncertainty) in the data.\nTo see variation between the different diets we use geom_boxplot to plot a box-whisker plot. A note of caution is that the number of chicks per diet is relatively low to produce this plot.\n\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\")\n\n\n\n\nInterpretation: Diet 3 seems to have the highest “average” weight gain but it has more variation than diet 4 which is consistent with our findings so far.\nLet’s finish with a plot that you might include in a publication.\n\nggplot(CW, aes(Time, weight, group=Diet, \n                             colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_point() +\n  # geom_jitter() +\n  stat_summary(fun=\"mean\", geom=\"line\",\n               colour=\"black\") +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\") + \n  xlab(\"Time (days)\") +\n  ylab(\"Weight (grams)\")\n\n\n\n\n\n\n\n\n\n\nIn this section we will learn how to wrangle (manipulate) datasets using the tidyverse package. Let’s start with the mutate(), select(), rename(), filter() and arrange() functions.\nmutate(): Adds a new variable (column) or modifies an existing one. We already used this above to create factor variables.\n\n# Added a column\nCWm1 <- mutate(CW, weightKg = weight/1000)\nCWm1\n\n# A tibble: 578 × 5\n  Chick Diet  Time  weight weightKg\n  <dbl> <fct> <fct>  <dbl>    <dbl>\n1    18 1     0         39    0.039\n2    18 1     2         35    0.035\n3    16 1     0         41    0.041\n# … with 575 more rows\n\n# Modify an existing column\nCWm2 <- mutate(CW, Diet = str_c(\"Diet \", Diet))\nCWm2\n\n# A tibble: 578 × 4\n  Chick Diet   Time  weight\n  <dbl> <chr>  <fct>  <dbl>\n1    18 Diet 1 0         39\n2    18 Diet 1 2         35\n3    16 Diet 1 0         41\n# … with 575 more rows\n\n\nselect(): Keeps, drops or reorders variables.\n\n# Drop the weight variable from CWm1 using minus\ndplyr::select(CWm1, -weight)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weightKg\n  <dbl> <fct> <fct>    <dbl>\n1    18 1     0        0.039\n2    18 1     2        0.035\n3    16 1     0        0.041\n# … with 575 more rows\n\n# Keep variables Time, Diet and weightKg\ndplyr::select(CWm1, Chick, Time, Diet, weightKg)\n\n# A tibble: 578 × 4\n  Chick Time  Diet  weightKg\n  <dbl> <fct> <fct>    <dbl>\n1    18 0     1        0.039\n2    18 2     1        0.035\n3    16 0     1        0.041\n# … with 575 more rows\n\n\nrename(): Renames variables whilst keeping all variables.\n\ndplyr::rename(CW, Group = Diet, Weight = weight)\n\n# A tibble: 578 × 4\n  Chick Group Time  Weight\n  <dbl> <fct> <fct>  <dbl>\n1    18 1     0         39\n2    18 1     2         35\n3    16 1     0         41\n# … with 575 more rows\n\n\nfilter(): Keeps or drops observations (rows).\n\ndplyr::filter(CW, Time==21 & weight>300)\n\n# A tibble: 8 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1     7 1     21       305\n2    29 2     21       309\n3    21 2     21       331\n# … with 5 more rows\n\n\nFor comparing values in vectors use: < (less than), > (greater than), <= (less than and equal to), >= (greater than and equal to), == (equal to) and != (not equal to). These can be combined logically using & (and) and | (or).\narrange(): Changes the order of the observations.\n\ndplyr::arrange(CW, Chick, Time)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1     1 1     0         42\n2     1 1     2         51\n3     1 1     4         59\n# … with 575 more rows\n\ndplyr::arrange(CW, desc(weight))\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1    35 3     21       373\n2    35 3     20       361\n3    34 3     21       341\n# … with 575 more rows\n\n\nWhat does the desc() do? Try using desc(Time).\n\n\n\nIn reality you will end up doing multiple data wrangling steps that you want to save. The pipe operator %>% makes your code nice and readable:\n\nCW21 <- CW %>% \n  dplyr::filter(Time %in% c(0, 21)) %>% \n  dplyr::rename(Weight = weight) %>% \n  dplyr::mutate(Group = factor(str_c(\"Diet \", Diet))) %>% \n  dplyr::select(Chick, Group, Time, Weight) %>% \n  dplyr::arrange(Chick, Time) \nCW21\n\n# A tibble: 95 × 4\n  Chick Group  Time  Weight\n  <dbl> <fct>  <fct>  <dbl>\n1     1 Diet 1 0         42\n2     1 Diet 1 21       205\n3     2 Diet 1 0         40\n# … with 92 more rows\n\n\nHint: To understand the code above we should read the pipe operator %>% as “then”.\n\nCreate a new dataset (object) called CW21 using dataset CW then keep the data for days 0 and 21 then rename variable weight to Weight then create a variable called Group then keep variables Chick, Group, Time and Weight and then finally arrange the data by variables Chick and Time.\n\nThis is the same code:\n\nCW21 <- CW %>% \n  dplyr::filter(., Time %in% c(0, 21)) %>% \n  dplyr::rename(., Weight = weight) %>% \n  dplyr::mutate(., Group=factor(str_c(\"Diet \",Diet))) %>% \n  dplyr::select(., Chick, Group, Time, Weight) %>% \n  dplyr::arrange(., Chick, Time) \n\nThe pipe operator, %>%, replaces the dots (.) with whatever is returned from code preceding it. For example, the dot in filter(., Time %in% c(0, 21)) is replaced by CW. The output of the filter(...) then replaces the dot in rename(., Weight = weight) and so on. Think of it as a data assembly line with each function doing its thing and passing it to the next.\n\n\n\nFrom the data visualizations above we concluded that the diet 3 has the highest mean and diet 4 the least variation. In this section, we will quantify the effects of the diets using summmary statistics. We start by looking at the number of observations and the mean by diet and time.\n\nmnsdCW <- CW %>% \n  dplyr::group_by(Diet, Time) %>% \n  dplyr::summarise(N = n(), Mean = mean(weight)) %>% \n  dplyr::arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nmnsdCW\n\n# A tibble: 48 × 4\n# Groups:   Diet [4]\n  Diet  Time      N  Mean\n  <fct> <fct> <int> <dbl>\n1 1     0        20  41.4\n2 1     2        20  47.2\n3 1     4        19  56.5\n# … with 45 more rows\n\n\nFor each distinct combination of Diet and Time, the chick weight data is summarized into the number of observations (N) and the mean (Mean) of weight.\nFurther summaries: Let’s also calculate the standard deviation, median, minimum and maximum values but only at days 0 and 21.\n\nsumCW <-  CW %>% \n  dplyr::filter(Time %in% c(0, 21)) %>% \n  dplyr::group_by(Diet, Time) %>% \n  dplyr::summarise(N = n(),\n            Mean = mean(weight),\n            SD = sd(weight),\n            Median = median(weight),\n            Min = min(weight),\n            Max = max(weight)) %>% \n  dplyr::arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nsumCW\n\n# A tibble: 8 × 8\n# Groups:   Diet [4]\n  Diet  Time      N  Mean     SD Median   Min   Max\n  <fct> <fct> <int> <dbl>  <dbl>  <dbl> <dbl> <dbl>\n1 1     0        20  41.4  0.995   41      39    43\n2 1     21       16 178.  58.7    166      96   305\n3 2     0        10  40.7  1.49    40.5    39    43\n# … with 5 more rows\n\n\nLet’s make the summaries “prettier”, say, for a report or publication.\n\nlibrary(\"knitr\") # to use the kable() function\nprettySumCW <- sumCW %>% \n dplyr::mutate(`Mean (SD)` = str_c(format(Mean, digits=1),\n           \" (\", format(SD, digits=2), \")\")) %>% \n dplyr::mutate(Range = str_c(Min, \" - \", Max)) %>% \n dplyr::select(Diet, Time, N, `Mean (SD)`, Median, Range) %>%\n dplyr::arrange(Diet, Time) %>% \n kable(format = \"latex\")\nprettySumCW\n\n\n\n\n\n \n  \n    Diet \n    Time \n    N \n    Mean (SD) \n    Median \n    Range \n  \n \n\n  \n    1 \n    0 \n    20 \n    41 ( 0.99) \n    41.0 \n    39 - 43 \n  \n  \n    1 \n    21 \n    16 \n    178 (58.70) \n    166.0 \n    96 - 305 \n  \n  \n    2 \n    0 \n    10 \n    41 ( 1.5) \n    40.5 \n    39 - 43 \n  \n  \n    2 \n    21 \n    10 \n    215 (78.1) \n    212.5 \n    74 - 331 \n  \n  \n    3 \n    0 \n    10 \n    41 ( 1) \n    41.0 \n    39 - 42 \n  \n  \n    3 \n    21 \n    10 \n    270 (72) \n    281.0 \n    147 - 373 \n  \n  \n    4 \n    0 \n    10 \n    41 ( 1.1) \n    41.0 \n    39 - 42 \n  \n  \n    4 \n    21 \n    9 \n    239 (43.3) \n    237.0 \n    196 - 322 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: This summary table offers the same interpretation as before, namely that diet 3 has the highest mean and median weights at day 21 but a higher variation than group 4. However it should be noted that at day 21, diet 1 lost 4 chicks from 20 that started and diet 4 lost 1 from 10. This could be a sign of some health related issues.\n\n\n\n\n\n\n\n\n\n\nhttps://eddelbuettel.github.io/gsir-te/Getting-Started-in-R.pdf\nhttps://www.datacamp.com/courses/free-introduction-to-r\nhttps://swcarpentry.github.io/r-novice-gapminder/\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\n\n\n\n\n\nhttps://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN\nhttp://happygitwithr.com/\nhttps://www.gitkraken.com/\n\n\n\n\n\nhttps://rladies.org/"
  },
  {
    "objectID": "Ch2_StatLearning.html#exercises",
    "href": "Ch2_StatLearning.html#exercises",
    "title": "2  Statistical Learning",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\nPrepare the following exercises of Chapter 2 in our course textbook ISLR:\n\nExercise 7\nExercise 8\nExercise 9\n\n\n2.2.1 Solutions\n\nExercise 7\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Suppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X_1 = X_2 = X_3 = 0\\) using K-nearest neighbors.\n\n\n\nObs.\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(Y\\)\n\n\n\n\n1\n0\n3\n0\nRed\n\n\n2\n2\n0\n0\nRed\n\n\n3\n0\n1\n3\nRed\n\n\n4\n0\n1\n2\nGreen\n\n\n5\n−1\n0\n1\nGreen\n\n\n6\n1\n1\n1\nRed\n\n\n\n7. a) Compute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\).\nAnswer:\n\n# Outcome\nY    <- c(\"red\", \"red\", \"red\", \"green\", \"green\", \"red\")\n# Predictor values\nobs1 <- c( 0, 3, 0)\nobs2 <- c( 2, 0, 0)\nobs3 <- c( 0, 1, 3)\nobs4 <- c( 0, 1, 2)\nobs5 <- c(-1, 0, 1)\nobs6 <- c( 1, 1, 1)\n\n# Test Point\nobs0 <- c(0, 0, 0)\n\n# Create a Vector Dist_vec to store the results\nDist <- numeric(length = 6)\n\n# Compute and store the Euclidean distances\nDist[1] <- sqrt(sum((obs1-obs0)^2)) \nDist[2] <- sqrt(sum((obs2-obs0)^2)) \nDist[3] <- sqrt(sum((obs3-obs0)^2)) \nDist[4] <- sqrt(sum((obs4-obs0)^2)) \nDist[5] <- sqrt(sum((obs5-obs0)^2)) \nDist[6] <- sqrt(sum((obs6-obs0)^2))  \n\n# Print the results\nDist\n\n[1] 3.000000 2.000000 3.162278 2.236068 1.414214 1.732051\n\n\n7. b) What is your prediction with \\(K = 1\\)? Why?\nAnswer:\n\nwhich.min(Dist)\n\n[1] 5\n\nY[which.min(Dist)]\n\n[1] \"green\"\n\n\nClosest \\(K=1\\) neighbor is obs5 and thus, our prediction is Green because Green is the \\(Y\\) value associated to obs5.\n7. c) What is your prediction with \\(K = 3\\)? Why?\nAnswer:\n\norder(Dist)[1:3]\n\n[1] 5 6 2\n\nY[order(Dist)[1:3]]\n\n[1] \"green\" \"red\"   \"red\"  \n\n\nClosest \\(K=3\\) neighbors are obs5, obs6, obs2 and thus, our prediction is Red because it is the \\(Y\\) value associated to obs2 and obs6 (majority rule).\n7. d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for \\(K\\) to be large or small? Why?\nAnswer:\n\nIn the case of a highly nonlinear decision boundary, the neighborhoods of similar \\(Y\\)-values become generally small. Therefore, also \\(K\\) must be chosen relatively small so that we can capture more of the non-linear decision boundary. \n\n\nExercise 8:\nThis exercise relates to the College data set, which can be found in the file College.csv (LINK-TO-DATA). It contains a number of variables for \\(777\\) different universities and colleges in the US. The variables are:\n\nPrivate : Public/private indicator\nApps : Number of applications received\nAccept : Number of applicants accepted\nEnroll : Number of new students enrolled\nTop10perc : New students from top 10% of high school class\nTop25perc : New students from top 25% of high school class\nF.Undergrad : Number of full-time undergraduates\nP.Undergrad : Number of part-time undergraduates\nOutstate : Out-of-state tuition\nRoom.Board : Room and board costs\nBooks : Estimated book costs\nPersonal : Estimated personal spending\nPhD : Percent of faculty with Ph.D.’s\nTerminal : Percent of faculty with terminal degree\nS.F.Ratio : Student/faculty ratio\nperc.alumni : Percent of alumni who donate\nExpend : Instructional expenditure per student\nGrad.Rate : Graduation rate\n\n8. a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\nAnswer:\n\n# Store data into dataframe college\ncollege <- read.csv(\"DATA/College.csv\")\n\n# Print first 10 rows and 5 collumns of the data\nprint(college[c(1:10),c(1:5)])\n\n                              X Private Apps Accept Enroll\n1  Abilene Christian University     Yes 1660   1232    721\n2            Adelphi University     Yes 2186   1924    512\n3                Adrian College     Yes 1428   1097    336\n4           Agnes Scott College     Yes  417    349    137\n5     Alaska Pacific University     Yes  193    146     55\n6             Albertson College     Yes  587    479    158\n7       Albertus Magnus College     Yes  353    340    103\n8                Albion College     Yes 1899   1720    489\n9              Albright College     Yes 1038    839    227\n10    Alderson-Broaddus College     Yes  582    498    172\n\n\n8. b) Look at the data using the fix() function.\nAnswer:\nYou should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:\n\n# Store row names\nrownames(college) <- college[,1]\n\n# pops up a window for data visualization\n# fix(college)\n\n# Alteratively you can use: \n# View(college)\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try:\n\n# Eliminates first column (containing the row names)\ncollege <- college[,-1]\n# fix(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n8. c. i) Use the summary() function to produce a numerical summary of the variables in the data set.\nAnswer:\n\nsummary(college[, 1:5])\n\n   Private               Apps           Accept          Enroll    \n Length:777         Min.   :   81   Min.   :   72   Min.   :  35  \n Class :character   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242  \n Mode  :character   Median : 1558   Median : 1110   Median : 434  \n                    Mean   : 3002   Mean   : 2019   Mean   : 780  \n                    3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902  \n                    Max.   :48094   Max.   :26330   Max.   :6392  \n   Top10perc    \n Min.   : 1.00  \n 1st Qu.:15.00  \n Median :23.00  \n Mean   :27.56  \n 3rd Qu.:35.00  \n Max.   :96.00  \n\n\n8. c. ii) Use the pairs() function to produce a scatterplot matrix of the 2nd to 10th column or variables of the data. Recall that you can reference the 2nd to 10th column of a matrix A using A[,2:10].\nAnswer:\n\npairs(x = college[,2:10])\n\n\n\n\n8. c. iii) Use the boxplot() function to produce side-by-side boxplots of Outstate versus Private.\nAnswer:\n\nboxplot(Outstate~Private, \n        data = college, \n        xlab = \"Private\", \n        ylab = \"Outstate\")\n\n\n\n\n8. c. iv) Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.\n\n# Creating a vector called ELite with only \"No\" entrances amounting the number of college rows\nElite <- rep(\"No\",nrow(college))\n\n# Replacing \"No\" with \"Yes\" if the proportion of students coming from the top 10% of their HS classes exceeds 50%.\nElite[college$Top10perc > 50] <- \"Yes\"\n\n# Encode a vector as a factor\nElite <- as.factor(Elite)\n\n# Add Elite variable to our current dataset \"college\"\ncollege <- data.frame(college, Elite)\n\nUse the summary() function to see how many elite universities there are. Now use the boxplot() function to produce side-by-side boxplots of Outstate versus Elite.\nAnswer:\n\nsummary(college$Elite)\n\n No Yes \n699  78 \n\n\nThere are \\(78\\) elite Universities. The boxplots of Outstate versus Elite-Status are generated as following:\n\nboxplot(Outstate ~ Elite, \n        data = college, xlab=\"Elite\", ylab=\"Outstate\")\n\n\n\n\n8. c. v) Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\nAnswer:\n\npar(mfrow=c(2,2))\nhist(college$Apps,     breaks=50, xlim=c(0,25000), \n     main=\"Apps\")\nhist(college$Enroll,   breaks=25, main=\"Enroll\")\nhist(college$Expend,   breaks=25, main=\"Expend\")\nhist(college$Outstate, main=\"Outstate\")\n\n\n\npar(mfrow=c(1,1))\n\n\n\nExercise 9:\nThis exercise involves the Auto data set. Make sure that the missing values have been removed from the data.\n\n# Store data into dataframe college\nAuto <- read.csv(\"DATA/Auto.csv\", header=T, na.strings=\"?\")\n\n# Remove missing values from the data\nAuto <- na.omit(Auto)\n\n# Print first 10 rows of the data\nprint(Auto[c(1:10),])\n\n   mpg cylinders displacement horsepower weight acceleration year origin\n1   18         8          307        130   3504         12.0   70      1\n2   15         8          350        165   3693         11.5   70      1\n3   18         8          318        150   3436         11.0   70      1\n4   16         8          304        150   3433         12.0   70      1\n5   17         8          302        140   3449         10.5   70      1\n6   15         8          429        198   4341         10.0   70      1\n7   14         8          454        220   4354          9.0   70      1\n8   14         8          440        215   4312          8.5   70      1\n9   14         8          455        225   4425         10.0   70      1\n10  15         8          390        190   3850          8.5   70      1\n                        name\n1  chevrolet chevelle malibu\n2          buick skylark 320\n3         plymouth satellite\n4              amc rebel sst\n5                ford torino\n6           ford galaxie 500\n7           chevrolet impala\n8          plymouth fury iii\n9           pontiac catalina\n10        amc ambassador dpl\n\n# Find more info on the variables here: https://rstudio-pubs-static.s3.amazonaws.com/61800_faea93548c6b49cc91cd0c5ef5059894.html\n\n9. a) Which of the predictors are quantitative, and which are qualitative?\nAnswer:\n\n# Summarize dataset\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:392        \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.54   Mean   :75.98   Mean   :1.577                     \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n\n\n\nQuantitative predictors: mpg, cylinders, displacement, horsepower, weight, acceleration, year\nQualitative predictors: name, origin\n\n9. b) What is the range of each quantitative predictor? You can answer this using the range() function.\nAnswer:\n\n# apply the range function to the first seven columns of Auto\nc <- sapply(Auto[, 1:7], range)\n# print to console\nc\n\n      mpg cylinders displacement horsepower weight acceleration year\n[1,]  9.0         3           68         46   1613          8.0   70\n[2,] 46.6         8          455        230   5140         24.8   82\n\n\n9. c) What is the mean and standard deviation of each quantitative predictor?\nAnswer:\n\n# compute mean for the first seven variables and store it in a vector\nmean <- sapply(Auto[,1:7], mean)\n\n# round the values inside the vectors to 2 decimal cases\nmean <- sapply(mean,round,2)\n\n# compute the standard deviation and round it up \nsd <- sapply(Auto[, 1:7], sd)\nsd <- sapply(sd,round,2)\n\n# print both vectors\nmean\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n       23.45         5.47       194.41       104.47      2977.58        15.54 \n        year \n       75.98 \n\nsd\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n        7.81         1.71       104.64        38.49       849.40         2.76 \n        year \n        3.68 \n\n\n9.d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\nAnswer:\n\n# remove observations and store them \nnewAuto = Auto[-(10:85),]\n\n# Re-do exercises 9. b) and 9.c)\n# This time, create an empty Matrix \"Results\" to store the results\nResults <- matrix(NA, nrow = 4, ncol = 7, \n                  dimnames = list(c(\"Mean\", \"SD\", \"Minimum\", \"Maximum\"), \n                                  c(colnames(newAuto[,1:7]))))\n\n# Store the results\nResults[1,] <- sapply(newAuto[, 1:7], mean)\nResults[2,] <- sapply(newAuto[, 1:7], sd)  # Standard Deviation\nResults[3,] <- sapply(newAuto[, 1:7], min)\nResults[4,] <- sapply(newAuto[, 1:7], max)\n\n# Round them\nResults[] <- sapply(Results[],round,2)\n\n# Print the results\n# Results\nprint(Results[,1:6])\n\n          mpg cylinders displacement horsepower  weight acceleration\nMean    24.40      5.37       187.24     100.72 2935.97        15.73\nSD       7.87      1.65        99.68      35.71  811.30         2.69\nMinimum 11.00      3.00        68.00      46.00 1649.00         8.50\nMaximum 46.60      8.00       455.00     230.00 4997.00        24.80\n\n\n9. e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\nAnswer:\n\npairs(Auto[, -9])\n\n\n\n\n\nheavier weight is related with lower mpg and with higher horsepower;\nhigher horsepower correlates with lower acceleration;\nmpg (miles per gallon) mostly increases for newer model years meaning that cars become more efficient over time.\n\n9. f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\nAnswer:\nYes. On the one hand, as we can see from the plot above, all of the quantitative variables show some sort of relation (either linear or non-linear) with mpg and hence, they might be useful in predicting mpg. The origin qualitative variable might also be useful in predicting mpg, with cars originated from region 3 being associated with higher mpg. On the other hand, the name predictor has too little observations per name though, so using this as a predictor is likely to result in overfitting the data and will not generalize well."
  },
  {
    "objectID": "Ch3_LinearRegression.html#ch.-3.1-simple-linear-regression",
    "href": "Ch3_LinearRegression.html#ch.-3.1-simple-linear-regression",
    "title": "3  Linear Regression",
    "section": "(Ch. 3.1) Simple Linear Regression",
    "text": "(Ch. 3.1) Simple Linear Regression\nThe linear regression model assumes a linear relationship between \\(Y\\) and the predictor(s) \\(X\\).\nThe simple (only one predictor) linear regression model: \\[\nY\\approx \\beta_0 + \\beta_1 X\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV\n\n\n(Ch. 3.1.1) Estimating the Coefficients\nWe choose \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) such that the Residual Sum of Squares criterion is minimized: \\[\n\\begin{align*}\n\\operatorname{RSS}\\equiv \\operatorname{RSS}(\\hat{\\beta}_0,\\hat{\\beta_1})\n& = e_1^2 + \\dots + e_n^2\\\\\n&=)\\sum_{i=1}^n\\left(y_i - \\left(\\hat\\beta_0 + \\hat\\beta_1x_i\\right)\\right)^2\n\\end{align*}\n\\] The minimizers are \\[\n\\hat\\beta_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\] and \\[\n\\hat\\beta_0=\\bar{y} - \\hat\\beta_1\\bar{x},\n\\] where \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i\\) and \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i\\).\n\n\n\n\n(Ch. 3.1.2) Assessing the Accuracy of the Coefficient Estimates\nTrue unknown model \\[\nY=f(X)+\\epsilon\n\\]\nIn in linear regression analysis, we assume1 that \\[\nf(X) = \\beta_0 + \\beta_1 X\n\\]\nOrdinary least squares estimators \\[\n\\hat\\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\n\\] are unbiased, that is \\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat\\beta_0)&=E(\\hat\\beta_0)-\\beta_0=0\\\\\n\\operatorname{Bias}(\\hat\\beta_1)&=E(\\hat\\beta_1)-\\beta_1=0\n\\end{align*}\n\\] I.e., on average, the estimation results equal the true (unknown) parameters. However, in an actual data analysis, we only have one realization of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) computed from one give dataset and thus we cannot compute averages of estimation results. Each single estimation result will have estimation errors, i.e., \\[\n\\hat\\beta_0\\neq \\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\\neq \\beta_1.\n\\]\nThe following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook ISLR.\n\n## ###############################\n## A function to generate data \n## similar to that shown in Fig 3.3\n## ##############################\n\nbeta_0 <- 0.1                          # intercept parameter\nbeta_1 <- 5  \n\n## A Function to simulate data\nmyDataGenerator <- function(){\n  n      <- 50                           # sample size\n  beta_0 <- 0.1                          # intercept parameter\n  beta_1 <- 5                            # slope parameter\n  X      <- runif(n, min = -2, max = 2)  # predictor\n  error  <- rnorm(n, mean = 0, sd = 8.5) # error term\n  Y      <- beta_0 + beta_1 * X + error  # outcome \n  ##\n  return(data.frame(\"Y\" = Y, \"X\" = X))\n}\n\n## Generate a first realization of the data\nset.seed(123)\ndata_sim <- myDataGenerator()\nhead(data_sim)\n\n            Y          X\n1 -18.4853427 -0.8496899\n2  12.9872926  1.1532205\n3  -0.4167901 -0.3640923\n4  -1.9138159  1.5320696\n5  19.5667725  1.7618691\n6  -5.3639241 -1.8177740\n\n\nUsing repeated samples form the data generating process defined in myDataGenerator(), we can generate multiple estimation results of the unknown simple linear regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) and plot the corresponding empirical regression lines:\n\n## Estimation\nlm_obj <- lm(Y ~ X, data = data_sim)\n\n## Plotting the results\npar(mfrow=c(1,2)) # Two plots side by side\n\n## First Plot (fit for the first realization of the data)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\")\nabline(a = beta_0, b = beta_1, col = \"red\")\nabline(lm_obj, col = \"blue\")\n\n## Second Plot (fits for multiple data realizations)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\", type = \"n\") # type = \"n\": empty plot\n##\nfor(r in 1:10){\n  data_sim_new <- myDataGenerator()\n  lm_obj_new   <- lm(Y ~ X, data=data_sim_new)\n  abline(lm_obj_new, col = \"lightskyblue\")\n}\n## Adding the first fit\nabline(a = beta_0, b = beta_1, col = \"red\", lwd = 2)\nabline(lm_obj, col = \"blue\", lwd = 2)\n\n\n\n\n\nCoding-Questions: Can you do this animated? https://gganimate.com/articles/gganimate.html\n\nThe magnitude of the estimation errors is expressed in unites of standard errors: \\[\n\\operatorname{SE}(\\hat\\beta_0)=\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right]\n\\] and \\[\n\\operatorname{SE}(\\hat\\beta_1)=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2},\n\\] where \\(Var(\\epsilon)=\\sigma^2\\Leftrightarrow \\operatorname{SD}(\\epsilon)=\\sqrt{Var(\\epsilon)}=\\sigma\\).\nTypically, \\(\\sigma\\) is unknown, but can be estimated by \\[\n\\sigma\\approx\\hat{\\sigma}=\\operatorname{RSE}=\\sqrt{\\frac{\\operatorname(RSS)}{n-2}},\n\\] where we subtract \\(2\\) from the sampel size \\(n\\) since \\(n-2\\) are the remaining degrees of freedom in the data after estimating two parameters \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\nKnowing \\(\\operatorname{SE}(\\hat\\beta_0)\\) and \\(\\operatorname{SE}(\\hat\\beta_1)\\) allows us to construct Confidence Intervals: \\[\n\\begin{align*}\n\\operatorname{CI}_{\\beta_1}\n&=\\left[\\hat{\\beta_1}-2\\operatorname{SE}(\\hat\\beta_1),\\;\n        \\hat{\\beta_1}+2\\operatorname{SE}(\\hat\\beta_1)\\right]\\\\\n&=\\hat\\beta_1\\pm 2\\operatorname{SE}(\\hat\\beta_1)\n\\end{align*}\n\\] likewise for \\(\\operatorname{CI}_{\\beta_1}\\).\nInterpretation: There is approximately a 95% change (in infinite resamplings) that the (random) confidence interval \\(\\operatorname{CI}_{\\beta_1}\\) contains the true (fix) parameter value \\(\\beta_1\\).\nThus, a given confidence interval either contains the true parameter value or not and we usually do not know it. To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:\n\nInteractive visualization for interpreting confidence intervals\n\nStandard errors can also be used to do hypothesis testing:\n\\[\n\\begin{align*}\nH_0:&\\;\\text{There is no relationship between $Y$ and $X$; i.e. $\\beta_1=0$}\\\\\nH_1:&\\;\\text{There is a relationship between $Y$ and $X$; i.e. $\\beta_1\\neq 0$}\n\\end{align*}\n\\]\n\\(t\\)-test statistic\n\\[\nt=\\frac{\\hat\\beta_1 - 0}{\\operatorname{SE}(\\hat\\beta_1)}\\overset{H_0}{\\sim}t_{(n-1)}\n\\]\n\\(p\\)-value\n\\[\n\\begin{align*}\np\n&=P_{H_0}\\left(|t|\\geq|t_{obs}|\\right)\n&=2\\cdot\\min\\{P_{H_0}\\left(t\\geq t_{obs} \\right),\\; P_{H_0}\\left(t\\leq t_{obs} \\right)\\},\n\\end{align*}\n\\] where \\(t_{obs}\\) denotes the observed value of the \\(t\\)-test statistic and where \\(t\\) is \\(t\\)-distributed with \\((n-2)\\) degrees of freedom.\nSelect a significance level \\(\\alpha\\) (e.g. \\(\\alpha=0.01\\) or \\(\\alpha=0.05\\)) and reject \\(H_0\\) if \\[\np<\\alpha\n\\]\n\n\n\n(Ch. 3.1.3) Assessing the Accuracy of the Model\nIn tendency an accurate model has …\n\na low \\(\\operatorname{RSE}\\) \\[\n\\operatorname{RSE}=\\hat\\sigma=\\sqrt{\\frac{\\operatorname{RSS}}{n-2}}\n\\]\na high \\(R^2\\)\n\n\\[\nR^2=\\frac{\\operatorname{TSS}-\\operatorname{RSS}}{\\operatorname{TSS}}=1-\\frac{\\operatorname{RSS}}{\\operatorname{TSS}},\n\\]\nwhere \\(0\\leq R^2\\leq 1\\) and\n\\[\n\\begin{align*}\n\\operatorname{TSS}&=\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2\\\\\n\\operatorname{RSS}&=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2\\\\\n\\hat{y}_i&=\\hat\\beta_0+\\hat\\beta_1x_i\n\\end{align*}\n\\]\nCaution: Do not forget that there is a irreducible error \\(Var(\\epsilon)=\\sigma^2>0\\). Thus\n\nvery low \\(\\operatorname{RSE}\\) values, \\(\\operatorname{RSE}\\approx 0\\), and\nvery high \\(R^2\\) values, \\(R^2\\approx 1\\),\n\ncan be warning signals indicating overfitting.\nIn the case of the simple linear regression model, \\(R^2\\) equals the squared sample correlation coefficient between \\(Y\\) and \\(X\\), \\[\nR^2 = r^2,\n\\] where \\[\nr=\\widehat{cor}(Y,X)=\\frac{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}.\n\\]"
  },
  {
    "objectID": "Ch3_LinearRegression.html#multiple-linear-regression-ch.-3.2",
    "href": "Ch3_LinearRegression.html#multiple-linear-regression-ch.-3.2",
    "title": "3  Linear Regression",
    "section": "Multiple Linear Regression (Ch. 3.2)",
    "text": "Multiple Linear Regression (Ch. 3.2)\nThe multiple linear regression model allows for more than only one predictor:\n\\[\nY\\approx \\beta_0 + \\beta_1 X_1 +  \\dots + \\beta_p X_p + \\epsilon\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV \\(+\\beta_2\\) radio \\(+\\beta_3\\) newspaper \\(+\\epsilon\\)\n\n\n(Ch. 3.2.1) Estimating the Regression Coefficients\nSelect\n\\[\n\\hat\\beta_0,\\dots,\\hat\\beta_p\n\\] by minimizing \\[\n\\operatorname{RSS}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2,\n\\] where \\[\n\\hat{y}_i=\\hat\\beta_0 + \\hat\\beta_1 x_{i1} \\dots + \\hat\\beta_p x_{ip}\n\\]\n\nMultiple linear regression is more than mere composition of single simple linear regression models.\nTake a look at the following two simple linear regression results:\n\nThus in separate simple linear regressions, the effects of radio and the effect of newspaper on sales are both (but separately) statistically.\nBy contrast, when looking at the multiple linear regression when regressing sales onto both radio and newspaper, only the effect of radio remains statistically significant:\n\nReason: Omitted Variable Bias\n\nradio has an effect on sales\nnewspaper has actually no effect on sales\nBut, newspaper is “strongly” correlated with radio (cor(newspaper,radio)=0.3541); see Table 3.5\n\n\n\nThus, when omitting radio from the multiple regression model, newspaper becomes a surrogate for radio. This is called a Omitted Variable Bias.\n\nConclusion: Simple linear regression can be dangerous. We need to control for all possibly relevant variables if we want to interpret the estimation results (“Inference”).\nInterpretation of the Coefficients in Table 3.5\nFor fixed values of TV and newspaper, spending additionally 1000 USD for radio, increases on average sales by approximately 189 units.\n\n\n(Ch. 3.2.2) Some Important Questions\n1. Is There a Relationship Between the Response and Predictors?\n\\[\n\\begin{align*}\nH_0:&\\;\\beta_1=\\beta_2=\\dots=\\beta_p=0\\\\\nH_1:&\\;\\text{at least one $\\beta_j\\neq 0$; $j=1,\\dots,p$}\n\\end{align*}\n\\]\n\\(F\\)-test statistic \\[\nF=\\frac{(\\operatorname{TSS}-\\operatorname{RSS})/p}{\\operatorname{\n  RSS}/(n-p-1)}\n\\]\nIf \\(H_0\\) is correct \\[\n\\begin{align*}\nE(\\operatorname{RSS}/(n-p-1))&=\\sigma^2\\\\\nE((\\operatorname{TSS}-\\operatorname{RSS})/p)&=\\sigma^2\\\\\n\\end{align*}\n\\]\n\nThus, if \\(H_0\\) is correct, we expect values of \\(F\\approx 1\\).\nBut if \\(H_1\\) is correct, we expect values of \\(F\\gg 1\\).\n\nCaution: Cannot be computed if \\(p>n\\). (Chapter 6 on “high dimensional problems”)"
  },
  {
    "objectID": "Ch3_LinearRegression.html#ch.-3.3-other-considerations-in-the-regression-model",
    "href": "Ch3_LinearRegression.html#ch.-3.3-other-considerations-in-the-regression-model",
    "title": "3  Linear Regression",
    "section": "(Ch. 3.3) Other Considerations in the Regression Model",
    "text": "(Ch. 3.3) Other Considerations in the Regression Model\n\n(Ch. 3.3.1) Qualitative Predictors\nOften some predictors are qualitative variables (also known as a factor variables). For instance, the Credit dataset contains the following qualitative predictors:\n\nown (house ownership)\nstudent (student status)\nstatus (marital status)\nregion (East, West or South)\n\n\nPredictors with Only Two Levels\nIf a qualitative predictor (factor) only has two levels (i.e. possible values), then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values; for instance, \\[\nx_{i} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person owns a house}\\\\\n  0&\\quad \\text{if the $i$th person does not own a house.}\n  \\end{array}\\right.\n\\] Using this dummy variable as a predictor in the regression equation results in the following regression model: \\[\ny_{i}=\\beta_0 + \\beta_1 x_i + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1 + \\epsilon_i &\\quad \\text{if the $i$th person owns a house}\\\\\n  \\beta_0 + \\epsilon_i           &\\quad \\text{if the $i$th person does not own a house}\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The average credit card balance among those who do not own a house\n\\(\\beta_0+\\beta_1\\): The average credit card balance among those who do own a house\n\\(\\beta_1\\): The average difference in credit card balance between owners and non-owners\n\n\nAlternatively, instead of a 0/1 coding scheme, we could create a dummy variable \\[\nx_{i} = \\left\\{\n  \\begin{array}{ll}\n  1 &\\quad \\text{if the $i$th person owns a house}\\\\\n-1 &\\quad \\text{if the $i$th person does not own a house.}\n  \\end{array}\\right.\n\\] \\[\ny_{i}=\\beta_0 + \\beta_1 x_i + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1 + \\epsilon_i&\\quad \\text{if the $i$th person owns a house}\\\\\n  \\beta_0 - \\beta_1 + \\epsilon_i&\\quad \\text{if the $i$th person does not own a house}\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The overall average credit card balance (ignoring the house ownership effect)\n\\(\\beta_1\\): The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.\n\n\n\nQualitative Predictors with More than Two Levels\nWhen a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the region \\(\\in\\{\\)South, West, East\\(\\}\\) variable we create two dummy variables. The first could be \\[\nx_{i1} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person is from the South}\\\\\n  0&\\quad \\text{if the $i$th person is not from the South,}\n  \\end{array}\\right.\n\\] and the second could be \\[\nx_{i2} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person is from the West}\\\\\n  0&\\quad \\text{if the $i$th person is not from the West.}\n  \\end{array}\\right.\n\\] Using both of these dummy variables results in the following regression model: order to obtain the model \\[\ny_{i}=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1  + \\epsilon_i& \\quad \\text{if the $i$th person is from the South}\\\\\n  \\beta_0 + \\beta_2  + \\epsilon_i& \\quad \\text{if the $i$th person is from the West}\\\\\n  \\beta_0            + \\epsilon_i& \\quad \\text{if the $i$th person is from the East.}\\\\\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The average credit card balance for individuals from the East\n\\(\\beta_1\\): The difference in the average balance between people from the South versus the East\n\\(\\beta_2\\): The difference in the average balance between people from the West versus the East\n\n\nThere are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular contrasts. (A detailed discussion of contrasts is beyond the scope of this lecture.)\n\n\n\n(Ch. 3.3.2) Extensions of the Linear Model\n\nInteraction Effects: Removing the Additive Assumption using Interaction Effects\nPreviously, we used the following model\n\nsales \\(= \\beta_0 + \\beta_1\\) TV \\(+ \\beta_2\\) radio \\(+ \\beta_3\\) newspaper \\(+\\epsilon\\)\n\nwhich states that the average increase in sales associated with a one-unit increase in TV is always \\(\\beta_1,\\) regardless of the amount spent on radio.\nHowever, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on radio advertising actually increases the effectiveness of TV advertising.\nFigure 3.5 suggests that such an effect may be present in the advertising data:\n\nWhen levels of either TV or radio are low, then the true sales are lower than predicted by the linear model.\nBut when advertising is split between the two media, then the model tends to underestimate sales. \n\nSolution: Interaction Effects:\nConsider the standard linear regression model with two variables, \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon.\n\\] Here each predictor \\(X_1\\) and \\(X_2\\) has a given effect, \\(\\beta_1\\) and \\(\\beta_2\\), on \\(Y\\) and this effect does not depend on the value of the other predictor. (Additive Assumption)\nOne way of extending this model is to include a third predictor, called an interaction term, which is constructed by computing the product of \\(X_1\\) and \\(X_2.\\) This results in the model \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2 + \\epsilon.\n\\] This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as \\[\n\\begin{align*}\nY &= \\beta_0 + \\underbrace{(\\beta_1 + \\beta_3 X_2)}_{=\\tilde{\\beta}_1} X_1 + \\beta_2 X_2 + \\epsilon,\n\\end{align*}\n\\] where the new slope parameter \\(\\tilde{\\beta}_2\\) is a linear function of \\(X_2\\) \\[\n\\tilde{\\beta}_1\\equiv\\tilde{\\beta}_1(X_2)=\\beta_1 + \\beta_3 X_2.\n\\]\nThus, a change in the value of \\(X_2\\) will change the association between \\(X_1\\) and \\(Y.\\)\nA similar argument shows that a change in the value of \\(X_1\\) changes the association between \\(X_2\\) and \\(Y.\\)\nLet us return to the Advertising example. A linear model that uses radio, TV, and an interaction, radio\\(\\times\\)radio, between the two to predict sales takes the form\n\nsales \\(= \\beta_0 + \\beta_1\\times\\) TV \\(+ \\beta_2\\times\\) radio \\(+ \\beta_3\\times(\\) radio\\(\\times\\) TV\\()+\\epsilon\\)\n\nwhich can be rewritten as\n\nsales \\(=\\beta_0 + (\\beta_1+ \\beta_3\\times\\) radio \\()\\times\\) TV \\(+ \\beta_2\\times\\) radio \\(+\\epsilon\\)\n\n\nInterpretation:\n\n\\(\\beta_3\\) denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising (or vice-versa).\n\n\nInterpretation of Table 3.9:\n\nBoth (separate) main effects, TV and radio, are statistically significant (\\(p\\)-values smaller than 0.01).\nAdditionally, the \\(p\\)-value for the interaction term, TV\\(\\times\\)radio, is extremely low, indicating that there is strong evidence for \\(H_1: \\beta_3\\neq 0.\\) In other words, it is clear that the true relationship is not additive.\n\nHierarchical Principle:\nIf we include an interaction in a model, we should also include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant.\nInteractions with Qualitative Variables:\nAn interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.\nConsider the Credit data set and suppose that we wish to predict balance using the predictors:\n\nincome (quantitative) and\nstudent (qualitative) using a dummy variable with \\(x_{i2}=1\\) if \\(i\\)th person is a student and \\(x_{i2}=0\\) if not.\n\nIn the absence of an interaction term, the model takes the form \nThus, the regression lines for students and non-students have different intercepts, \\(\\beta_0+\\beta_2\\) versus \\(\\beta_0\\), but the same slope \\(\\beta_1\\).\nThis represents a potentially serious limitation of the model, since in fact a change in income may have a very different effect on the credit card balance of a student versus a non-student.\nThis limitation can be addressed by adding an interaction variable, created by multiplying income with the dummy variable for student. Our model now becomes \nNow we have different intercepts for students and non-students but also different slopes for these groups. \n\n\nPolynomial Regression: Non-linear Relationships\nPolynomial regression allows to accommodate non-linear relationships between the predictors \\(X\\) and the outcome \\(Y.\\) \nFor example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form\n\nmpg \\(=\\beta_0 + \\beta_1\\times\\) horsepower \\(+ \\beta_2\\times(\\)horsepower\\()^2+\\epsilon\\)\n\nThis regression model involves predicting mpg using a non-linear function of horsepower. But it is still a linear model! It’s simply a multiple linear regression model with \\(X_1=\\)horsepower and \\(X_2 =(\\)horsepower\\()^2.\\)\nSo we can use standard linear regression software to estimate \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) in order to produce a non-linear fit.\n\n\n\n\n(Ch. 3.3.3) Potential Problems\n1. Non-linearity of the response-predictor relationships.\nDiagnostic residual plots are most useful to detect possible non-linear response-predictor relationships.\n\nlibrary(\"ISLR2\")\ndata(Auto) \n\n## Gives the variable names in the Auto dataset\n# names(Auto)\n\n## Simple linear regression\nlmobj_1 <- lm(mpg ~ horsepower, data = Auto)\n\n## Quadratic regression \nlmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n\n## Diagnostic Plot\npar(mfrow = c(1,2))\nplot(lmobj_1, which = 1)\nplot(lmobj_2, which = 1)\n\n\n\n\nResidual plots are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals, \\[\ne_i = y_i - \\hat{y}_i,\n\\] versus the predictor \\(x_i.\\)\nIn the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values \\(\\hat{y}_i.\\) Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\nIf the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as \\[\n\\log(X),\\; \\sqrt{X},\\; \\text{or}\\; X^2\n\\] in the regression model. In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue.\n2. Correlation of Error Terms\nAn important assumption of the linear regression model is that the error terms, \\(\\epsilon_1, \\epsilon_2, \\dots , \\epsilon_n\\), are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that \\(\\epsilon_i\\) is positive provides little or no information about the sign of \\(\\epsilon_{i+1}.\\) The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors.\nCorrelations among the error terms typically occur in time series data (see Fig. 3.10).\n\n3. Non-Constant Variance of Error Terms\nAnother important assumption of the linear regression model is that the error terms have a constant variance, \\[\nVar(\\epsilon_i) = \\sigma^2.\n\\] The standard formulas for standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.\nOne can identify non-constant variances “heteroscedasticity” in the errors, using diagnostic residual plots.\nOften one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values which indicates. When faced with this problem, one possible solution is to transform the response \\(Y\\) using a concave function such as \\[\n\\log(Y)\\;\\text{ or }\\; \\sqrt{Y}.\n\\] Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.\n\n## Quadratic regression \nlmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n\n## Quadratic regression with transformed response log(Y)\nlmobj_3 <- lm(I(log(mpg)) ~ horsepower + I(horsepower^2), data = Auto)\n\n## Diagnostic Plot\npar(mfrow = c(1,2))\nplot(lmobj_2, which = 1)\nplot(lmobj_3, which = 1)\n\n\n\n\n4. Outliers\nAn outlier is a point for which \\(y_i\\) is far from the value predicted by the outlier model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.\nOutliers typically have a strong effect on the \\(R^2\\) value since they add a very large residual to its computation.\nFigure 3.12 in the textbook ISLR shows a clear outlier (observation 20) which, however, has a typical predictor value \\(x_i\\). Such outliers have little effect on the regression fit. \nFigure 3.13 in the textbook ISLR shows again a clear outlier (observation 41) which has a predictor value \\(x_i\\) that is very atypical. Such outliers are said to have large leverage giving them power to affect the regression fit considerably. \nSummary: Critical outliers have both, large residuals and large leverage.\n5. High Leverage Points\nIn order to quantify an observation’s leverage, we compute the leverage statistic \\(h_i\\) for each observation \\(i=1,\\dots,n.\\) A large value of this statistic indicates an observation with high leverage. For a simple linear regression, \\[\nh_i = \\frac{1}{n} + \\frac{(x_i-\\bar{x})^2}{\\sum_{j=1}^n(x_j-\\bar{x})^2}\n\\] There is a simple extension of \\(h_i\\) to the case of multiple predictors, though we do not provide the formula here.\n\nThe leverage statistic \\(h_i\\) is always between \\(1/n\\) and \\(1\\)\nThe average leverage for all the observations is equal to \\(\\bar{h}=\\frac{1}{n}\\sum_{i=1}^n h_i=(p + 1)/n.\\)\nIf a given observation has a leverage statistic \\(h_i\\) that greatly exceeds \\((p+1)/n,\\) then we may suspect that the corresponding point has high leverage.\n\n6. Collinearity\nCollinearity refers to the situation in which two or more predictor variables are closely related to one another.\n\nlibrary(\"ISLR2\")\ndata(Credit) # names(Credit)\n\npar(mfrow=c(1,2))\nplot(y = Credit$Age,    x = Credit$Limit, main = \"No Collinearity\", ylab = \"Age\", xlab = \"Limit\")\nplot(y = Credit$Rating, x = Credit$Limit, main = \"Strong Collinearity\", ylab = \"Rating\", xlab = \"Limit\")\n\n\n\n\n\n\nWe call this situation multicollinearity.\nTo detect multicollinearity issues, one can use the variance inflation factor (VIF) \\[\n\\operatorname{VIF}(\\hat{\\beta}_j)=\\frac{1}{1-R^2_{X_j|X_-j}},\n\\] where \\(R^2_{X_j|X_-j}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors.\n\nIf \\(R^2_{X_j|X_-j}\\) is close to one, then multicollinearity is present, and \\(\\operatorname{VIF}(\\hat{\\beta}_j)\\) will be large.\n\nIn the Credit data, a regression of balance on age, rating, and limit indicates that the predictors have VIF values of 1.01 (age), 160.67 (rating), and 160.59 (limit). Thus, as we suspected, there is considerable collinearity in the data!\nPossible solutions:\n\nDrop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables.  Caution: In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.\nCombine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.\nUse a different estimation procedure like ridge regression.\nLive with it. Sometimes you’re not allowed to drop or combine variables (e.g. important control variables) and also no other estimation procedure can be used. Then you have to live with large standard errors due to multicollinearity. But at least you know where the large stand errors are coming from."
  },
  {
    "objectID": "Ch3_LinearRegression.html#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors",
    "href": "Ch3_LinearRegression.html#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors",
    "title": "3  Linear Regression",
    "section": "(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors",
    "text": "(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors\nLinear regression is an example of a parametric approach because it assumes a linear model form for \\(f(X).\\)\nAdvantages of parametric approaches:\n\nTypically easy to fit\nSimple interpretation\nSimple inference\n\nDisadvantages of parametric approaches:\n\nThe parametric model assumption can be far from true; i.e. \\[\nf(X) \\neq \\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\n\\]\n\nAlternative: Non-parametric methods such as K-nearest neighbors regression since non-parametric approaches do not explicitly assume a parametric form for \\(f(X).\\)\n\nK-nearest neighbors regression (KNN regression)\nGiven a value for \\(K\\) and a prediction point \\(x_0,\\) KNN regression regression …\n\nidentifies the \\(K\\) training observations that are closest to \\(x_0\\), represented by the index set \\(\\mathcal{N}_0\\subset\\{1,2,\\dots,n_{Train}\\}.\\)\nestimates \\(f(x_0)\\) using the average of all the training responses \\(y_i\\) with \\(i\\in\\mathcal{N}_0.\\)\n\nIn other words, \\[\n\\hat{f}(x_0)=\\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}y_i.\n\\]\n\nIn general, the optimal value for \\(K\\) will depend on the bias-variance tradeoff, which we introduced in Chapter 2.\n\nA small value for \\(K\\) provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation of \\(K=1\\).\nA large value of \\(K\\) provide a smoother and less wiggly fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in \\(f(X).\\)\n\nIn Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of \\(K\\) in KNN regression.\nGenerally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of \\(f\\) and vice versa.\nFigure 3.17 of our textbook ISLR provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent the true \\(f(X)\\), while the blue curves correspond to the KNN fits using \\(K = 1\\) (left plot) and \\(K = 9\\) (right plot). In this case, the \\(K = 1\\) predictions are far too variable, while the smoother \\(K = 9\\) fit is much closer to the true \\(f(X).\\) However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. \nThe blue dashed line in the left-hand panel of Figure 3.18 represents the linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. \nFigure 3.19 displays a non-linear situations in which KNN performs much better than linear regression. \nCurse of dimensionality:\nUnfortunately, in higher dimensions, KNN often performs worse than linear regression, since non-parametric approaches suffer from the curse of dimensionality. Figure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise (i.e. redundant) predictors that are not associated with the response.\n\nWhen \\(p = 1\\) or \\(p = 2\\), KNN outperforms linear regression.\nBut for \\(p = 3\\) the results are mixed, and for \\(p\\geq 4\\) linear regression is superior to KNN. \n\nWhen \\(p=1\\), \\(50\\) data points can provide enough information to estimate \\(f(X)\\) accurately using non-parametric methods since the \\(K\\) nearest neighbors can actually be close to a given test observation \\(x_0.\\) However, when spreading the \\(50\\) data points over a large number of, for instance, \\(p=20\\) dimensions, even the \\(K\\) nearest neighbors tend to become far away from \\(x_0.\\)"
  },
  {
    "objectID": "Ch3_LinearRegression.html#r-lab-linear-regression",
    "href": "Ch3_LinearRegression.html#r-lab-linear-regression",
    "title": "3  Linear Regression",
    "section": "3.2 R-Lab: Linear Regression",
    "text": "3.2 R-Lab: Linear Regression\n\n3.2.1 Libraries\nThe library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with this book.\n\nsuppressPackageStartupMessages(library(MASS))\nsuppressPackageStartupMessages(library(ISLR2))\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session.\n\n\n3.2.2 Simple Linear Regression\nThe ISLR2 library contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (average age of houses), and lstat (percent of households with low socioeconomic status).\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm.fit <- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat.\nThe next line tells R that the variables are in Boston:\n\nlm.fit <- lm(medv ~ lstat, data = Boston)\n\nAlternatively, we can attach the Boston object:\n\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\nWe can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\nType confint(lm.fit) at the command line to obtain the confidence intervals for the linear regression coefficients.\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), \n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), \n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nFor instance, the 95% confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95% prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nThe abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a, b). Below we experiment with some additional settings for plotting lines and points. The lwd = 3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\n\nplot(lstat, medv)\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\nplot(lstat, medv, col = \"red\")\n\n\n\nplot(lstat, medv, pch = 20)\n\n\n\nplot(lstat, medv, pch = \"+\")\n\n\n\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity.\nLeverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nplot(hatvalues(lm.fit))\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\nsort(hatvalues(lm.fit), decreasing = TRUE)[1:3]\n\n       375        415        374 \n0.02686517 0.02495670 0.02097101 \n\n\nThe sort() function can be used to sort and print values of a vector like hatvalues(lm.fit).\n\n\n3.2.3 Multiple Linear Regression\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nsuppressPackageStartupMessages(library(car)) # contains the vif() function\nsort(vif(lm.fit)) # computes the VIF statistics and sorts them\n\n    chas    black     crim  ptratio       rm       zn    lstat      age \n1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 \n     dis    indus      nox      rad      tax \n3.955945 3.991596 4.393720 7.484496 9.008554 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 <- update(lm.fit, ~ . - age)\n\n\n\n3.2.4 Interaction Terms\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells R to include an interaction term between lstat and black. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\n\n\n\n\n3.2.5 Non-linear Transformations of the Predictors\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n3.2.6 Qualitative Predictors\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n3.2.7 Writing Functions\nAs we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError in eval(expr, envir, enclos): object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function.\n\nLoadLibraries <- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries\n\nfunction() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "Ch3_LinearRegression.html#exercises",
    "href": "Ch3_LinearRegression.html#exercises",
    "title": "3  Linear Regression",
    "section": "3.3 Exercises",
    "text": "3.3 Exercises\nPrepare the following exercises of Chapter 3 in our course textbook ISLR:\n\nExercise 1\nExercise 2\nExercise 3\nExercise 8\nExercise 9"
  },
  {
    "objectID": "Ch3_LinearRegression.html#solutions",
    "href": "Ch3_LinearRegression.html#solutions",
    "title": "3  Linear Regression",
    "section": "3.4 Solutions",
    "text": "3.4 Solutions\n\nExercise 1\n1 a) Describe the null hypotheses to which the \\(p\\)-values given in Table 3.4 correspond.\n\n1 b) Explain what conclusions you can draw based on these \\(p\\)-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\nAnswers:\n1 a) In Table 3.4, the null hypothesis for TV is that in the presence of radio ads and newspaper ads, TV ads have no effect on sales. Similarly, the null hypothesis for radio is that in the presence of TV ads and newspaper ads, radio ads have no effect on sales.\n1 b) On the one hand, the low p-values of TV and radio allow us to reject the “no effect” null hypotheses for TV and radio. Hence, we believe that TV (radio) ads have an effect on sales in the presence of radio (TV) and newspaper ads. On the other hand, the high p-value of newspaper does not allow us to reject the “no effect” null-hypothesis. This constitutes an inconclusive result and only says that the possible effects of newspaper ads are not large enough to stand out from the estimation errors.\nRemember: An insignificant hypothesis test result is never informative about whether the tested null hypothesis is true. We do not have an error-control for falsely accepting the null-hypothesis. We only have an error-control (by the significance level) for falsely rejecting the null-hypothesis.\n\n\nExercise 2\nCarefully explain the main difference between the KNN classifier and KNN regression methods.\nAnswer:\nKNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for \\(Y\\) (qualitative), given a certain predictor \\(x_0\\), where as the output for a KNN regression predicts the quantitative value for \\(f(x_0)\\), given a certain predictor \\(x_0\\).\n\n\nExercise 3\nSuppose we have a data set with five predictors:\n\\(X_1 =GPA\\)\n\\(X_2 = IQ\\)\n\\(X_3 = Gender\\) (\\(1\\) for Female and \\(0\\) for Male)\n\\(X_4 =\\) Interaction between \\(GPA\\) and \\(IQ\\)\n\\(X_5 =\\) Interaction between \\(GPA\\) and \\(Gender\\)\nThe response variable (in thousands of dollars) is defined as:\n\\(Y =\\) starting salary after graduation\nSuppose we use least squares to fit the model, and get:\n\\(\\hat{\\beta}_0 = 50\\), \\(\\hat{\\beta}_1 = 20\\), \\(\\hat{\\beta}_2 = 0.07\\), \\(\\hat{\\beta}_3 = 35\\), \\(\\hat{\\beta}_4 = 0.01\\), and \\(\\hat{\\beta}_5 = −10\\).\nThus we have:\n\\[\n\\begin{align*}\n&E[Y|X] = \\\\\n& 50 + 20\\,\\overbrace{GPA}^{X_1} + 0.07\\,\\overbrace{IQ}^{X_2} + 35\\,\\overbrace{Gender}^{X_3} +\n0.01\\,\\overbrace{GPA\\cdot IQ}^{X_4=X_1\\cdot X_2} - 10\\,\\overbrace{GPA\\cdot Gender}^{X_5=X_1\\cdot X_3}\n\\end{align*}\n\\]\n3 a) Which answer is correct, and why?\n\nFor a fixed value of \\(IQ\\) and \\(GPA\\), males earn more on average than females.\nFor a fixed value of \\(IQ\\) and \\(GPA\\), females earn more on average than males.\nFor a fixed value of \\(IQ\\) and \\(GPA\\), males earn more on average than females provided that the \\(GPA\\) is high enough.\nFor a fixed value of \\(IQ\\) and \\(GPA\\), females earn more on average than males provided that the \\(GPA\\) is high enough.\n\nAnswer: Observe that: \\[\n\\begin{align*}\n\\text{Male\\; $(X_3 = 0)$:}\\quad   & 50 + 20 X_1 + 0.07 X_2 + \\phantom{3}0 + 0.01\\,(X_1 \\cdot X_2) -0     \\\\[1.5ex]\n\\text{Female\\; $(X_3 = 1)$:}\\quad & 50 + 20 X_1 + 0.07 X_2 + 35 + 0.01(X_1 \\cdot X_2) - 10\\,X_1\n\\end{align*}\n\\]\nThus 3 a) iii. is correct, since once the \\(X_1=\\)GPA is high enough (\\(35-10\\,X_1<0 \\Leftrightarrow X_1>3.5\\)), males earn more on average.\n3 b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.\nAnswer:\n\nGPA    <-   4\nIQ     <- 110\nGender <-   1 # female = 1\n## Prediction\nY_hat  <- 50 + 20*GPA + 0.07*IQ + 35*Gender + 0.01*GPA*IQ - 10*GPA\nY_hat\n\n[1] 137.1\n\n\n3 c) True or false: Since the coefficient for the GPA\\(\\times\\)IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\nAnswer:\nFalse. We must examine the \\(p\\)-value (or the \\(t\\)-statistic) of the regression coefficient to determine if the interaction term is statistically significant or not.\n\n\nExercise 8\nThis question involves the use of simple linear regression on the Auto data set.\n8 a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results.\n\nlibrary(\"ISLR2\")\n\ndata(\"Auto\")\n\n# Perform linear regression\nlmObj_1 <- lm(mpg ~ horsepower, data=Auto)\n\n# Use summary function to print the results\nsummary(lmObj_1)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\nComment on the output. For example:\ni) Is there a relationship between the predictor and the response?\nAnswer:\nYes, there is. The predictor horsepower has a statistically significant (\\(p<0.001\\)) linear relationship with the response.\nii) How strong is the relationship between the predictor and the response?\nAnswer:\nStatistical significance does not necessarily mean a practically strong or important relationship.\nTo quantify the strength of the relationship between the predictor and the response, we can look at the following quantities:\n\nResidual Standard Error (RSE) (estimate of the standard deviation of \\(\\epsilon\\)) in comparison to the RSE of the trivial linear regression model with only an intercept.\nThe \\(R^2\\) Statistic (the proportion of variance explained by the model)\nThe \\(F\\)-Statistic\n\nThe Residual Standard Error (RSE) of the regression model with intercept and horsepower as predictors is given by:\n\n## RSE of lm(mpg ~ horsepower):\nRSS <- sum(resid(lmObj_1)^2)\nn   <- length(resid(lmObj_1))\nRSE <- sqrt(RSS/(n-2))\nround(RSE, 3)\n\n[1] 4.906\n\n## Alternatively: \nround(summary(lmObj_1)$sigma, 3)\n\n[1] 4.906\n\n\nThis RSE value is considerable smaller than the RSE of a model with only an intercept:\n\nlmObj_onlyIntercept <- lm(mpg ~ +1, data = Auto)\nRSS_onlyIntercept   <- sum(resid(lmObj_onlyIntercept)^2)\nn                   <- length(resid(lmObj_onlyIntercept))\nRSE_onlyIntercept   <- sqrt(RSS_onlyIntercept/(n-1))\nround(RSE_onlyIntercept, 3)\n\n[1] 7.805\n\n\nThus, the larger model with horsepower included explains more of the variances in the response variable mpg. Including horsepower as a predictor reduces the RSE by ((RSE_onlyIntercept - RSE)/RSE_onlyIntercept)*100 %; i.e. by 37.15%.\nThe \\(R^2\\) value:\n\nround(summary(lmObj_1)$r.squared, 2)\n\n[1] 0.61\n\n\nshows that \\(60\\%\\) of variability in \\(Y\\) can be explained using an intercept and horsepower as predictors.\nThe value of the \\(F\\) statistic ::: {.cell}\nround(summary(lmObj_1)$fstatistic, 2)\n\n value  numdf  dendf \n599.72   1.00 390.00 \n\n::: is much larger than \\(1\\) which means that the linear regression model with intercept and horsepower fits the data significantly better than the trivial regression model with only an intercept.\niii) Is the relationship between the predictor and the response positive or negative?\nAnswer:\nThe relationship is negative, as we can see from the parameter estimate for horsepower\n\ncoef(lmObj_1)[2]\n\nhorsepower \n-0.1578447 \n\n\niv) What is the predicted mpg associated with a horsepower of \\(98\\)? What are the associated \\(95\\%\\) confidence and prediction intervals?\nAnswer:\nThe predicted value plus confidence interval:\n\n# Horsepower of 98\nnew_df <- data.frame(horsepower = 98)\n\n# confidence interval \npredict(object = lmObj_1, newdata = new_df, interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\n\nThe predicted value plus prediction interval: ::: {.cell}\n# Horsepower of 98\nnew_df <- data.frame(horsepower = 98)\n\n# prediction interval\npredict(object = lmObj_1, newdata = new_df, interval = \"prediction\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\n:::\n8 b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.\nAnswer:\n\nplot(x = Auto$horsepower, y = Auto$mpg, ylab = \"MPG\", xlab = \"Horsepower\")\nabline(lmObj_1, col=\"blue\")\nlegend(\"topright\", \n       legend = c(\"(y,x)\", expression(paste(\"(\",hat(y),\",x)\"))), \n       pch=c(1,NA), lty=c(NA,1), col=c(\"black\", \"blue\"))\n\n\n\n\n8 c) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\nAnswer:\n\npar(mfrow=c(2,2))\nplot(lmObj_1, col='blue')\n\n\n\n\nLooking at the smoothing line of the residuals (\\(e_i=y_i−\\hat{y}_i\\)) vs. the fitted values (\\(\\hat{y}_i\\)), there is a strong pattern in the residuals, indicating non-linearity. You can see evidence of this also in the scatter plot in the answer for question 8 b).\nThere also appears to be non-constant variance in the error terms (heteroscedasticity), but this may be corrected to an extent when trying a quadratic fit. If not, transformations such as \\(log(y)\\) or \\(\\sqrt{y}\\) can shrink larger responses by a greater amount and reduce this issue.\nThere are some observations with large standardized residuals & high leverage (hence, high Cook’s Distance) that we need to review.\n\n\nExercise 9\nThis question involves the use of multiple linear regression on the Auto data set.\n9 a) Produce a scatterplot matrix which includes all of the variables in the data set.\nAnswer:\n\nlibrary(\"ISLR2\")\n\ndata(\"Auto\")\n\n# Produce scatterplot matrix\npairs(Auto)\n\n\n\n\n9 b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.\nAnswer:\n\nround(cor(subset(Auto, select = -name)), 1)\n\n              mpg cylinders displacement horsepower weight acceleration year\nmpg           1.0      -0.8         -0.8       -0.8   -0.8          0.4  0.6\ncylinders    -0.8       1.0          1.0        0.8    0.9         -0.5 -0.3\ndisplacement -0.8       1.0          1.0        0.9    0.9         -0.5 -0.4\nhorsepower   -0.8       0.8          0.9        1.0    0.9         -0.7 -0.4\nweight       -0.8       0.9          0.9        0.9    1.0         -0.4 -0.3\nacceleration  0.4      -0.5         -0.5       -0.7   -0.4          1.0  0.3\nyear          0.6      -0.3         -0.4       -0.4   -0.3          0.3  1.0\norigin        0.6      -0.6         -0.6       -0.5   -0.6          0.2  0.2\n             origin\nmpg             0.6\ncylinders      -0.6\ndisplacement   -0.6\nhorsepower     -0.5\nweight         -0.6\nacceleration    0.2\nyear            0.2\norigin          1.0\n\n\n9 c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output by answering the below questions 9 c i) to 9 c iii).\nAnswer:\n\n# Perform multiplie linear regression\nfit.lm <- lm(mpg ~ . -name, data=Auto)\n\n# Print results\nsummary(fit.lm)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n\n9 c i) Is there a relationship between the predictors and the response?\nAnswer:\nYes, there is a relationship between the predictors and the response. By testing the null hypothesis of whether all (except intercept) the regression coefficients are zero (i.e. H\\(_0\\): \\(\\beta_1=\\dots=\\beta_7=0\\)), we can see that the \\(F\\)-statistic is big and its \\(p\\)-value is close to zero, indicating evidence against the null hypothesis.\n9 c ii) Which predictors appear to have a statistically significant relationship to the response?\nAnswer:\nLooking at the \\(p\\)-values associated with each predictor’s \\(t\\)-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower, and acceleration do not.\nCaution: This consideration neglects issues due to multiple testing. When testing at the significance level \\(\\alpha=0.05\\), then each single test has a type I error (false H\\(_0\\) rejections) rate of up to \\(5\\%\\). These type I error rates accumulate since we consider seven hypothesis tests simultaneously, and thus the probability of seeing one type I error among the seven tests is up to \\(7\\cdot 5\\%=35\\%\\). So is quite likely to see one type I error.\nBonferroni correction for multiple testing: To determine if any of the seven predictors is statistically significant, the corresponding \\(p\\)-value must be smaller than \\(\\alpha/7\\). For instance, with \\(\\alpha/7=0.05/7\\approx 0.007\\), only weight, year, and origin have a statistically significant relationships to the response.\n9 c iii) What does the coefficient for the year variable suggest?\nAnswer:\nThe regression coefficient for year suggests that, on average, one year later year-of-construction is associated with an increased mpg by \\(0.75\\), when holding every other predictor value constant.\n9 d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\nAnswer:\n\npar(mfrow=c(4,1))\nplot(fit.lm)\n\n\n\n\n\nThe “Residuals vs Fitted” plot (1st plot) shows some systematic deviations of the residuals from \\(0\\). The reason is that we are imposing a straight “line” (better hyper plane) fit for the conditional mean function \\(E[Y|X]=f(X)\\) which appears non-linear here. This results in a systematic underestimation of the true conditional mean function for large and small fitted values \\(\\hat{y}=\\hat\\beta_0+\\hat\\beta_1x_1+\\dots+\\hat\\beta_px_p\\).\nThe “Normal Q-Q” plot (2nd plot) suggests non-normally distributed residuals–particularly the upper tail deviates from that of a normal distribution.\nThe “Residuals vs Leverage” plot (3rd plot) shows that there are some potential outliers that we can see when: standardized residuals are below \\(-2\\) or above \\(+2\\). Moreover, the plot shows also potentially problematic “high-leverage” points with leverage values heavily exceeding the rule-of-thumb threshold \\((p+1)/n=8/392=0.02\\). All points with simultaneously high-leverages and large absolute standardized residuals should be handled with care since these may distort the estimation.\nThe “Scale-Location” plot (4th plot) shows is rather inconclusive about heteroscedasticity. However the “Residuals vs Fitted” plot (1st plot)shows some clear sign of heteroscedastic residuals.\n\n9 e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\nAnswer:\nViolating the hierarchy principle:\n\nfit.lm0 <- lm(mpg ~ horsepower+cylinders+year+weight:displacement, \n              data=Auto)\nsummary(fit.lm0)\n\n\nCall:\nlm(formula = mpg ~ horsepower + cylinders + year + weight:displacement, \n    data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.1046 -2.8861 -0.2415  2.3967 15.3221 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -1.343e+01  5.043e+00  -2.663  0.00807 ** \nhorsepower          -3.914e-02  1.278e-02  -3.063  0.00234 ** \ncylinders           -1.358e+00  3.233e-01  -4.201 3.31e-05 ***\nyear                 6.661e-01  6.019e-02  11.067  < 2e-16 ***\nweight:displacement -3.354e-06  1.352e-06  -2.480  0.01355 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.985 on 387 degrees of freedom\nMultiple R-squared:  0.7419,    Adjusted R-squared:  0.7393 \nF-statistic: 278.2 on 4 and 387 DF,  p-value: < 2.2e-16\n\n\nFollowing the hierarchical principle: ::: {.cell}\nfit.lm1 <- lm(mpg~horsepower+cylinders+year+weight*displacement, \n              data=Auto)\nsummary(fit.lm1)\n\n\nCall:\nlm(formula = mpg ~ horsepower + cylinders + year + weight * displacement, \n    data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7530 -1.8228 -0.0602  1.5780 12.6133 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -2.210e+00  3.819e+00  -0.579  0.56316    \nhorsepower          -3.396e-02  9.560e-03  -3.552  0.00043 ***\ncylinders            2.072e-01  2.914e-01   0.711  0.47756    \nyear                 7.858e-01  4.555e-02  17.250  < 2e-16 ***\nweight              -1.084e-02  6.346e-04 -17.076  < 2e-16 ***\ndisplacement        -7.947e-02  9.905e-03  -8.023 1.26e-14 ***\nweight:displacement  2.431e-05  2.141e-06  11.355  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.976 on 385 degrees of freedom\nMultiple R-squared:  0.8568,    Adjusted R-squared:  0.8546 \nF-statistic: 384.1 on 6 and 385 DF,  p-value: < 2.2e-16\n\n:::\nNote that there is a difference between using A:B and A*B when running a regression. While the first includes only the interaction term between the variable A and B, the second one also includes the stand-alone variables A and B.\nGenerally, you should follow the hierarchical principle for interaction effects: If we include an interaction in a model, we should also include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant.\n9 f)\nTry a few different transformations of the variables, such as \\(\\log(X)\\), \\(\\sqrt{X}\\), \\(X^2\\). Comment on your findings.\nAnswer:\n\nfit.lm2 <- lm(mpg~log(weight)+sqrt(horsepower)+\n                acceleration+I(acceleration^2),\n              data=Auto)\nsummary(fit.lm2)\n\n\nCall:\nlm(formula = mpg ~ log(weight) + sqrt(horsepower) + acceleration + \n    I(acceleration^2), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2932  -2.5082  -0.2237   2.0237  15.7650 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       178.30303   10.80451  16.503  < 2e-16 ***\nlog(weight)       -14.74259    1.73994  -8.473 5.06e-16 ***\nsqrt(horsepower)   -1.85192    0.36005  -5.144 4.29e-07 ***\nacceleration       -2.19890    0.63903  -3.441 0.000643 ***\nI(acceleration^2)   0.06139    0.01857   3.305 0.001037 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.99 on 387 degrees of freedom\nMultiple R-squared:  0.7414,    Adjusted R-squared:  0.7387 \nF-statistic: 277.3 on 4 and 387 DF,  p-value: < 2.2e-16\n\n##\npar(mfrow=c(4,1))\nplot(fit.lm2)\n\n\n\n\nThis try suffers basically from the same issues as the model considered in 9 d)\nLet’s consider again the model with all predictors (except name), but with transforming the outcome variable mpg by a \\(\\log\\)-transformation.\n\nfit.lm3 <-lm(log(mpg)~ . -name, data=Auto)\nsummary(fit.lm3)\n\n\nCall:\nlm(formula = log(mpg) ~ . - name, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40955 -0.06533  0.00079  0.06785  0.33925 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.751e+00  1.662e-01  10.533  < 2e-16 ***\ncylinders    -2.795e-02  1.157e-02  -2.415  0.01619 *  \ndisplacement  6.362e-04  2.690e-04   2.365  0.01852 *  \nhorsepower   -1.475e-03  4.935e-04  -2.989  0.00298 ** \nweight       -2.551e-04  2.334e-05 -10.931  < 2e-16 ***\nacceleration -1.348e-03  3.538e-03  -0.381  0.70339    \nyear          2.958e-02  1.824e-03  16.211  < 2e-16 ***\norigin        4.071e-02  9.955e-03   4.089 5.28e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1191 on 384 degrees of freedom\nMultiple R-squared:  0.8795,    Adjusted R-squared:  0.8773 \nF-statistic: 400.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n##\npar(mfrow=c(4,1))\nplot(fit.lm3)\n\n\n\n\nThis model specification is much better!\n\nNo clear issues of systematic under/over estimations for given fitted values.\nNo clear issues of heteroscedastic residuals.\nNormality assumption may be wrong, but this isn’t problematic since we have a large dataset, such that a central limit theorem will make the estimators asymptotically normal distributed.\nOne large leverage point which, however, has a small residual."
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.1-an-overview-of-classification",
    "href": "Ch4_Classification.html#ch.-4.1-an-overview-of-classification",
    "title": "4  Classification",
    "section": "(Ch. 4.1) An Overview of Classification",
    "text": "(Ch. 4.1) An Overview of Classification\nClassification problems occur often, perhaps even more so than regression problems.\nSome examples include:\n\nA person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\nAn online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.\nOn the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.\n\nOne of the running example for this chapter: The (simulated) Default data set which is part of the online resources of our textbook ISLR, but also contained in the R package ISLR2. \nLet’s take a first look at the a priori default rate in this dataset:\n\nsuppressPackageStartupMessages(library(ISLR2))\ndata(Default)\n\nn <- nrow(Default)       # sample size\n\ntable(Default$default)/n # Overall no-default and default-rate\n\n\n    No    Yes \n0.9667 0.0333 \n\n\n\nOverall default rate: approx. \\(3\\%.\\) (Figure 4.1 shows only a small fraction of the individuals who did not default.)"
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.2-why-not-linear-regression",
    "href": "Ch4_Classification.html#ch.-4.2-why-not-linear-regression",
    "title": "4  Classification",
    "section": "(Ch. 4.2) Why Not Linear Regression?",
    "text": "(Ch. 4.2) Why Not Linear Regression?\nLinear regression is often not appropriate in the case of a qualitative response \\(Y.\\)\nSuppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses:\n\nstroke\ndrug overdose, and\nepileptic seizure\n\nWe can encoding these values as a quantitative response variable, \\[\nY=\\left\\{\n    \\begin{array}{ll}\n    1&\\quad\\text{if }\\texttt{stroke}\\\\\n    2&\\quad\\text{if }\\texttt{drug overdose}\\\\\n    3&\\quad\\text{if }\\texttt{epileptic seizure}\\\\\n    \\end{array}\n\\right.\n\\] Using this coding, least squares could be used to fit a linear regression model to predict \\(Y,\\) but:\n\nThe results would then depend on the numeric ordering \\(1<2<3\\), even though the ordering was completely arbitrary and could have been made differently.\nThe results would then depend on the assumption the gap \\((2-1=1)\\) between stroke and drug overdose is comparable to the gap \\((3-2=1)\\) between drug overdose and epileptic seizure.\n\nGenerally, both points are quite a lot of nonsense for most applications.\nOnly if the response variable’s values did take on a natural ordering, such as “mild”, “moderate”, and “severe”, and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable.\nFor a binary (two level) qualitative response, the situation is better. For instance, if there are only two conditions that we need to predict (either stroke or drug overdoes), we can use a dummy variable coding \\[\nY=\\left\\{\n    \\begin{array}{ll}\n    0&\\quad\\text{if }\\texttt{stroke}\\\\\n    1&\\quad\\text{if }\\texttt{drug overdose}\\\\\n    \\end{array}\n\\right.\n\\] We could then fit a linear regression to this binary response, and predict drug overdose if \\(\\hat{Y}> 0.5\\) and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.\nFor binary responses with a \\(0/1\\) coding, linear regression is not completely unreasonable. It can be shown that \\[\nPr(\\texttt{drug overdose})\\approx \\beta_0+ \\beta_1 X_1+\\dots +\\beta_p X_p.\n\\]\nHowever, if we use linear regression, some of our estimates might be outside the \\([0, 1]\\) interval (see Figure 4.2), which doesn’t make sense when predicting probabilities. \nSummary:\n\nA classic regression method cannot accommodate a qualitative response with more than two classes\nA classic regression method may not provide meaningful estimates of \\(Pr(Y |X),\\) even with just two classes.\n\nThus, it is often preferable to use a classification method that is truly suited for qualitative response values."
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.3-logistic-regression",
    "href": "Ch4_Classification.html#ch.-4.3-logistic-regression",
    "title": "4  Classification",
    "section": "(Ch. 4.3) Logistic Regression",
    "text": "(Ch. 4.3) Logistic Regression\nLogistic regression models the probability that \\(Y\\) belongs to a particular category.\nFor the Default data, logistic regression models the conditional probability of default given values for the predictor(s). For example, the probability of default given balance: \\[\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{balance}) = p(\\texttt{balance}),\n\\] where \\(p(\\texttt{balance})\\in[0,1]\\) is used as a short hand notation.\nOne might predict default\\(=\\)Yes for any individual for whom \\(p(\\texttt{balance}) > 0.5.\\)\nHowever, \\(0.5\\) this is not the only reasonable classification threshold!\nFor instance, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as \\(p(\\texttt{balance}) > 0.1.\\)\n\n(Ch. 4.3.1) The Logistic Model\nFor a binary coded dependen variable \\(Y\\in\\{0,1\\}\\) we aim to model the relationship between \\[\np(X)=Pr(Y=1|X)\\quad\\text{and}\\quad X.\n\\]\nAs discussed above, a linear regression model, e.g., \\[\np(X)=\\beta_0+\\beta_1 X\n\\] can produce nonsense predictions \\(p(X)<0\\) or \\(p(X)>1\\); see the left-hand panel of Figure 4.2.\nLogistic regression avoids this problem by using the logistic function, \\[\np(X)=\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}.\n\\] To fit the parameters \\(\\beta_0\\) and \\(\\beta_1\\) we use an estimation method called maximum likelihood.\nThe right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data.\nNote that \\[\n\\begin{align*}\np(X) & = \\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{1-p(X)} \\\\\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{1-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{\\frac{1+e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n\\quad \\Leftrightarrow\\quad  \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1 X}\n\\end{align*}\n\\]\nThe quantity \\[\n\\frac{p(X)}{1 − p(X)}\n\\] is called the odds, and can take any value between 0 and plus infinity.\n\nA small odds value (close to zero) indicates a low probability of default.\nA large odds value indicates a high probability of default.\n\nFor instance\n\nAn odds value of \\(\\frac{1}{4}\\) means that \\(0.2=20\\%\\) of the people will default \\[\n\\frac{0.2}{1-0.2}=\\frac{1}{4}\n\\]\nAn odds value of \\(9\\) means that \\(0.9=90\\%\\) of the people will default \\[\n\\frac{0.9}{1-0.9}=9\n\\]\n\nBy taking the logarithm, we arrive at log odds or logit \\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0+\\beta_1 X\n\\]\nThus increasing \\(X\\) by one unit …\n\n… changes the log odds by \\(\\beta_1\\)\n… multiplies the odds by \\(e^{\\beta_1}\\)\n\nCaution: The amount that \\(p(X)\\) changes due to a one-unit change in \\(X\\) depends on the current value of \\(X.\\) (The logistic regression model is a non-linear model.)\nBut regardless of the value of \\(X\\), if \\(\\beta_1\\) is positive then increasing \\(X\\) will be associated with increasing \\(p(X)\\), and if \\(\\beta_1\\) is negative then increasing \\(X\\) will be associated with decreasing \\(p(X).\\)\n\n\n(Ch. 4.3.2) Estimating the Regression Coefficients\nIn logistic regression analysis, the unknown model parameters are estimated using maximum likelihood.\nBasic intuition:\nFind estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) such that the predicted probability \\(\\hat{p}(x_i)\\) for each person \\(i\\) corresponds as close as possible to its default status. (I.e. \\(\\hat{p}(x_i)\\approx 1\\) if person \\(i\\) defaulted and \\(\\hat{p}(x_i)\\approx 0\\) if not.)\nThis intuition can be formalized using a mathematical equation called a likelihood function: \\[\n\\ell(\\beta_0,\\beta_1)=\\prod_{i:y_{i}=1} p(x_i)\\prod_{i:y_{i}=0} (1-p(x_i))\n\\] The estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are chosen to maximize this likelihood function.\nMaximum likelihood is a very general estimation method that allows to estimate also non-linear models (like the logistic regression model).\nTable 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default\\(=\\)Yes using balance as the only predictor. \nInterpretation:\n\nWe see that \\(\\hat\\beta_1=0.0055\\); this indicates that an increase in balance is associated with an increase in the probability of default.\n\nTo be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0.0055 units.\n\nThe \\(z\\)-statistic in Table 4.1 plays the same role as the \\(t\\)-statistic in the linear regression output: a large (absolute) value of the \\(z\\)-statistic indicates evidence against the null hypothesis \\(H_0: \\beta_1= 0.\\)\n\n\n\n(Ch. 4.3.3) Making Predictions\nOnce the coefficients have been estimated, we can compute the probability of default\\(=1\\) for any given credit card balance.\nFor example, using the coefficient estimates given in Table 4.1, we predict that the default probability for an individual with a balance-value of 1,000 [USD] is \\[\n\\begin{align*}\n\\hat{p}(\\texttt{balance})\n&=Pr(\\texttt{default}=\\texttt{Yes}|\\texttt{balance})\\\\\n&=\\frac{e^{\\hat\\beta_0+\\hat\\beta_1 \\texttt{balance}}}{1+e^{\\hat\\beta_0+\\hat\\beta_1 \\texttt{balance}}}\\\\\n&=\\frac{e^{-10.6513+ 0.0055\\times 1000}}{1+e^{-10.6513+ 0.0055\\times 1000}} = 0.00576 < 1\\%\n\\end{align*}\n\\]\nBy contrast, the default probability for an individual with a balance-value of 2,000 [USD] equals \\(0.586\\) (or \\(58,6\\%\\)) and is thus much higher.\n\nQualitative Predictors:\n\n\\[\n\\begin{align*}\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{student}=\\texttt{Yes})\n&=\\frac{e^{-3.5041+0.4049\\times 1}}{1+e^{-3.5041+0.4049\\times 1}}\\\\\n&= 0.0431\\\\\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{student}=\\texttt{No})\n&=\\frac{e^{-3.5041+0.4049\\times 0}}{1+e^{-3.5041+0.4049\\times 0}}\\\\\n&= 0.0292\\\\\n\\end{align*}\n\\]\nThis may indicate that students tend to have higher default probabilities than non-students. But we may missed further factors here since we only use one predictor variable.\n\n\n\n(Ch. 4.3.4) Multiple Logistic Regression\nBy analogy with the extension from simple to multiple linear, we can generalize the (simple) logistic regression model as following \\[\n\\begin{align*}\np(X)\n&=\\frac{e^{\\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p}}{1+e^{\\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p}}\\\\\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right)\n&= \\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p,\\\\\n\\end{align*}\n\\] where \\(X=(X_1,\\dots,X_p)\\), and where the unknown parameters \\(\\beta_0,\\dots,\\beta_p\\) are estimated by maximum likelihood. \nThe negative coefficient for student in the multiple logistic regression indicates that for a fixed value of balance and income, a student is less likely to default than a non-student.\n\nInterestingly, the effect of the dummy variable student[Yes] is now negative, in contrast to the estimation results of the (simple) logistic regression in Table 4.2 where it was positive.\nThe left-hand panel of Figure 4.3 provides a graphical illustration of this apparent paradox:\n\nWithout considering balance, the (overall) default rates of students are higher than those of non-students (horizontal broken lines). This overall effect was shown in Table 4.2.\nHowever, for given balance-values, the default rate for students is lower than for non-students (solid lines).\n\n\nThe right-hand panel of Figure 4.3 provides an explanation for this discrepancy: Students tend to hold higher levels of debt, which is in turn associated with higher probability of default.\nSummary:\n\nAn individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance\nHowever, overall, students tend to default at a higher rate than non-students since, overall, they tend to have higher credit card balances.\n\nIn other words: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!\n\n\n(Ch. 4.3.5) Multinomial Logistic Regression\nIt is possible to extend the two-class logistic regression approach to the setting of \\(K > 2\\) classes. This extension is sometimes known as multinomial logistic regression.\nTo do this, we first select a single class to serve as the baseline; without loss of generality, we select the \\(K\\)th class for this role.\nWe model the probabilities that \\(Y=k\\), for \\(k=1,\\dots,K\\), using \\(k\\)-specific parameters \\(\\beta_{k0},\\dots,\\beta_{kp}\\) with \\[\np_k(x)=Pr(Y=k|X=x)=\\frac{e^{\\beta_{k0} + \\beta_{k1} x_1 + \\dots +  \\beta_{kp} x_p}}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots +  \\beta_{lp} x_l}}\n\\] for \\(k=1,\\dots,K-1\\), and \\[\np_K(x)=Pr(Y=K|X=x)=\\frac{1}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots +  \\beta_{lp} x_l}}\n\\]\nFor \\(k=1,\\dots,K-1\\) it holds that \\[\n\\log\\left(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\\right)=\\beta_{k0} + \\beta_{k1} x_1 + \\dots +  \\beta_{kp} x_p\n\\] which is the counterpart to the log odds equation for \\(K=2.\\)\nNote that:\n\nThe predictions \\(\\hat{p}_k(x)\\), for \\(k=1,\\dots,K\\) do not depend on the choice of the baseline class.\nHowever, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline."
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.4-discriminant-analysis-generative-models-for-classification",
    "href": "Ch4_Classification.html#ch.-4.4-discriminant-analysis-generative-models-for-classification",
    "title": "4  Classification",
    "section": "(Ch. 4.4) Discriminant Analysis: Generative Models for Classification",
    "text": "(Ch. 4.4) Discriminant Analysis: Generative Models for Classification\nWe now consider an alternative and less direct approach to estimating the probabilities \\(Pr(Y=K|X=x).\\)\nSuppose that we wish to classify an observation into one of \\(K\\geq 2\\) classes.\nPrior probability: Let \\[\n\\pi_k=Pr(Y=k)\n\\] represent the overall prior probability that a randomly chosen observation comes from class \\(k.\\) We have that \\[\n\\pi_1+\\dots+\\pi_K=1.\n\\]\nDensity function of \\(X\\): Let \\[\nf_k(x)=Pr(X=x|Y=k)\n\\] denote the (conditional) density of \\(X\\) for an observation that comes from class \\(k.\\)1\nPosterior probability: Then Bayes’ theorem states that the probability that a observation with predictor values \\(X=x\\) comes from class \\(k\\) (i.e. the posterior probability), is given by \\[\np_k(x) = Pr(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_l f_l(x)}.\n\\tag{4.1}\\]\nWhile logistic regression aims at estimating the posterior probability \\(p_k(x)\\) directly, Bayes’s theorem gives us a way to estimate \\(p_k(x)\\) indirectly simply by plugging in estimates of \\(\\pi_k\\) and \\(f_k(x)\\).\nHowever, estimating the densities \\(f_k(x)\\) can be very challenging and we therefore have to make some simplifying assumptions. (E.g. assuming that the data is normal distributed.)\nWe know from Chapter 2 that the Bayes classifier, which classifies an observation \\(x\\) to the class \\(k\\) for which \\(p_k(x)\\) is largest, has the lowest possible error rate out of all classifiers.\nIn the following sections, we discuss three classifiers that use different estimates of \\(f_k(x)\\) to approximate the Bayes classifier:\n\nlinear discriminant analysis\nquadratic discriminant analysis\nNaive Bayes\n\n\n(Ch. 4.4.1) Linear Discriminant Analysis for \\(p = 1\\)\nFor the beginning, let us assume that we have only one predictor, i.e., \\(p=1.\\)\nTo estimate \\(f_k(x)\\), we will assume that \\(f_k\\) is normal (or Gaussian). In the simple \\(p=1\\) dimensional setting, the normal distribution is \\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\left(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2\\right),\n\\] where\n\n\\(\\mu_k\\) is the mean of the \\(k\\)th class and\n\\(\\sigma_k^2\\) is the variance of the \\(k\\)th class\n\\(\\pi\\approx 3.14159\\) is the mathematical constant \\(\\pi\\). (Do not confuse it with the prior probabilities \\(\\pi_k.\\))\n\nFor now, let us further assume the simplifying case of equal variances across all classes, i.e.  \\[\n\\sigma_1^2=\\dots = \\sigma_K^2\\equiv \\sigma^2.\n\\] Plugging this assumed version of \\(f_k\\) into Equation 4.1, leads to \\[\np_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\right)}{\\sum_{l=1}^K\\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\right)}.\n\\tag{4.2}\\]\nThe Bayes classifier involves assigning an observation \\(X = x\\) to the class \\(k\\) for which \\(p_k(x)\\) is largest. Taking the \\(\\log\\) of Equation 4.2 (i.e. a monotonic transformation) and rearranging terms shows that this is equivalent to assigning an observation \\(X=x\\) to the class \\(k\\) for which \\[\n\\delta_k(x)=x\\cdot\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\] is largest. The function \\(\\delta_k(x)\\) is called the discriminant function.\nExample:\nIn the case of only two classes, \\(K=2\\), with equal a priori probabilities \\(\\pi_1=\\pi_2\\equiv \\pi^*\\), the Bayes classifier assigns an observation \\(X=x\\) to class \\(1\\) if \\[\n\\begin{align*}\n\\delta_1(x) & > \\delta_2(x)\\\\\n%%%\nx\\cdot\\frac{\\mu_1}{\\sigma^2} - \\frac{\\mu_1^2}{2\\sigma^2} + \\log(\\pi^*)\n& >\nx\\cdot\\frac{\\mu_2}{\\sigma^2} - \\frac{\\mu_2^2}{2\\sigma^2} + \\log(\\pi^*)\\\\\n%%%\nx\\cdot\\frac{\\mu_1}{\\sigma^2} - \\frac{\\mu_1^2}{2\\sigma^2}  \n& >\nx\\cdot\\frac{\\mu_2}{\\sigma^2} - \\frac{\\mu_2^2}{2\\sigma^2}\\\\\n%%%\nx\\cdot\\mu_1 - \\frac{\\mu_1^2}{2}  \n& >\nx\\cdot\\mu_2 - \\frac{\\mu_2^2}{2}\\\\\n%%%\n2x\\cdot(\\mu_1-\\mu_2)   \n& >\n\\mu_1^2 - \\mu_2^2\\\\\n%%%\nx   \n& > \\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1-\\mu_2)}\n\\end{align*}\n\\] The Bayes decision boundary is the point for which \\(\\delta_1(x)=\\delta_2(x)\\) \\[\n\\begin{align*}\nx   \n=\\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1-\\mu_2)}\n&=\\frac{(\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2)}{2(\\mu_1-\\mu_2)}\\\\\n&=\\frac{(\\mu_1 + \\mu_2)}{2}\n\\end{align*}\n\\] Figure 4.4 shows a specific example with\n\n\\(\\pi_1=\\pi_2\\)\n\\(\\mu_1=-1.25\\) and \\(\\mu_2=1.25\\) and\n\\(\\sigma_1=\\sigma_2\\equiv\\sigma =1.\\)\n\nThe two densities \\(f_1\\) and \\(f_2\\) overlap such that for given \\(X_x\\) there is some uncertainty about the class to which the observation belongs to. In this example, the Bayes classifier assigns an observation \\(X=x\\) …\n\n… to class 1 if \\(x<0\\)\n… to class 2 if \\(x>0\\)\n\n\nIn the above example (Figure 4.4) we know all parameters \\(\\pi_k\\), \\(\\mu_k\\), \\(k=1,\\dots,K\\), and \\(\\sigma\\). In practice, however, these parameters are usually unknown, and thus need to estimated from the data.\nThe Linear Discriminant Analysis (LDA) method approximates the Bayes classifier by using the normality assumption and by plugging in estimates for the unknown parameters \\(\\pi_k\\), \\(\\mu_k\\), \\(k=1,\\dots,K\\), and \\(\\sigma\\). The following estimates are used: \\[\n\\begin{align*}\n\\hat\\pi_k & = \\frac{n_k}{n}\\\\\n\\hat\\mu   & = \\frac{1}{n_k}\\sum_{i:y_i=k}x_i\\\\\n\\hat\\sigma   & = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat\\mu_k)^2\\\\\n\\end{align*}\n\\] The LDA classifier assigns an observation \\(X=x\\) to the class \\(k\\) for which \\[\n\\hat\\delta_k(x)=x\\cdot\\frac{\\hat\\mu_k}{\\hat\\sigma^2} - \\frac{\\hat\\mu_k^2}{2\\hat\\sigma^2} + \\log(\\hat\\pi_k)\n\\] is largest.\nLDA is named linear since the discriminant functions \\(\\hat\\delta_k(x)\\), \\(k=1,\\dots,K\\) are linear functions of \\(x.\\)"
  },
  {
    "objectID": "Ch4_Classification.html#r-lab-classification",
    "href": "Ch4_Classification.html#r-lab-classification",
    "title": "4  Classification",
    "section": "4.2 R-Lab: Classification",
    "text": "4.2 R-Lab: Classification\n\n4.2.1 The Stock Market Data\nWe will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over \\(1,250\\)~days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, lagone through lagfive. We have also recorded volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and direction (whether the market was Up or Down on this date). Our goal is to predict direction (a qualitative response) using the other features.\n\nlibrary(ISLR2)\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)\n\n[1] 1250    9\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\npairs(Smarket)\n\n\n\n\nThe cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the direction variable is qualitative.\n\ncor(Smarket)\n\nError in cor(Smarket): 'x' must be numeric\n\ncor(Smarket[, -9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n\n\nAs one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and volume. By plotting the data, which is ordered chronologically, we see that volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.\n\nattach(Smarket)\nplot(Volume)\n\n\n\n\n\n\n4.2.2 Logistic Regression\nNext, we will fit a logistic regression model in order to predict direction using lagone through lagfive and volume. The glm() function can be used to fit many types of generalized linear models , including logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\n\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.446  -1.203   1.065   1.145   1.326  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe smallest \\(p\\)-value here is associated with lagone. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of \\(0.15\\), the \\(p\\)-value is still relatively large, and so there is no clear evidence of a real association between lagone and direction.\nWe use the coef() function in order to access just the coefficients for this fitted model. We can also use the summary() function to access particular aspects of the fitted model, such as the \\(p\\)-values for the coefficients.\n\ncoef(glm.fits)\n\n (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 \n-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 \n      Volume \n 0.135440659 \n\nsummary(glm.fits)$coef\n\n                Estimate Std. Error    z value  Pr(>|z|)\n(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\nLag1        -0.073073746 0.05016739 -1.4565986 0.1452272\nLag2        -0.042301344 0.05008605 -0.8445733 0.3983491\nLag3         0.011085108 0.04993854  0.2219750 0.8243333\nLag4         0.009358938 0.04997413  0.1872757 0.8514445\nLag5         0.010313068 0.04951146  0.2082966 0.8349974\nVolume       0.135440659 0.15835970  0.8552723 0.3924004\n\nsummary(glm.fits)$coef[, 4]\n\n(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 \n  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 \n     Volume \n  0.3924004 \n\n\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form \\(P(Y=1|X)\\), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\n\nglm.probs <- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than \\(0.5\\).\n\nglm.pred <- rep(\"Down\", 1250)\nglm.pred[glm.probs > .5] = \"Up\"\n\nThe first command creates a vector of 1,250 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds \\(0.5\\). Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified. %By inputting two qualitative vectors R will create a two by two table with counts of the number of times each combination occurred e.g. predicted {} and market increased, predicted {} and the market decreased etc.\n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n(507 + 145) / 1250\n\n[1] 0.5216\n\nmean(glm.pred == Direction)\n\n[1] 0.5216\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on \\(507\\)~days and that it would go down on \\(145\\)~days, for a total of \\(507+145 = 652\\) correct predictions. The mean() function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market \\(52.2\\),% of the time.\nAt first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of \\(1,250\\) observations. In other words, \\(100\\%-52.2\\%=47.8\\%\\), is the training error rate. As we have seen previously, the training error rate is often overly optimistic—it tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.\nTo implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.\n\ntrain <- (Year < 2005)\nSmarket.2005 <- Smarket[!train, ]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 <- Direction[!train]\n\nThe object train is a vector of \\(1{,}250\\) elements, corresponding to the observations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to TRUE, whereas those that correspond to observations in 2005 are set to FALSE. The object train is a Boolean vector, since its elements are TRUE and FALSE. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command Smarket[train, ] would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of train are TRUE. The ! symbol can be used to reverse all of the elements of a Boolean vector. That is, !train is a vector similar to train, except that the elements that are TRUE in train get swapped to FALSE in !train, and the elements that are FALSE in train get swapped to TRUE in !train. Therefore, Smarket[!train, ] yields a submatrix of the stock market data containing only the observations for which train is FALSE—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.\n\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\nglm.probs <- predict(glm.fits, Smarket.2005,\n    type = \"response\")\n\nNotice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is \\(52\\),%, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)\nWe recall that the logistic regression model had very underwhelming \\(p\\)-values associated with all of the predictors, and that the smallest \\(p\\)-value, though not very small, corresponded to lagone. Perhaps by removing the variables that appear not to be helpful in predicting direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. Below we have refit the logistic regression using just lagone and lagtwo, which seemed to have the highest predictive power in the original logistic regression model.\n\nglm.fits <- glm(Direction ~ Lag1 + Lag2, data = Smarket,\n    family = binomial, subset = train)\nglm.probs <- predict(glm.fits, Smarket.2005,\n    type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nNow the results appear to be a little better: \\(56\\%\\) of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct \\(56\\%\\) of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a \\(58\\%\\) accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of lagone and lagtwo. In particular, we want to predict direction on a day when lagone and lagtwo equal 1.2 and~1.1, respectively, and on a day when they equal 1.5 and $-$0.8. We do this using the predict() function.\n\npredict(glm.fits,\n    newdata =\n      data.frame(Lag1 = c(1.2, 1.5),  Lag2 = c(1.1, -0.8)),\n    type = \"response\"\n  )\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\n4.2.3 Linear Discriminant Analysis\nNow we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. We fit the model using only the observations before 2005.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit)\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\),% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2~days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The coefficients of linear discriminants output provides the linear combination of lagone and lagtwo that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in ( 4.24). If \\(-0.642\\times `lagone` - 0.514 \\times `lagtwo`\\) is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe plot() function produces plots of the linear discriminants, obtained by computing \\(-0.642\\times `lagone` - 0.514 \\times `lagtwo`\\) for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns a list with three elements. The first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from ( 4.15). Finally, x contains the linear discriminants, described earlier.\n\nlda.pred <- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nAs we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.\n\nlda.class <- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction.2005)\n\n[1] 0.5595238\n\n\nApplying a \\(50\\),% threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.\n\nsum(lda.pred$posterior[, 1] >= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] < .5)\n\n[1] 182\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will decrease:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\),% in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least \\(90\\),%.\n\nsum(lda.pred$posterior[, 1] > .9)\n\n[1] 0\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\),%.\n\n\n4.2.4 Quadratic Discriminant Analysis\nWe will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. The syntax is identical to that of lda().\n\nqda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\nqda.class <- predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\),% of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!\n\n\n4.2.5 Naive Bayes\nNext, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\n\nlibrary(e1071)\nnb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for lagone is \\(0.0428\\) for\nDirection=Down, and the standard deviation is \\(1.23\\). We can easily verify this:\n\nmean(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nThe predict() function is straightforward.\n\nnb.class <- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class. %\n\nnb.preds <- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110\n\n\n\n\n4.2.6 \\(K\\)-Nearest Neighbors\nWe will now perform KNN using the knn() function, which is part of the class library. This function works rather differently from the other model-fitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, knn() forms predictions using a single command. The function requires four inputs.\n\nA matrix containing the predictors associated with the training data, labeled train.X below.\nA matrix containing the predictors associated with the data for which we wish to make predictions, labeled test.X below.\nA vector containing the class labels for the training observations, labeled train.Direction below.\nA value for \\(K\\), the number of nearest neighbors to be used by the classifier.\n\nWe use the cbind() function, short for column bind, to bind the lagone and lagtwo variables together into two matrices, one for the training set and the other for the test set.\n\nlibrary(class)\ntrain.X <- cbind(Lag1, Lag2)[train, ]\ntest.X <- cbind(Lag1, Lag2)[!train, ]\ntrain.Direction <- Direction[train]\n\nNow the knn() function can be used to predict the market’s movement for the dates in 2005. We set a random seed before we apply knn() because if several observations are tied as nearest neighbors, then R will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.\n\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Direction, k = 1)\ntable(knn.pred, Direction.2005)\n\n        Direction.2005\nknn.pred Down Up\n    Down   43 58\n    Up     68 83\n\n(83 + 43) / 252\n\n[1] 0.5\n\n\nThe results using \\(K=1\\) are not very good, since only \\(50\\),% of the observations are correctly predicted. Of course, it may be that \\(K=1\\) results in an overly flexible fit to the data. Below, we repeat the analysis using \\(K=3\\).\n\nknn.pred <- knn(train.X, test.X, train.Direction, k = 3)\ntable(knn.pred, Direction.2005)\n\n        Direction.2005\nknn.pred Down Up\n    Down   48 54\n    Up     63 87\n\nmean(knn.pred == Direction.2005)\n\n[1] 0.5357143\n\n\nThe results have improved slightly. But increasing \\(K\\) further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.\nKNN does not perform well on the Smarket data but it does often provide impressive results. As an example we will apply the KNN approach to the Insurance data set, which is part of the ISLR2 library. This data set includes \\(85\\) predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only \\(6\\),% of people purchased caravan insurance.\n\ndim(Caravan)\n\n[1] 5822   86\n\nattach(Caravan)\nsummary(Purchase)\n\n  No  Yes \n5474  348 \n\n348 / 5822\n\n[1] 0.05977327\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of \\(50\\)~years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of $\\(1{,}000\\) is quite small compared to an age difference of \\(50\\)~years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column \\(86\\), because that is the qualitative Purchase variable.\n\nstandardized.X <- scale(Caravan[, -86])\nvar(Caravan[, 1])\n\n[1] 165.0378\n\nvar(Caravan[, 2])\n\n[1] 0.1647078\n\nvar(standardized.X[, 1])\n\n[1] 1\n\nvar(standardized.X[, 2])\n\n[1] 1\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using \\(K=1\\), and evaluate its performance on the test data.%\n\ntest <- 1:1000\ntrain.X <- standardized.X[-test, ]\ntest.X <- standardized.X[test, ]\ntrain.Y <- Purchase[-test]\ntest.Y <- Purchase[test]\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\n\n[1] 0.118\n\nmean(test.Y != \"No\")\n\n[1] 0.059\n\n\nThe vector test is numeric, with values from \\(1\\) through \\(1,000\\). Typing standardized.X[test, ] yields the submatrix of the data containing the observations whose indices range from \\(1\\) to \\(1,000\\), whereas typing\nstandardized.X[-test, ] yields the submatrix containing the observations whose indices do not range from \\(1\\) to \\(1,000\\). The KNN error rate on the 1,000 test observations is just under \\(12\\),%. At first glance, this may appear to be fairly good. However, since only \\(6\\),% of customers purchased insurance, we could get the error rate down to \\(6\\),% by always predicting No regardless of the values of the predictors!\nSuppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, perhaps a salesperson must visit each potential customer. If the company tries to sell insurance to a random selection of customers, then the success rate will be only \\(6\\),%, which may be far too low given the costs involved. Instead, the company would like to try to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest. Instead, the fraction of individuals that are correctly predicted to buy insurance is of interest.\nIt turns out that KNN with \\(K=1\\) does far better than random guessing among the customers that are predicted to buy insurance. Among \\(77\\) such customers, \\(9\\), or \\(11.7\\),%, actually do purchase insurance. This is double the rate that one would obtain from random guessing.\n\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nUsing \\(K=3\\), the success rate increases to \\(19\\),%, and with \\(K=5\\) the rate is \\(26.7\\),%. This is over four times the rate that results from random guessing. It appears that KNN is finding some real patterns in a difficult data set!\n\nknn.pred <- knn(train.X, test.X, train.Y, k = 3)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  920  54\n     Yes  21   5\n\n5 / 26\n\n[1] 0.1923077\n\nknn.pred <- knn(train.X, test.X, train.Y, k = 5)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  930  55\n     Yes  11   4\n\n4 / 15\n\n[1] 0.2666667\n\n\nHowever, while this strategy is cost-effective, it is worth noting that only 15 customers are predicted to purchase insurance using KNN with \\(K=5\\). In practice, the insurance company may wish to expend resources on convincing more than just 15 potential customers to buy insurance.\nAs a comparison, we can also fit a logistic regression model to the data. If we use \\(0.5\\) as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of \\(0.5\\). If we instead predict a purchase any time the predicted probability of purchase exceeds \\(0.25\\), we get much better results: we predict that 33 people will purchase insurance, and we are correct for about \\(33\\),% of these people. This is over five times better than random guessing!\n\nglm.fits <- glm(Purchase ~ ., data = Caravan,\n    family = binomial, subset = -test)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm.probs <- predict(glm.fits, Caravan[test, ],\n    type = \"response\")\nglm.pred <- rep(\"No\", 1000)\nglm.pred[glm.probs > .5] <- \"Yes\"\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  934  59\n     Yes   7   0\n\nglm.pred <- rep(\"No\", 1000)\nglm.pred[glm.probs > .25] <- \"Yes\"\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  919  48\n     Yes  22  11\n\n11 / (22 + 11)\n\n[1] 0.3333333\n\n\n\n\n4.2.7 Poisson Regression\nFinally, we fit a Poisson regression model to the Bikeshare data set, which measures the number of bike rentals (bikers) per hour in Washington, DC. The data can be found in the ISLR2 library.\n\nattach(Bikeshare)\ndim(Bikeshare)\n\n[1] 8645   15\n\nnames(Bikeshare)\n\n [1] \"season\"     \"mnth\"       \"day\"        \"hr\"         \"holiday\"   \n [6] \"weekday\"    \"workingday\" \"weathersit\" \"temp\"       \"atemp\"     \n[11] \"hum\"        \"windspeed\"  \"casual\"     \"registered\" \"bikers\"    \n\n\nWe begin by fitting a least squares linear regression model to the data.\n\nmod.lm <- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -68.632      5.307 -12.932  < 2e-16 ***\nmnthFeb                      6.845      4.287   1.597 0.110398    \nmnthMarch                   16.551      4.301   3.848 0.000120 ***\nmnthApril                   41.425      4.972   8.331  < 2e-16 ***\nmnthMay                     72.557      5.641  12.862  < 2e-16 ***\nmnthJune                    67.819      6.544  10.364  < 2e-16 ***\nmnthJuly                    45.324      7.081   6.401 1.63e-10 ***\nmnthAug                     53.243      6.640   8.019 1.21e-15 ***\nmnthSept                    66.678      5.925  11.254  < 2e-16 ***\nmnthOct                     75.834      4.950  15.319  < 2e-16 ***\nmnthNov                     60.310      4.610  13.083  < 2e-16 ***\nmnthDec                     46.458      4.271  10.878  < 2e-16 ***\nhr1                        -14.579      5.699  -2.558 0.010536 *  \nhr2                        -21.579      5.733  -3.764 0.000168 ***\nhr3                        -31.141      5.778  -5.389 7.26e-08 ***\nhr4                        -36.908      5.802  -6.361 2.11e-10 ***\nhr5                        -24.135      5.737  -4.207 2.61e-05 ***\nhr6                         20.600      5.704   3.612 0.000306 ***\nhr7                        120.093      5.693  21.095  < 2e-16 ***\nhr8                        223.662      5.690  39.310  < 2e-16 ***\nhr9                        120.582      5.693  21.182  < 2e-16 ***\nhr10                        83.801      5.705  14.689  < 2e-16 ***\nhr11                       105.423      5.722  18.424  < 2e-16 ***\nhr12                       137.284      5.740  23.916  < 2e-16 ***\nhr13                       136.036      5.760  23.617  < 2e-16 ***\nhr14                       126.636      5.776  21.923  < 2e-16 ***\nhr15                       132.087      5.780  22.852  < 2e-16 ***\nhr16                       178.521      5.772  30.927  < 2e-16 ***\nhr17                       296.267      5.749  51.537  < 2e-16 ***\nhr18                       269.441      5.736  46.976  < 2e-16 ***\nhr19                       186.256      5.714  32.596  < 2e-16 ***\nhr20                       125.549      5.704  22.012  < 2e-16 ***\nhr21                        87.554      5.693  15.378  < 2e-16 ***\nhr22                        59.123      5.689  10.392  < 2e-16 ***\nhr23                        26.838      5.688   4.719 2.41e-06 ***\nworkingday                   1.270      1.784   0.711 0.476810    \ntemp                       157.209     10.261  15.321  < 2e-16 ***\nweathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.494      2.965 -22.425  < 2e-16 ***\nweathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16\n\n\nDue to space constraints, we truncate the output of summary(mod.lm). In mod.lm, the first level of hr (0) and mnth (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines. For example, the Feb coefficient of \\(6.845\\) signifies that, holding all other variables constant, there are on average about 7 more riders in February than in January. Similarly there are about 16.5 more riders in March than in January.\nThe results seen in Section 4.6.1 used a slightly different coding of the variables hr and mnth, as follows:\n\ncontrasts(Bikeshare$hr) = contr.sum(24)\ncontrasts(Bikeshare$mnth) = contr.sum(12)\nmod.lm2 <- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm2)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 73.5974     5.1322  14.340  < 2e-16 ***\nmnth1                      -46.0871     4.0855 -11.281  < 2e-16 ***\nmnth2                      -39.2419     3.5391 -11.088  < 2e-16 ***\nmnth3                      -29.5357     3.1552  -9.361  < 2e-16 ***\nmnth4                       -4.6622     2.7406  -1.701  0.08895 .  \nmnth5                       26.4700     2.8508   9.285  < 2e-16 ***\nmnth6                       21.7317     3.4651   6.272 3.75e-10 ***\nmnth7                       -0.7626     3.9084  -0.195  0.84530    \nmnth8                        7.1560     3.5347   2.024  0.04295 *  \nmnth9                       20.5912     3.0456   6.761 1.46e-11 ***\nmnth10                      29.7472     2.6995  11.019  < 2e-16 ***\nmnth11                      14.2229     2.8604   4.972 6.74e-07 ***\nhr1                        -96.1420     3.9554 -24.307  < 2e-16 ***\nhr2                       -110.7213     3.9662 -27.916  < 2e-16 ***\nhr3                       -117.7212     4.0165 -29.310  < 2e-16 ***\nhr4                       -127.2828     4.0808 -31.191  < 2e-16 ***\nhr5                       -133.0495     4.1168 -32.319  < 2e-16 ***\nhr6                       -120.2775     4.0370 -29.794  < 2e-16 ***\nhr7                        -75.5424     3.9916 -18.925  < 2e-16 ***\nhr8                         23.9511     3.9686   6.035 1.65e-09 ***\nhr9                        127.5199     3.9500  32.284  < 2e-16 ***\nhr10                        24.4399     3.9360   6.209 5.57e-10 ***\nhr11                       -12.3407     3.9361  -3.135  0.00172 ** \nhr12                         9.2814     3.9447   2.353  0.01865 *  \nhr13                        41.1417     3.9571  10.397  < 2e-16 ***\nhr14                        39.8939     3.9750  10.036  < 2e-16 ***\nhr15                        30.4940     3.9910   7.641 2.39e-14 ***\nhr16                        35.9445     3.9949   8.998  < 2e-16 ***\nhr17                        82.3786     3.9883  20.655  < 2e-16 ***\nhr18                       200.1249     3.9638  50.488  < 2e-16 ***\nhr19                       173.2989     3.9561  43.806  < 2e-16 ***\nhr20                        90.1138     3.9400  22.872  < 2e-16 ***\nhr21                        29.4071     3.9362   7.471 8.74e-14 ***\nhr22                        -8.5883     3.9332  -2.184  0.02902 *  \nhr23                       -37.0194     3.9344  -9.409  < 2e-16 ***\nworkingday                   1.2696     1.7845   0.711  0.47681    \ntemp                       157.2094    10.2612  15.321  < 2e-16 ***\nweathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.4944     2.9652 -22.425  < 2e-16 ***\nweathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16\n\n\nWhat is the difference between the two codings? In mod.lm2, a coefficient estimate is reported for all but the last level of hr and mnth. Importantly, in mod.lm2, the coefficient estimate for the last level of mnth is not zero: instead, it equals the negative of the sum of the coefficient estimates for all of the other levels. Similarly, in mod.lm2, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all of the other levels. This means that the coefficients of hr and mnth in mod.lm2 will always sum to zero, and can be interpreted as the difference from the mean level. For example, the coefficient for January of \\(-46.087\\) indicates that, holding all other variables constant, there are typically 46 fewer riders in January relative to the yearly average.\nIt is important to realize that the choice of coding really does not matter, provided that we interpret the model output correctly in light of the coding used. For example, we see that the predictions from the linear model are the same regardless of coding:\n\nsum((predict(mod.lm) - predict(mod.lm2))^2)\n\n[1] 1.586608e-18\n\n\nThe sum of squared differences is zero. We can also see this using the all.equal() function:\n\nall.equal(predict(mod.lm), predict(mod.lm2))\n\n[1] TRUE\n\n\nTo reproduce the left-hand side of Figure 4.13, we must first obtain the coefficient estimates associated with mnth. The coefficients for January through November can be obtained directly from the mod.lm2 object. The coefficient for December must be explicitly computed as the negative sum of all the other months.\n\ncoef.months <- c(coef(mod.lm2)[2:12],\n    -sum(coef(mod.lm2)[2:12]))\n\nTo make the plot, we manually label the \\(x\\)-axis with the names of the months.\n\nplot(coef.months, xlab = \"Month\", ylab = \"Coefficient\",\n    xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\",\n    \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n\n\n\n\nReproducing the right-hand side of Figure 4.13 follows a similar process.\n\ncoef.hours <- c(coef(mod.lm2)[13:35],\n    -sum(coef(mod.lm2)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n\n\n\n\nNow, we consider instead fitting a Poisson regression model to the Bikeshare data. Very little changes, except that we now use the function glm() with the argument family = poisson to specify that we wish to fit a Poisson regression model:\n\nmod.pois <- glm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare, family = poisson\n  )\nsummary(mod.pois)\n\n\nCall:\nglm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    family = poisson, data = Bikeshare)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-20.7574   -3.3441   -0.6549    2.6999   21.9628  \n\nCoefficients:\n                           Estimate Std. Error  z value Pr(>|z|)    \n(Intercept)                4.118245   0.006021  683.964  < 2e-16 ***\nmnth1                     -0.670170   0.005907 -113.445  < 2e-16 ***\nmnth2                     -0.444124   0.004860  -91.379  < 2e-16 ***\nmnth3                     -0.293733   0.004144  -70.886  < 2e-16 ***\nmnth4                      0.021523   0.003125    6.888 5.66e-12 ***\nmnth5                      0.240471   0.002916   82.462  < 2e-16 ***\nmnth6                      0.223235   0.003554   62.818  < 2e-16 ***\nmnth7                      0.103617   0.004125   25.121  < 2e-16 ***\nmnth8                      0.151171   0.003662   41.281  < 2e-16 ***\nmnth9                      0.233493   0.003102   75.281  < 2e-16 ***\nmnth10                     0.267573   0.002785   96.091  < 2e-16 ***\nmnth11                     0.150264   0.003180   47.248  < 2e-16 ***\nhr1                       -0.754386   0.007879  -95.744  < 2e-16 ***\nhr2                       -1.225979   0.009953 -123.173  < 2e-16 ***\nhr3                       -1.563147   0.011869 -131.702  < 2e-16 ***\nhr4                       -2.198304   0.016424 -133.846  < 2e-16 ***\nhr5                       -2.830484   0.022538 -125.586  < 2e-16 ***\nhr6                       -1.814657   0.013464 -134.775  < 2e-16 ***\nhr7                       -0.429888   0.006896  -62.341  < 2e-16 ***\nhr8                        0.575181   0.004406  130.544  < 2e-16 ***\nhr9                        1.076927   0.003563  302.220  < 2e-16 ***\nhr10                       0.581769   0.004286  135.727  < 2e-16 ***\nhr11                       0.336852   0.004720   71.372  < 2e-16 ***\nhr12                       0.494121   0.004392  112.494  < 2e-16 ***\nhr13                       0.679642   0.004069  167.040  < 2e-16 ***\nhr14                       0.673565   0.004089  164.722  < 2e-16 ***\nhr15                       0.624910   0.004178  149.570  < 2e-16 ***\nhr16                       0.653763   0.004132  158.205  < 2e-16 ***\nhr17                       0.874301   0.003784  231.040  < 2e-16 ***\nhr18                       1.294635   0.003254  397.848  < 2e-16 ***\nhr19                       1.212281   0.003321  365.084  < 2e-16 ***\nhr20                       0.914022   0.003700  247.065  < 2e-16 ***\nhr21                       0.616201   0.004191  147.045  < 2e-16 ***\nhr22                       0.364181   0.004659   78.173  < 2e-16 ***\nhr23                       0.117493   0.005225   22.488  < 2e-16 ***\nworkingday                 0.014665   0.001955    7.502 6.27e-14 ***\ntemp                       0.785292   0.011475   68.434  < 2e-16 ***\nweathersitcloudy/misty    -0.075231   0.002179  -34.528  < 2e-16 ***\nweathersitlight rain/snow -0.575800   0.004058 -141.905  < 2e-16 ***\nweathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance:  228041  on 8605  degrees of freedom\nAIC: 281159\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe can plot the coefficients associated with mnth and hr, in order to reproduce Figure 4.15:\n\ncoef.mnth <- c(coef(mod.pois)[2:12],\n    -sum(coef(mod.pois)[2:12]))\nplot(coef.mnth, xlab = \"Month\", ylab = \"Coefficient\",\n     xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n\n\n\ncoef.hours <- c(coef(mod.pois)[13:35],\n     -sum(coef(mod.pois)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n\n\n\n\nWe can once again use the predict() function to obtain the fitted values (predictions) from this Poisson regression model. However, we must use the argument type = \"response\" to specify that we want R to output \\(\\exp(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots +\\hat\\beta_p X_p)\\) rather than \\(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots + \\hat\\beta_p X_p\\), which it will output by default.\n\nplot(predict(mod.lm2), predict(mod.pois, type = \"response\"))\nabline(0, 1, col = 2, lwd = 3)\n\n\n\n\nThe predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative. As a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership.\nIn this section, we used the glm() function with the argument family = poisson in order to perform Poisson regression. Earlier in this lab we used the glm() function with family = binomial to perform logistic regression. Other choices for the family argument can be used to fit other types of GLMs. For instance, family = Gamma fits a gamma regression model."
  },
  {
    "objectID": "Ch4_Classification.html#exercises",
    "href": "Ch4_Classification.html#exercises",
    "title": "4  Classification",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\nPrepare the following exercises of Chapter 4 in our course textbook ISLR:\n\nExercise 1\nExercise 6\nExercise 13\nExercise 15\nExercise 16"
  },
  {
    "objectID": "Ch5_ResamplingMethods.html",
    "href": "Ch5_ResamplingMethods.html",
    "title": "5  Resampling Methods",
    "section": "",
    "text": "In this lab, we explore the resampling techniques covered in this chapter. Some of the commands in this lab may take a while to run on your computer.\n\n\nWe explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the Auto data set.\nBefore we begin, we use the set.seed() function in order to set a for R’s random number generator, so that the reader of this book will obtain precisely the same results as those shown below. It is generally a good idea to set a random seed when performing an analysis such as cross-validation that contains an element of randomness, so that the results obtained can be reproduced precisely at a later time.\nWe begin by using the sample() function to split the set of observations into two halves, by selecting a random subset of \\(196\\) observations out of the original \\(392\\) observations. We refer to these observations as the training set.\n\nlibrary(ISLR2)\nset.seed(1)\ntrain <- sample(392, 196)\n\n(Here we use a shortcut in the sample command; see ?sample for details.) We then use the subset option in lm() to fit a linear regression using only the observations corresponding to the training set.\n\nlm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)\n\nWe now use the predict() function to estimate the response for all \\(392\\) observations, and we use the mean() function to calculate the MSE of the \\(196\\) observations in the validation set. Note that the -train index below selects only the observations that are not in the training set.\n\nattach(Auto)\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 23.26601\n\n\nTherefore, the estimated test MSE for the linear regression fit is \\(23.27\\). We can use the poly() function to estimate the test error for the quadratic and cubic regressions.\n\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2)\n\n[1] 18.71646\n\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2)\n\n[1] 18.79401\n\n\nThese error rates are \\(18.72\\) and \\(18.79\\), respectively. If we choose a different training set instead, then we will obtain somewhat different errors on the validation set.\n\nset.seed(2)\ntrain <- sample(392, 196)\nlm.fit <- lm(mpg ~ horsepower, subset = train)\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 25.72651\n\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2)\n\n[1] 20.43036\n\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2)\n\n[1] 20.38533\n\n\nUsing this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are \\(25.73\\), \\(20.43\\), and \\(20.39\\), respectively.\nThese results are consistent with our previous findings: a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence in favor of a model that uses a cubic function of horsepower.\n\n\n\nThe LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions. In the lab for Chapter 4, we used the glm() function to perform logistic regression by passing in the family = \"binomial\" argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function. So for instance,\n\nglm.fit <- glm(mpg ~ horsepower, data = Auto)\ncoef(glm.fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nand\n\nlm.fit <- lm(mpg ~ horsepower, data = Auto)\ncoef(lm.fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nyield identical linear regression models. In this lab, we will perform linear regression using the glm() function rather than the lm() function because the former can be used together with cv.glm(). The cv.glm() function is part of the boot library.\n\nlibrary(boot)\nglm.fit <- glm(mpg ~ horsepower, data = Auto)\ncv.err <- cv.glm(Auto, glm.fit)\ncv.err$delta\n\n[1] 24.23151 24.23114\n\n\nThe cv.glm() function produces a list with several components. The two numbers in the delta vector contain the cross-validation results. In this case the numbers are identical (up to two decimal places) and correspond to the LOOCV statistic given in ( 5.1). Below, we discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately \\(24.23\\).\nWe can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for() function to initiate a which iteratively fits polynomial regressions for polynomials of order \\(i=1\\) to \\(i=10\\), computes the associated cross-validation error, and stores it in the \\(i\\)th element of the vector cv.error. We begin by initializing the vector.\n\ncv.error <- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]\n}\ncv.error\n\n [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n [9] 19.06863 19.49093\n\n\nAs in Figure 5.4, we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.\n\n\n\nThe cv.glm() function can also be used to implement \\(k\\)-fold CV. Below we use \\(k=10\\), a common choice for \\(k\\), on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.\n\nset.seed(17)\ncv.error.10 <- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\nNotice that the computation time is shorter than that of LOOCV. (In principle, the computation time for LOOCV for a least squares linear model should be faster than for \\(k\\)-fold CV, due to the availability of the formula ( 5.2) for LOOCV; however, unfortunately the cv.glm() function does not make use of this formula.) We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.\nWe saw in Section 5.3.2 that the two numbers associated with delta are essentially the same when LOOCV is performed. When we instead perform \\(k\\)-fold CV, then the two numbers associated with delta differ slightly. The first is the standard \\(k\\)-fold CV estimate, as in ( 5.3). The second is a bias-corrected version. On this data set, the two estimates are very similar to each other.\n\n\n\nWe illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in R entails only two steps. First, we must create a function that computes the statistic of interest. Second, we use the boot() function, which is part of the boot library, to perform the bootstrap by repeatedly sampling observations from the data set with replacement.\nThe Portfolio data set in the ISLR2 package is simulated data of \\(100\\) pairs of returns, generated in the fashion described in Section 5.2. To illustrate the use of the bootstrap on this data, we must first create a function, alpha.fn(), which takes as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations.\n\nalpha.fn <- function(data, index) {\n  X <- data$X[index]\n  Y <- data$Y[index]\n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\n\nThis function returns, or outputs, an estimate for \\(\\alpha\\) based on applying ( 5.7) to the observations indexed by the argument index. For instance, the following command tells R to estimate \\(\\alpha\\) using all \\(100\\) observations.\n\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.5758321\n\n\nThe next command uses the sample() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing \\(\\hat{\\alpha}\\) based on the new data set.\n\nset.seed(7)\nalpha.fn(Portfolio, sample(100, 100, replace = T))\n\n[1] 0.5385326\n\n\nWe can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. However, the boot() function automates this approach. Below we produce \\(R=1,000\\) bootstrap estimates for \\(\\alpha\\).\n\nboot(Portfolio, alpha.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 0.0007959475  0.08969074\n\n\nThe final output shows that using the original data, \\(\\hat{\\alpha}=0.5758\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\alpha})\\) is \\(0.0897\\).\nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\) described in Section 3.1.2.\nWe first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3. Note that we do not need the { and } at the beginning and end of the function because it is only one line long.\n\nboot.fn <- function(data, index)\n  coef(lm(mpg ~ horsepower, data = data, subset = index))\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nThe boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. Here we give two examples.\n\nset.seed(1)\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.1186906  -0.1577063 \n\n\nNext, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0544513229 0.841289790\nt2* -0.1578447 -0.0006170901 0.007343073\n\n\nThis indicates that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.84\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\). As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.\n\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\nThe standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained using the formulas from Section 3.1.2 are \\(0.717\\) for the intercept and \\(0.0064\\) for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 on page66 rely on certain assumptions. For example, they depend on the unknown parameter \\(\\sigma^2\\), the noise variance. We then estimate \\(\\sigma^2\\) using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does. We see in Figure 3.8 on page91 that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\). Secondly, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function.\nBelow we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\).\n\nboot.fn <- function(data, index)\n  coef(\n      lm(mpg ~ horsepower + I(horsepower^2), \n        data = data, subset = index)\n    )\nset.seed(1)\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\nsummary(\n    lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n  )$coef\n\n                    Estimate   Std. Error   t value      Pr(>|t|)\n(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\nhorsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\nI(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21"
  },
  {
    "objectID": "Ch5_ResamplingMethods.html#exercises",
    "href": "Ch5_ResamplingMethods.html#exercises",
    "title": "5  Resampling Methods",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\nPrepare the following exercises of Chapter 5 in our course textbook ISLR:\n\nExercise 3\nExercise 4\nExercise 5\nExercise 6\nExercise 8"
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html",
    "href": "Ch6_LinModSelectRegul.html",
    "title": "6  Linear Model Selection and Regularization",
    "section": "",
    "text": "Here we apply the best subset selection approach to the Hitters data. We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.\nFirst of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements.\n\nlibrary(ISLR2)\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\nHence we see that Salary is missing for \\(59\\) players. The na.omit() function removes all of the rows that have missing values in any variable.\n\nHitters <- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\nsum(is.na(Hitters))\n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.\n\nlibrary(leaps)\nregfit.full <- regsubsets(Salary ~ ., Hitters)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nAn asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.\n\nregfit.full <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\nreg.summary <- summary(regfit.full)\n\nThe summary() function also returns \\(R^2\\), RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC. We can examine these to try to select the best overall model.\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nFor instance, we see that the \\(R^2\\) statistic increases from \\(32\\,\\%\\), when only one variable is included in the model, to almost \\(55\\,\\%\\), when all variables are included. As expected, the \\(R^2\\) statistic increases monotonically as more variables are included.\n\nreg.summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nPlotting RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC for all of the models at once will help us decide which model to select. Note the type = \"l\" option tells R to connect the plotted points with lines.\n\npar(mfrow = c(1, 2))\nplot(reg.summary$rss, xlab = \"Number of Variables\",\n    ylab = \"RSS\", type = \"l\")\nplot(reg.summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\n\n\n\n\nThe points() command works like the plot() command, except that it puts points on a plot that has already been created, instead of creating a new plot. The which.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted \\(R^2\\) statistic.\n\nwhich.max(reg.summary$adjr2)\n\n[1] 11\n\nplot(reg.summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\npoints(11, reg.summary$adjr2[11], col = \"red\", cex = 2, \n    pch = 20)\n\n\n\n\nIn a similar fashion we can plot the \\(C_p\\) and BIC statistics, and indicate the models with the smallest statistic using which.min().\n\nplot(reg.summary$cp, xlab = \"Number of Variables\",\n    ylab = \"Cp\", type = \"l\")\nwhich.min(reg.summary$cp)\n\n[1] 10\n\npoints(10, reg.summary$cp[10], col = \"red\", cex = 2,\n    pch = 20)\n\n\n\nwhich.min(reg.summary$bic)\n\n[1] 6\n\nplot(reg.summary$bic, xlab = \"Number of Variables\",\n    ylab = \"BIC\", type = \"l\")\npoints(6, reg.summary$bic[6], col = \"red\", cex = 2,\n    pch = 20)\n\n\n\n\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, \\(C_p\\), adjusted \\(R^2\\), or AIC. To find out more about this function, type ?plot.regsubsets.\n\nplot(regfit.full, scale = \"r2\")\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\nThe top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to \\(-150\\). However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the coef() function to see the coefficient estimates associated with this model.\n\ncoef(regfit.full, 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n\nWe can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method = \"forward\" or method = \"backward\".\n\nregfit.fwd <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\nregfit.bwd <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\nFor instance, we see that using forward stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.\n\ncoef(regfit.full, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd, 7)\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\ncoef(regfit.bwd, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n\nWe just saw that it is possible to choose among a set of models of different sizes using \\(C_p\\), BIC, and adjusted \\(R^2\\). We will now consider how to do this using the validation set and cross-validation approaches.\nIn order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting—including variable selection. Therefore, the determination of which model of a given size is best must be made using only the training observations. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\nIn order to use the validation set approach, we begin by splitting the observations into a training set and a test set. We do this by creating a random vector, train, of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. The vector test has a TRUE if the observation is in the test set, and a FALSE otherwise. Note the ! in the command to create test causes TRUEs to be switched to FALSEs and vice versa. We also set a random seed so that the user will obtain the same training set/test set split.\n\nset.seed(1)\ntrain <- sample(c(TRUE, FALSE), nrow(Hitters),\n    replace = TRUE)\ntest <- (!train)\n\nNow, we apply regsubsets() to the training set in order to perform best subset selection.\n\nregfit.best <- regsubsets(Salary ~ .,\n    data = Hitters[train, ], nvmax = 19)\n\nNotice that we subset the Hitters data frame directly in the call in order to access only the training subset of the data, using the expression Hitters[train, ]. We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.\n\ntest.mat <- model.matrix(Salary ~ ., data = Hitters[test, ])\n\nThe model.matrix() function is used in many regression packages for building an `X'' matrix from data.  Now we run a loop, and for each sizei, we extract the coefficients fromregfit.best` for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.\n\nval.errors <- rep(NA, 19)\nfor (i in 1:19) {\n coefi <- coef(regfit.best, id = i)\n pred <- test.mat[, names(coefi)] %*% coefi\n val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)\n}\n\nWe find that the best model is the one that contains seven variables.\n\nval.errors\n\n [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4\n [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4\n[17] 141767.4 142339.6 142238.2\n\nwhich.min(val.errors)\n\n[1] 7\n\ncoef(regfit.best, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\nThis was a little tedious, partly because there is no predict() method for regsubsets(). Since we will be using this function again, we can capture our steps above and write our own predict method.\n\n predict.regsubsets <- function(object, newdata, id, ...) {\n  form <- as.formula(object$call[[2]])\n  mat <- model.matrix(form, newdata)\n  coefi <- coef(object, id = id)\n  xvars <- names(coefi)\n  mat[, xvars] %*% coefi\n }\n\nOur function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to regsubsets(). We demonstrate how we use this function below, when we do cross-validation.\nFinally, we perform best subset selection on the full data set, and select the best seven-variable model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates. Note that we perform best subset selection on the full data set and select the best seven-variable model, rather than simply using the variables that were obtained from the training set, because the best seven-variable model on the full data set may differ from the corresponding model on the training set.\n\nregfit.best <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\nIn fact, we see that the best seven-variable model on the full data set has a different set of variables than the best seven-variable model on the training set.\nWe now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy. First, we create a vector that allocates each observation to one of \\(k=10\\) folds, and we create a matrix in which we will store the results.\n\nk <- 10\nn <- nrow(Hitters)\nset.seed(1)\nfolds <- sample(rep(1:k, length = n))\ncv.errors <- matrix(NA, k, 19,\n    dimnames = list(NULL, paste(1:19)))\n\nNow we write a for loop that performs cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors. Note that in the following code R will automatically use our predict.regsubsets() function when we call predict() because the best.fit object has class regsubsets.\n\nfor (j in 1:k) {\n  best.fit <- regsubsets(Salary ~ .,\n       data = Hitters[folds != j, ],\n       nvmax = 19)\n  for (i in 1:19) {\n    pred <- predict(best.fit, Hitters[folds == j, ], id = i)\n    cv.errors[j, i] <-\n         mean((Hitters$Salary[folds == j] - pred)^2)\n   }\n }\n\nThis has given us a \\(10 \\times 19\\) matrix, of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\nmean.cv.errors <- apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\npar(mfrow = c(1, 1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\nWe see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.\n\nreg.best <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(reg.best, 10)\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 \n        CRBI       CWalks    DivisionW      PutOuts      Assists \n   0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680 \n\n\n\n\n\n\nWe will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far in this book. In particular, we must pass in an x matrix as well as a y vector, and we do not use the {} syntax. We will now perform ridge regression and the lasso in order to predict Salary on the Hitters data. Before proceeding ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nx <- model.matrix(Salary ~ ., Hitters)[, -1]\ny <- Hitters$Salary\n\nThe model.matrix() function is particularly useful for creating x; not only does it produce a matrix corresponding to the \\(19\\) predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs.\n\n\nThe glmnet() function has an alpha argument that determines what type of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit. We first fit a ridge regression model.\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-4\n\ngrid <- 10^seq(10, -2, length = 100)\nridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)\n\nBy default the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. However, here we have chosen to implement the function over a grid of values ranging from \\(\\lambda=10^{10}\\) to \\(\\lambda=10^{-2}\\), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of \\(\\lambda\\) that is not one of the original grid values. Note that by default, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize = FALSE.\nAssociated with each value of \\(\\lambda\\) is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a \\(20 \\times 100\\) matrix, with \\(20\\) rows (one for each predictor, plus an intercept) and \\(100\\) columns (one for each value of \\(\\lambda\\)).\n\ndim(coef(ridge.mod))\n\n[1]  20 100\n\n\nWe expect the coefficient estimates to be much smaller, in terms of \\(\\ell_2\\) norm, when a large value of \\(\\lambda\\) is used, as compared to when a small value of \\(\\lambda\\) is used. These are the coefficients when \\(\\lambda=11{,}498\\), along with their \\(\\ell_2\\) norm:\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\nsqrt(sum(coef(ridge.mod)[-1, 50]^2))\n\n[1] 6.360612\n\n\nIn contrast, here are the coefficients when \\(\\lambda=705\\), along with their \\(\\ell_2\\) norm. Note the much larger \\(\\ell_2\\) norm of the coefficients associated with this smaller value of \\(\\lambda\\).\n\nridge.mod$lambda[60]\n\n[1] 705.4802\n\ncoef(ridge.mod)[, 60]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 \n      Errors   NewLeagueN \n -0.70358655   8.61181213 \n\nsqrt(sum(coef(ridge.mod)[-1, 60]^2))\n\n[1] 57.11001\n\n\nWe can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of \\(\\lambda\\), say \\(50\\):\n\npredict(ridge.mod, s = 50, type = \"coefficients\")[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n 4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 \n          RBI         Walks         Years        CAtBat         CHits \n 8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n 6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 \n\n\nWe now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a data set. The first is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to TRUE for the training data. The second is to randomly choose a subset of numbers between \\(1\\) and \\(n\\); these can then be used as the indices for the training observations. The two approaches work equally well. We used the former method in Section 6.5.1. Here we demonstrate the latter approach.\nWe first set a random seed so that the results obtained will be reproducible.\n\nset.seed(1)\ntrain <- sample(1:nrow(x), nrow(x) / 2)\ntest <- (-train)\ny.test <- y[test]\n\nNext we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using \\(\\lambda=4\\). Note the use of the predict() function again. This time we get predictions for a test set, by replacing type=\"coefficients\" with the newx argument.\n\nridge.mod <- glmnet(x[train, ], y[train], alpha = 0,\n    lambda = grid, thresh = 1e-12)\nridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\nThe test MSE is \\(142{,}199\\). Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:\n\nmean((mean(y[train]) - y.test)^2)\n\n[1] 224669.9\n\n\nWe could also get the same result by fitting a ridge regression model with a very large value of \\(\\lambda\\). Note that 1e10 means \\(10^{10}\\).\n\nridge.pred <- predict(ridge.mod, s = 1e10, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 224669.8\n\n\nSo fitting a ridge regression model with \\(\\lambda=4\\) leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with \\(\\lambda=4\\) instead of just performing least squares regression. Recall that least squares is simply ridge regression with \\(\\lambda=0\\).\n\nridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ],\n    exact = T, x = x[train, ], y = y[train])\nmean((ridge.pred - y.test)^2)\n\n[1] 168588.6\n\nlm(y ~ x, subset = train)\n\n\nCall:\nlm(formula = y ~ x, subset = train)\n\nCoefficients:\n(Intercept)       xAtBat        xHits       xHmRun        xRuns         xRBI  \n   274.0145      -0.3521      -1.6377       5.8145       1.5424       1.1243  \n     xWalks       xYears      xCAtBat       xCHits      xCHmRun       xCRuns  \n     3.7287     -16.3773      -0.6412       3.1632       3.4008      -0.9739  \n      xCRBI      xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists  \n    -0.6005       0.3379     119.1486    -144.0831       0.1976       0.6804  \n    xErrors  xNewLeagueN  \n    -4.7128     -71.0951  \n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\",\n    x = x[train, ], y = y[train])[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 274.0200994   -0.3521900   -1.6371383    5.8146692    1.5423361    1.1241837 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n   3.7288406  -16.3795195   -0.6411235    3.1629444    3.4005281   -0.9739405 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  -0.6003976    0.3378422  119.1434637 -144.0853061    0.1976300    0.6804200 \n      Errors   NewLeagueN \n  -4.7127879  -71.0898914 \n\n\nIn general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients.\nIn general, instead of arbitrarily choosing \\(\\lambda=4\\), it would be better to use cross-validation to choose the tuning parameter \\(\\lambda\\). We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs ten-fold cross-validation, though this can be changed using the argument nfolds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random.\n\nset.seed(1)\ncv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)\nplot(cv.out)\n\n\n\nbestlam <- cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\nTherefore, we see that the value of \\(\\lambda\\) that results in the smallest cross-validation error is \\(326\\). What is the test MSE associated with this value of \\(\\lambda\\)?\n\nridge.pred <- predict(ridge.mod, s = bestlam,\n    newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\nThis represents a further improvement over the test MSE that we got using \\(\\lambda=4\\). Finally, we refit our ridge regression model on the full data set, using the value of \\(\\lambda\\) chosen by cross-validation, and examine the coefficient estimates.\n\nout <- glmnet(x, y, alpha = 0)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\nAs expected, none of the coefficients are zero—ridge regression does not perform variable selection!\n\n\n\nWe saw that ridge regression with a wise choice of \\(\\lambda\\) can outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. Other than that change, we proceed just as we did in fitting a ridge model.\n\nlasso.mod <- glmnet(x[train, ], y[train], alpha = 1,\n    lambda = grid)\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\nWe can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.\n\nset.seed(1)\ncv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\n\n\n\nbestlam <- cv.out$lambda.min\nlasso.pred <- predict(lasso.mod, s = bestlam,\n    newx = x[test, ])\nmean((lasso.pred - y.test)^2)\n\n[1] 143673.6\n\n\nThis is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with \\(\\lambda\\) chosen by cross-validation.\nHowever, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with \\(\\lambda\\) chosen by cross-validation contains only eleven variables.\n\nout <- glmnet(x, y, alpha = 1, lambda = grid)\nlasso.coef <- predict(out, type = \"coefficients\",\n    s = bestlam)[1:20, ]\nlasso.coef\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\nlasso.coef[lasso.coef != 0]\n\n  (Intercept)         AtBat          Hits         Walks         Years \n   1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 \n       CHmRun         CRuns          CRBI       LeagueN     DivisionW \n   0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 \n      PutOuts        Errors \n   0.23752385   -0.85629148 \n\n\n\n\n\n\n\n\nPrincipal components regression (PCR) can be performed using the pcr() function, which is part of the pls library. We now apply PCR to the Hitters data, in order to predict Salary. Again, we ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nset.seed(2)\npcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE,\n    validation = \"CV\")\n\nThe syntax for the pcr() function is similar to that for lm(), with a few additional options. Setting scale = TRUE has the effect of standardizing each predictor, using ( 6.6), prior to generating the principal components, so that the scale on which each variable is measured will not have an effect. Setting validation = \"CV\" causes pcr() to compute the ten-fold cross-validation error for each possible value of \\(M\\), the number of principal components used. The resulting fit can be examined using summary().\n\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452    351.9    353.2    355.0    352.8    348.4    343.6\nadjCV          452    351.6    352.7    354.4    352.1    347.6    342.7\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       345.5    347.7    349.6     351.4     352.1     353.5     358.2\nadjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        349.7     349.4     339.9     341.6     339.2     339.6\nadjCV     348.0     347.7     338.2     339.7     337.2     337.6\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n\nThe CV score is provided for each possible number of components, ranging from \\(M=0\\) onwards. (We have printed the CV output only up to \\(M=4\\).) Note that pcr() reports the root mean squared error; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of \\(352.8\\) corresponds to an MSE of \\(352.8^2=124{,}468\\).\nOne can also plot the cross-validation scores using the validationplot() function. Using val.type = \"MSEP\" will cause the cross-validation MSE to be plotted.\n\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\nWe see that the smallest cross-validation error occurs when \\(M=18\\) components are used. This is barely fewer than \\(M=19\\), which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.\nThe summary() function also provides the percentage of variance explained in the predictors and in the response using different numbers of components. This concept is discussed in greater detail in Chapter 12. Briefly, we can think of this as the amount of information about the predictors or the response that is captured using \\(M\\) principal components. For example, setting \\(M=1\\) only captures \\(38.31\\,\\%\\) of all the variance, or information, in the predictors. In contrast, using \\(M=5\\) increases the value to \\(84.29\\,\\%\\). If we were to use all \\(M=p=19\\) components, this would increase to \\(100\\,\\%\\).\nWe now perform PCR on the training data and evaluate its test set performance.\n\nset.seed(1)\npcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train,\n    scale = TRUE, validation = \"CV\")\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\nNow we find that the lowest cross-validation error occurs when \\(M=5\\) components are used. We compute the test MSE as follows.\n\npcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)\nmean((pcr.pred - y.test)^2)\n\n[1] 142811.8\n\n\nThis test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates.\nFinally, we fit PCR on the full data set, using \\(M=5\\), the number of components identified by cross-validation.\n\npcr.fit <- pcr(y ~ x, scale = TRUE, ncomp = 5)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 5\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps  5 comps\nX    38.31    60.16    70.84    79.03    84.29\ny    40.63    41.58    42.17    43.22    44.90\n\n\n\n\n\nWe implement partial least squares (PLS) using the plsr() function, also in the pls library. The syntax is just like that of the pcr() function.\n\nset.seed(1)\npls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = \"CV\")\nsummary(pls.fit)\n\nData:   X dimension: 131 19 \n    Y dimension: 131 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV           428.3    325.5    329.9    328.8    339.0    338.9    340.1\nadjCV        428.3    325.0    328.2    327.2    336.6    336.1    336.6\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       339.0    347.1    346.4     343.4     341.5     345.4     356.4\nadjCV    336.2    343.4    342.8     340.2     338.3     341.8     351.1\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        348.4     349.1     350.0     344.2     344.5     345.0\nadjCV     344.2     345.0     345.9     340.4     340.6     341.1\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71\nSalary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         93.17     96.05     97.08     97.61     97.97     98.70     99.12\nSalary    55.95     56.12     56.47     56.68     57.37     57.76     58.08\n        16 comps  17 comps  18 comps  19 comps\nX          99.61     99.70     99.95    100.00\nSalary     58.17     58.49     58.56     58.62\n\nvalidationplot(pls.fit, val.type = \"MSEP\")\n\n\n\n\nThe lowest cross-validation error occurs when only \\(M=1\\) partial least squares directions are used. We now evaluate the corresponding test set MSE.\n\npls.pred <- predict(pls.fit, x[test, ], ncomp = 1)\nmean((pls.pred - y.test)^2)\n\n[1] 151995.3\n\n\nThe test MSE is comparable to, but slightly higher than, the test MSE obtained using ridge regression, the lasso, and PCR.\nFinally, we perform PLS using the full data set, using \\(M=1\\), the number of components identified by cross-validation.\n\npls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE,\n    ncomp = 1)\nsummary(pls.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: kernelpls\nNumber of components considered: 1\nTRAINING: % variance explained\n        1 comps\nX         38.08\nSalary    43.05\n\n\nNotice that the percentage of variance in Salary that the one-component PLS fit explains, \\(43.05\\,\\%\\), is almost as much as that explained using the final five-component model PCR fit, \\(44.90\\,\\%\\). This is because PCR only attempts to maximize the amount of variance explained in the predictors, while PLS searches for directions that explain variance in both the predictors and the response."
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#exercises",
    "href": "Ch6_LinModSelectRegul.html#exercises",
    "title": "6  Linear Model Selection and Regularization",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\nPrepare the following exercises of Chapter 6 in our course textbook ISLR:\n\nExercise 2\nExercise 4\nExercise 8\nExercise 10"
  }
]