[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer-Aided Statistical Analysis (B.Sc.)",
    "section": "",
    "text": "Monday afternoon is computational-statistics-afternoon. We’ll have both theory and practice sessions and a couple of breaks in between.\n\n\n\n\n \n  \n    Mondays \n    Time \n    Lecture Hall \n  \n \n\n  \n    Theory 1 \n    14:15-15:15 \n    Jur / Hörsaal K \n  \n  \n    Q/A & Coffee Break \n    15:15-15:30 \n    Jur / Hörsaal K \n  \n  \n    Theory 2 \n    15:30-16:30 \n    Jur / Hörsaal K \n  \n  \n    Q/A & Coffee Break \n    16:30-17:00 \n    Jur / Hörsaal K \n  \n  \n    Practice \n    17:00-18:00 \n    Jur / Hörsaal K \n  \n\n\n\n\n\n\n\n\n\nMain Course Textbook (ISLR):\n\nAn Introduction to Statistical Learning, by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\nThe PDF-version and further resources (datasets, etc.) of the textbook can be downloaded for free from https://www.statlearning.com/\n\n\n\n\nThis online script\nTo-Do List for October 9, 2023: To-Do List (PDF)\n\nThe above links to the lecture materials can also be found at eCampus"
  },
  {
    "objectID": "Ch2_StatLearning.html",
    "href": "Ch2_StatLearning.html",
    "title": "1  Statistical Learning",
    "section": "",
    "text": "Reading: Chapter 2 of our course textbook ISLR An Introduction to Statistical Learning, by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani"
  },
  {
    "objectID": "Ch2_StatLearning.html#exercises",
    "href": "Ch2_StatLearning.html#exercises",
    "title": "1  Statistical Learning",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nPrepare the following exercises of Chapter 2 in our course textbook ISLR:\n\nExercise 7\nExercise 8\nExercise 9\n\n\n1.5.1 Solutions\n\nExercise 7\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Suppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X_1 = X_2 = X_3 = 0\\) using K-nearest neighbors.\n\n\n\nObs.\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(Y\\)\n\n\n\n\n1\n0\n3\n0\nRed\n\n\n2\n2\n0\n0\nRed\n\n\n3\n0\n1\n3\nRed\n\n\n4\n0\n1\n2\nGreen\n\n\n5\n−1\n0\n1\nGreen\n\n\n6\n1\n1\n1\nRed\n\n\n\n7. a) Compute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\).\nAnswer:\n\n# Outcome\nY    <- c(\"red\", \"red\", \"red\", \"green\", \"green\", \"red\")\n# Predictor values\nobs1 <- c( 0, 3, 0)\nobs2 <- c( 2, 0, 0)\nobs3 <- c( 0, 1, 3)\nobs4 <- c( 0, 1, 2)\nobs5 <- c(-1, 0, 1)\nobs6 <- c( 1, 1, 1)\n\n# Test Point\nobs0 <- c(0, 0, 0)\n\n# Create a Vector Dist_vec to store the results\nDist <- numeric(length = 6)\n\n# Compute and store the Euclidean distances\nDist[1] <- sqrt(sum((obs1-obs0)^2)) \nDist[2] <- sqrt(sum((obs2-obs0)^2)) \nDist[3] <- sqrt(sum((obs3-obs0)^2)) \nDist[4] <- sqrt(sum((obs4-obs0)^2)) \nDist[5] <- sqrt(sum((obs5-obs0)^2)) \nDist[6] <- sqrt(sum((obs6-obs0)^2))  \n\n# Print the results\nDist\n\n[1] 3.000000 2.000000 3.162278 2.236068 1.414214 1.732051\n\n\n7. b) What is your prediction with \\(K = 1\\)? Why?\nAnswer:\n\nwhich.min(Dist)\n\n[1] 5\n\nY[which.min(Dist)]\n\n[1] \"green\"\n\n\nClosest \\(K=1\\) neighbor is obs5 and thus, our prediction is Green because Green is the \\(Y\\) value associated to obs5.\n7. c) What is your prediction with \\(K = 3\\)? Why?\nAnswer:\n\norder(Dist)[1:3]\n\n[1] 5 6 2\n\nY[order(Dist)[1:3]]\n\n[1] \"green\" \"red\"   \"red\"  \n\n\nClosest \\(K=3\\) neighbors are obs5, obs6, obs2 and thus, our prediction is Red because it is the \\(Y\\) value associated to obs2 and obs6 (majority rule).\n7. d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for \\(K\\) to be large or small? Why?\nAnswer:\n\nIn the case of a highly nonlinear decision boundary, the neighborhoods of similar \\(Y\\)-values become generally small. Therefore, also \\(K\\) must be chosen relatively small so that we can capture more of the non-linear decision boundary. \n\n\nExercise 8:\nThis exercise relates to the College data set, which can be found in the file College.csv (LINK-TO-DATA). It contains a number of variables for \\(777\\) different universities and colleges in the US. The variables are:\n\nPrivate : Public/private indicator\nApps : Number of applications received\nAccept : Number of applicants accepted\nEnroll : Number of new students enrolled\nTop10perc : New students from top 10% of high school class\nTop25perc : New students from top 25% of high school class\nF.Undergrad : Number of full-time undergraduates\nP.Undergrad : Number of part-time undergraduates\nOutstate : Out-of-state tuition\nRoom.Board : Room and board costs\nBooks : Estimated book costs\nPersonal : Estimated personal spending\nPhD : Percent of faculty with Ph.D.’s\nTerminal : Percent of faculty with terminal degree\nS.F.Ratio : Student/faculty ratio\nperc.alumni : Percent of alumni who donate\nExpend : Instructional expenditure per student\nGrad.Rate : Graduation rate\n\n8. a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\nAnswer:\n\n# Store data into dataframe college\ncollege <- read.csv(\"DATA/College.csv\")\n\n# Print first 10 rows and 5 collumns of the data\nprint(college[c(1:10),c(1:5)])\n\n                              X Private Apps Accept Enroll\n1  Abilene Christian University     Yes 1660   1232    721\n2            Adelphi University     Yes 2186   1924    512\n3                Adrian College     Yes 1428   1097    336\n4           Agnes Scott College     Yes  417    349    137\n5     Alaska Pacific University     Yes  193    146     55\n6             Albertson College     Yes  587    479    158\n7       Albertus Magnus College     Yes  353    340    103\n8                Albion College     Yes 1899   1720    489\n9              Albright College     Yes 1038    839    227\n10    Alderson-Broaddus College     Yes  582    498    172\n\n\n8. b) Look at the data using the fix() function.\nAnswer:\nYou should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:\n\n# Store row names\nrownames(college) <- college[,1]\n\n# pops up a window for data visualization\n# fix(college)\n\n# Alteratively you can use: \n# View(college)\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try:\n\n# Eliminates first column (containing the row names)\ncollege <- college[,-1]\n# fix(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n8. c. i) Use the summary() function to produce a numerical summary of the variables in the data set.\nAnswer:\n\nsummary(college[, 1:5])\n\n   Private               Apps           Accept          Enroll    \n Length:777         Min.   :   81   Min.   :   72   Min.   :  35  \n Class :character   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242  \n Mode  :character   Median : 1558   Median : 1110   Median : 434  \n                    Mean   : 3002   Mean   : 2019   Mean   : 780  \n                    3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902  \n                    Max.   :48094   Max.   :26330   Max.   :6392  \n   Top10perc    \n Min.   : 1.00  \n 1st Qu.:15.00  \n Median :23.00  \n Mean   :27.56  \n 3rd Qu.:35.00  \n Max.   :96.00  \n\n\n8. c. ii) Use the pairs() function to produce a scatterplot matrix of the 2nd to 10th column or variables of the data. Recall that you can reference the 2nd to 10th column of a matrix A using A[,2:10].\nAnswer:\n\npairs(x = college[,2:10])\n\n\n\n\n8. c. iii) Use the boxplot() function to produce side-by-side boxplots of Outstate versus Private.\nAnswer:\n\nboxplot(Outstate~Private, \n        data = college, \n        xlab = \"Private\", \n        ylab = \"Outstate\")\n\n\n\n\n8. c. iv) Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.\n\n# Creating a vector called ELite with only \"No\" entrances amounting the number of college rows\nElite <- rep(\"No\",nrow(college))\n\n# Replacing \"No\" with \"Yes\" if the proportion of students coming from the top 10% of their HS classes exceeds 50%.\nElite[college$Top10perc > 50] <- \"Yes\"\n\n# Encode a vector as a factor\nElite <- as.factor(Elite)\n\n# Add Elite variable to our current dataset \"college\"\ncollege <- data.frame(college, Elite)\n\nUse the summary() function to see how many elite universities there are. Now use the boxplot() function to produce side-by-side boxplots of Outstate versus Elite.\nAnswer:\n\nsummary(college$Elite)\n\n No Yes \n699  78 \n\n\nThere are \\(78\\) elite Universities. The boxplots of Outstate versus Elite-Status are generated as following:\n\nboxplot(Outstate ~ Elite, \n        data = college, xlab=\"Elite\", ylab=\"Outstate\")\n\n\n\n\n8. c. v) Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\nAnswer:\n\npar(mfrow=c(2,2))\nhist(college$Apps,     breaks=50, xlim=c(0,25000), \n     main=\"Apps\")\nhist(college$Enroll,   breaks=25, main=\"Enroll\")\nhist(college$Expend,   breaks=25, main=\"Expend\")\nhist(college$Outstate, main=\"Outstate\")\n\n\n\npar(mfrow=c(1,1))\n\n\n\nExercise 9:\nThis exercise involves the Auto data set. Make sure that the missing values have been removed from the data.\n\n# Store data into dataframe college\nAuto <- read.csv(\"DATA/Auto.csv\", header=T, na.strings=\"?\")\n\n# Remove missing values from the data\nAuto <- na.omit(Auto)\n\n# Print first 10 rows of the data\nprint(Auto[c(1:10),])\n\n   mpg cylinders displacement horsepower weight acceleration year origin\n1   18         8          307        130   3504         12.0   70      1\n2   15         8          350        165   3693         11.5   70      1\n3   18         8          318        150   3436         11.0   70      1\n4   16         8          304        150   3433         12.0   70      1\n5   17         8          302        140   3449         10.5   70      1\n6   15         8          429        198   4341         10.0   70      1\n7   14         8          454        220   4354          9.0   70      1\n8   14         8          440        215   4312          8.5   70      1\n9   14         8          455        225   4425         10.0   70      1\n10  15         8          390        190   3850          8.5   70      1\n                        name\n1  chevrolet chevelle malibu\n2          buick skylark 320\n3         plymouth satellite\n4              amc rebel sst\n5                ford torino\n6           ford galaxie 500\n7           chevrolet impala\n8          plymouth fury iii\n9           pontiac catalina\n10        amc ambassador dpl\n\n# Find more info on the variables here: https://rstudio-pubs-static.s3.amazonaws.com/61800_faea93548c6b49cc91cd0c5ef5059894.html\n\n9. a) Which of the predictors are quantitative, and which are qualitative?\nAnswer:\n\n# Summarize dataset\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:392        \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.54   Mean   :75.98   Mean   :1.577                     \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n\n\n\nQuantitative predictors: mpg, cylinders, displacement, horsepower, weight, acceleration, year\nQualitative predictors: name, origin\n\n9. b) What is the range of each quantitative predictor? You can answer this using the range() function.\nAnswer:\n\n# apply the range function to the first seven columns of Auto\nc <- sapply(Auto[, 1:7], range)\n# print to console\nc\n\n      mpg cylinders displacement horsepower weight acceleration year\n[1,]  9.0         3           68         46   1613          8.0   70\n[2,] 46.6         8          455        230   5140         24.8   82\n\n\n9. c) What is the mean and standard deviation of each quantitative predictor?\nAnswer:\n\n# compute mean for the first seven variables and store it in a vector\nmean <- sapply(Auto[,1:7], mean)\n\n# round the values inside the vectors to 2 decimal cases\nmean <- sapply(mean,round,2)\n\n# compute the standard deviation and round it up \nsd <- sapply(Auto[, 1:7], sd)\nsd <- sapply(sd,round,2)\n\n# print both vectors\nmean\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n       23.45         5.47       194.41       104.47      2977.58        15.54 \n        year \n       75.98 \n\nsd\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n        7.81         1.71       104.64        38.49       849.40         2.76 \n        year \n        3.68 \n\n\n9.d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\nAnswer:\n\n# remove observations and store them \nnewAuto = Auto[-(10:85),]\n\n# Re-do exercises 9. b) and 9.c)\n# This time, create an empty Matrix \"Results\" to store the results\nResults <- matrix(NA, nrow = 4, ncol = 7, \n                  dimnames = list(c(\"Mean\", \"SD\", \"Minimum\", \"Maximum\"), \n                                  c(colnames(newAuto[,1:7]))))\n\n# Store the results\nResults[1,] <- sapply(newAuto[, 1:7], mean)\nResults[2,] <- sapply(newAuto[, 1:7], sd)  # Standard Deviation\nResults[3,] <- sapply(newAuto[, 1:7], min)\nResults[4,] <- sapply(newAuto[, 1:7], max)\n\n# Round them\nResults[] <- sapply(Results[],round,2)\n\n# Print the results\n# Results\nprint(Results[,1:6])\n\n          mpg cylinders displacement horsepower  weight acceleration\nMean    24.40      5.37       187.24     100.72 2935.97        15.73\nSD       7.87      1.65        99.68      35.71  811.30         2.69\nMinimum 11.00      3.00        68.00      46.00 1649.00         8.50\nMaximum 46.60      8.00       455.00     230.00 4997.00        24.80\n\n\n9. e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\nAnswer:\n\npairs(Auto[, -9])\n\n\n\n\n\nheavier weight is related with lower mpg and with higher horsepower;\nhigher horsepower correlates with lower acceleration;\nmpg (miles per gallon) mostly increases for newer model years meaning that cars become more efficient over time.\n\n9. f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\nAnswer:\nYes. On the one hand, as we can see from the plot above, all of the quantitative variables show some sort of relation (either linear or non-linear) with mpg and hence, they might be useful in predicting mpg. The origin qualitative variable might also be useful in predicting mpg, with cars originated from region 3 being associated with higher mpg. On the other hand, the name predictor has too little observations per name though, so using this as a predictor is likely to result in overfitting the data and will not generalize well."
  },
  {
    "objectID": "Ch3_LinearRegression.html#ch.-3.1-simple-linear-regression",
    "href": "Ch3_LinearRegression.html#ch.-3.1-simple-linear-regression",
    "title": "2  Linear Regression",
    "section": "(Ch. 3.1) Simple Linear Regression",
    "text": "(Ch. 3.1) Simple Linear Regression\nThe linear regression model assumes a linear relationship between \\(Y\\) and the predictor(s) \\(X\\).\nThe simple (only one predictor) linear regression model: \\[\nY\\approx \\beta_0 + \\beta_1 X\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV\n\n\n(Ch. 3.1.1) Estimating the Coefficients\nWe choose \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) such that the Residual Sum of Squares criterion is minimized: \\[\n\\begin{align*}\n\\operatorname{RSS}\\equiv \\operatorname{RSS}(\\hat{\\beta}_0,\\hat{\\beta_1})\n& = e_1^2 + \\dots + e_n^2\\\\\n&=)\\sum_{i=1}^n\\left(y_i - \\left(\\hat\\beta_0 + \\hat\\beta_1x_i\\right)\\right)^2\n\\end{align*}\n\\] The minimizers are \\[\n\\hat\\beta_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\] and \\[\n\\hat\\beta_0=\\bar{y} - \\hat\\beta_1\\bar{x},\n\\] where \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i\\) and \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i\\).\n\n\n\n\n(Ch. 3.1.2) Assessing the Accuracy of the Coefficient Estimates\nTrue unknown model \\[\nY=f(X)+\\epsilon\n\\]\nIn in linear regression analysis, we assume1 that \\[\nf(X) = \\beta_0 + \\beta_1 X\n\\]\nOrdinary least squares estimators \\[\n\\hat\\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\n\\] are unbiased, that is \\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat\\beta_0)&=E(\\hat\\beta_0)-\\beta_0=0\\\\\n\\operatorname{Bias}(\\hat\\beta_1)&=E(\\hat\\beta_1)-\\beta_1=0\n\\end{align*}\n\\] I.e., on average, the estimation results equal the true (unknown) parameters. However, in an actual data analysis, we only have one realization of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) computed from one give dataset and thus we cannot compute averages of estimation results. Each single estimation result will have estimation errors, i.e., \\[\n\\hat\\beta_0\\neq \\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\\neq \\beta_1.\n\\]\nThe following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook ISLR.\n\n## ###############################\n## A function to generate data \n## similar to that shown in Fig 3.3\n## ##############################\n\nbeta_0 <- 0.1                          # intercept parameter\nbeta_1 <- 5  \n\n## A Function to simulate data\nmyDataGenerator <- function(){\n  n      <- 50                           # sample size\n  beta_0 <- 0.1                          # intercept parameter\n  beta_1 <- 5                            # slope parameter\n  X      <- runif(n, min = -2, max = 2)  # predictor\n  error  <- rnorm(n, mean = 0, sd = 8.5) # error term\n  Y      <- beta_0 + beta_1 * X + error  # outcome \n  ##\n  return(data.frame(\"Y\" = Y, \"X\" = X))\n}\n\n## Generate a first realization of the data\nset.seed(123)\ndata_sim <- myDataGenerator()\nhead(data_sim)\n\n            Y          X\n1 -18.4853427 -0.8496899\n2  12.9872926  1.1532205\n3  -0.4167901 -0.3640923\n4  -1.9138159  1.5320696\n5  19.5667725  1.7618691\n6  -5.3639241 -1.8177740\n\n\nUsing repeated samples form the data generating process defined in myDataGenerator(), we can generate multiple estimation results of the unknown simple linear regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) and plot the corresponding empirical regression lines:\n\n## Estimation\nlm_obj <- lm(Y ~ X, data = data_sim)\n\n## Plotting the results\npar(mfrow=c(1,2)) # Two plots side by side\n\n## First Plot (fit for the first realization of the data)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\")\nabline(a = beta_0, b = beta_1, col = \"red\")\nabline(lm_obj, col = \"blue\")\n\n## Second Plot (fits for multiple data realizations)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\", type = \"n\") # type = \"n\": empty plot\n##\nfor(r in 1:10){\n  data_sim_new <- myDataGenerator()\n  lm_obj_new   <- lm(Y ~ X, data=data_sim_new)\n  abline(lm_obj_new, col = \"lightskyblue\")\n}\n## Adding the first fit\nabline(a = beta_0, b = beta_1, col = \"red\", lwd = 2)\nabline(lm_obj, col = \"blue\", lwd = 2)\n\n\n\n\n\nCoding-Questions: Can you do this animated? https://gganimate.com/articles/gganimate.html\n\nThe magnitude of the estimation errors is expressed in unites of standard errors: \\[\n\\operatorname{SE}(\\hat\\beta_0)=\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right]\n\\] and \\[\n\\operatorname{SE}(\\hat\\beta_1)=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2},\n\\] where \\(Var(\\epsilon)=\\sigma^2\\Leftrightarrow \\operatorname{SD}(\\epsilon)=\\sqrt{Var(\\epsilon)}=\\sigma\\).\nTypically, \\(\\sigma\\) is unknown, but can be estimated by \\[\n\\sigma\\approx\\hat{\\sigma}=\\operatorname{RSE}=\\sqrt{\\frac{\\operatorname(RSS)}{n-2}},\n\\] where we subtract \\(2\\) from the sampel size \\(n\\) since \\(n-2\\) are the remaining degrees of freedom in the data after estimating two parameters \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\nKnowing \\(\\operatorname{SE}(\\hat\\beta_0)\\) and \\(\\operatorname{SE}(\\hat\\beta_1)\\) allows us to construct Confidence Intervals: \\[\n\\begin{align*}\n\\operatorname{CI}_{\\beta_1}\n&=\\left[\\hat{\\beta_1}-2\\operatorname{SE}(\\hat\\beta_1),\\;\n        \\hat{\\beta_1}+2\\operatorname{SE}(\\hat\\beta_1)\\right]\\\\\n&=\\hat\\beta_1\\pm 2\\operatorname{SE}(\\hat\\beta_1)\n\\end{align*}\n\\] likewise for \\(\\operatorname{CI}_{\\beta_1}\\).\nInterpretation: There is approximately a 95% change (in infinite resamplings) that the (random) confidence interval \\(\\operatorname{CI}_{\\beta_1}\\) contains the true (fix) parameter value \\(\\beta_1\\).\nThus, a given confidence interval either contains the true parameter value or not and we usually do not know it. To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:\n\nInteractive visualization for interpreting confidence intervals\n\nStandard errors can also be used to do hypothesis testing:\n\\[\n\\begin{align*}\nH_0:&\\;\\text{There is no relationship between $Y$ and $X$; i.e. $\\beta_1=0$}\\\\\nH_1:&\\;\\text{There is a relationship between $Y$ and $X$; i.e. $\\beta_1\\neq 0$}\n\\end{align*}\n\\]\n\\(t\\)-test statistic\n\\[\nt=\\frac{\\hat\\beta_1 - 0}{\\operatorname{SE}(\\hat\\beta_1)}\\overset{H_0}{\\sim}t_{(n-1)}\n\\]\n\\(p\\)-value\n\\[\n\\begin{align*}\np\n&=P_{H_0}\\left(|t|\\geq|t_{obs}|\\right)\n&=2\\cdot\\min\\{P_{H_0}\\left(t\\geq t_{obs} \\right),\\; P_{H_0}\\left(t\\leq t_{obs} \\right)\\},\n\\end{align*}\n\\] where \\(t_{obs}\\) denotes the observed value of the \\(t\\)-test statistic and where \\(t\\) is \\(t\\)-distributed with \\((n-2)\\) degrees of freedom.\nSelect a significance level \\(\\alpha\\) (e.g. \\(\\alpha=0.01\\) or \\(\\alpha=0.05\\)) and reject \\(H_0\\) if \\[\np<\\alpha\n\\]\n\n\n\n(Ch. 3.1.3) Assessing the Accuracy of the Model\nIn tendency an accurate model has …\n\na low \\(\\operatorname{RSE}\\) \\[\n\\operatorname{RSE}=\\hat\\sigma=\\sqrt{\\frac{\\operatorname{RSS}}{n-2}}\n\\]\na high \\(R^2\\)\n\n\\[\nR^2=\\frac{\\operatorname{TSS}-\\operatorname{RSS}}{\\operatorname{TSS}}=1-\\frac{\\operatorname{RSS}}{\\operatorname{TSS}},\n\\]\nwhere \\(0\\leq R^2\\leq 1\\) and\n\\[\n\\begin{align*}\n\\operatorname{TSS}&=\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2\\\\\n\\operatorname{RSS}&=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2\\\\\n\\hat{y}_i&=\\hat\\beta_0+\\hat\\beta_1x_i\n\\end{align*}\n\\]\nCaution: Do not forget that there is a irreducible error \\(Var(\\epsilon)=\\sigma^2>0\\). Thus\n\nvery low \\(\\operatorname{RSE}\\) values, \\(\\operatorname{RSE}\\approx 0\\), and\nvery high \\(R^2\\) values, \\(R^2\\approx 1\\),\n\ncan be warning signals indicating overfitting.\nIn the case of the simple linear regression model, \\(R^2\\) equals the squared sample correlation coefficient between \\(Y\\) and \\(X\\), \\[\nR^2 = r^2,\n\\] where \\[\nr=\\widehat{cor}(Y,X)=\\frac{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}.\n\\]"
  },
  {
    "objectID": "Ch3_LinearRegression.html#multiple-linear-regression-ch.-3.2",
    "href": "Ch3_LinearRegression.html#multiple-linear-regression-ch.-3.2",
    "title": "2  Linear Regression",
    "section": "Multiple Linear Regression (Ch. 3.2)",
    "text": "Multiple Linear Regression (Ch. 3.2)\nThe multiple linear regression model allows for more than only one predictor:\n\\[\nY\\approx \\beta_0 + \\beta_1 X_1 +  \\dots + \\beta_p X_p + \\epsilon\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV \\(+\\beta_2\\) radio \\(+\\beta_3\\) newspaper \\(+\\epsilon\\)\n\n\n(Ch. 3.2.1) Estimating the Regression Coefficients\nSelect\n\\[\n\\hat\\beta_0,\\dots,\\hat\\beta_p\n\\] by minimizing \\[\n\\operatorname{RSS}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2,\n\\] where \\[\n\\hat{y}_i=\\hat\\beta_0 + \\hat\\beta_1 x_{i1} \\dots + \\hat\\beta_p x_{ip}\n\\]\n\nMultiple linear regression is more than mere composition of single simple linear regression models.\nTake a look at the following two simple linear regression results:\n\nThus in separate simple linear regressions, the effects of radio and the effect of newspaper on sales are both (but separately) statistically.\nBy contrast, when looking at the multiple linear regression when regressing sales onto both radio and newspaper, only the effect of radio remains statistically significant:\n\nReason: Omitted Variable Bias\n\nradio has an effect on sales\nnewspaper has actually no effect on sales\nBut, newspaper is “strongly” correlated with radio (cor(newspaper,radio)=0.3541); see Table 3.5\n\n\n\nThus, when omitting radio from the multiple regression model, newspaper becomes a surrogate for radio. This is called a Omitted Variable Bias.\n\nConclusion: Simple linear regression can be dangerous. We need to control for all possibly relevant variables if we want to interpret the estimation results (“Inference”).\nInterpretation of the Coefficients in Table 3.5\nFor fixed values of TV and newspaper, spending additionally 1000 USD for radio, increases on average sales by approximately 189 units.\n\n\n(Ch. 3.2.2) Some Important Questions\n1. Is There a Relationship Between the Response and Predictors?\n\\[\n\\begin{align*}\nH_0:&\\;\\beta_1=\\beta_2=\\dots=\\beta_p=0\\\\\nH_1:&\\;\\text{at least one $\\beta_j\\neq 0$; $j=1,\\dots,p$}\n\\end{align*}\n\\]\n\\(F\\)-test statistic \\[\nF=\\frac{(\\operatorname{TSS}-\\operatorname{RSS})/p}{\\operatorname{\n  RSS}/(n-p-1)}\n\\]\nIf \\(H_0\\) is correct \\[\n\\begin{align*}\nE(\\operatorname{RSS}/(n-p-1))&=\\sigma^2\\\\\nE((\\operatorname{TSS}-\\operatorname{RSS})/p)&=\\sigma^2\\\\\n\\end{align*}\n\\]\n\nThus, if \\(H_0\\) is correct, we expect values of \\(F\\approx 1\\).\nBut if \\(H_1\\) is correct, we expect values of \\(F\\gg 1\\).\n\nCaution: Cannot be computed if \\(p>n\\). (Chapter 6 on “high dimensional problems”)"
  },
  {
    "objectID": "Ch3_LinearRegression.html#ch.-3.3-other-considerations-in-the-regression-model",
    "href": "Ch3_LinearRegression.html#ch.-3.3-other-considerations-in-the-regression-model",
    "title": "2  Linear Regression",
    "section": "(Ch. 3.3) Other Considerations in the Regression Model",
    "text": "(Ch. 3.3) Other Considerations in the Regression Model\n\n(Ch. 3.3.1) Qualitative Predictors\nOften some predictors are qualitative variables (also known as a factor variables). For instance, the Credit dataset contains the following qualitative predictors:\n\nown (house ownership)\nstudent (student status)\nstatus (marital status)\nregion (East, West or South)\n\n\nPredictors with Only Two Levels\nIf a qualitative predictor (factor) only has two levels (i.e. possible values), then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values; for instance, \\[\nx_{i} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person owns a house}\\\\\n  0&\\quad \\text{if the $i$th person does not own a house.}\n  \\end{array}\\right.\n\\] Using this dummy variable as a predictor in the regression equation results in the following regression model: \\[\ny_{i}=\\beta_0 + \\beta_1 x_i + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1 + \\epsilon_i &\\quad \\text{if the $i$th person owns a house}\\\\\n  \\beta_0 + \\epsilon_i           &\\quad \\text{if the $i$th person does not own a house}\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The average credit card balance among those who do not own a house\n\\(\\beta_0+\\beta_1\\): The average credit card balance among those who do own a house\n\\(\\beta_1\\): The average difference in credit card balance between owners and non-owners\n\n\nAlternatively, instead of a 0/1 coding scheme, we could create a dummy variable \\[\nx_{i} = \\left\\{\n  \\begin{array}{ll}\n  1 &\\quad \\text{if the $i$th person owns a house}\\\\\n-1 &\\quad \\text{if the $i$th person does not own a house.}\n  \\end{array}\\right.\n\\] \\[\ny_{i}=\\beta_0 + \\beta_1 x_i + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1 + \\epsilon_i&\\quad \\text{if the $i$th person owns a house}\\\\\n  \\beta_0 - \\beta_1 + \\epsilon_i&\\quad \\text{if the $i$th person does not own a house}\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The overall average credit card balance (ignoring the house ownership effect)\n\\(\\beta_1\\): The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.\n\n\n\nQualitative Predictors with More than Two Levels\nWhen a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the region \\(\\in\\{\\)South, West, East\\(\\}\\) variable we create two dummy variables. The first could be \\[\nx_{i1} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person is from the South}\\\\\n  0&\\quad \\text{if the $i$th person is not from the South,}\n  \\end{array}\\right.\n\\] and the second could be \\[\nx_{i2} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person is from the West}\\\\\n  0&\\quad \\text{if the $i$th person is not from the West.}\n  \\end{array}\\right.\n\\] Using both of these dummy variables results in the following regression model: order to obtain the model \\[\ny_{i}=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1  + \\epsilon_i& \\quad \\text{if the $i$th person is from the South}\\\\\n  \\beta_0 + \\beta_2  + \\epsilon_i& \\quad \\text{if the $i$th person is from the West}\\\\\n  \\beta_0            + \\epsilon_i& \\quad \\text{if the $i$th person is from the East.}\\\\\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The average credit card balance for individuals from the East\n\\(\\beta_1\\): The difference in the average balance between people from the South versus the East\n\\(\\beta_2\\): The difference in the average balance between people from the West versus the East\n\n\nThere are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular contrasts. (A detailed discussion of contrasts is beyond the scope of this lecture.)\n\n\n\n(Ch. 3.3.2) Extensions of the Linear Model\n\nInteraction Effects: Removing the Additive Assumption using Interaction Effects\nPreviously, we used the following model\n\nsales \\(= \\beta_0 + \\beta_1\\) TV \\(+ \\beta_2\\) radio \\(+ \\beta_3\\) newspaper \\(+\\epsilon\\)\n\nwhich states that the average increase in sales associated with a one-unit increase in TV is always \\(\\beta_1,\\) regardless of the amount spent on radio.\nHowever, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on radio advertising actually increases the effectiveness of TV advertising.\nFigure 3.5 suggests that such an effect may be present in the advertising data:\n\nWhen levels of either TV or radio are low, then the true sales are lower than predicted by the linear model.\nBut when advertising is split between the two media, then the model tends to underestimate sales. \n\nSolution: Interaction Effects:\nConsider the standard linear regression model with two variables, \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon.\n\\] Here each predictor \\(X_1\\) and \\(X_2\\) has a given effect, \\(\\beta_1\\) and \\(\\beta_2\\), on \\(Y\\) and this effect does not depend on the value of the other predictor. (Additive Assumption)\nOne way of extending this model is to include a third predictor, called an interaction term, which is constructed by computing the product of \\(X_1\\) and \\(X_2.\\) This results in the model \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2 + \\epsilon.\n\\] This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as \\[\n\\begin{align*}\nY &= \\beta_0 + \\underbrace{(\\beta_1 + \\beta_3 X_2)}_{=\\tilde{\\beta}_1} X_1 + \\beta_2 X_2 + \\epsilon,\n\\end{align*}\n\\] where the new slope parameter \\(\\tilde{\\beta}_2\\) is a linear function of \\(X_2\\) \\[\n\\tilde{\\beta}_1\\equiv\\tilde{\\beta}_1(X_2)=\\beta_1 + \\beta_3 X_2.\n\\]\nThus, a change in the value of \\(X_2\\) will change the association between \\(X_1\\) and \\(Y.\\)\nA similar argument shows that a change in the value of \\(X_1\\) changes the association between \\(X_2\\) and \\(Y.\\)\nLet us return to the Advertising example. A linear model that uses radio, TV, and an interaction, radio\\(\\times\\)radio, between the two to predict sales takes the form\n\nsales \\(= \\beta_0 + \\beta_1\\times\\) TV \\(+ \\beta_2\\times\\) radio \\(+ \\beta_3\\times(\\) radio\\(\\times\\) TV\\()+\\epsilon\\)\n\nwhich can be rewritten as\n\nsales \\(=\\beta_0 + (\\beta_1+ \\beta_3\\times\\) radio \\()\\times\\) TV \\(+ \\beta_2\\times\\) radio \\(+\\epsilon\\)\n\n\nInterpretation:\n\n\\(\\beta_3\\) denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising (or vice-versa).\n\n\nInterpretation of Table 3.9:\n\nBoth (separate) main effects, TV and radio, are statistically significant (\\(p\\)-values smaller than 0.01).\nAdditionally, the \\(p\\)-value for the interaction term, TV\\(\\times\\)radio, is extremely low, indicating that there is strong evidence for \\(H_1: \\beta_3\\neq 0.\\) In other words, it is clear that the true relationship is not additive.\n\nHierarchical Principle:\nIf we include an interaction in a model, we should also include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant.\nInteractions with Qualitative Variables:\nAn interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.\nConsider the Credit data set and suppose that we wish to predict balance using the predictors:\n\nincome (quantitative) and\nstudent (qualitative) using a dummy variable with \\(x_{i2}=1\\) if \\(i\\)th person is a student and \\(x_{i2}=0\\) if not.\n\nIn the absence of an interaction term, the model takes the form \nThus, the regression lines for students and non-students have different intercepts, \\(\\beta_0+\\beta_2\\) versus \\(\\beta_0\\), but the same slope \\(\\beta_1\\).\nThis represents a potentially serious limitation of the model, since in fact a change in income may have a very different effect on the credit card balance of a student versus a non-student.\nThis limitation can be addressed by adding an interaction variable, created by multiplying income with the dummy variable for student. Our model now becomes \nNow we have different intercepts for students and non-students but also different slopes for these groups. \n\n\nPolynomial Regression: Non-linear Relationships\nPolynomial regression allows to accommodate non-linear relationships between the predictors \\(X\\) and the outcome \\(Y.\\) \nFor example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form\n\nmpg \\(=\\beta_0 + \\beta_1\\times\\) horsepower \\(+ \\beta_2\\times(\\)horsepower\\()^2+\\epsilon\\)\n\nThis regression model involves predicting mpg using a non-linear function of horsepower. But it is still a linear model! It’s simply a multiple linear regression model with \\(X_1=\\)horsepower and \\(X_2 =(\\)horsepower\\()^2.\\)\nSo we can use standard linear regression software to estimate \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) in order to produce a non-linear fit.\n\n\n\n\n(Ch. 3.3.3) Potential Problems\n1. Non-linearity of the response-predictor relationships.\nDiagnostic residual plots are most useful to detect possible non-linear response-predictor relationships.\n\nlibrary(\"ISLR2\")\ndata(Auto) \n\n## Gives the variable names in the Auto dataset\n# names(Auto)\n\n## Simple linear regression\nlmobj_1 <- lm(mpg ~ horsepower, data = Auto)\n\n## Quadratic regression \nlmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n\n## Diagnostic Plot\npar(mfrow = c(1,2))\nplot(lmobj_1, which = 1)\nplot(lmobj_2, which = 1)\n\n\n\n\nResidual plots are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals, \\[\ne_i = y_i - \\hat{y}_i,\n\\] versus the predictor \\(x_i.\\)\nIn the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values \\(\\hat{y}_i.\\) Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\nIf the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as \\[\n\\log(X),\\; \\sqrt{X},\\; \\text{or}\\; X^2\n\\] in the regression model. In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue.\n2. Correlation of Error Terms\nAn important assumption of the linear regression model is that the error terms, \\(\\epsilon_1, \\epsilon_2, \\dots , \\epsilon_n\\), are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that \\(\\epsilon_i\\) is positive provides little or no information about the sign of \\(\\epsilon_{i+1}.\\) The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors.\nCorrelations among the error terms typically occur in time series data (see Fig. 3.10).\n\n3. Non-Constant Variance of Error Terms\nAnother important assumption of the linear regression model is that the error terms have a constant variance, \\[\nVar(\\epsilon_i) = \\sigma^2.\n\\] The standard formulas for standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.\nOne can identify non-constant variances “heteroscedasticity” in the errors, using diagnostic residual plots.\nOften one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values which indicates. When faced with this problem, one possible solution is to transform the response \\(Y\\) using a concave function such as \\[\n\\log(Y)\\;\\text{ or }\\; \\sqrt{Y}.\n\\] Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.\n\n## Quadratic regression \nlmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n\n## Quadratic regression with transformed response log(Y)\nlmobj_3 <- lm(I(log(mpg)) ~ horsepower + I(horsepower^2), data = Auto)\n\n## Diagnostic Plot\npar(mfrow = c(1,2))\nplot(lmobj_2, which = 1)\nplot(lmobj_3, which = 1)\n\n\n\n\n4. Outliers\nAn outlier is a point for which \\(y_i\\) is far from the value predicted by the outlier model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.\nOutliers typically have a strong effect on the \\(R^2\\) value since they add a very large residual to its computation.\nFigure 3.12 in the textbook ISLR shows a clear outlier (observation 20) which, however, has a typical predictor value \\(x_i\\). Such outliers have little effect on the regression fit. \nFigure 3.13 in the textbook ISLR shows again a clear outlier (observation 41) which has a predictor value \\(x_i\\) that is very atypical. Such outliers are said to have large leverage giving them power to affect the regression fit considerably. \nSummary: Critical outliers have both, large residuals and large leverage.\n5. High Leverage Points\nIn order to quantify an observation’s leverage, we compute the leverage statistic \\(h_i\\) for each observation \\(i=1,\\dots,n.\\) A large value of this statistic indicates an observation with high leverage. For a simple linear regression, \\[\nh_i = \\frac{1}{n} + \\frac{(x_i-\\bar{x})^2}{\\sum_{j=1}^n(x_j-\\bar{x})^2}\n\\] There is a simple extension of \\(h_i\\) to the case of multiple predictors, though we do not provide the formula here.\n\nThe leverage statistic \\(h_i\\) is always between \\(1/n\\) and \\(1\\)\nThe average leverage for all the observations is equal to \\(\\bar{h}=\\frac{1}{n}\\sum_{i=1}^n h_i=(p + 1)/n.\\)\nIf a given observation has a leverage statistic \\(h_i\\) that greatly exceeds \\((p+1)/n,\\) then we may suspect that the corresponding point has high leverage.\n\n6. Collinearity\nCollinearity refers to the situation in which two or more predictor variables are closely related to one another.\n\nlibrary(\"ISLR2\")\ndata(Credit) # names(Credit)\n\npar(mfrow=c(1,2))\nplot(y = Credit$Age,    x = Credit$Limit, main = \"No Collinearity\", ylab = \"Age\", xlab = \"Limit\")\nplot(y = Credit$Rating, x = Credit$Limit, main = \"Strong Collinearity\", ylab = \"Rating\", xlab = \"Limit\")\n\n\n\n\n\n\nWe call this situation multicollinearity.\nTo detect multicollinearity issues, one can use the variance inflation factor (VIF) \\[\n\\operatorname{VIF}(\\hat{\\beta}_j)=\\frac{1}{1-R^2_{X_j|X_-j}},\n\\] where \\(R^2_{X_j|X_-j}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors.\n\nIf \\(R^2_{X_j|X_-j}\\) is close to one, then multicollinearity is present, and \\(\\operatorname{VIF}(\\hat{\\beta}_j)\\) will be large.\n\nIn the Credit data, a regression of balance on age, rating, and limit indicates that the predictors have VIF values of 1.01 (age), 160.67 (rating), and 160.59 (limit). Thus, as we suspected, there is considerable collinearity in the data!\nPossible solutions:\n\nDrop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables.  Caution: In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.\nCombine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.\nUse a different estimation procedure like ridge regression.\nLive with it. Sometimes you’re not allowed to drop or combine variables (e.g. important control variables) and also no other estimation procedure can be used. Then you have to live with large standard errors due to multicollinearity. But at least you know where the large stand errors are coming from."
  },
  {
    "objectID": "Ch3_LinearRegression.html#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors",
    "href": "Ch3_LinearRegression.html#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors",
    "title": "2  Linear Regression",
    "section": "(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors",
    "text": "(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors\nLinear regression is an example of a parametric approach because it assumes a linear model form for \\(f(X).\\)\nAdvantages of parametric approaches:\n\nTypically easy to fit\nSimple interpretation\nSimple inference\n\nDisadvantages of parametric approaches:\n\nThe parametric model assumption can be far from true; i.e. \\[\nf(X) \\neq \\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\n\\]\n\nAlternative: Non-parametric methods such as K-nearest neighbors regression since non-parametric approaches do not explicitly assume a parametric form for \\(f(X).\\)\n\nK-nearest neighbors regression (KNN regression)\nGiven a value for \\(K\\) and a prediction point \\(x_0,\\) KNN regression regression …\n\nidentifies the \\(K\\) training observations that are closest to \\(x_0\\), represented by the index set \\(\\mathcal{N}_0\\subset\\{1,2,\\dots,n_{Train}\\}.\\)\nestimates \\(f(x_0)\\) using the average of all the training responses \\(y_i\\) with \\(i\\in\\mathcal{N}_0.\\)\n\nIn other words, \\[\n\\hat{f}(x_0)=\\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}y_i.\n\\]\n\nIn general, the optimal value for \\(K\\) will depend on the bias-variance tradeoff, which we introduced in Chapter 2.\n\nA small value for \\(K\\) provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation of \\(K=1\\).\nA large value of \\(K\\) provide a smoother and less wiggly fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in \\(f(X).\\)\n\nIn Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of \\(K\\) in KNN regression.\nGenerally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of \\(f\\) and vice versa.\nFigure 3.17 of our textbook ISLR provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent the true \\(f(X)\\), while the blue curves correspond to the KNN fits using \\(K = 1\\) (left plot) and \\(K = 9\\) (right plot). In this case, the \\(K = 1\\) predictions are far too variable, while the smoother \\(K = 9\\) fit is much closer to the true \\(f(X).\\) However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. \nThe blue dashed line in the left-hand panel of Figure 3.18 represents the linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. \nFigure 3.19 displays a non-linear situations in which KNN performs much better than linear regression. \nCurse of dimensionality:\nUnfortunately, in higher dimensions, KNN often performs worse than linear regression, since non-parametric approaches suffer from the curse of dimensionality. Figure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise (i.e. redundant) predictors that are not associated with the response.\n\nWhen \\(p = 1\\) or \\(p = 2\\), KNN outperforms linear regression.\nBut for \\(p = 3\\) the results are mixed, and for \\(p\\geq 4\\) linear regression is superior to KNN. \n\nWhen \\(p=1\\), \\(50\\) data points can provide enough information to estimate \\(f(X)\\) accurately using non-parametric methods since the \\(K\\) nearest neighbors can actually be close to a given test observation \\(x_0.\\) However, when spreading the \\(50\\) data points over a large number of, for instance, \\(p=20\\) dimensions, even the \\(K\\) nearest neighbors tend to become far away from \\(x_0.\\)"
  },
  {
    "objectID": "Ch3_LinearRegression.html#r-lab-linear-regression",
    "href": "Ch3_LinearRegression.html#r-lab-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.2 R-Lab: Linear Regression",
    "text": "2.2 R-Lab: Linear Regression\n\n2.2.1 Libraries\nThe library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with this book.\n\nsuppressPackageStartupMessages(library(MASS))\nsuppressPackageStartupMessages(library(ISLR2))\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session.\n\n\n2.2.2 Simple Linear Regression\nThe ISLR2 library contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (average age of houses), and lstat (percent of households with low socioeconomic status).\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm.fit <- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat.\nThe next line tells R that the variables are in Boston:\n\nlm.fit <- lm(medv ~ lstat, data = Boston)\n\nAlternatively, we can attach the Boston object:\n\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\nWe can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\nType confint(lm.fit) at the command line to obtain the confidence intervals for the linear regression coefficients.\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), \n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), \n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nFor instance, the 95% confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95% prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nThe abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a, b). Below we experiment with some additional settings for plotting lines and points. The lwd = 3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\n\nplot(lstat, medv)\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\nplot(lstat, medv, col = \"red\")\n\n\n\nplot(lstat, medv, pch = 20)\n\n\n\nplot(lstat, medv, pch = \"+\")\n\n\n\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity.\nLeverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nplot(hatvalues(lm.fit))\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\nsort(hatvalues(lm.fit), decreasing = TRUE)[1:3]\n\n       375        415        374 \n0.02686517 0.02495670 0.02097101 \n\n\nThe sort() function can be used to sort and print values of a vector like hatvalues(lm.fit).\n\n\n2.2.3 Multiple Linear Regression\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nsuppressPackageStartupMessages(library(car)) # contains the vif() function\nsort(vif(lm.fit)) # computes the VIF statistics and sorts them\n\n    chas    black     crim  ptratio       rm       zn    lstat      age \n1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 \n     dis    indus      nox      rad      tax \n3.955945 3.991596 4.393720 7.484496 9.008554 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 <- update(lm.fit, ~ . - age)\n\n\n\n2.2.4 Interaction Terms\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells R to include an interaction term between lstat and black. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\n\n\n\n\n2.2.5 Non-linear Transformations of the Predictors\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n2.2.6 Qualitative Predictors\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n2.2.7 Writing Functions\nAs we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError in eval(expr, envir, enclos): object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function.\n\nLoadLibraries <- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries\n\nfunction() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "Ch3_LinearRegression.html#exercises",
    "href": "Ch3_LinearRegression.html#exercises",
    "title": "2  Linear Regression",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\nPrepare the following exercises of Chapter 3 in our course textbook ISLR:\n\nExercise 1\nExercise 2\nExercise 3\nExercise 8\nExercise 9"
  },
  {
    "objectID": "Ch3_LinearRegression.html#solutions",
    "href": "Ch3_LinearRegression.html#solutions",
    "title": "2  Linear Regression",
    "section": "2.4 Solutions",
    "text": "2.4 Solutions\n\nExercise 1\n1 a) Describe the null hypotheses to which the \\(p\\)-values given in Table 3.4 correspond.\n\n1 b) Explain what conclusions you can draw based on these \\(p\\)-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\nAnswers:\n1 a) In Table 3.4, the null hypothesis for TV is that in the presence of radio ads and newspaper ads, TV ads have no effect on sales. Similarly, the null hypothesis for radio is that in the presence of TV ads and newspaper ads, radio ads have no effect on sales.\n1 b) On the one hand, the low p-values of TV and radio allow us to reject the “no effect” null hypotheses for TV and radio. Hence, we believe that TV (radio) ads have an effect on sales in the presence of radio (TV) and newspaper ads. On the other hand, the high p-value of newspaper does not allow us to reject the “no effect” null-hypothesis. This constitutes an inconclusive result and only says that the possible effects of newspaper ads are not large enough to stand out from the estimation errors.\nRemember: An insignificant hypothesis test result is never informative about whether the tested null hypothesis is true. We do not have an error-control for falsely accepting the null-hypothesis. We only have an error-control (by the significance level) for falsely rejecting the null-hypothesis.\n\n\nExercise 2\nCarefully explain the main difference between the KNN classifier and KNN regression methods.\nAnswer:\nKNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for \\(Y\\) (qualitative), given a certain predictor \\(x_0\\), where as the output for a KNN regression predicts the quantitative value for \\(f(x_0)\\), given a certain predictor \\(x_0\\).\n\n\nExercise 3\nSuppose we have a data set with five predictors:\n\\(X_1 =GPA\\)\n\\(X_2 = IQ\\)\n\\(X_3 = Gender\\) (\\(1\\) for Female and \\(0\\) for Male)\n\\(X_4 =\\) Interaction between \\(GPA\\) and \\(IQ\\)\n\\(X_5 =\\) Interaction between \\(GPA\\) and \\(Gender\\)\nThe response variable (in thousands of dollars) is defined as:\n\\(Y =\\) starting salary after graduation\nSuppose we use least squares to fit the model, and get:\n\\(\\hat{\\beta}_0 = 50\\), \\(\\hat{\\beta}_1 = 20\\), \\(\\hat{\\beta}_2 = 0.07\\), \\(\\hat{\\beta}_3 = 35\\), \\(\\hat{\\beta}_4 = 0.01\\), and \\(\\hat{\\beta}_5 = −10\\).\nThus we have:\n\\[\n\\begin{align*}\n&E[Y|X] = \\\\\n& 50 + 20\\,\\overbrace{GPA}^{X_1} + 0.07\\,\\overbrace{IQ}^{X_2} + 35\\,\\overbrace{Gender}^{X_3} +\n0.01\\,\\overbrace{GPA\\cdot IQ}^{X_4=X_1\\cdot X_2} - 10\\,\\overbrace{GPA\\cdot Gender}^{X_5=X_1\\cdot X_3}\n\\end{align*}\n\\]\n3 a) Which answer is correct, and why?\n\nFor a fixed value of \\(IQ\\) and \\(GPA\\), males earn more on average than females.\nFor a fixed value of \\(IQ\\) and \\(GPA\\), females earn more on average than males.\nFor a fixed value of \\(IQ\\) and \\(GPA\\), males earn more on average than females provided that the \\(GPA\\) is high enough.\nFor a fixed value of \\(IQ\\) and \\(GPA\\), females earn more on average than males provided that the \\(GPA\\) is high enough.\n\nAnswer: Observe that: \\[\n\\begin{align*}\n\\text{Male\\; $(X_3 = 0)$:}\\quad   & 50 + 20 X_1 + 0.07 X_2 + \\phantom{3}0 + 0.01\\,(X_1 \\cdot X_2) -0     \\\\[1.5ex]\n\\text{Female\\; $(X_3 = 1)$:}\\quad & 50 + 20 X_1 + 0.07 X_2 + 35 + 0.01(X_1 \\cdot X_2) - 10\\,X_1\n\\end{align*}\n\\]\nThus 3 a) iii. is correct, since once the \\(X_1=\\)GPA is high enough (\\(35-10\\,X_1<0 \\Leftrightarrow X_1>3.5\\)), males earn more on average.\n3 b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.\nAnswer:\n\nGPA    <-   4\nIQ     <- 110\nGender <-   1 # female = 1\n## Prediction\nY_hat  <- 50 + 20*GPA + 0.07*IQ + 35*Gender + 0.01*GPA*IQ - 10*GPA\nY_hat\n\n[1] 137.1\n\n\n3 c) True or false: Since the coefficient for the GPA\\(\\times\\)IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\nAnswer:\nFalse. We must examine the \\(p\\)-value (or the \\(t\\)-statistic) of the regression coefficient to determine if the interaction term is statistically significant or not.\n\n\nExercise 8\nThis question involves the use of simple linear regression on the Auto data set.\n8 a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results.\n\nlibrary(\"ISLR2\")\n\ndata(\"Auto\")\n\n# Perform linear regression\nlmObj_1 <- lm(mpg ~ horsepower, data=Auto)\n\n# Use summary function to print the results\nsummary(lmObj_1)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\nComment on the output. For example:\ni) Is there a relationship between the predictor and the response?\nAnswer:\nYes, there is. The predictor horsepower has a statistically significant (\\(p<0.001\\)) linear relationship with the response.\nii) How strong is the relationship between the predictor and the response?\nAnswer:\nStatistical significance does not necessarily mean a practically strong or important relationship.\nTo quantify the strength of the relationship between the predictor and the response, we can look at the following quantities:\n\nResidual Standard Error (RSE) (estimate of the standard deviation of \\(\\epsilon\\)) in comparison to the RSE of the trivial linear regression model with only an intercept.\nThe \\(R^2\\) Statistic (the proportion of variance explained by the model)\nThe \\(F\\)-Statistic\n\nThe Residual Standard Error (RSE) of the regression model with intercept and horsepower as predictors is given by:\n\n## RSE of lm(mpg ~ horsepower):\nRSS <- sum(resid(lmObj_1)^2)\nn   <- length(resid(lmObj_1))\nRSE <- sqrt(RSS/(n-2))\nround(RSE, 3)\n\n[1] 4.906\n\n## Alternatively: \nround(summary(lmObj_1)$sigma, 3)\n\n[1] 4.906\n\n\nThis RSE value is considerable smaller than the RSE of a model with only an intercept:\n\nlmObj_onlyIntercept <- lm(mpg ~ +1, data = Auto)\nRSS_onlyIntercept   <- sum(resid(lmObj_onlyIntercept)^2)\nn                   <- length(resid(lmObj_onlyIntercept))\nRSE_onlyIntercept   <- sqrt(RSS_onlyIntercept/(n-1))\nround(RSE_onlyIntercept, 3)\n\n[1] 7.805\n\n\nThus, the larger model with horsepower included explains more of the variances in the response variable mpg. Including horsepower as a predictor reduces the RSE by ((RSE_onlyIntercept - RSE)/RSE_onlyIntercept)*100 %; i.e. by 37.15%.\nThe \\(R^2\\) value:\n\nround(summary(lmObj_1)$r.squared, 2)\n\n[1] 0.61\n\n\nshows that \\(60\\%\\) of variability in \\(Y\\) can be explained using an intercept and horsepower as predictors.\nThe value of the \\(F\\) statistic ::: {.cell}\nround(summary(lmObj_1)$fstatistic, 2)\n\n value  numdf  dendf \n599.72   1.00 390.00 \n\n::: is much larger than \\(1\\) which means that the linear regression model with intercept and horsepower fits the data significantly better than the trivial regression model with only an intercept.\niii) Is the relationship between the predictor and the response positive or negative?\nAnswer:\nThe relationship is negative, as we can see from the parameter estimate for horsepower\n\ncoef(lmObj_1)[2]\n\nhorsepower \n-0.1578447 \n\n\niv) What is the predicted mpg associated with a horsepower of \\(98\\)? What are the associated \\(95\\%\\) confidence and prediction intervals?\nAnswer:\nThe predicted value plus confidence interval:\n\n# Horsepower of 98\nnew_df <- data.frame(horsepower = 98)\n\n# confidence interval \npredict(object = lmObj_1, newdata = new_df, interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\n\nThe predicted value plus prediction interval: ::: {.cell}\n# Horsepower of 98\nnew_df <- data.frame(horsepower = 98)\n\n# prediction interval\npredict(object = lmObj_1, newdata = new_df, interval = \"prediction\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\n:::\n8 b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.\nAnswer:\n\nplot(x = Auto$horsepower, y = Auto$mpg, ylab = \"MPG\", xlab = \"Horsepower\")\nabline(lmObj_1, col=\"blue\")\nlegend(\"topright\", \n       legend = c(\"(y,x)\", expression(paste(\"(\",hat(y),\",x)\"))), \n       pch=c(1,NA), lty=c(NA,1), col=c(\"black\", \"blue\"))\n\n\n\n\n8 c) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\nAnswer:\n\npar(mfrow=c(2,2))\nplot(lmObj_1, col='blue')\n\n\n\n\nLooking at the smoothing line of the residuals (\\(e_i=y_i−\\hat{y}_i\\)) vs. the fitted values (\\(\\hat{y}_i\\)), there is a strong pattern in the residuals, indicating non-linearity. You can see evidence of this also in the scatter plot in the answer for question 8 b).\nThere also appears to be non-constant variance in the error terms (heteroscedasticity), but this may be corrected to an extent when trying a quadratic fit. If not, transformations such as \\(log(y)\\) or \\(\\sqrt{y}\\) can shrink larger responses by a greater amount and reduce this issue.\nThere are some observations with large standardized residuals & high leverage (hence, high Cook’s Distance) that we need to review.\n\n\nExercise 9\nThis question involves the use of multiple linear regression on the Auto data set.\n9 a) Produce a scatterplot matrix which includes all of the variables in the data set.\nAnswer:\n\nlibrary(\"ISLR2\")\n\ndata(\"Auto\")\n\n# Produce scatterplot matrix\npairs(Auto)\n\n\n\n\n9 b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.\nAnswer:\n\nround(cor(subset(Auto, select = -name)), 1)\n\n              mpg cylinders displacement horsepower weight acceleration year\nmpg           1.0      -0.8         -0.8       -0.8   -0.8          0.4  0.6\ncylinders    -0.8       1.0          1.0        0.8    0.9         -0.5 -0.3\ndisplacement -0.8       1.0          1.0        0.9    0.9         -0.5 -0.4\nhorsepower   -0.8       0.8          0.9        1.0    0.9         -0.7 -0.4\nweight       -0.8       0.9          0.9        0.9    1.0         -0.4 -0.3\nacceleration  0.4      -0.5         -0.5       -0.7   -0.4          1.0  0.3\nyear          0.6      -0.3         -0.4       -0.4   -0.3          0.3  1.0\norigin        0.6      -0.6         -0.6       -0.5   -0.6          0.2  0.2\n             origin\nmpg             0.6\ncylinders      -0.6\ndisplacement   -0.6\nhorsepower     -0.5\nweight         -0.6\nacceleration    0.2\nyear            0.2\norigin          1.0\n\n\n9 c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output by answering the below questions 9 c i) to 9 c iii).\nAnswer:\n\n# Perform multiplie linear regression\nfit.lm <- lm(mpg ~ . -name, data=Auto)\n\n# Print results\nsummary(fit.lm)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n\n9 c i) Is there a relationship between the predictors and the response?\nAnswer:\nYes, there is a relationship between the predictors and the response. By testing the null hypothesis of whether all (except intercept) the regression coefficients are zero (i.e. H\\(_0\\): \\(\\beta_1=\\dots=\\beta_7=0\\)), we can see that the \\(F\\)-statistic is big and its \\(p\\)-value is close to zero, indicating evidence against the null hypothesis.\n9 c ii) Which predictors appear to have a statistically significant relationship to the response?\nAnswer:\nLooking at the \\(p\\)-values associated with each predictor’s \\(t\\)-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower, and acceleration do not.\nCaution: This consideration neglects issues due to multiple testing. When testing at the significance level \\(\\alpha=0.05\\), then each single test has a type I error (false H\\(_0\\) rejections) rate of up to \\(5\\%\\). These type I error rates accumulate since we consider seven hypothesis tests simultaneously, and thus the probability of seeing one type I error among the seven tests is up to \\(7\\cdot 5\\%=35\\%\\). So is quite likely to see one type I error.\nBonferroni correction for multiple testing: To determine if any of the seven predictors is statistically significant, the corresponding \\(p\\)-value must be smaller than \\(\\alpha/7\\). For instance, with \\(\\alpha/7=0.05/7\\approx 0.007\\), only weight, year, and origin have a statistically significant relationships to the response.\n9 c iii) What does the coefficient for the year variable suggest?\nAnswer:\nThe regression coefficient for year suggests that, on average, one year later year-of-construction is associated with an increased mpg by \\(0.75\\), when holding every other predictor value constant.\n9 d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\nAnswer:\n\npar(mfrow=c(4,1))\nplot(fit.lm)\n\n\n\n\n\nThe “Residuals vs Fitted” plot (1st plot) shows some systematic deviations of the residuals from \\(0\\). The reason is that we are imposing a straight “line” (better hyper plane) fit for the conditional mean function \\(E[Y|X]=f(X)\\) which appears non-linear here. This results in a systematic underestimation of the true conditional mean function for large and small fitted values \\(\\hat{y}=\\hat\\beta_0+\\hat\\beta_1x_1+\\dots+\\hat\\beta_px_p\\).\nThe “Normal Q-Q” plot (2nd plot) suggests non-normally distributed residuals–particularly the upper tail deviates from that of a normal distribution.\nThe “Residuals vs Leverage” plot (3rd plot) shows that there are some potential outliers that we can see when: standardized residuals are below \\(-2\\) or above \\(+2\\). Moreover, the plot shows also potentially problematic “high-leverage” points with leverage values heavily exceeding the rule-of-thumb threshold \\((p+1)/n=8/392=0.02\\). All points with simultaneously high-leverages and large absolute standardized residuals should be handled with care since these may distort the estimation.\nThe “Scale-Location” plot (4th plot) shows is rather inconclusive about heteroscedasticity. However the “Residuals vs Fitted” plot (1st plot)shows some clear sign of heteroscedastic residuals.\n\n9 e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\nAnswer:\nViolating the hierarchy principle:\n\nfit.lm0 <- lm(mpg ~ horsepower+cylinders+year+weight:displacement, \n              data=Auto)\nsummary(fit.lm0)\n\n\nCall:\nlm(formula = mpg ~ horsepower + cylinders + year + weight:displacement, \n    data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.1046 -2.8861 -0.2415  2.3967 15.3221 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -1.343e+01  5.043e+00  -2.663  0.00807 ** \nhorsepower          -3.914e-02  1.278e-02  -3.063  0.00234 ** \ncylinders           -1.358e+00  3.233e-01  -4.201 3.31e-05 ***\nyear                 6.661e-01  6.019e-02  11.067  < 2e-16 ***\nweight:displacement -3.354e-06  1.352e-06  -2.480  0.01355 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.985 on 387 degrees of freedom\nMultiple R-squared:  0.7419,    Adjusted R-squared:  0.7393 \nF-statistic: 278.2 on 4 and 387 DF,  p-value: < 2.2e-16\n\n\nFollowing the hierarchical principle: ::: {.cell}\nfit.lm1 <- lm(mpg~horsepower+cylinders+year+weight*displacement, \n              data=Auto)\nsummary(fit.lm1)\n\n\nCall:\nlm(formula = mpg ~ horsepower + cylinders + year + weight * displacement, \n    data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7530 -1.8228 -0.0602  1.5780 12.6133 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -2.210e+00  3.819e+00  -0.579  0.56316    \nhorsepower          -3.396e-02  9.560e-03  -3.552  0.00043 ***\ncylinders            2.072e-01  2.914e-01   0.711  0.47756    \nyear                 7.858e-01  4.555e-02  17.250  < 2e-16 ***\nweight              -1.084e-02  6.346e-04 -17.076  < 2e-16 ***\ndisplacement        -7.947e-02  9.905e-03  -8.023 1.26e-14 ***\nweight:displacement  2.431e-05  2.141e-06  11.355  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.976 on 385 degrees of freedom\nMultiple R-squared:  0.8568,    Adjusted R-squared:  0.8546 \nF-statistic: 384.1 on 6 and 385 DF,  p-value: < 2.2e-16\n\n:::\nNote that there is a difference between using A:B and A*B when running a regression. While the first includes only the interaction term between the variable A and B, the second one also includes the stand-alone variables A and B.\nGenerally, you should follow the hierarchical principle for interaction effects: If we include an interaction in a model, we should also include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant.\n9 f)\nTry a few different transformations of the variables, such as \\(\\log(X)\\), \\(\\sqrt{X}\\), \\(X^2\\). Comment on your findings.\nAnswer:\n\nfit.lm2 <- lm(mpg~log(weight)+sqrt(horsepower)+\n                acceleration+I(acceleration^2),\n              data=Auto)\nsummary(fit.lm2)\n\n\nCall:\nlm(formula = mpg ~ log(weight) + sqrt(horsepower) + acceleration + \n    I(acceleration^2), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2932  -2.5082  -0.2237   2.0237  15.7650 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       178.30303   10.80451  16.503  < 2e-16 ***\nlog(weight)       -14.74259    1.73994  -8.473 5.06e-16 ***\nsqrt(horsepower)   -1.85192    0.36005  -5.144 4.29e-07 ***\nacceleration       -2.19890    0.63903  -3.441 0.000643 ***\nI(acceleration^2)   0.06139    0.01857   3.305 0.001037 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.99 on 387 degrees of freedom\nMultiple R-squared:  0.7414,    Adjusted R-squared:  0.7387 \nF-statistic: 277.3 on 4 and 387 DF,  p-value: < 2.2e-16\n\n##\npar(mfrow=c(4,1))\nplot(fit.lm2)\n\n\n\n\nThis try suffers basically from the same issues as the model considered in 9 d)\nLet’s consider again the model with all predictors (except name), but with transforming the outcome variable mpg by a \\(\\log\\)-transformation.\n\nfit.lm3 <-lm(log(mpg)~ . -name, data=Auto)\nsummary(fit.lm3)\n\n\nCall:\nlm(formula = log(mpg) ~ . - name, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40955 -0.06533  0.00079  0.06785  0.33925 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.751e+00  1.662e-01  10.533  < 2e-16 ***\ncylinders    -2.795e-02  1.157e-02  -2.415  0.01619 *  \ndisplacement  6.362e-04  2.690e-04   2.365  0.01852 *  \nhorsepower   -1.475e-03  4.935e-04  -2.989  0.00298 ** \nweight       -2.551e-04  2.334e-05 -10.931  < 2e-16 ***\nacceleration -1.348e-03  3.538e-03  -0.381  0.70339    \nyear          2.958e-02  1.824e-03  16.211  < 2e-16 ***\norigin        4.071e-02  9.955e-03   4.089 5.28e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1191 on 384 degrees of freedom\nMultiple R-squared:  0.8795,    Adjusted R-squared:  0.8773 \nF-statistic: 400.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n##\npar(mfrow=c(4,1))\nplot(fit.lm3)\n\n\n\n\nThis model specification is much better!\n\nNo clear issues of systematic under/over estimations for given fitted values.\nNo clear issues of heteroscedastic residuals.\nNormality assumption may be wrong, but this isn’t problematic since we have a large dataset, such that a central limit theorem will make the estimators asymptotically normal distributed.\nOne large leverage point which, however, has a small residual."
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.1-an-overview-of-classification",
    "href": "Ch4_Classification.html#ch.-4.1-an-overview-of-classification",
    "title": "4  Classification",
    "section": "(Ch. 4.1) An Overview of Classification",
    "text": "(Ch. 4.1) An Overview of Classification\nClassification problems occur often, perhaps even more so than regression problems.\nSome examples include:\n\nA person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\nAn online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.\nOn the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.\n\nOne of the running example for this chapter: The (simulated) Default data set which is part of the online resources of our textbook ISLR, but also contained in the R package ISLR2. \nLet’s take a first look at the a priori default rate in this dataset:\n\nsuppressPackageStartupMessages(library(\"ISLR2\"))\ndata(Default)\n\nn <- nrow(Default)       # sample size\n\ntable(Default$default)/n # Overall no-default and default-rate\n\n\n    No    Yes \n0.9667 0.0333 \n\n\n\nOverall default rate: approx. \\(3\\%.\\) (Figure 4.1 shows only a small fraction of the individuals who did not default.)"
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.2-why-not-linear-regression",
    "href": "Ch4_Classification.html#ch.-4.2-why-not-linear-regression",
    "title": "4  Classification",
    "section": "(Ch. 4.2) Why Not Linear Regression?",
    "text": "(Ch. 4.2) Why Not Linear Regression?\nLinear regression is often not appropriate in the case of a qualitative response \\(Y.\\)\nSuppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses:\n\nstroke\ndrug overdose, and\nepileptic seizure\n\nWe can encoding these values as a quantitative response variable, \\[\nY=\\left\\{\n    \\begin{array}{ll}\n    1&\\quad\\text{if }\\texttt{stroke}\\\\\n    2&\\quad\\text{if }\\texttt{drug overdose}\\\\\n    3&\\quad\\text{if }\\texttt{epileptic seizure}\\\\\n    \\end{array}\n\\right.\n\\] Using this coding, least squares could be used to fit a linear regression model to predict \\(Y,\\) but:\n\nThe results would then depend on the numeric ordering \\(1<2<3\\), even though the ordering was completely arbitrary and could have been made differently.\nThe results would then depend on the assumption the gap \\((2-1=1)\\) between stroke and drug overdose is comparable to the gap \\((3-2=1)\\) between drug overdose and epileptic seizure.\n\nGenerally, both points are quite a lot of nonsense for most applications.\nOnly if the response variable’s values did take on a natural ordering, such as “mild”, “moderate”, and “severe”, and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable.\nFor a binary (two level) qualitative response, the situation is better. For instance, if there are only two conditions that we need to predict (e.g. either default\\(=\\)Yes or default\\(=\\)No), we can use a dummy variable coding \\[\nY=\\left\\{\n\\begin{array}{ll}\n    0&\\quad\\text{if }\\texttt{default}=\\texttt{Yes}\\\\\n    1&\\quad\\text{if }\\texttt{default}=\\texttt{No}\\\\\n\\end{array}\n\\right.\n\\] We could then fit a linear regression to this binary response, and predict drug overdose if \\(\\hat{Y}> 0.5\\) and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.\nFor binary responses with a \\(0/1\\) coding, linear regression is not completely unreasonable. It can be shown that \\[\nPr(\\texttt{default}=\\texttt{Yes}|X)\\approx \\hat{Y}=\\beta_0+ \\beta_1 X_1+\\dots +\\beta_p X_p.\n\\]\nHowever, if we use linear regression, some of our estimates might be outside the \\([0, 1]\\) interval (see Figure 4.2), which doesn’t make sense when predicting probabilities. \nSummary:\n\nA classic regression method cannot accommodate a qualitative response with more than two classes\nA classic regression method may not provide meaningful estimates of \\(Pr(Y |X),\\) even with just two classes.\n\nThus, it is often preferable to use a classification method that is truly suited for qualitative response values."
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.3-logistic-regression",
    "href": "Ch4_Classification.html#ch.-4.3-logistic-regression",
    "title": "4  Classification",
    "section": "(Ch. 4.3) Logistic Regression",
    "text": "(Ch. 4.3) Logistic Regression\nLogistic regression models the probability that \\(Y\\) belongs to a particular category.\nFor the Default data, logistic regression models the conditional probability of default given values for the predictor(s). For example, the probability of default given balance: \\[\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{balance}) = p(\\texttt{balance}),\n\\] where \\(p(\\texttt{balance})\\in[0,1]\\) is used as a short hand notation.\nOne might predict default\\(=\\)Yes for any individual for whom \\(p(\\texttt{balance}) > 0.5.\\)\nHowever, \\(0.5\\) this is not the only reasonable classification threshold!\nFor instance, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as \\(p(\\texttt{balance}) > 0.1.\\)\n\n(Ch. 4.3.1) The Logistic Model\nFor a binary coded dependen variable \\(Y\\in\\{0,1\\}\\) we aim to model the relationship between \\[\np(X)=Pr(Y=1|X)\\quad\\text{and}\\quad X.\n\\]\nAs discussed above, a linear regression model, e.g., \\[\np(X)=\\beta_0+\\beta_1 X\n\\] can produce nonsense predictions \\(p(X)<0\\) or \\(p(X)>1\\); see the left-hand panel of Figure 4.2.\nLogistic regression avoids this problem by using the logistic function, \\[\np(X)=\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}.\n\\] To fit the parameters \\(\\beta_0\\) and \\(\\beta_1\\) we use an estimation method called maximum likelihood.\nThe right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data.\nNote that \\[\n\\begin{align*}\np(X) & = \\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{1-p(X)} \\\\\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{1-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{\\frac{1+e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n\\quad \\Leftrightarrow\\quad  \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1 X}\n\\end{align*}\n\\]\nThe quantity \\[\n\\frac{p(X)}{1 − p(X)}\n\\] is called the odds, and can take any value between 0 and plus infinity.\n\nA small odds value (close to zero) indicates a low probability of default.\nA large odds value indicates a high probability of default.\n\nFor instance\n\nAn odds value of \\(\\frac{1}{4}\\) means that \\(0.2=20\\%\\) of the people will default \\[\n\\frac{0.2}{1-0.2}=\\frac{1}{4}\n\\]\nAn odds value of \\(9\\) means that \\(0.9=90\\%\\) of the people will default \\[\n\\frac{0.9}{1-0.9}=9\n\\]\n\nBy taking the logarithm, we arrive at log odds or logit \\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0+\\beta_1 X\n\\]\nThus increasing \\(X\\) by one unit …\n\n… changes the log odds by \\(\\beta_1\\)\n… multiplies the odds by \\(e^{\\beta_1}\\)\n\nCaution: The amount that \\(p(X)\\) changes due to a one-unit change in \\(X\\) depends on the current value of \\(X.\\) (The logistic regression model is a non-linear model.)\nBut regardless of the value of \\(X\\), if \\(\\beta_1\\) is positive then increasing \\(X\\) will be associated with increasing \\(p(X)\\), and if \\(\\beta_1\\) is negative then increasing \\(X\\) will be associated with decreasing \\(p(X).\\)\n\n\n(Ch. 4.3.2) Estimating the Regression Coefficients\nIn logistic regression analysis, the unknown model parameters are estimated using maximum likelihood.\nBasic intuition:\nFind estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) such that the predicted probability \\(\\hat{p}(x_i)\\) for each person \\(i\\) corresponds as close as possible to its default status. (I.e. \\(\\hat{p}(x_i)\\approx 1\\) if person \\(i\\) defaulted and \\(\\hat{p}(x_i)\\approx 0\\) if not.)\nThis intuition can be formalized using a mathematical equation called a likelihood function: \\[\n\\ell(\\beta_0,\\beta_1)=\\prod_{i:y_{i}=1} p(x_i)\\prod_{i:y_{i}=0} (1-p(x_i))\n\\] The estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are chosen to maximize this likelihood function.\nMaximum likelihood is a very general estimation method that allows to estimate also non-linear models (like the logistic regression model).\nTable 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default\\(=\\)Yes using balance as the only predictor. \nInterpretation:\n\nWe see that \\(\\hat\\beta_1=0.0055\\); this indicates that an increase in balance is associated with an increase in the probability of default.\n\nTo be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0.0055 units.\n\nThe \\(z\\)-statistic in Table 4.1 plays the same role as the \\(t\\)-statistic in the linear regression output: a large (absolute) value of the \\(z\\)-statistic indicates evidence against the null hypothesis \\(H_0: \\beta_1= 0.\\)\n\n\n\n(Ch. 4.3.3) Making Predictions\nOnce the coefficients have been estimated, we can compute the probability of default\\(=1\\) for any given credit card balance.\nFor example, using the coefficient estimates given in Table 4.1, we predict that the default probability for an individual with a balance-value of 1,000 [USD] is \\[\n\\begin{align*}\n\\hat{p}(\\texttt{balance})\n&=Pr(\\texttt{default}=\\texttt{Yes}|\\texttt{balance})\\\\\n&=\\frac{e^{\\hat\\beta_0+\\hat\\beta_1 \\texttt{balance}}}{1+e^{\\hat\\beta_0+\\hat\\beta_1 \\texttt{balance}}}\\\\\n&=\\frac{e^{-10.6513+ 0.0055\\times 1000}}{1+e^{-10.6513+ 0.0055\\times 1000}} = 0.00576 < 1\\%\n\\end{align*}\n\\]\nBy contrast, the default probability for an individual with a balance-value of 2,000 [USD] equals \\(0.586\\) (or \\(58,6\\%\\)) and is thus much higher.\n\nQualitative Predictors:\n\n\\[\n\\begin{align*}\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{student}=\\texttt{Yes})\n&=\\frac{e^{-3.5041+0.4049\\times 1}}{1+e^{-3.5041+0.4049\\times 1}}\\\\\n&= 0.0431\\\\\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{student}=\\texttt{No})\n&=\\frac{e^{-3.5041+0.4049\\times 0}}{1+e^{-3.5041+0.4049\\times 0}}\\\\\n&= 0.0292\\\\\n\\end{align*}\n\\]\nThis may indicate that students tend to have higher default probabilities than non-students. But we may missed further factors here since we only use one predictor variable.\n\n\n\n(Ch. 4.3.4) Multiple Logistic Regression\nBy analogy with the extension from simple to multiple linear, we can generalize the (simple) logistic regression model as following \\[\n\\begin{align*}\np(X)\n&=\\frac{e^{\\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p}}{1+e^{\\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p}}\\\\\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right)\n&= \\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p,\\\\\n\\end{align*}\n\\] where \\(X=(X_1,\\dots,X_p)\\), and where the unknown parameters \\(\\beta_0,\\dots,\\beta_p\\) are estimated by maximum likelihood. \nThe negative coefficient for student in the multiple logistic regression indicates that for a fixed value of balance and income, a student is less likely to default than a non-student.\n\nInterestingly, the effect of the dummy variable student[Yes] is now negative, in contrast to the estimation results of the (simple) logistic regression in Table 4.2 where it was positive.\nThe left-hand panel of Figure 4.3 provides a graphical illustration of this apparent paradox:\n\nWithout considering balance, the (overall) default rates of students are higher than those of non-students (horizontal broken lines). This overall effect was shown in Table 4.2.\nHowever, for given balance-values, the default rate for students is lower than for non-students (solid lines).\n\n\nThe right-hand panel of Figure 4.3 provides an explanation for this discrepancy: Students tend to hold higher levels of debt, which is in turn associated with higher probability of default.\nSummary:\n\nAn individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance\nHowever, overall, students tend to default at a higher rate than non-students since, overall, they tend to have higher credit card balances.\n\nIn other words: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, a student is less risky than a non-student if both have the same credit card balance.\n\n\n(Ch. 4.3.5) Multinomial Logistic Regression\nIt is possible to extend the two-class logistic regression approach to the setting of \\(K > 2\\) classes. This extension is sometimes known as multinomial logistic regression.\nTo do this, we first select a single class to serve as the baseline; without loss of generality, we select the \\(K\\)th class for this role.\nWe model the probabilities that \\(Y=k\\), for \\(k=1,\\dots,K\\), using \\(k\\)-specific parameters \\(\\beta_{k0},\\dots,\\beta_{kp}\\) with \\[\np_k(x)=Pr(Y=k|X=x)=\\frac{e^{\\beta_{k0} + \\beta_{k1} x_1 + \\dots +  \\beta_{kp} x_p}}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots +  \\beta_{lp} x_l}}\n\\] for \\(k=1,\\dots,K-1\\), and \\[\np_K(x)=Pr(Y=K|X=x)=\\frac{1}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots +  \\beta_{lp} x_l}}\n\\]\nFor \\(k=1,\\dots,K-1\\) it holds that \\[\n\\log\\left(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\\right)=\\beta_{k0} + \\beta_{k1} x_1 + \\dots +  \\beta_{kp} x_p\n\\] which is the counterpart to the log odds equation for \\(K=2.\\)\nNote that:\n\nThe predictions \\(\\hat{p}_k(x)\\), for \\(k=1,\\dots,K\\) do not depend on the choice of the baseline class.\nHowever, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline."
  },
  {
    "objectID": "Ch4_Classification.html#ch.-4.4-discriminant-analysis-generative-models-for-classification",
    "href": "Ch4_Classification.html#ch.-4.4-discriminant-analysis-generative-models-for-classification",
    "title": "4  Classification",
    "section": "(Ch. 4.4) Discriminant Analysis: Generative Models for Classification",
    "text": "(Ch. 4.4) Discriminant Analysis: Generative Models for Classification\nWe now consider an alternative and less direct approach to estimating the probabilities \\(Pr(Y=K|X=x).\\)\nSuppose that we wish to classify an observation into one of \\(K\\geq 2\\) classes.\nPrior probability: Let \\[\n\\pi_k=Pr(Y=k)\n\\] represent the overall prior probability that a randomly chosen observation comes from class \\(k.\\) We have that \\[\n\\pi_1+\\dots+\\pi_K=1.\n\\]\nDensity function of \\(X\\): Let \\[\nf_k(x)=Pr(X=x|Y=k)\n\\] denote the (conditional) density of \\(X\\) for an observation that comes from class \\(k.\\)1\nPosterior probability: Then Bayes’ theorem states that the probability that a observation with predictor values \\(X=x\\) comes from class \\(k\\) (i.e. the posterior probability), is given by \\[\np_k(x) = Pr(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_l f_l(x)}.\n\\tag{4.1}\\]\nWhile logistic regression aims at estimating the posterior probability \\(p_k(x)\\) directly, Bayes’s theorem gives us a way to estimate \\(p_k(x)\\) indirectly simply by plugging in estimates of \\(\\pi_k\\) and \\(f_k(x)\\).\nHowever, estimating the densities \\(f_k(x)\\) can be very challenging and we therefore have to make some simplifying assumptions; namely, assuming that the data is normal distributed.\nWe know from Chapter 2 that the Bayes classifier, which classifies an observation \\(x\\) to the class \\(k\\) for which \\(p_k(x)\\) is largest, has the lowest possible error rate out of all classifiers.\nIn the following sections, we discuss three classifiers that use different estimates of \\(f_k(x)\\) to approximate the Bayes classifier:\n\nlinear discriminant analysis\nquadratic discriminant analysis\nNaive Bayes\n\n\n(Ch. 4.4.1) Linear Discriminant Analysis for \\(p = 1\\)\nFor the beginning, let us assume that we have only one predictor, i.e., \\(p=1.\\)\nTo estimate \\(f_k(x)\\), we will assume that \\(f_k\\) is normal (or Gaussian). In the simple \\(p=1\\) dimensional setting, the normal distribution is \\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\left(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2\\right),\n\\] where\n\n\\(\\mu_k\\) is the mean of the \\(k\\)th class and\n\\(\\sigma_k^2\\) is the variance of the \\(k\\)th class\n\\(\\pi\\approx 3.14159\\) is the mathematical constant \\(\\pi\\). (Do not confuse it with the prior probabilities \\(\\pi_k.\\))\n\nFor now, let us further assume the simplifying case of equal variances across all classes, i.e.  \\[\n\\sigma_1^2=\\dots = \\sigma_K^2\\equiv \\sigma^2.\n\\] Plugging this assumed version of \\(f_k\\) into Equation 4.1, leads to \\[\np_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\right)}{\\sum_{l=1}^K\\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\right)}.\n\\tag{4.2}\\]\nThe Bayes classifier involves assigning an observation \\(X = x\\) to the class \\(k\\) for which \\(p_k(x)\\) is largest. Taking the \\(\\log\\) of Equation 4.2 (i.e. a monotonic transformation) and rearranging terms shows that this is equivalent to assigning an observation \\(X=x\\) to the class \\(k\\) for which \\[\n\\delta_k(x)=x\\cdot\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\] is largest. The function \\(\\delta_k(x)\\) is called the discriminant function.\nExample:\nIn the case of only two classes, \\(K=2\\), with equal a priori probabilities \\(\\pi_1=\\pi_2\\equiv \\pi^*\\), the Bayes classifier assigns an observation \\(X=x\\) to class \\(1\\) if \\[\n\\begin{align*}\n\\delta_1(x) & > \\delta_2(x)\\\\\n%%%\nx\\cdot\\frac{\\mu_1}{\\sigma^2} - \\frac{\\mu_1^2}{2\\sigma^2} + \\log(\\pi^*)\n& >\nx\\cdot\\frac{\\mu_2}{\\sigma^2} - \\frac{\\mu_2^2}{2\\sigma^2} + \\log(\\pi^*)\\\\\n%%%\nx\\cdot\\frac{\\mu_1}{\\sigma^2} - \\frac{\\mu_1^2}{2\\sigma^2}  \n& >\nx\\cdot\\frac{\\mu_2}{\\sigma^2} - \\frac{\\mu_2^2}{2\\sigma^2}\\\\\n%%%\nx\\cdot\\mu_1 - \\frac{\\mu_1^2}{2}  \n& >\nx\\cdot\\mu_2 - \\frac{\\mu_2^2}{2}\\\\\n%%%\n2x\\cdot(\\mu_1-\\mu_2)   \n& >\n\\mu_1^2 - \\mu_2^2\\\\\n%%%\nx   \n& > \\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1-\\mu_2)}\n\\end{align*}\n\\] The Bayes decision boundary is the point for which \\(\\delta_1(x)=\\delta_2(x)\\) \\[\n\\begin{align*}\nx   \n=\\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1-\\mu_2)}\n&=\\frac{(\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2)}{2(\\mu_1-\\mu_2)}\\\\\n&=\\frac{(\\mu_1 + \\mu_2)}{2}\n\\end{align*}\n\\] Figure 4.4 shows a specific example with\n\n\\(\\pi_1=\\pi_2\\)\n\\(\\mu_1=-1.25\\) and \\(\\mu_2=1.25\\) and\n\\(\\sigma_1=\\sigma_2\\equiv\\sigma =1.\\)\n\nThe two densities \\(f_1\\) and \\(f_2\\) overlap such that for given \\(X=x\\) there is some uncertainty about the class to which the observation belongs to. In this example, the Bayes classifier assigns an observation \\(X=x\\) …\n\n… to class 1 if \\(x<0\\)\n… to class 2 if \\(x>0\\)\n\n\nIn the above example (Figure 4.4) we know all parameters \\(\\pi_k\\), \\(\\mu_k\\), \\(k=1,\\dots,K\\), and \\(\\sigma\\). In practice, however, these parameters are usually unknown, and thus need to be estimated from the data.\nThe Linear Discriminant Analysis (LDA) method approximates the Bayes classifier by using the normality assumption and by plugging in estimates for the unknown parameters \\(\\pi_k\\), \\(\\mu_k\\), \\(k=1,\\dots,K\\), and \\(\\sigma\\). The following estimates are used: \\[\n\\begin{align*}\n\\hat\\pi_k    & = \\frac{n_k}{n}\\\\\n\\hat\\mu_k    & = \\frac{1}{n_k}\\sum_{i:y_i=k}x_i\\\\\n\\hat\\sigma   & = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat\\mu_k)^2\\\\\n\\end{align*}\n\\] The LDA classifier assigns an observation \\(X=x\\) to the class \\(k\\) for which \\[\n\\hat\\delta_k(x)=x\\cdot\\frac{\\hat\\mu_k}{\\hat\\sigma^2} - \\frac{\\hat\\mu_k^2}{2\\hat\\sigma^2} + \\log(\\hat\\pi_k)\n\\] is largest.\nLDA is named linear since the discriminant functions \\(\\hat\\delta_k(x)\\), \\(k=1,\\dots,K\\) are linear functions of \\(x.\\)\nAssumptions made by the LDA method:\n\nObservation for each class come from a normal distribution\nThe class-specific normal distributions have class-specific means, \\(\\mu_k\\), but equal variances \\(\\sigma\\)\n\n\n\n(Ch. 4.4.2) Linear Discriminant Analysis for \\(p > 1\\)\nIn the case of multiple predictors \\[\nX=(X_1,\\dots,X_p)\\quad \\text{with}\\quad p>1\n\\] we need to assume that the observations for each class \\(k\\) come from a multivariate Gaussian distribution with\n\nClass-specific \\((p\\times 1)\\) mean vectors \\(\\mu_k\\)\nEqual \\((p\\times p)\\) covariance matrix \\(\\Sigma\\) for all classes \\(k=1,\\dots,K\\)\n\nThat is, a \\((p\\times 1)\\) random vector \\(X\\) of class \\(k\\) is distributed as \\[\nX\\sim N\\left(\\mu_k,\\Sigma\\right)\n\\] with class-specific \\((p\\times 1)\\) mean vector \\(\\mu_k\\), \\[\nE(X)=\\mu_k,\n\\] and \\((p\\times p)\\) covariance matrix \\[\nCov(X)=\\Sigma\n\\] that is common for all classes \\(k=1,\\dots,K.\\)\nThe multivariate Gaussian density for class \\(k=1,\\dots,K\\) is defined as \\[\nf_k(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)\\right)\n\\]\nFigure 4.5 shows the case of bivariate (\\(p=2\\)) Gaussian distribution predictors.\n\nUnder the multivariate normality assumption (with common covariance matrix and group-specific mean vectors) the Bayes classifier assigns an multivariate observation \\(X=x\\) to the class for which the discriminant function \\[\n\\delta_k(x)=x^T \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log(\\pi_k)\n\\tag{4.3}\\] is largest.\nAn example for \\(p=2\\) and \\(K=3\\) with three equally sized (i.e. \\(\\pi_1=\\pi_2=\\pi_3\\)) Gaussian classes is shown in the left-hand panel of Figure 4.6. The three ellipses represent regions that contain \\(95\\%\\) of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which \\(\\delta_k(x) = \\delta_l(x)\\), \\(k\\neq l\\).\n\nAgain, the unknown parameters \\(\\mu_k\\), \\(\\pi_k\\), and \\(\\Sigma\\) must be estimated from the data with formulas similar to those for the \\(p=1\\) case.\n\nThe Default data example\nWe can perform LDA on the Default data in order to predict whether or not an individual will default on the basis of\n\ncredit card balance (real variable) and\nstudent status (qualitative variable).\n\nNote: Student status is qualitative, and thus the normality assumption made by LDA is clearly violated in this example! However, LDA is often remarkably robust to model violations, as this example shows.\nThe LDA model fit to the \\(10,000\\) training samples results in an overall training error rate of \\(2.75\\%:\\)\n\n# MASS package contains the lda() function\nsuppressPackageStartupMessages(library(MASS)) \n\n## Sample size \nn              <- nrow(Default)\n\nlda_obj        <- lda(default ~ balance + student, \n                      data = Default)\nldaPredict_obj <- predict(lda_obj)\n\n## Training error rate:\nsum(Default$default != ldaPredict_obj$class)/n \n\n[1] 0.0275\n\n\nThis sounds like a low error rate, but two caveats must be noted:\n\nTraining error rates are usually smaller than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a new set of individuals will default.\n\nThe reason is that we specifically adjust the parameters of our model to do well on the training data. (We may “overfit” the training data.)\nGenerally, the higher the ratio of parameters \\(p\\) to number of samples \\(n\\), the more we expect overfitting to play a role. However, for these data we don’t expect this to be a problem, since \\(p = 2\\) and \\(n = 10,000.\\)\n\nSince only \\(3.33\\%\\) of the individuals in the training sample defaulted, a simple naive classifier that always predicts “not default” for every person, regardless of his or her credit card balance and student status, will result in an error rate of \\(3.33\\%.\\) In other words, the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate.\n\n\n## Training error rate of the \"null classifier\":\nsum(Default$default != rep(\"No\", n))/n \n\n[1] 0.0333\n\n\n\n\n\nClassification Errors of Binary Classifiers\nAny binary classifier can make two types of errors:\n\nFalse Negatives (FN): Predicting “no default” for a person who actually defaults\nFalse Positives (FP): Predicing “default” for a person who actually does not default\n\nDepending on the context, both error can have different consequential damages.\nA confusion matrix shows both types of errors. The upper off-diagonal entry shows the FNs; the lower off-diagonal entry shows the FPs. A perfect confusion matrix has zeros in the off-diagonal entries.\n\n## Confusion matrix\ntable(ldaPredict_obj$class, Default$default, \n      dnn = c(\"Predicted default status\", \n              \"True default status\"))\n\n                        True default status\nPredicted default status   No  Yes\n                     No  9644  252\n                     Yes   23   81\n\n\n\nTrue/False Positive Predictions:\n\nLDA predicts that P\\(^*=104\\) people default (Predicted Positives, P\\(^*\\)).\n\n\\(81\\) of these actually default (True Positives, TP).\n\\(23\\) of these actually do not default (False Positives, FP).\n\n\nTrue/False Negative Predictions:\n\nLDA predicts that N\\(^*=9896\\) people do not default (Predicted Negatives, N\\(^*\\)).\n\n\\(9644\\) of these actually do not default (True Negatives, TN)\n\\(252\\) of these actually default (FALSE Negatives, FN)\n\n\nTrue Positive/Negative Rates:\n\nTrue Positive Rate (TP/P): \\((81/333)\\cdot 100\\%\\approx 24.3\\%\\)  \nTrue Negative Rate (TN/N): \\((9644/9667)\\cdot 100\\%\\approx 99.8\\%\\) \n\nFalse Positive/Negative Rates:\n\nFalse Positive Rate (FP/N): \\((23/9667)\\cdot 100\\%\\approx 0.2\\%\\)\n \nFalse Negative Rate (FN/P): \\((252/333)\\cdot 100\\%\\approx 75.7\\%\\)\n\n\nThe small False Positive Rate of \\(0.2\\%\\) is good since this means that trustworthy “no default” people are rarely labeled as non-trustworthy “default” people.\nHowever, the high False Negative Rate of \\(75.7\\%\\) is problematic for a bank since this means that the bank often does not detect the non-trustworthy “default” people.\n\nChoosing the Classification Threshold\nIf a “False Negative Event” costs the bank more than a “False Positive Event” (or vice versa), then the bank will want to adjust the classifying threshold of the classifier.\n\nThe Bayes classifier, and thus also all classifiers which try to approximate the Bayes classifier, use a threshold of \\(50\\%\\). I.e., a person \\(i\\) with predictor value \\(X=x_i\\) is assigned to the default\\(=\\)Yes class if \\[\nPr(\\texttt{default}_i=\\texttt{Yes}|X=x_i)>0.5.\n\\]\nHowever, if the costs of False Negatives (FN) are higher than the costs of False Positives (FP), one can consider lowering the classifying threshold. This decreases the predicted negatives (N\\(^*\\)) and thus also decreases the False Negatives (FN) since N\\(^*=\\)TN\\(+\\)FN.\nFor instance, we might classify any person \\(i\\) with a posterior probability of default above \\(20\\%,\\) \\[\nPr(\\texttt{default}=\\texttt{Yes}|X=x_i)>0.2,\n\\] into the default\\(=\\)Yes class.\n\n## Predicted posterior probabilities \n## head(ldaPredict_obj$posterior)\n\n## Container for the predicted classes\nPredictedDefault <- rep(\"No\", n)\n## Fill in the \"Yes\" classifications \nPredictedDefault[ldaPredict_obj$posterior[,2] > 0.2] <- \"Yes\"\n\n## Confusion matrix for 0.2 threshold:\ntable(PredictedDefault, Default$default, \n      dnn = c(\"Predicted default status\", \n              \"True default status\"))\n\n                        True default status\nPredicted default status   No  Yes\n                     No  9432  138\n                     Yes  235  195\n\n\n\nChanging the threshold from \\(0.5\\) to \\(0.2\\) …\n\n… lowered the False Negative Rate (FN/P) 😎\n\nfrom \\(252/333\\approx 75.7\\%\\)\nto \\(138/333\\approx 41.4\\%\\)\n\n\n… increased the False Positive Rate (FP/N) 😕\n\nfrom \\(23/9667\\approx 0.2\\%\\)\nto \\(235/9667\\approx 2.4\\%\\)\n\n… increased the Overall Error Rate ((FN+FP)/n) 😕\n\nfrom \\((23 + 252)/10000\\approx 2.75\\%\\)\nto \\((235 + 138)/10000\\approx 3.73\\%\\)\n\n\nFor a credit card company, this slight increase in the overall error rate can be a small price to pay for a more accurate identification of people who indeed default.\n\n\n\nClassification Performance Measures\nTable 4.6 and 4.7 summarize the different classification performance measures.\n\n\n\n\nThe ROC curve\nThe (Receiver Operating Characteristics) ROC curve is a popular graphic for displaying the \\[\n\\text{TP Rate}=\\text{TP/P}\\quad \\text{(``sensitivity'' or ``power'')}\n\\] against the \\[\n\\text{FP Rate}=\\text{FP/N} \\quad \\text{(``}1-\\text{specificity'' or ``type I error'')}\n\\] for all possible threshold values (from zero to one).\n\n## Considered thresholds (exclude the trivial 0/1 values)\nthresholds       <- seq(from = 0, to = 1, length.out = 250)\nn_thresholds     <- length(thresholds)\n\n## Container for the predicted classes\nPredictedDefault_mat <- matrix(\"No\", n, n_thresholds)\n\n## Fill in the \"Yes\" classifications \nfor(j in 1:n_thresholds){\n  PredicedYes  <- ldaPredict_obj$posterior[,2] > thresholds[j]\n  PredictedDefault_mat[PredicedYes, j] <- \"Yes\"\n}\n\n## Number of actual positives\nP <- length(Default$default[Default$default == \"Yes\"])\n## Number of actual negatives\nN <- length(Default$default[Default$default == \"No\"])\n\nTP_Rate <- numeric(n_thresholds)\nFP_Rate <- numeric(n_thresholds)\n\nfor(j in 1:n_thresholds){\n  ## Classification results among the actually positives\n  Classifications_among_trueYes <- PredictedDefault_mat[,j][Default$default == \"Yes\"]\n  ## Classification results among the actually negatives\n  Classifications_among_trueNo <- PredictedDefault_mat[,j][Default$default == \"No\"]\n  ## Number of True Positives\n  TP <- length(Classifications_among_trueYes[Classifications_among_trueYes == \"Yes\"])\n  ## Number of False Positives\n  FP <- length(Classifications_among_trueNo[Classifications_among_trueNo == \"Yes\"])\n\n  ## TP error rate:\n  TP_Rate[j] <- TP/P\n  ## FP error rate:\n  FP_Rate[j] <- FP/N \n}\n\n## Layout for the plotting region (two plots side-by-side)\nlayout(matrix(1:2, ncol=2), width = c(2,1),height = c(1,1))\n\ncol_range <- hcl.colors(n       = n_thresholds, \n                       palette = \"viridis\")\n\n## Plot the ROC curve \nplot(x    = FP_Rate, \n     y    = TP_Rate, \n     col  = col_range, \n     xlab = \"False Positive Rate\",\n     ylab = \"True Positive Rate\",\n     main = \"ROC Curve\", \n     type = \"o\", lwd=1.5, pch = 19)\nabline(a = 0, b = 1, lty = 3, col = \"gray\")\n\n## Color-Legend for threshold values\nlegend_image <- as.raster(matrix(rev(col_range), ncol=1))\nplot(c(0,2),c(0,1), type = \"n\", axes = FALSE, xlab = \"\", ylab = \"\", main = 'Threshold-Value')\ntext(x      = 1.5, \n     y      = seq(from = 0, to = 1, l = 5), \n     labels = seq(from = 0, to = 1, l=5))\nrasterImage(legend_image, xleft = 0, ybottom = 0, xright = 1, ytop = 1)\n\n\n\n\nFigure 4.1: The ROC curve traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are often not shown. Here, however, we show them using a color code from yellow [threshold = 1] to purple [threshold = 0]. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default.\n\n\n\n\n\nThe commonly reported summary statistic for the ROC curve is the “Area Under the (ROC) Curve” AUC\n\nIdeal AUC-value is AUC\\(=1.\\)\nA “no information” classifier has AUC\\(=0.5.\\)\n\nThe R package ROCR contains function for computing and plotting the ROC curve and the AUC value.\n\n## install.packages(\"ROCR\")\nlibrary(\"ROCR\")\n\n## Caution: Choose the posterior probability column carefully, \n## it may be lda.pred$posterior[,1] or lda.pred$posterior[,2], \n## depending on your factor levels\n\n## Predicted and actual classes:\npredict <- prediction(ldaPredict_obj$posterior[,2], Default$default) \n## compute TP-Ratios and FP-Ratios:\nperform <- performance(predict, \"tpr\", \"fpr\")\n## ROC curve\nplot(perform, colorize = TRUE)\n\n\n\n## AUC\nAUC     <- performance(predict, measure = \"auc\")\nprint(AUC@y.values)\n\n[[1]]\n[1] 0.9495584\n\n\n\n\n(Ch. 4.4.3) Quadratic Discriminant Analysis (QDA)\nUnlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form \\[\nX|Y=k \\sim N(\\mu_k,\\Sigma_k),\n\\] where \\(\\Sigma_k\\) is a \\((p\\times p)\\) covariance matrix for the \\(k\\)th class.\nUnder this assumption, the Bayes classifier assigns an observation \\(X=x\\) to the class for which \\[\n\\begin{align*}\n&\\delta_k(x)=\\\\\n&= - \\frac{1}{2}\\left(x-\\mu_k\\right)^T\\Sigma_k^{-1}\\left(x-\\mu_k\\right)\n   - \\frac{1}{2}\\log\\left(|\\Sigma_k|\\right)\n   + \\log(\\pi_k)\\\\\n&= - \\frac{1}{2}x^T    \\Sigma_k^{-1}x\n   + \\frac{1}{2}x^T    \\Sigma_k^{-1}\\mu_k\n   - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k\n   - \\frac{1}{2}\\log\\left(|\\Sigma_k|\\right)\n   + \\log(\\pi_k)\n\\end{align*}\n\\] is largest.\nThis classifier is called quadratic discriminant analysis since the quantity \\(x\\) appears as a quadratic function.\nQDA has much more parameters than LDA and is thus a much more flexible classifier than LDA:\n\nLDA requires\n\n\\(K\\) a-priori probability estimates\n\\(K\\cdot p\\) mean estimations\n\\((p+1)/2\\) covariance estimates\n\nQDA requires\n\n\\(K\\) a-priori probability estimates\n\\(K\\cdot p\\) mean estimations\n\\({\\color{red}K}\\cdot(p+1)/2\\) covariance estimates\n\n\nThe difference in flexibility needs to be considered under the bias variance trade-off:\n\nLow flexibility generally means low variance, but large bias\nHigh flexibility generally means high variance, but low bias\n\nRoughly:\n\nLDA tends to be better than QDA if there are relatively few training observations (small \\(n_{Train}\\)) and so reducing variance is crucial.\nQDA is recommended\n\nif the training set \\(n_{Train}\\) is large, so that the variance of the classifier is not a major concern, or\nif the assumption of a common covariance matrix for the \\(K\\) classes is clearly unrealistic.\n\n\n\n\n\n(Ch. 4.4.4) Naive Bayes\nRemember, that Bayes’ theorem states that the probability that a observation with predictor values \\(X=x\\) comes from class \\(k\\) (i.e. the posterior probability), is given by \\[\np_k(x) = Pr(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_l f_l(x)}.\n\\]\nWe have seen above, that estimating the prior probabilities \\(\\pi_1,\\dots,\\pi_K\\) is typically straight forward using the relative frequencies \\[\n\\hat\\pi_k=\\frac{n_k}{n},\\quad k=1,\\dots,K.\n\\] However, estimating the \\(p\\) dimensional density functions \\[\nf_1(x),\\dots,f_K(x)\n\\] was and is substantially more difficult.\nTo make estimation tractable and useful, LDA and QDA assume parametric families for these density functions; namely, multivariate normals with and without common covariances.\nBy contrast, Naive Bayes only makes the assumption that the \\(p\\) dimensional components of the predictor are independent from each other. I.e. that the joint density \\(f_k(x)\\) can be written as the product of the \\(p\\) marginal densities  \\[\nf_k(x) = f_{k1}(x_1)\\times f_{k2}(x_2)\\times \\dots \\times f_{kp}(x_p)\n\\]\nThis simplifying assumption may not be completely correct, but it often leads to very good classification results - particularly, in classification problems with smallish sample sizes.\nUnder the naive Bayes assumption, we have that \\[\nPr(Y=k|X=x)=\\frac{\\pi_k f_{k1}(x_1)\\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K\\pi_l f_{l1}(x_1)\\times \\dots \\times f_{lp}(x_p)}.\n\\]\nFor estimating the one dimensional marginal density functions \\[\nf_{kj}(x),\\quad j=1,\\dots,p\\quad\\text{and}\\quad k=1,\\dots,K\n\\] we can use …\n\n(univariate) normal distribution assumptions, i.e. \\(X_j|Y=k \\sim N(\\mu_{jk},\\sigma_{jk})\\)\nnon-parametric kernel density estimation (for quantitative predictors)\nsimple relative frequencies (for qualitative predictors)"
  },
  {
    "objectID": "Ch4_Classification.html#r-lab-classification",
    "href": "Ch4_Classification.html#r-lab-classification",
    "title": "4  Classification",
    "section": "4.2 R-Lab: Classification",
    "text": "4.2 R-Lab: Classification\n\n4.2.1 The Stock Market Data\nWe will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over \\(1,250\\) days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days: Lag1, Lag2, … Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date). Our goal is to predict Direction (a qualitative response) using the other predictors/features.\n\nlibrary(ISLR2)   # package contains the data\nattach(Smarket)  # attach(Smarket) allows to use the variables \n                 # contained Smarket directly \n                 # (i.e. 'Direction' instead of 'Smarket$Direction')\nnames(Smarket)   # names of the variables in the Smarket data\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)     # total sample size n, number of variables\n\n[1] 1250    9\n\nsummary(Smarket) # descriptive statistics (mean, median, ...)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\npairs(Smarket)   # pairwise scatter plots\n\n\n\n\nThe cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the Direction variable is qualitative (Up and Down).\n\ncor(Smarket)\n\nError in cor(Smarket): 'x' must be numeric\n\nround(cor(Smarket[, names(Smarket) != \"Direction\"]), 2)\n\n       Year  Lag1  Lag2  Lag3  Lag4  Lag5 Volume Today\nYear   1.00  0.03  0.03  0.03  0.04  0.03   0.54  0.03\nLag1   0.03  1.00 -0.03 -0.01  0.00 -0.01   0.04 -0.03\nLag2   0.03 -0.03  1.00 -0.03 -0.01  0.00  -0.04 -0.01\nLag3   0.03 -0.01 -0.03  1.00 -0.02 -0.02  -0.04  0.00\nLag4   0.04  0.00 -0.01 -0.02  1.00 -0.03  -0.05 -0.01\nLag5   0.03 -0.01  0.00 -0.02 -0.03  1.00  -0.02 -0.03\nVolume 0.54  0.04 -0.04 -0.04 -0.05 -0.02   1.00  0.01\nToday  0.03 -0.03 -0.01  0.00 -0.01 -0.03   0.01  1.00\n\n\nAs one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and Volume.\n\n\n\n4.2.2 Logistic Regression\nNext, we will fit a logistic regression model in order to predict Direction using Lag1 through Lag5 and Volume. The glm() function can be used to fit many types of generalized linear models , including logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\n\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.446  -1.203   1.065   1.145   1.326  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe smallest \\(p\\)-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of \\(0.145\\), the \\(p\\)-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction.\nWe use the coef() function in order to access just the coefficients for this fitted model. We can also use the summary() function to access particular aspects of the fitted model, such as the \\(p\\)-values for the coefficients.\n\ncoef(glm.fits)\n\n (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 \n-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 \n      Volume \n 0.135440659 \n\nsummary(glm.fits)$coef\n\n                Estimate Std. Error    z value  Pr(>|z|)\n(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\nLag1        -0.073073746 0.05016739 -1.4565986 0.1452272\nLag2        -0.042301344 0.05008605 -0.8445733 0.3983491\nLag3         0.011085108 0.04993854  0.2219750 0.8243333\nLag4         0.009358938 0.04997413  0.1872757 0.8514445\nLag5         0.010313068 0.04951146  0.2082966 0.8349974\nVolume       0.135440659 0.15835970  0.8552723 0.3924004\n\nsummary(glm.fits)$coef[, 4]\n\n(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 \n  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 \n     Volume \n  0.3924004 \n\n\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form \\(P(Y=1|X)\\), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities.\n\n## Posterior probabilities\nglm.probs <- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\n\nWe know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\n\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than \\(0.5\\).\n\nglm.pred                 <- rep(\"Down\", 1250) # sample size: 1250\nglm.pred[glm.probs > .5] <- \"Up\"\n\nThe first command creates a vector of 1,250 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds \\(0.5\\). Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified. \n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on \\(507\\) days and that it would go down on \\(145\\) days, for a total of \\(507+145 = 652\\) correct predictions. The overall training error rate is\n\n## Overall training error rate \n(507 + 145) / 1250\n\n[1] 0.5216\n\n\nAlternatively, the mean() function can be used to compute the overall training error rate; i.e., the fraction of days for which the prediction was correct (within the training data). In this case, logistic regression correctly predicted the movement of the market \\(52.2\\%\\) of the time.\n\n## Alternative way to compute the \n## overall training error rate \nmean(glm.pred == Direction)\n\n[1] 0.5216\n\n\nAt first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of \\(1,250\\) observations. In other words, \\(100\\%-52.2\\%=47.8\\%\\), is the training error rate.\nThe training error rate is often overly optimistic. It tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data.\nThis will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.\nTo implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.\n\ntrain          <- (Year < 2005)\n## Test data \nSmarket_2005   <- Smarket[!train, ] \n## Test sample size \ndim(Smarket_2005)[1] \n\n[1] 252\n\n## Dependent variable of the test data \nDirection_2005 <- Direction[!train]\n\nThe object train is a vector of \\(1250\\) elements, corresponding to the observations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to TRUE, whereas those that correspond to observations in 2005 are set to FALSE.\nThe object train is a Boolean vector, since its elements are TRUE and FALSE. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command Smarket[train, ] would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of train are TRUE.\nThe ! symbol can be used to reverse all of the elements of a Boolean vector. That is, !train is a vector similar to train, except that the elements that are TRUE in train get swapped to FALSE in !train, and the elements that are FALSE in train get swapped to TRUE in !train. Therefore, Smarket[!train, ] yields a submatrix of the stock market data containing only the observations for which train is FALSE—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.\n\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\n## Using the trained model to predict the held out test data  \nglm.probs <- predict(glm.fits, Smarket_2005,\n    type = \"response\")\n\nNotice that we have trained and tested our model on two completely separate data sets:\n\ntraining (estimation) was performed using only the dates before 2005\ntesting was performed using only the dates in 2005.\n\nFinally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\nglm.pred                 <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\n## Test data confusion matrix \ntable(glm.pred, Direction_2005)\n\n        Direction_2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n## Rate of correct predictions in the test data\nmean(glm.pred == Direction_2005)\n\n[1] 0.4801587\n\n## Rate of prediction errors in the test data \nmean(glm.pred != Direction_2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is \\(52\\%\\), which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)\nWe recall that the logistic regression model had very underwhelming \\(p\\)-values associated with all of the predictors, and that the smallest \\(p\\)-value, though not very small, corresponded to Lag1. Perhaps by removing the variables that appear not to be helpful in predicting Direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. Below we have refit the logistic regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model.\n\n## Train model using the training data\nglm.fits  <- glm(Direction ~ Lag1 + Lag2, \n                 family = binomial, \n                 data   = Smarket, \n                 subset = train)\n## Predict the test data                  \nglm.probs <- predict(glm.fits, Smarket_2005, type = \"response\")\n##\nglm.pred                 <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\n## Test data confusion matrix\ntable(glm.pred, Direction_2005)\n\n        Direction_2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\n## Test data rate of correct predictions\nmean(glm.pred == Direction_2005)\n\n[1] 0.5595238\n\n## Test data positive predictive value TP/P*\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nNow the results appear to be a little better: \\(56\\%\\) of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct \\(56\\%\\) of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a \\(58\\%\\) accuracy rate (Positive predictive value TP/P*).\nThis suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction on a day when Lag1 and Lag2 equal \\(1.2\\) and \\(1.1\\), respectively, and on a day when they equal \\(1.5\\) and \\(-0.8.\\) We do this using the predict() function.\n\npredict(glm.fits,\n        newdata = data.frame(Lag1 = c(1.2,  1.5),  \n                         Lag2 = c(1.1, -0.8)),\n        type    = \"response\")\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\n4.2.3 Linear Discriminant Analysis\nNow we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. We fit the model using only the observations before 2005.\n\nlibrary(MASS)\nlda.fit <- lda(Direction ~ Lag1 + Lag2, \n               data   = Smarket,\n               subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit)\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\%\\) of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The “coefficients of linear discriminants” output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in Equation 4.3. If \\(-0.642\\times\\)Lag1\\(- 0.514 \\times\\) Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe plot() function produces plots of the linear discriminants, obtained by computing \\(-0.642\\times\\)Lag1\\(- 0.514 \\times\\) Lag2 for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns a list with three elements. The first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15). Finally, x contains the linear discriminants, described earlier.\n\nlda.pred <- predict(lda.fit, Smarket_2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nThe LDA and logistic regression predictions are almost identical. This observation holds true also general since LDA and logistic regression are effectively very similar classifiers; see Section 4.5 in our textbook ISLR2.\n\nlda.class <- lda.pred$class\ntable(lda.class, Direction_2005)\n\n         Direction_2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction_2005)\n\n[1] 0.5595238\n\n\nApplying a \\(50\\%\\) threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.\n\nsum(lda.pred$posterior[, 1] >= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] < .5)\n\n[1] 182\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will decrease:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\%\\) in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability for a decrease is at least \\(90\\%.\\)\n\nsum(lda.pred$posterior[, 1] > .9)\n\n[1] 0\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\%.\\)\n\n\n4.2.4 Quadratic Discriminant Analysis\nWe will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. The syntax is identical to that of lda().\n\nqda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\nqda.class <- predict(qda.fit, Smarket_2005)$class\ntable(qda.class, Direction_2005)\n\n         Direction_2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction_2005)\n\n[1] 0.5992063\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\%\\) of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!\n\n\n4.2.5 Naive Bayes\nNext, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a univariate Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\n\n## install.packages(\"e1071\") \nlibrary(\"e1071\")\nnb_fit <- naiveBayes(Direction ~ Lag1 + Lag2, \n                     data   = Smarket,\n                     subset = train)\nnb_fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for Lag1 is \\(0.0428\\) for Direction=Down, and the standard deviation is \\(1.23\\). We can easily verify this:\n\nmean(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nThe predict() function is straightforward to use:\n\nnb.class <- predict(nb_fit, Smarket_2005)\n## Test data confusion matrix\ntable(nb.class, Direction_2005)\n\n        Direction_2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\n## Test data overall rate of correct classifications\nmean(nb.class == Direction_2005)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class.\n\nnb.preds <- predict(nb_fit, Smarket_2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110\n\n\n\n\n4.2.6 \\(K\\)-Nearest Neighbors\nWe will now perform KNN using the knn() function, which is part of the class library. This function works rather differently from the other model-fitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, knn() forms predictions using a single command. The function requires four inputs.\n\nA matrix containing the predictors associated with the training data, labeled train.X below.\nA matrix containing the predictors associated with the data for which we wish to make predictions, labeled test.X below.\nA vector containing the class labels for the training observations, labeled train.Direction below.\nA value for \\(K\\), the number of nearest neighbors to be used by the classifier.\n\nWe use the cbind() function, short for column bind, to bind the Lag1 and Lag2 variables together into two matrices, one for the training set and the other for the test set.\n\n## install.packages(\"class\")\nlibrary(\"class\")\ntrain.X         <- cbind(Lag1, Lag2)[train, ]\ntest.X          <- cbind(Lag1, Lag2)[!train, ]\ntrain.Direction <- Direction[train]\n\nNow the knn() function can be used to predict the market’s movement for the dates in 2005. We set a random seed before we apply knn() because if several observations are tied as nearest neighbors, then R will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.\n\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Direction, k = 1)\n## Test data confusion matrix\ntable(knn.pred, Direction_2005)\n\n        Direction_2005\nknn.pred Down Up\n    Down   43 58\n    Up     68 83\n\n## Test data overall rate of correct classifications\nmean(knn.pred == Direction_2005)\n\n[1] 0.5\n\n\nThe results using \\(K=1\\) are not very good, since only \\(50\\%\\) of the observations are correctly predicted. Of course, it may be that \\(K=1\\) results in an overly flexible fit to the data. Below, we repeat the analysis using \\(K=3\\).\n\nknn.pred <- knn(train.X, test.X, train.Direction, k = 3)\n## Test data confusion matrix\ntable(knn.pred, Direction_2005)\n\n        Direction_2005\nknn.pred Down Up\n    Down   48 54\n    Up     63 87\n\n## Test data overall rate of correct classifications\nmean(knn.pred == Direction_2005)\n\n[1] 0.5357143\n\n\nThe results have improved slightly. But increasing \\(K\\) further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.\nKNN does not perform well on the Smarket data, but generally, KNN does often provide impressive results. As an example we will apply the KNN approach to the Insurance data set, which is part of the ISLR2 library. This data set includes \\(85\\) predictors that measure demographic characteristics for \\(5822\\) individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only \\(6\\%\\) of people purchased caravan insurance.\n\ndim(Caravan)     # sample size and number of variables in the Caravan dataset \n\n[1] 5822   86\n\nattach(Caravan)  # makes the variables in Caravan (e.g. Purchase) directly usable\n\nsummary(Purchase)\n\n  No  Yes \n5474  348 \n\n## Empirical prior probability of 'Purchase = Yes' \n348 / 5822\n\n[1] 0.05977327\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of \\(1000\\) in salary is enormous compared to a difference of \\(50\\) years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of \\(1000\\) is quite small compared to an age difference of \\(50\\) years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column \\(86\\), because that is the qualitative Purchase variable.\n\nstandardized.X <- scale(Caravan[, -86])\nvar(Caravan[, 1])\n\n[1] 165.0378\n\nvar(Caravan[, 2])\n\n[1] 0.1647078\n\nvar(standardized.X[, 1]) \n\n[1] 1\n\nvar(standardized.X[, 2])\n\n[1] 1\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first \\(1000\\) observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using \\(K=1\\), and evaluate its performance on the test data.\n\ntest     <- 1:1000\n## Training data \ntrain.X  <- standardized.X[-test, ]\ntrain.Y  <- Purchase[-test]\n## Testing data \ntest.X   <- standardized.X[test, ]\ntest.Y   <- Purchase[test]\n\nset.seed(1)\n## KNN (K=1)\nknn.pred <- knn(train.X, test.X, train.Y, k = 1)\n## Test data overall classification error \nmean(test.Y != knn.pred)\n\n[1] 0.118\n\n## Test data overall classification error \n## of the no information classifier (\"always No\")\nmean(test.Y != \"No\")\n\n[1] 0.059\n\n\nThe vector test is numeric, with values from \\(1\\) through \\(1000\\). Typing standardized.X[test, ] yields the submatrix of the data containing the observations whose indices range from \\(1\\) to \\(1000\\), whereas typing standardized.X[-test, ] yields the submatrix containing the observations whose indices do not range from \\(1\\) to \\(1,000\\).\nThe KNN overall test error rate on the \\(1000\\) test observations is just under \\(12\\%.\\) At first glance, this may appear to be fairly good. However, since only \\(6\\%\\) of customers purchased insurance, we could get the error rate down to \\(6\\%\\) by always predicting No regardless of the values of the predictors!\nSuppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, perhaps a salesperson must visit each potential customer.\nIf the company tries to sell insurance to a random selection of customers, then the success rate will be only \\(6\\%\\) (the empirical prior probability of Purchase = Yes), which may be far too low given the costs involved. Instead, the company would like to try to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest. Instead, the fraction of individuals that are correctly predicted to buy insurance is of interest.\nIt turns out that KNN with \\(K=1\\) does far better than random guessing among the customers that are predicted to buy insurance. Among the P* \\(=77\\) Purchase=Yes predictions, \\(9\\) (TP), or \\(11.7\\%\\) (TP/P*), actually do purchase the insurance. This is double the rate that one would obtain from random guessing.\n\n## Test data confusion matrix (KNN, K=1)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n## Test data positive predictive value TP/P*\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nUsing \\(K=3\\), the success rate increases to \\(19\\%,\\) and with \\(K=5\\) the rate is \\(26.7\\%.\\) This is over four times the rate that results from random guessing. It appears that KNN is finding some real patterns in a difficult data set!\n\n## K = 3\nknn.pred <- knn(train.X, test.X, train.Y, k = 3)\n\n## Test data confusion matrix (KNN, K=3)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  920  54\n     Yes  21   5\n\n## Test data positive predictive value TP/P*\n5 / 26\n\n[1] 0.1923077\n\n## K = 5\nknn.pred <- knn(train.X, test.X, train.Y, k = 5)\n\n## Test data confusion matrix (KNN, K=5)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  930  55\n     Yes  11   4\n\n## Test data positive predictive value TP/P*\n4 / 15\n\n[1] 0.2666667\n\n\nHowever, while this strategy is cost-effective, it is worth noting that only 15 customers are predicted to purchase insurance using KNN with \\(K=5\\). In practice, the insurance company may wish to expend resources on convincing more than just 15 potential customers to buy insurance.\nAs a comparison, we can also fit a logistic regression model to the data. If we use \\(0.5\\) as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of \\(0.5\\). If we instead predict a purchase any time the predicted probability of purchase exceeds \\(0.25\\), we get much better results: we predict that 33 people will purchase insurance, and we are correct for about \\(33\\%\\) of these people. This is over five times better than random guessing!\n\nglm.fits  <- glm(Purchase ~ ., data = Caravan,\n                 family = binomial, \n                 subset = -test)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm.probs <- predict(glm.fits, Caravan[test, ], type = \"response\")\n\n## Classifications (Bayes threshold)\nglm.pred                 <- rep(\"No\", 1000)\nglm.pred[glm.probs > .5] <- \"Yes\"\n\n## confusion matrix (test set)\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  934  59\n     Yes   7   0\n\n## Classifications (adjusted threshold)\nglm.pred                  <- rep(\"No\", 1000)\nglm.pred[glm.probs > .25] <- \"Yes\"\n\n## confusion matrix (test set)\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  919  48\n     Yes  22  11\n\n11 / (22 + 11)\n\n[1] 0.3333333"
  },
  {
    "objectID": "Ch4_Classification.html#exercises",
    "href": "Ch4_Classification.html#exercises",
    "title": "4  Classification",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\nPrepare the following exercises of Chapter 4 in our course textbook ISLR:\n\nExercise 1\nExercise 6\nExercise 13\nExercise 15\nExercise 16"
  },
  {
    "objectID": "Ch4_Classification.html#solutions",
    "href": "Ch4_Classification.html#solutions",
    "title": "4  Classification",
    "section": "4.4 Solutions",
    "text": "4.4 Solutions\n\nExercise 1\nUsing a little bit of algebra, use (4.2) to achieve (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\nAnswer:\nEquations (4.2) and (4.3) are the following:\n\\[\n(4.2) \\quad p(X) = \\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}\\qquad\\qquad\n(4.3)  \\quad  \\frac {p(X)} {1 - p(X)} =e^{\\beta_0 + \\beta_1 X}\n\\]\nDerivations:\n\\[\n\\begin{align*}\n\\frac {p(X)} {1 - p(X)}\n&= \\frac {\\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}{1 - \\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}\\\\[2ex]\n&= \\frac {\\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}{\\frac {1 + e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}} - \\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}\\\\[2ex]  \n&= \\frac {\\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}{\\frac {1} {1 + e^{\\beta_0 + \\beta_1 X}}}\n= e^{\\beta_0 + \\beta_1 X}\n\\end{align*}\n\\]\n\n\nExercise 6\nSuppose we collect data for a group of students in a statistics class with variables\n\n\\(X_1=\\)hours studied,\n\\(X_2 =\\) undergrad GPA (GPA: Grade Point Average), and\n\\(Y =\\) receive an A.\n\nWe fit a logistic regression and produce estimated coefficients:\n\\[\\hat{\\beta}_0 = -6, \\quad \\hat{\\beta}_1 = 0.05, \\quad \\hat{\\beta}_2 = 1.\\]\n6 a) Estimate the probability that a student who studies for \\(40\\) h and has an undergrad GPA of \\(3.5\\) gets an A in the class.\nAnswer:\nRemember from the previous exercise that:\n\\[\np(X) = \\frac {\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}\n             {1 + \\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}\n\\\\\n\\]\nThus, the probability of \\(Y=1\\) given \\(X=[x_1,x_2]\\) with \\(x_1 =40\\) (hours) and \\(x_2=3.5\\) (GPA) yields:\n\\[\np(X) =  \\frac {\\exp(-6 + 0.05\\cdot 40 + 3.5)} {1 + \\exp(-6 + 0.05\\cdot 40 + 3.5)} = \\frac {\\exp(-0.5)} {1 + \\exp(-0.5)} = 37.75\\%\n\\]\nCalculations in R: ::: {.cell}\n( exp(-0.5) )/( 1 + exp(-0.5) )\n\n[1] 0.3775407\n\n:::\n6 b) How many hours would the student in part (a) need to study to have a \\(50\\%\\) chance of getting an A in the class?\nAnswer:\nFinding \\(x_1\\), where \\(X = [x_1, 3.5]\\), such that \\(p(X) = 0.5\\) yields:\n\\[\n\\begin{align*}\n0.50 &= \\frac {\\exp(-6 + 0.05 x_1 + 3.5)} {1 + \\exp(-6 + 0.05 x_1 + 3.5)} \\\\\n\\Leftrightarrow 0.50 (1 + \\exp(-2.5 + 0.05\\,x_1)) &= \\exp(-2.5 + 0.05\\,x_1)\\\\\n\\Leftrightarrow 0.50 + 0.50 \\exp(-2.5 + 0.05\\,x_1)) &= \\exp(-2.5 + 0.05\\,x_1)\\\\\n\\Leftrightarrow 0.50 &= 0.50 \\exp(-2.5 + 0.05\\,x_1)\\\\\n\\Leftrightarrow \\log(1) &= -2.5 + 0.05\\,x_1 \\\\\n\\Leftrightarrow x_1 &= 2.5 / 0.05 = 50\n\\end{align*}\n\\]\nThus, on average, a student with an undergrad GPA of \\(3.5\\) needs to study \\(50\\) hours to have a \\(50\\%\\) chance of getting an A.\n\n\nExercise 13\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains \\(1,089\\) weekly returns for \\(21\\) years, from the beginning of \\(1990\\) to the end of \\(2010\\).\n13 a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\nAnswer:\n\n# You may need to install first the following R-packages:\n# install.packages(\"ISLR2\")\n# install.packages(\"MASS\")\n# install.packages(\"class\")\n# install.packages(\"e1071\") # naiveBayes()\n\n# Load the packages you need\nlibrary(\"ISLR2\")\n\n# Eliminates the need of referring to a variable \n# like 'Weekly$Year', and thus allows direct use of 'Year'\nattach(Weekly) \n\nThe following objects are masked from Smarket (pos = 7):\n\n    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year\n\n\nThe following objects are masked from Smarket (pos = 8):\n\n    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year\n\n# Use summary function to produce a numerical summary for each variable\nsummary(Weekly)\n\n      Year           Lag1               Lag2               Lag3         \n Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950  \n 1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580  \n Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410  \n Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472  \n 3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090  \n Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260  \n      Lag4               Lag5              Volume            Today         \n Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747   Min.   :-18.1950  \n 1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202   1st Qu.: -1.1540  \n Median :  0.2380   Median :  0.2340   Median :1.00268   Median :  0.2410  \n Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462   Mean   :  0.1499  \n 3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373   3rd Qu.:  1.4050  \n Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821   Max.   : 12.0260  \n Direction \n Down:484  \n Up  :605  \n           \n           \n           \n           \n\n\n\n# Use cor function to produce a table of correlations for all variables \n# (excluding the non-numerical variable 'Direction')\nround(cor(Weekly[,-9]), 2)\n\n        Year  Lag1  Lag2  Lag3  Lag4  Lag5 Volume Today\nYear    1.00 -0.03 -0.03 -0.03 -0.03 -0.03   0.84 -0.03\nLag1   -0.03  1.00 -0.07  0.06 -0.07 -0.01  -0.06 -0.08\nLag2   -0.03 -0.07  1.00 -0.08  0.06 -0.07  -0.09  0.06\nLag3   -0.03  0.06 -0.08  1.00 -0.08  0.06  -0.07 -0.07\nLag4   -0.03 -0.07  0.06 -0.08  1.00 -0.08  -0.06 -0.01\nLag5   -0.03 -0.01 -0.07  0.06 -0.08  1.00  -0.06  0.01\nVolume  0.84 -0.06 -0.09 -0.07 -0.06 -0.06   1.00 -0.03\nToday  -0.03 -0.08  0.06 -0.07 -0.01  0.01  -0.03  1.00\n\n\n\n# Use pairs function to produce pairwise scatter plots\npairs(Weekly)\n\n\n\n\nYes, it appears that Year and Volume have a strong positive, but non-linear relationship.\n13 b) Use the full data set to perform a logistic regression with Direction as the response and the five Lag variables plus Volume as predictors. Use the summary() function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\nAnswer:\n\n# Estimate a generalized linear regression model where the third input family is a description of the error distribution \n# and link function to be used in the model, supplied as the result of a call to a family function - here use binomial.\n# Why binomial? Because our independent variable Direction takes two values.\n\nglm_fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n              data   = Weekly,\n              family = binomial)\n\n# Use summary function to print the results\nsummary(glm_fit)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Weekly)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6949  -1.2565   0.9913   1.0849   1.4579  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)  0.26686    0.08593   3.106   0.0019 **\nLag1        -0.04127    0.02641  -1.563   0.1181   \nLag2         0.05844    0.02686   2.175   0.0296 * \nLag3        -0.01606    0.02666  -0.602   0.5469   \nLag4        -0.02779    0.02646  -1.050   0.2937   \nLag5        -0.01447    0.02638  -0.549   0.5833   \nVolume      -0.02274    0.03690  -0.616   0.5377   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1496.2  on 1088  degrees of freedom\nResidual deviance: 1486.4  on 1082  degrees of freedom\nAIC: 1500.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nConclusion: The predictor Lag2 appears to have some statistical significance with a \\(p\\)-value smaller than \\(3\\%\\). (Ignoring issues due to multiple testing.)\n13 c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\nAnswer:\n\n# Use predict function on results of previous regression in 10 b)\nglm_probs <-  predict(glm_fit, type=\"response\")\n\n# Create a vector containing the string \"Down\" \nglm_pred <- rep(\"Down\", times = length(glm_probs))\n\n# Substitute \"Down\" for \"Up\", whenever the estimated probability is above 0.5\nglm_pred[glm_probs > 0.5] <- \"Up\"\n\n# Construct a summary table with the predictions against \n# the actual 'Direction'-values\ntable(glm_pred, Direction)\n\n        Direction\nglm_pred Down  Up\n    Down   54  48\n    Up    430 557\n\ncontrasts(Weekly$Direction)\n\n     Up\nDown  0\nUp    1\n\n\nCounts of the classification errors are found at the off-diagonal entries of the confusion matrix\n\nUpper off-diagonal entry: Number of False Negatives\nLower off-diagonal entry: Number of False Positives\n\nCounts of the correct classifications are found at the diagonal entries of the confusion matrix\n\nUpper diagonal entry: Number of True Negatives\nLower diagonal entry: Number of True Positives\n\nPossible Conclusions:\n\nPercentage of correct predictions (TP+TN)/n: \\((54+557)/(54+557+48+430) = 56.1\\%\\)\nPercentage of false predictions (FP+FN)/n: \\((48+430)/(54+557+48+430) = 43.9\\%\\)\nDuring weeks when the market goes Up, the logistic regression is right about (True Pos. Rate TP/P) \\(557/(557+48) = 92.1\\%\\) of the time.\nDuring weeks when the market goes Down, the logistic regression is right about (True Neg. Rate TN/N) \\(54/(430+54) = 11.2\\%\\) of the time.\n\nCaution: All these answers are with respect to the training errors - not the test errors.\n13 d) Now fit the logistic regression model using a training data period from \\(1990\\) to \\(2008\\), with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from \\(2009\\) and \\(2010\\)).\nAnswer:\n\n# generate condition for our training data\ntrain = (Year < 2009)\n\n# create data frame for the Weekly data from 2009 and 2010 \n# (usage of ! to define the \"opposite\")\nWeekly_0910 <- Weekly[!train,]\n\n# run regression on the training data subset\nglm_fit <- glm(Direction ~ Lag2,\n               data   = Weekly,\n               family = binomial,\n               subset = train)\n\n# create data frame\nglm_probs <- predict(glm_fit, Weekly_0910, type=\"response\")\n\n# fill with our predictions\nglm_pred                  <- rep(\"Down\", length(glm_probs))\nglm_pred[glm_probs > 0.5] <- \"Up\"\n\n# construct confusion table using only the test data\nDirection_0910 <- Direction[!train]\ntable(glm_pred, Direction_0910)\n\n        Direction_0910\nglm_pred Down Up\n    Down    9  5\n    Up     34 56\n\n# compute the overall rate of correct predictions \n# in the test set \nmean(glm_pred == Direction_0910)\n\n[1] 0.625\n\n\n13 e) Repeat (d) using LDA.\nAnswer:\n\n#call the packages you need\nsuppressPackageStartupMessages(library(\"MASS\"))\n\n# same approach as before but now using LDA method\nlda_fit  <- lda(Direction ~ Lag2, data=Weekly, subset=train)\nlda_pred <- predict(lda_fit, Weekly_0910)\n\n# confusion table using only the test data\ntable(lda_pred$class, Direction_0910)\n\n      Direction_0910\n       Down Up\n  Down    9  5\n  Up     34 56\n\n# compute the overall rate of correct predictions \n# in the test set \nmean(lda_pred$class == Direction_0910)\n\n[1] 0.625\n\n\n13 f) Repeat (d) using QDA.\nAnswer:\n\n# same approach as before but now using QDA method\nqda_fit   <- qda(Direction~Lag2, data=Weekly, subset=train)\nqda_class <- predict(qda_fit, Weekly_0910)$class\n\n# confusion table using only the test data\ntable(qda_class, Direction_0910)\n\n         Direction_0910\nqda_class Down Up\n     Down    0  0\n     Up     43 61\n\n# compute the overall rate of correct predictions \n# in the test set \nmean(qda_class == Direction_0910)\n\n[1] 0.5865385\n\n\n13 g) Repeat (d) using KNN with \\(K = 1.\\)\nAnswer:\n\n# call the package you need\nlibrary(\"class\")\n\n# same approach as before but now using KNN method with K=1\ntrain_X         <- as.matrix(Lag2[train])\ntest_X          <- as.matrix(Lag2[!train])\ntrain_Direction <- Direction[train]\n\n# Note: If several observations are tied as nearest neighbors, \n# then R will randomly break the tie. \n# Setting a common seed guarantees that we get the same results \nset.seed(1)\n\n# Caution: KNN prediction uses a different function\nknn_pred <- knn(train_X, test_X, train_Direction, k=1)\n\n# confusion table using only the test data\ntable(knn_pred, Direction_0910)\n\n        Direction_0910\nknn_pred Down Up\n    Down   21 30\n    Up     22 31\n\n# compute the overall rate of correct predictions \n# in the test set \nmean(knn_pred == Direction_0910)\n\n[1] 0.5\n\n\n13 h) Repeat (d) using naive Bayes.\nAnswer:\n\nlibrary(\"e1071\")\nnb_fit  <- naiveBayes(Direction ~ Lag2, \n                      data   = Weekly, \n                      subset = train)\nnb_pred <- predict(nb_fit, Weekly_0910)\n\n# confusion table using only the test data\ntable(nb_pred, Direction_0910)\n\n       Direction_0910\nnb_pred Down Up\n   Down    0  0\n   Up     43 61\n\n# compute the overall rate of correct predictions \n# in the test set \nmean(nb_pred == Direction_0910)\n\n[1] 0.5865385\n\n\n13 i) Which of these methods appears to provide the best results on this data?\nAnswer:\nLogistic regression and LDA have the largest rates of (overall) correct predictions on the held out test set.\n13 j) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for \\(K\\) in the KNN classifier.\nAnswer:\n\n# Logistic regression with Lag2:Lag1\nglm_fit   <- glm(Direction~Lag2:Lag1, data=Weekly, family=binomial, subset=train)\nglm_probs <- predict(glm_fit, Weekly_0910, type=\"response\")\nglm_pred  <- rep(\"Down\", length(glm_probs))\nglm_pred[glm_probs>.5] <- \"Up\"\nDirection_0910 <- Direction[!train]\ntable(glm_pred, Direction_0910)\n\n        Direction_0910\nglm_pred Down Up\n    Down    1  1\n    Up     42 60\n\nmean(glm_pred == Direction_0910)\n\n[1] 0.5865385\n\n\n\n# LDA with Lag2 interaction with Lag1\nlda_fit  <- lda(Direction ~ Lag2:Lag1, data=Weekly, subset=train)\nlda_pred <- predict(lda_fit, Weekly_0910)\nmean(lda_pred$class == Direction_0910)\n\n[1] 0.5769231\n\n\n\n# QDA with sqrt(abs(Lag2))\nqda_fit   <- qda(Direction~Lag2+sqrt(abs(Lag2)), data=Weekly, subset=train)\nqda_class <- predict(qda_fit, Weekly_0910)$class\ntable(qda_class, Direction_0910)\n\n         Direction_0910\nqda_class Down Up\n     Down   12 13\n     Up     31 48\n\nmean(qda_class == Direction_0910)\n\n[1] 0.5769231\n\n\n\n# KNN k =10, as before KNN uses a different command\nset.seed(1)\nknn_pred <- knn(train_X, test_X, train_Direction, k=10)\ntable(knn_pred, Direction_0910)\n\n        Direction_0910\nknn_pred Down Up\n    Down   17 21\n    Up     26 40\n\nmean(knn_pred == Direction_0910)\n\n[1] 0.5480769\n\n\n\n# KNN k = 100\nset.seed(1)\nknn_pred <- knn(train_X, test_X, train_Direction, k=100)\ntable(knn_pred, Direction_0910)\n\n        Direction_0910\nknn_pred Down Up\n    Down   10 11\n    Up     33 50\n\nmean(knn_pred == Direction_0910)\n\n[1] 0.5769231\n\n\nConclusion: Out of these experiments, the original LDA and logistic regression have better performances in terms of overall correct prediction rates on the held out test set.\n\n\nExercise 15\nThis problem involves writing functions.\n15 a) Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute \\(2^3\\) and print out the results. Hint: Recall that x^a raises x to the power a. Use the print() function to output the result.\nAnswer:\n\nPower <- function() {\n  2^3\n}\nPower()\n\n[1] 8\n\n\n15 b) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line Power2 <- function (x,a){. You should be able to call your function by entering, for instance, Power2 (3,8) on the command line. This should output the value of \\(38\\), namely, \\(6,561\\).\nAnswer:\n\nPower2 <- function(x, a) {\n  x^a\n}\nPower2(3, 8)\n\n[1] 6561\n\n\n15 c) Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\).\nAnswer:\n\nPower2(10, 3)\n\n[1] 1000\n\nPower2(8, 17)\n\n[1] 2.2518e+15\n\nPower2(131, 3)\n\n[1] 2248091\n\n\n15 d) Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line: return(result). This should be the last line in your function, before the } symbol.\nAnswer:\n\nPower3 <- function(x, a) {\n  result <- x^a\n  return(result)\n}\n\n15 e) Now using the Power3() function, create a plot of f(x) = \\(x^2\\). The x-axis should display a range of integers from 1 to 10, and the y-axis should display \\(x^2\\). Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log=\"x\", log=\"y\", or log=\"xy\" as arguments to the plot() function.\nAnswer:\n\nx <- 1:10\npar(mfrow=c(1,3))# graph parameters (three plots in one column)\nplot(x = x, y = Power3(x, 2), type=\"b\", log=\"x\", \n     ylab = expression(x^2), \n     xlab = expression(x),\n     main = \"log-transformed\\nx-axis\")\nplot(x = x, y = Power3(x, 2), type=\"b\", log=\"y\", \n     ylab = expression(x^2), \n     xlab = expression(x),\n     main = \"log-transformed\\ny-axis\")\nplot(x = x, y = Power3(x, 2), type=\"b\", log=\"xy\", \n     ylab = expression(x^2), \n     xlab = expression(x),\n     main = \"log-transformed\\nx and y-axis\")\n\n\n\npar(mfrow=c(1,1))# reset graphic parameters\n\n15 f) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call PlotPower (1:10 ,3) then a plot should be created with an x-axis taking on values \\(1, 2, \\dots , 10\\), and a y-axis taking on values \\(1^3\\), \\(2^3\\), . . . , \\(10^3\\).\nAnswer:\n\nPlotPower = function(x, a) {\n  ylab_text <- bquote('x'^.(a)) # write y-axis label\n  plot(x = x, y = Power3(x, a), type = \"b\",\n       ylab = ylab_text)\n}\nPlotPower(1:10, 3)\n\n\n\n\n\n\nExercise 16\nUsing the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.\nAnswer:\n\nattach(Boston)\n\ncrime01                      <- rep(0, length(crim))\ncrime01[crim > median(crim)] <- 1\n\nBoston <- data.frame(Boston, crime01)\n\ntrain <- 1:(dim(Boston)[1]/2)\ntest  <- (dim(Boston)[1]/2+1):dim(Boston)[1]\n\nBoston.train <- Boston[train,]\nBoston.test  <- Boston[test,]\ncrime01.test <- crime01[test]\n\n\n# logistic regression of crime01 on all predictors \n# except 'crime01' and 'crim'\nglm_fit <- glm(crime01 ~ . -crime01 - crim, \n              data   = Boston, \n              family = binomial, \n              subset = train)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm_probs <- predict(glm_fit, Boston.test, type=\"response\")\n\nglm_pred                  <- rep(0, length(glm_probs))\nglm_pred[glm_probs > 0.5] <- 1\n\nmean(glm_pred != crime01.test)\n\n[1] 0.1818182\n\n\nConclusion: This logistic regression has a test error rate of \\(18.2\\%\\).\n\nglm_fit   <- glm(crime01 ~ . -crime01 -crim -chas -tax, \n              data=Boston, family=binomial, subset=train)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm_probs <- predict(glm_fit, Boston.test, type=\"response\")\n##\nglm_pred                  <- rep(0, length(glm_probs))\nglm_pred[glm_probs > 0.5] <- 1\nmean(glm_pred != crime01.test)\n\n[1] 0.1857708\n\n\nConclusion: This logistic regression has a test error rate of \\(18.6\\%\\).\n\n# LDA\nlda_fit  <- lda(crime01~.-crime01-crim, data=Boston, subset=train)\nlda_pred <- predict(lda_fit, Boston.test)\nmean(lda_pred$class != crime01.test)\n\n[1] 0.1343874\n\n\nConclusion: This LDA has a test error rate of \\(13.4\\%\\).\n\nlda_fit  <- lda(crime01~.-crime01-crim-chas-tax, data=Boston, subset=train)\nlda_pred <- predict(lda_fit, Boston.test)\nmean(lda_pred$class != crime01.test)\n\n[1] 0.1225296\n\n\nConclusion: This LDA has a test error rate of \\(12.3\\%\\).\n\nlda_fit  <- lda(crime01~.-crime01-crim-chas-tax-lstat-indus-age,\n              data=Boston, subset=train)\nlda_pred <- predict(lda_fit, Boston.test)\nmean(lda_pred$class != crime01.test)\n\n[1] 0.1185771\n\n\nConclusion: This LDA has a test error rate of \\(11.9\\%\\).\n\n# KNN\nlibrary(class)\ntrain_X <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black,\n                lstat, medv)[train,]\ntest_X  <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black,\n                lstat, medv)[test,]\n\ntrain.crime01 <- crime01[train]\n\nset.seed(1)\n# KNN(k=1)\nknn_pred  <- knn(train_X, test_X, train.crime01, k=1)\n##\nmean(knn_pred != crime01.test)\n\n[1] 0.458498\n\n\nConclusion: This KNN prediction has a test error rate of \\(45.8\\%\\).\n\n# KNN(k=10)\nset.seed(1)\nknn_pred <- knn(train_X, test_X, train.crime01, k=10)\nmean(knn_pred != crime01.test)\n\n[1] 0.1106719\n\n\nConclusion: This KNN prediction has a test error rate of \\(11.1\\%\\).\n\n# KNN(k=100)\nset.seed(1)\nknn_pred = knn(train_X, test_X, train.crime01, k=100)\nmean(knn_pred != crime01.test)\n\n[1] 0.486166\n\n\nConclusion: This KNN prediction has a test error rate of \\(48.6\\%\\).\nOverall conclusion: The best models are the ones with the smaller test error rates. In our case, this means that the smallest (fewest predictors) LDA-model and the KNN prediction with K=10 are the best prediction models."
  },
  {
    "objectID": "Ch5_ResamplingMethods.html",
    "href": "Ch5_ResamplingMethods.html",
    "title": "5  Resampling Methods",
    "section": "",
    "text": "Resampling methods involve repeatedly drawing samples from a training data set and refitting a model of interest on each of these samples. The different estimation results across resamples can be used, for instance, to estimate the variability of a linear regression fit.\nIn the following, we consider the resampling methods:\n\nCross-Validation and\nBootstrap"
  },
  {
    "objectID": "Ch5_ResamplingMethods.html#ch.-5.1-cross-validation",
    "href": "Ch5_ResamplingMethods.html#ch.-5.1-cross-validation",
    "title": "5  Resampling Methods",
    "section": "(Ch. 5.1) Cross-Validation",
    "text": "(Ch. 5.1) Cross-Validation\nIn this section, we consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.\n\n(Ch. 5.1.1) Validation Set Approach\nThe validation set approach randomly divides the available set of observations into two parts:\n\na training set and\na validation set (or hold-out set)\n\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\n\n\nIllustration\nReconsider the Auto data set. In Chapter 3, we found that a model that predicts mpg using horsepower and horsepower\\(^2\\) predicts better than a model that uses only the linear term. But maybe a cubic or a higher order polynomial regression model predicts even better? The validation set approach can be used to select the degree \\(p\\) of the polynomial regression model \\[\n\\texttt{mpg}=\\beta_0 + \\sum_{j=1}^p\\beta_j \\texttt{horsepower}^j + \\epsilon.\n\\]\nStep 1: Randomly split the total data set into mutually exclusive training and test (validation) sets of roughly equal subsample sizes:\n\nTraining set: \\(\\{(x_i,y_i), i\\in\\mathcal{I}_{Train}\\},\\) where \\(n_{Train}=|\\mathcal{I}_{Train}|<n\\)\nTest set: \\(\\{(x_i,y_i), i\\in\\mathcal{I}_{Test}\\},\\) where \\(n_{Test}=|\\mathcal{I}_{Test}|<n\\)\n\nsuch that \\(n_{Train}\\approx n_{Test}\\) with \\(n=n_{Train} + n_{Test}\\) and \\[\n\\mathcal{I}_{Train}\\cap \\mathcal{I}_{Test}=\\emptyset.\n\\] Code for splitting data randomly into training and validation sets:\n\nlibrary(\"ISLR2\")\ndata(\"Auto\")\n\nn        <- nrow(Auto)    # Sample size\nn_Train  <- 200           # Sample size of training set \nn_Valid  <- n - n_Train   # Sample size of test/validation set \n\nset.seed(1234) \n\n## Index-Sets for selecting the training and validation sets\nI_Train  <- sample(x = 1:n, size = n_Train, replace = FALSE)\nI_Valid  <- c(1:n)[-I_Train]\n\n## Trainingsdaten \nAuto_Train_df <- Auto[I_Train, ]\n## Validierungsdaten \nAuto_Valid_df <- Auto[I_Valid, ]\n\nStep 2: Estimation of the polynomial regression model, e.g., for \\(p=2\\) using the training set:\n\np            <- 2\nTrain_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw=TRUE), \n                   data = Auto_Train_df)\n\nStep 3: Validation of the polynomial regression model by computing the test mean squared (prediction) error using the validation set: \\[\n\\operatorname{MSE}_{Test}^{ValidationSetApproach}=\\frac{1}{n_{Test}}\\sum_{i\\in\\mathcal{I}_{Test}}(y_i - \\hat{y}_i)^2,\n\\] where \\(\\hat{f}\\) in \\(\\hat{y}_i=\\hat{f}(x_i)\\) is computed from the training data, but evaluated at the test data \\(x_i,\\) \\(i\\in\\mathcal{I}_{Test}.\\)\n\ny_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\nRSS_Valid     <- sum((Auto_Valid_df$mpg - y_fit_Valid)^2)\nMSE           <- RSS_Valid / n_Valid\n\nRepeating Steps 1-3 for a series of polynomial degrees \\(p=1,\\dots,10\\) allows us to search for the polynomial degree with lowest test MSE.\n\np_max         <- 10\nMSE           <- numeric(p_max)\nfor(p in 1:p_max){\n  ## Step 1\n  Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw=TRUE), \n                     data = Auto_Train_df)\n  ## Step 2\n  y_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\n  ## Step 3\n  RSS_Valid     <- sum( (Auto_Valid_df$mpg - y_fit_Valid)^2 )\n  MSE[p]        <- RSS_Valid / n_Valid\n}\n\nplot(x = 1:p_max, y = MSE, type = \"b\", \n     col = \"black\", bg = \"black\", pch = 21,  \n     xlab = \"Degree of Polynomial\", ylab = \"MSE\")\npoints(y = MSE[which.min(MSE)], \n       x = c(1:p_max)[which.min(MSE)], \n       col = \"red\", bg = \"red\", pch = 21)     \n\n\n\n\nFigure 5.1: Validation error estimates for a single split into training and validation data sets. This result suggests that \\(p=9\\) minimizes the test MSE; however, the test MSE values for polynomial degrees from \\(p=2\\) to \\(p=10\\) are all of comparable order of magnitude.\n\n\n\n\nFigure 5.1 shows the test MSE values based on one random split of the dataset. The result that \\(p=9\\) minimizes the test MSE, however, may depend on the random split. Different random splits may lead to different model selection (choices of \\(p\\)).\nThe following code repeats the above computations for multiple random splits of the dataset into training and validation sets:\n\n## R = 10 random splits\nR        <- 10\n## Container for the MSE results\nMSE      <- matrix(NA, R, p_max)\n\nfor(r in 1:R){\n  ## Index sets for training and validation sets\n  I_Train  <- sample(x = 1:n, size = n_Train, replace = FALSE)\n  I_Valid  <- c(1:n)[-I_Train]\n\n  ## Training set \n  Auto_Train_df <- Auto[I_Train, ]\n  ## Validation set\n  Auto_Valid_df <- Auto[I_Valid, ]\n\n  for(p in 1:p_max){\n    ## Step 1\n    Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                       data = Auto_Train_df)\n    ## Step 2\n    y_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\n    ## Step 3\n    RSS_Valid     <- sum( (Auto_Valid_df$mpg - y_fit_Valid)^2 )\n    MSE[r,p]      <- RSS_Valid / n_Valid\n  }\n}\n\nmatplot(y = t(MSE), type=\"b\", ylab=\"MSE\", xlab=\"Degree of Polynomial\", \n        pch=21, col=\"black\", bg=\"black\", lty = 1, main=\"\")\nfor(r in 1:R){\n  points(y = MSE[r,][which.min(MSE[r,])], \n       x = c(1:p_max)[which.min(MSE[r,])], \n       col = \"red\", bg = \"red\", pch = 21)\n}\n\n\n\n\nFigure 5.2: Validation error estimates for ten different random splits into training and validation data sets. The polynomial degrees that minimize the test MSE strongly vary across the different random splits.\n\n\n\n\nFigure 5.2 shows that the validation set approach can be highly variable. The selected polynomial degrees (minimal test MSE) strongly varies across the different random splits and thus depend on the data included in the test and validation sets.\nA further serious problem with the validation set approach is that the evaluated predictions \\(\\hat{y}_i=\\hat{f}(x_i)\\) are based on estimates \\(\\hat{f}\\) computed from the training set, where, however, the training set sample size \\(n_{Train}\\) is typically substantially smaller than the actual sample size \\(n.\\) This leads to increased (i.e. biased) test MSE values which do not reflect the actual test MSE values for the total sample size \\(n.\\)\nLeave-One-Out and \\(k\\)-fold Cross-validation are refinements of the validation set approach that addresses these issues.\n\n\n\n(Ch. 5.1.2) Leave-One-Out Cross-Validation (LOOCV)\nLike the validation set approach, LOOCV involves splitting the set of validation observations into two parts.\nHowever, instead of creating two subsets of comparable size, a single observation is used for the validation set, and the remaining observations are used for the training set. , i.e.\n\nTraining set: \\(\\{(x_1,y_1),\\dots,(x_{i-1},y_{i-1}),(x_{i+1},y_{i+1}),\\dots,(x_{n},y_{n})\\}\\) with \\(n_{Train}=n-1\\)\nTest set: \\(\\{(x_i,y_i)\\}\\) with \\(n_{Test}=1\\)\n\nThe \\(i\\)th estimate for the test MSE is thus \\[\n\\operatorname{MSE}_i = \\left(y_i - \\hat{y}_i\\right)^2,\n\\] which is an (approximately) unbiased estimate for the test MSE, although a poor estimate with a high variance as it is based on only one observation in the test set.\nRepeating this leave-one-out splitting approach for each \\(i=1,\\dots,n,\\) produces \\(n\\) many estimates of the test MSE: \\[\n\\operatorname{MSE}_1, \\operatorname{MSE}_2,\\dots,\\operatorname{MSE}_n\n\\]\nThe LOOCV estimate is then formed by the average of the \\(n\\) MSE estimates: \\[\n\\operatorname{LOOCV}=\\operatorname{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n\\operatorname{MSE}_i.\n\\tag{5.1}\\]\nFigure 5.3 shows schematically the leave-one-out data splitting approach.\n\nAdvantages of CV over the Validation Set approach:\n\nLower bias. Since the test MSE estimates are based on training sets with sample sizes \\(n_{Train}=n-1 \\approx n,\\), LOOCV does not overestimate the test error rate as much the validation set approach does.\nPerforming LOOCV multiple times, always yields the same result. I.e., there is no randomness due to the training/validation set splits as seen for the validation set approach.\n\nCodes to implement the LOOCV approach for the Auto data example:\n\nMSE_i      <- matrix(NA, n, p_max)\n\n## Save starting time of the loop\nstart_time <- Sys.time()\n\nfor(r in 1:n){\n  ## Training set \n  Auto_Train_df <- Auto[-r, ]\n  ## Validation set\n  Auto_Valid_df <- Auto[r, ]\n\n  for(p in 1:p_max){\n    ## Step 1\n    Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                       data = Auto_Train_df)\n    ## Step 2\n    y_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\n    ## Step 3\n    MSE_i[r,p]    <- (Auto_Valid_df$mpg - y_fit_Valid)^2  \n  }\n}\n## Save end time of the loop\nend_time <- Sys.time()\n\nLOOCV  <- colMeans(MSE_i)\n\nplot(x = 1:p_max, y = LOOCV, type = \"b\", \n     col = \"black\", bg = \"black\", pch = 21,  \n     xlab = \"Degree of Polynomial\", ylab = \"LOOCV\")\npoints(y = LOOCV[which.min(LOOCV)], \n       x = c(1:p_max)[which.min(LOOCV)], \n       col = \"red\", bg = \"red\", pch = 21)     \n\n\n\n\nFigure 5.3: LOOCV error estimates for different polynomial degrees \\(p.\\)\n\n\n\n\nLOOCV has the potential to be computationally expensive, since the model has to be fit \\(n\\) times. Indeed the above code, which represents a rather simple implementation of LOOCV for least squares fits of linear/polynomial regression models, takes\n\n\nend_time\\(-\\)start_time \\(=\\) 3.492 seconds\n\nfor the computations which is quite long.\nLuckily, for least squares fits of linear/polynomial regression models one can use the following short-cut formula \\[\n\\operatorname{LOOCV}=\\operatorname{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n\\left(\\frac{y_i - \\hat{y}_i}{1-h_i}\\right)^2,\n\\tag{5.2}\\] where\n\n\\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the original least squares fit, based on the total sample size \\(n,\\) and\n\\(h_i\\) is the leverage statistic for the \\(i\\)th observation, i.e.  \\[\nh_i=\\left[X(X'X)^{-1}X'\\right]_{ii}.\n\\]\n\nThe following codes implement this fast LOOCV version:\n\nLOOCV_fast <- numeric(p_max)\n\n## Save starting time \nstart_time2 <- Sys.time()\n\nfor(p in 1:p_max){\n  PolyReg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                           data = Auto)\n  h             <- lm.influence(PolyReg)$hat\n  LOOCV_fast[p] <- mean(((Auto$mpg - fitted.values(PolyReg))/(1 - h))^2)\n}\n## Save end time of the loop\nend_time2 <- Sys.time()\n\nIndeed, both approaches yield the same LOOCV values\n\n## Minimal absolute difference between \n## the naive and the fast implementation: \nround(max(abs(LOOCV - LOOCV_fast)), 3)\n\n[1] 0\n\n\nHowever, the fast version takes only\n\nend_time2\\(-\\)start_time2 \\(=\\) 0.018 seconds\n\nfor the computations.\nLOOCV is a very general method, and can be used with any kind of predictive modeling; e.g.\n\nLogistic regression\nLinear discriminant analysis\nQuadratic discriminant analysis\netc.\n\nand any statistical prediction method discussed in the lecture or in our textbook ISLR2.\nCaution: The fast LOOCV Equation 5.2 does not hold in general, but only for least squares fits of linear regression models, which includes, for instance, polynomial regressions, but, for instance, not logistic regression models."
  },
  {
    "objectID": "Ch5_ResamplingMethods.html#ch.-5.1.3-k-fold-cross-validation",
    "href": "Ch5_ResamplingMethods.html#ch.-5.1.3-k-fold-cross-validation",
    "title": "5  Resampling Methods",
    "section": "(Ch. 5.1.3) \\(k\\)-Fold Cross-Validation",
    "text": "(Ch. 5.1.3) \\(k\\)-Fold Cross-Validation\nAn alternative to LOOCV is \\(k\\)-fold CV.\nThis approach divides the total index set \\(\\mathcal{I}=\\{1,2,\\dots,n\\}\\) of the original data data set into \\(k\\) mutually exclusive subsets (folds) of roughly equal sizes \\[\n\\mathcal{I}_1,\\,\\mathcal{I}_2,\\dots,\\mathcal{I}_k\n\\] with \\(|\\mathcal{I}_1|\\approx |\\mathcal{I}_k|\\approx n/k.\\)\nThese \\(k\\) index sets allow us construct different training and test sets for each \\(j=1,2,\\dots,k\\)\n\nTraining set: \\(\\{(x_i,y_i),\\; i\\in\\mathcal{I}\\setminus \\mathcal{I}_j\\}\\) with sample size of \\(n_{Train}\\approx n - n/k\\)\nTest set: \\(\\{(x_i,y_i),\\;i\\in\\mathcal{I}_j\\}\\) with sample size of \\(n_{Test}\\approx n/k\\)\n\nEach pair of training and test set allow to compute a estimate of the test error \\[\n\\operatorname{MSE}_1, \\operatorname{MSE}_2,\\dots,\\operatorname{MSE}_k.\n\\] The \\(k\\)-fold CV estimate is computed by averaging these values \\[\n\\operatorname{CV}_{(k)}=\\frac{1}{k}\\sum_{j=1}^k\\operatorname{MSE}_j\n\\tag{5.3}\\]\nFigure 5.5 illustrates the data splitting for \\(k\\)-fold CV.\n\n\nLOOCV is a special case of \\(k\\)-fold CV with \\(k=n\\).\nMost often used \\(k\\)-values in practice are \\(k=5\\) or \\(k=10\\).\n\nWhy \\(k=5\\) or \\(k=10\\) instead of \\(k=n\\)?\n\nFaster computation times (\\(k=5\\) instead of \\(k=n\\) model fits)\nImproved estimates of the test MSE (see next section)\n\nThe following codes illustrate \\(k\\)-fold CV:\n\nset.seed(123)\n\n## number of folds for k-fold CV \nk              <- 5\n\n## container for storing the MSE results\nMSE_folds      <- matrix(NA, k, p_max)\n\n## selector for the folds \nfolds          <- sample(rep(1:k, length = n))\n\n## Save starting time of the loop\nstart_time     <- Sys.time()\n\nfor(j in 1:k){\n  ## Training set \n  Auto_Train_df <- Auto[folds != j, ]\n  ## Validation set\n  Auto_Valid_df <- Auto[folds == j, ]\n\n  for(p in 1:p_max){\n    ## Step 1\n    Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                       data = Auto_Train_df)\n    ## Step 2\n    y_fit_Valid    <- predict(Train_polreg, newdata = Auto_Valid_df)\n    ## Step 3\n    MSE_folds[j,p] <- mean((Auto_Valid_df$mpg - y_fit_Valid)^2)\n  }\n}\n## Save end time of the loop\nend_time  <- Sys.time()\n\nCV_kfold  <- colMeans(MSE_folds)\n\nplot(x = 1:p_max, y = CV_kfold, type = \"b\", \n     col = \"black\", bg = \"black\", pch = 21,  main=\"k-fold CV\", \n     xlab = \"Degree of Polynomial\", ylab = expression(\"CV\"[k]))\npoints(y = CV_kfold[which.min(CV_kfold)], \n       x = c(1:p_max)[which.min(CV_kfold)], \n       col = \"red\", bg = \"red\", pch = 21)     \n\n\n\n\n\n(Ch. 5.1.4) Bias-Variance Trade-Off for \\(k\\)-Fold Cross-Validation\nThere is a bias-variance trade-off associated with the choice of \\(k\\) in \\(k\\)-fold CV.\n\nBias:\n\nSmall \\(k\\) lead to test MSE estimates with large bias\nLarge \\(k\\) lead to test MSE estimates with small bias\n\nExplanation:\n\nA small \\(k\\) leads to trainings sets with samples sizes \\(n_{Train} \\ll n\\) substantially smaller than the actual sample size \\(n.\\) Thus, we estimate the MSE for a sample size that is substantially smaller than the sample size \\(n\\) we are actually interested in. This leads to systematic overestimations of the actual test MSE for sample size \\(n.\\)\nA large \\(k\\) reduces this bias since \\(n_{Train}\\approx n.\\) Thus we estimate essentially the actual test MSE for sample size \\(n.\\)\n\nVariance:\n\nSmall \\(k\\) lead to test MSE estimates with small variance\nLarge \\(k\\) lead to test MSE estimates with large variance\n\nExplanation:\n\nIn \\(k\\)-fold CV, the training sets overlap pairwise by roughly \\(((k-2)/k)\\times 100 \\%\\).\n\nFor \\(k=2\\) there is no overlap.\nFor \\(k=5\\) (\\(k\\)-fold CV) approximately \\((k-2)/k=(3/5)=60\\%\\) of the training data points are equal in each pair of training sets.\nFor \\(k=n\\) (LOOCV) approximately \\((n-2)/n=98\\%\\) of the training data points are equal in each pair of trainings sets.\n\n\nThus, the larger \\(k\\) the more similar the training data sets become. However, very similar training sets lead to highly correlated test MSE estimates. Since the mean of highly correlated quantities has higher variance than does the mean of quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from \\(k\\)-fold CV with \\(k<n.\\)\n\\(k\\)-fold CV with \\(k=5\\) or \\(k=10\\) is often considered a good compromise balancing these bias and variance issues.\n\n\n(Ch. 5.1.5) Cross-Validation on Classification Problems\nCross-validation can also be a very useful approach in the classification setting when \\(Y\\) is qualitative.\nIn the classification setting, the LOOCV error rate takes the form \\[\n\\operatorname{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^n\\operatorname{Err}_i,\n\\] where \\[\n\\operatorname{Err}_i=I(y_i\\neq \\hat{y}_i)\n\\] with \\(I(\\texttt{TRUE})=1\\) and \\(I(\\texttt{FALSE})=0.\\)\nAnalogously for the \\(k\\)-fold CV error rate and the validation set error rate.\nWe can, for instance, determine the degree \\(d\\) in logistic regression models \\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0 +\\sum_{j=1}^d X_j^d\n\\] by selecting that polynomial degree \\(d\\) that minimizes the CV error rate.\nLikewise, one can select the tuning parameter \\(K\\) in KNN classification by minimizing the CV error rate across different candidate values for \\(K.\\)"
  },
  {
    "objectID": "Ch5_ResamplingMethods.html#ch.-5.2-the-bootstrap",
    "href": "Ch5_ResamplingMethods.html#ch.-5.2-the-bootstrap",
    "title": "5  Resampling Methods",
    "section": "(Ch. 5.2) The Bootstrap",
    "text": "(Ch. 5.2) The Bootstrap\nThe bootstrap is a widely applicable and powerful statistical tool to quantify the uncertainty associated with a given estimator or statistical learning method.\n\nIllustration\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y.\\) These returns \\(X\\) and \\(Y\\) are random with\n\n\\(Var(X)=\\sigma^2_X\\)\n\\(Var(Y)=\\sigma^2_Y\\)\n\\(Cov(X,Y)=\\sigma_{XY}\\)\n\nWe want to invest a fraction \\(\\alpha\\in(0,1)\\) in \\(X\\) and invest the remaining \\(1-\\alpha\\) in \\(Y.\\)\nOur aim is to minimize the variance (risk) of our investment, i.e., we want to minimize \\[\nVar\\left(\\alpha X + (1-\\alpha)Y\\right).\n\\] One can show that the value \\(\\alpha\\) that minimizes this variance is \\[\n\\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}}.\n\\tag{5.4}\\] Using a data set that contains past measurements \\[\n((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] for \\(X\\) and \\(Y,\\) we can estimate the unknown \\(\\alpha\\) by plugging in estimates of the variances and covariances \\[\n\\hat\\alpha = \\frac{\\hat\\sigma^2_Y - \\hat\\sigma_{XY}}{\\hat\\sigma^2_X + \\hat\\sigma^2_Y - 2\\hat\\sigma_{XY}}\n\\tag{5.5}\\] with \\[\n\\begin{align*}\n\\hat{\\sigma}^2_X&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2\\\\\n\\hat{\\sigma}^2_Y&=\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2\\\\\n\\hat{\\sigma}_{XY}&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)\\left(Y_i-\\bar{Y}\\right),\n\\end{align*}\n\\] where \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i,\\) and likewise for \\(\\bar{Y}.\\)\nIt is natural to wish to quantify the accuracy of our estimate \\(\\hat\\alpha\\approx \\alpha.\\) I.e., we wish to know the standard error of the estimator \\(\\hat\\alpha\\), \\[\n\\sqrt{Var(\\hat\\alpha)} = \\operatorname{SE}(\\hat\\alpha)=?\n\\] Computing \\(\\operatorname{SE}(\\hat\\alpha)\\) is here difficult due to the definition of \\(\\hat\\alpha\\) in Equation 5.5 which contains variance estimates also in the denominator.\n\n\nThe Infeasible Bootstrap: A Monte Carlo Simulation\nLet us, for a moment, assume that we know the distributions of \\(X\\) and \\(Y.\\) For simplicity, let’s say \\[\n\\left(\\begin{matrix}X\\\\ Y\\end{matrix}\\right) \\sim F_{(X,Y)},\n\\] where \\(F_{(X,Y)}\\) is the distribution function of the bi-variate normal distribution \\[\n\\mathcal{N}\\left(\\left(\\begin{matrix}0\\\\0\\end{matrix}\\right),\\left[\\begin{matrix}\\sigma_X^2&\\sigma_{XY}\\\\\\sigma_{XY}&\\sigma_{Y}^2\\end{matrix}\\right]\\right).\n\\tag{5.6}\\] If this were true, i.e., if we would know the true population distribution of \\(X\\) and \\(Y,\\) we could simply generate a new dataset containing new observations for \\(X\\) and \\(Y\\) that allows us to compute a new estimate \\(\\hat\\alpha.\\)\nRepeatedly generating new datasets \\(((X_1,Y_1),\\dots,(X_n,Y_n))\\) by sampling new observations from the (here assumed) true population distribution Equation 5.6, for instance, \\(B=1000\\) many times, would allow us to compute \\(B=1000\\) estimates\n\\[\n\\hat\\alpha_1,\\;\\hat\\alpha_2,\\dots,\\hat\\alpha_{B}.\n\\] The empirical standard deviation \\[\n\\sqrt{\\frac{1}{B}\\sum_{b=1}^B\\left(\\hat\\alpha_b - \\bar{\\alpha}\\right)^2},\\quad\\text{with}\\quad \\bar{\\alpha} = \\frac{1}{B}\\sum_{b=1}^B\\hat\\alpha_b,\n\\] is then a very good estimate of the (unknown) true \\(\\operatorname{SE}(\\hat\\alpha).\\)\nIndeed, by the law of large numbers this sample standard deviation consistently estimates the true \\(\\operatorname{SE}(\\hat\\alpha)\\) as \\(B\\to\\infty,\\) provided that we sample from the true population distribution \\(F_{(X,Y)}.\\)\nR code for doing this Monte Carlo simulation:\n\nsuppressPackageStartupMessages(library(\"MASS\")) # for mvrnorm()\n\nn        <- 100 # sample size\n\n## Next: Defining the (usually unknown) population \n## distribution of (X,Y) ~ F_XY, where F_XY is \n## assumed to be a Bi-variate normal distribution \n## with the following parameters: \nmu_X     <- 0\nmu_Y     <- 0\n\nsigma2_X <- 3\nsigma2_Y <- 4\nsigma_XY <- 1\n\nSigma    <- rbind(c(sigma2_X, sigma_XY), \n                  c(sigma_XY, sigma2_Y))\n\n\n## The true (usually unknown) alpha value: \nalpha_true  <- (sigma2_Y - sigma_XY) / (sigma2_X + sigma2_X - 2 * sigma_XY)                  \n\n\n## Infeasible Bootstrap (i.e. a Monte Carlo (MC) Simulation)\nset.seed(333)\n\nB            <- 1000\nalpha_hat_MC <- numeric(B)\n\nfor(b in 1:B){\n  dat             <- mvrnorm(n = n, mu = c(mu_X, mu_Y), Sigma = Sigma)\n  X               <- dat[,1]\n  Y               <- dat[,2]\n  ##\n  sigma2_X_hat    <- var(X)\n  sigma2_Y_hat    <- var(Y)\n  sigma_XY_hat    <- cov(X,Y)\n  ##\n  alpha_hat_MC[b] <- (sigma2_Y_hat - sigma_XY_hat) / (sigma2_X_hat + sigma2_X_hat - 2 * sigma_XY_hat)\n}\n\n## Estimate of the standard error of the estimates for alpha:\nsd(alpha_hat_MC)\n\n[1] 0.2301389\n\n\nThus, this Monte Carlo simulation estimates that the true standard error equals 0.2301389, i.e. \n\n\\(\\operatorname{SE}(\\hat\\alpha) \\approx\\) sd(alpha_hat_MC) \\(=\\) 0.2301389,\n\nand by the law of large number (large B), we can expect this estimation to be really good and reliable.\nBut, unfortunately, this result depends on our **completely unrealistic assumption that we know the true population distribution \\(F_{(X,Y)}\\) of \\((X,Y),\\) which makes this simple resampling approach infeasible in practice. 😭\n\n\nThe Actual (Feasible) Bootstrap\nFortunately, we can use the empirical cumulative distribution function \\(F_{n,(X,Y)}\\) from the originally observed dataset of past measurements for \\(X\\) and \\(Y,\\) as an approximation to the true (unknown) population distribution \\(F_{(X,Y)}\\), \\[\nF_{n,(X,Y)}\\approx F_{(X,Y)}.\n\\]\nSo, instead of resampling from an unknown population distribution \\(F_{(X,Y)},\\) which is not possible in practice, we resample from the empirical distribution \\(F_{n,(X,Y)},\\) which is easily possible in practice. 🥳\nThis idea will work well, as long as \\(F_{n,(X,Y)}\\) serves as a good approximation of \\(F_{(X,Y)}\\) which will always be the case if the sample size \\(n\\) is sufficiently large since, by the famous Glivenko-Cantelli Theorem, \\(F_{n,(X,Y)}\\) is uniformly consistent for \\(F_{(X,Y)}.\\)\nSampling from an empirical cdf \\(F_{n}\\) simply means sampling from the observed dataset \\((X_i,Y_i)\\), \\(i=1,\\dots,n\\), with replacement, for instance like this:\n\nbootstrap_sample <- sample(x = 1:n, n, replace = TRUE)\nbootstrap_data   <- data_frame[bootstrap_sample, ]\n\nIn order to illustrate the bootstrap, let us generate some artificial data. We use again the bi-variate normal distribution as in the “infeasible bootstrap” illustration.\n\n## Generate some artificial data\nobserved_data  <- mvrnorm(n = n, mu = c(mu_X, mu_Y), Sigma = Sigma)\n\nIf the bootstrap works, then the bootstrap estimate of the standard error \\(\\operatorname{SE}(\\hat\\alpha)\\) should be close to the infeasible Monte Carlo estimate, even though the bootstrap method does not explicitly use the true data generating process, but only the observed data.\nThe following code implements the bootstrap:\n\nset.seed(123)\n## Bootstrap \nB              <- 1000\nalpha_hat_boot <- numeric(B)\n\nfor(b in 1:B){\n  bootstrap_sample  <- sample(x = 1:n, n, replace = TRUE)\n  bootstrap_data    <- observed_data[bootstrap_sample, ]\n  ##\n  X                 <- bootstrap_data[,1]\n  Y                 <- bootstrap_data[,2]\n  ##\n  sigma2_X_hat      <- var(X)\n  sigma2_Y_hat      <- var(Y)\n  sigma_XY_hat      <- cov(X,Y)\n  ##\n  alpha_hat_boot[b] <- (sigma2_Y_hat - sigma_XY_hat) / (sigma2_X_hat + sigma2_X_hat - 2 * sigma_XY_hat)\n}\n\n## Estimate of the standard error of the estimates for alpha:\nsd(alpha_hat_boot)\n\n[1] 0.2523776\n\n\nThe bootstrap estimate of the true standard error equals 0.2523776, i.e. \n\n\\(\\operatorname{SE}(\\hat\\alpha) \\approx\\) sd(alpha_hat_boot) \\(=\\) 0.2523776.\n\nThis is really close to the infeasible Monte Carlo simulation based estimate sd(alpha_hat_MC) \\(=\\) 0.2301389, but without making use of the unknown data generating process.\n\nThe bootstrap method is attributed to Bradley Efron, who received the International Prize in Statistics (the Nobel price of statistics) for his seminal works on the bootstrap method."
  },
  {
    "objectID": "Ch5_ResamplingMethods.html#r-lab-resampling-methods",
    "href": "Ch5_ResamplingMethods.html#r-lab-resampling-methods",
    "title": "5  Resampling Methods",
    "section": "5.2 R-Lab: Resampling Methods",
    "text": "5.2 R-Lab: Resampling Methods\nIn this lab, we explore the resampling techniques covered in this chapter. Some of the commands in this lab may take a while to run on your computer.\n\n5.2.1 The Validation Set Approach\nWe explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the Auto data set.\nBefore we begin, we use the set.seed() function in order to set a for R’s random number generator, so that the (pseudo) random data splits are reproducible.\nWe begin by using the sample() function to split the set of observations into two halves, by selecting a random subset of \\(196\\) observations out of the original \\(392\\) observations. We refer to these observations as the training set.\n\nlibrary(\"ISLR2\")\nattach(Auto)\n\n## One half of the sample size\nnrow(Auto)/2\n\n[1] 196\n\nset.seed(1)\ntrain <- sample(x = 1:392, size = 196)\n\nWe then use the subset option in lm() to fit a linear regression using only the observations corresponding to the training set.\n\nlm.fit <- lm(mpg ~ horsepower, \n             data   = Auto, \n             subset = train)\n\nWe now use the predict() function to estimate the response for all \\(392\\) observations, and we use the mean() function to calculate the MSE of the \\(196\\) observations in the validation set. Note that the -train index below selects only the observations that are not in the training set.\n\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 23.26601\n\n\nTherefore, the estimated test MSE for the linear regression fit is \\(23.27\\). We can use the poly() function to estimate the test error for the quadratic and cubic regressions.\n\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), \n             data   = Auto, \n             subset = train)\n\n## Test MSE \nmean((mpg - predict(lm.fit2, Auto))[-train]^2)\n\n[1] 18.71646\n\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), \n             data   = Auto, \n             subset = train)\n## Test MSE             \nmean((mpg - predict(lm.fit3, Auto))[-train]^2)\n\n[1] 18.79401\n\n\nThese error rates are \\(18.72\\) and \\(18.79\\), respectively. If we choose a different training set instead, then we will obtain somewhat different errors on the validation set.\n\nset.seed(2)\n\n## Polynomial degree 1\ntrain  <- sample(x = 1:392, size = 196)\nlm.fit <- lm(mpg ~ horsepower, subset = train)\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 25.72651\n\n## Polynomial degree 2\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2)\n\n[1] 20.43036\n\n## Polynomial degree 3\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2)\n\n[1] 20.38533\n\n\nUsing this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are \\(25.73\\), \\(20.43\\), and \\(20.39\\), respectively.\nThese results are consistent with our previous findings: a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence that a model that uses a cubic function of horsepower performance substantially better.\n\n\n5.2.2 Leave-One-Out Cross-Validation\nThe LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions. In the lab for Chapter 4, we used the glm() function to perform logistic regression by passing in the family = \"binomial\" argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function. So for instance,\n\nglm_fit <- glm(mpg ~ horsepower, data = Auto)\ncoef(glm_fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nand\n\nlm.fit <- lm(mpg ~ horsepower, data = Auto)\ncoef(lm.fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nyield identical linear regression models. In this lab, we will perform linear regression using the glm() function rather than the lm() function because the former can be used together with cv.glm(). The cv.glm() function is part of the boot library.\n\n## install.packages(\"boot\")\nlibrary(\"boot\")\n\nglm_fit <- glm(mpg ~ horsepower, data = Auto)\ncv.err  <- cv.glm(Auto, glm_fit)\ncv.err$delta\n\n[1] 24.23151 24.23114\n\n\nThe cv.glm() function produces a list with several components. The two numbers in the delta vector contain the cross-validation results. In this case the numbers are identical (up to two decimal places) and correspond to the LOOCV statistic given in Equation 5.1. Below, we discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately \\(24.23\\).\nWe can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for() function to initiate a which iteratively fits polynomial regressions for polynomials of order \\(i=1\\) to \\(i=10\\), computes the associated cross-validation error, and stores it in the \\(i\\)th element of the vector cv_error. We begin by initializing the vector.\n\ncv_error <- rep(0, 10)\nfor (i in 1:10) {\n  glm_fit     <- glm(mpg ~ poly(horsepower, i), \n                     data = Auto)\n  cv_error[i] <- cv.glm(Auto, glm_fit)$delta[1]\n}\nplot(cv_error, type=\"b\", ylab=\"Test MSE\", xlab=\"Polynomial Degree\",\n     col = \"black\", bg = \"black\", pch = 21, main=\"LOOCV\")\n\n\n\n\nWe see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.\n\n\n5.2.3 \\(k\\)-Fold Cross-Validation\nThe cv.glm() function can also be used to implement \\(k\\)-fold CV. Below we use \\(k=10\\), a common choice for \\(k\\), on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.\n\nset.seed(17)\ncv_error_10 <- rep(0, 10)\nfor (i in 1:10) {\n  glm_fit        <- glm(mpg ~ poly(horsepower, i), \n                        data = Auto)\n  cv_error_10[i] <- cv.glm(Auto, glm_fit, K = 10)$delta[1]\n}\n##\nplot(cv_error_10, type=\"b\", ylab=\"Test MSE\", xlab=\"Polynomial Degree\",\n     col = \"black\", bg = \"black\", pch = 21, main=\"10-fold CV\")\n\n\n\n\nWe still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.\nNotice that the computation time is shorter than that of LOOCV.\n(In principle, the computation time for LOOCV for a least squares linear model should be faster than for \\(k\\)-fold CV, due to the availability of the formula Equation 5.2 for LOOCV; however, unfortunately the cv.glm() function does not make use of this formula.)\nWe saw in Section 5.2.2 that the two numbers associated with delta are essentially the same when LOOCV is performed. When we instead perform \\(k\\)-fold CV, then the two numbers associated with delta differ slightly. The first number is the standard \\(k\\)-fold CV estimate, as in Equation 5.3. The second is a bias-corrected version. On this data set, however, the two estimates are very similar to each other.\n\n\n5.2.4 The Bootstrap\nWe illustrate the use of the bootstrap revisiting the portfolio choice example from above, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\n\n5.2.4.1 Estimating the Accuracy of a Statistic of Interest\nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in R entails only two steps:\n\nFirst, we must create a function that computes the statistic of interest.\nSecond, we use the boot() function, which is part of the boot library, to perform the bootstrap by repeatedly sampling observations from the data set with replacement.\n\nThe Portfolio data set in the ISLR2 package is simulated data of \\(100\\) pairs of returns, generated in the fashion described above, where we introduced the portfolio example.\nTo illustrate the use of the bootstrap on this data, we must first create a function, alpha_fn(), which takes as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations.\n\nalpha_fn <- function(data, index) {\n  X         <- data$X[index]\n  Y         <- data$Y[index]\n  alpha_hat <- (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n  return(alpha_hat)\n}\n\nThis function returns an estimate for \\(\\alpha\\) based on applying Equation 5.4 to the observations indexed by the argument index. For instance, the following command tells R to estimate \\(\\alpha\\) using all of the \\(100\\) observations.\n\nalpha_fn(data  = Portfolio, \n         index = 1:100) # complete original dataset\n\n[1] 0.5758321\n\n\nThe next command uses the sample() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing \\(\\hat{\\alpha}\\) based on the new data set.\n\nset.seed(7)\nalpha_fn(data  = Portfolio, \n         index = sample(x=1:100, size=100, replace = TRUE))\n\n[1] 0.5385326\n\n\nWe can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. (We use this approach above.)\nHowever, the boot() function automates this approach. Below we produce \\(R=1,000\\) bootstrap estimates for \\(\\alpha\\).\n\nboot(data      = Portfolio, \n     statistic = alpha_fn, \n     R         = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha_fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 0.0007959475  0.08969074\n\n\nThe final output shows that using the original data, \\(\\hat{\\alpha}=0.5758\\), and that the bootstrap estimate for \\(\\operatorname{SE}(\\hat{\\alpha})\\) is \\(0.0897\\).\n\n\n5.2.4.2 Estimating the Accuracy of a Linear Regression Model\nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\) described in Chapter 3.1.2.\nWe first create a simple function, boot_fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3.\n\n## Function to compute coefficient estimates using lm()\nboot_fn <- function(data, index){\n  coef(lm(mpg ~ horsepower, \n          data   = data, \n          subset = index))\n}\n\n## Sample size\nn <- nrow(Auto)\n\n## Coeficient estimates using the total sample\nboot_fn(data  = Auto, \n        index = 1:n)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nThe boot_fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. Here we give two examples.\n\nset.seed(1)\n\nboot_fn(data  = Auto, \n        index = sample(1:n, n, replace = TRUE))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\nboot_fn(data  = Auto, \n        index = sample(1:n, n, replace = TRUE))\n\n(Intercept)  horsepower \n 40.1186906  -0.1577063 \n\n\nNext, we use the boot() function to compute the standard errors of \\(R=1,000\\) bootstrap estimates for the intercept and slope terms.\n\nboot_obj <- boot(data      = Auto, \n                 statistic = boot_fn, \n                 R         = 1000)\nboot_obj                 \n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot_fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0544513229 0.841289790\nt2* -0.1578447 -0.0006170901 0.007343073\n\n\nThis indicates that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.84\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\).\nThe reported bias equals the difference between the sample means of the bootstrap realizations and the full sample estimates:\n\n## estimated biases: \ncolMeans(boot_obj$t) - boot_obj$t0\n\n  (Intercept)    horsepower \n 0.0544513229 -0.0006170901 \n\n\nAs discussed in Chapter 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.\n\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\nThe standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained using the formulas from Chapter 3.1.2 are \\(0.717\\) for the intercept and \\(0.0064\\) for the slope.\nInterestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 (Chapter 3.1.2) rely on certain assumptions. For example, they depend on the unknown parameter \\(\\sigma^2\\), the noise variance. We then estimate \\(\\sigma^2\\) by \\(\\operatorname{RSS}/(n-p-1).\\) Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does. We see in Figure 3.8 of our textbook that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\).\nMoreover, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function.\nBelow we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data.\n\nboot_fn <- function(data, index)\n  coef(lm(mpg ~ horsepower + I(horsepower^2), \n        data = data, subset = index))\n\nset.seed(1)\nboot(data      = Auto, \n     statistic = boot_fn, \n     R         = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot_fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\nsummary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$coef\n\n                    Estimate   Std. Error   t value      Pr(>|t|)\n(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\nhorsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\nI(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21\n\n\nSince the quadratic model provides a good fit to the data (Figure 3.8 of our textbook), there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\)."
  },
  {
    "objectID": "Ch5_ResamplingMethods.html#exercises",
    "href": "Ch5_ResamplingMethods.html#exercises",
    "title": "5  Resampling Methods",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\nPrepare the following exercises of Chapter 5 in our course textbook:\n\nExercise 3\nExercise 4\nExercise 5\nExercise 6\nExercise 8"
  },
  {
    "objectID": "Ch5_ResamplingMethods.html#solutions",
    "href": "Ch5_ResamplingMethods.html#solutions",
    "title": "5  Resampling Methods",
    "section": "5.4 Solutions",
    "text": "5.4 Solutions\n\nExercise 3\nWe now review k-fold cross-validation.\n3 a) Explain how k-fold cross-validation is implemented.\nAnswer:\nk-fold cross-validation is implemented by taking the set of \\(n\\) observations and randomly splitting into \\(K\\) non-overlapping groups of roughly equal group-size (approx. \\(n/K\\)). To compute the \\(k\\)-th \\((k=1,2,\\dots,K)\\) test MSE estimate, group \\(k\\) is used as a validation set and the remainder as a training set. The test error is estimated by averaging the \\(K\\) resulting test MSE estimates.\n3 b) What are the advantages and disadvantages of \\(k\\)-fold cross-validation relative to:\n\nthe validation set approach?\nLOOCV?\n\nAnswer:\nThe validation set approach is conceptually simple and easily implemented as you are simply partitioning the existing training data into two sets. However, there are two drawbacks:\n\nThe estimate of the test MSE can be highly variable/instable; i.e. may strongly depend on which observations are included in the training and validation sets.\nThe validation set error rate may tend to overestimate the test MSE for the actual model fit on the entire data set since the training set has a relatively small sample size in comparison to the actual data size \\(n\\).\n\nLOOCV is a special case of k-fold cross-validation with \\(k = n\\). Thus, LOOCV is the computationally most expensive cross-validation method since the model must be fit \\(n\\) times. Also, LOOCV has higher variance, but lower bias, than k-fold CV: On the one hand, the LOOCV cross-validation samples are highly correlated; one the other hand, the cross-validation samples are having sample sizes roughly equal to the actual sample size \\((n-1\\approx n).\\)\n\n\nExercise 4\nSuppose that we use some statistical learning method to make a prediction for the response \\(Y\\) for a particular value of the predictor \\(X\\). Carefully describe how we might estimate the standard deviation of our prediction.\nAnswer:\n\nWe can estimate the standard deviation of our prediction by using the bootstrap approach.\nLet \\(\\hat{Y} = \\hat{f}(X)\\) denote the prediction of \\(Y\\) for the given \\(X\\). The bootstrap approach works by repeatedly (\\(B\\) many times) sampling \\(n\\) observations (with replacement) from the original data set, and to compute new prediction results for each re-sampled bootstrap datasets. This yields to \\(B\\) many bootstrap predictions: \\[\n\\hat{f}^*_1(X), \\hat{f}^*_2(X), \\dots, \\hat{f}^*_B(X).\n\\]\nUsing these bootstrap predictions we can compute the standard deviation of our prediction by computing \\[\n\\sqrt{\\frac{1}{B}\\sum^B_{b=1}\\left(\\hat{f}^*_b(X)-\\left(\\frac{1}{B}\\sum_{r=1}^B\\hat{f}^*_r(X)\\right)\\right)^2}.\n\\]\n\n\nExercise 5\nIn Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default dataset. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n5 a) Fit a logistic regression model that uses income and balance to predict default.\nAnswer:\n\nlibrary(\"ISLR2\")\n\n# Attaching the data set\nattach(Default) \n\n# Estimate a GLM model where \"family=binomial\" selects a logistic regression\nglm.fit <- glm(default ~ income + balance, \n                data   = Default, \n                family = binomial)\n\n# Use summary function to print the results\nsummary(glm.fit)\n\n\nCall:\nglm(formula = default ~ income + balance, family = binomial, \n    data = Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4725  -0.1444  -0.0574  -0.0211   3.7245  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***\nincome       2.081e-05  4.985e-06   4.174 2.99e-05 ***\nbalance      5.647e-03  2.274e-04  24.836  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1579.0  on 9997  degrees of freedom\nAIC: 1585\n\nNumber of Fisher Scoring iterations: 8\n\n\n5 b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n\nSplit the sample set into a training set and a validation set.\nFit a multiple logistic regression model using only the training observations.\nObtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than \\(0.5\\).\nCompute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n\nAnswer:\n\n## We are going to do similar tasks several times in this exercise. \n## Therefore, it's convenient to build a function:\n\nfun5b <- function(){\n    # sample(): takes a sample of the specified size from the elements of x using either with or without replacement.\n    n     <- dim(Default)[1]\n    train <- sample(x = n, size = round(n/2, 0), replace = FALSE)\n    \n    # logistic function fit (training dataset)\n    glm.fit <- glm(default ~ income + balance, \n        data   = Default, \n        family = binomial, \n        subset = train)\n    \n    # predictions (test dataset)\n    glm.pred  <- rep(\"No\", times = round(n/2, 0))\n    glm.probs <- predict(glm.fit, Default[-train, ], \n                         type = \"response\")\n    glm.pred[glm.probs > 0.5] <- \"Yes\"\n    \n    # return the test (prediction) error rate \n    return(mean(glm.pred != Default[-train, ]$default))\n}\n\n## set seed\nset.seed(1110)\n\n## compute test prediction error using the \n## programmed validation set approach\nfun5b()\n\n[1] 0.0262\n\n\nAnswer: There is a 2.62% test error rate from the validation set approach.\n5 c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\n\nfun5b()\n\n[1] 0.028\n\nfun5b()\n\n[1] 0.0272\n\nfun5b()\n\n[1] 0.025\n\n\nAnswer: The estimates of the test error rates are in the range of 2.5% and 2.8% and are varying with respect to the different training and validation set splittings.\n5 d) Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.\nAnswer:\n\nset.seed(1)\n\n# generate our training data\nn     <- dim(Default)[1]\ntrain <- sample(n, round(n/2, 0))\n\n# run regression on the training data subset\nglm.fit <- glm(default ~ income + balance + student, \n                data   = Default, \n                family = binomial, \n                subset = train)\n\n# test sample predictions\nglm.pred <- rep(\"No\", times = round(n/2, 0))\n\n# fill with our predictions\nglm.probs <- predict(glm.fit, Default[-train, ], \n                    type = \"response\")\nglm.pred[glm.probs > 0.5] <- \"Yes\"\n\n# test (prediction) error rate \nmean(glm.pred != Default[-train, ]$default)\n\n[1] 0.026\n\n\nAnswer:\nThe test error rate is 2.6%. Thus adding the student dummy variable to our specification doesn’t appear to reduce the test error rate.\n\n\nExercise 6\nWe continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n6 a) Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\nAnswer:\n\n# Same as in 5 a)\n\n# Estimate a generalized linear regression model using glm(), \n# where the third function argument ('family') allows to specify \n# distribution---here, 'binomial' since our independent variable\n# 'default' takes two values '0' and '1'\n\nglm.fit <- glm(default ~ income + balance, \n                data   = Default, \n                family = binomial)\n\n# Use summary function to print a summary of the the results\nsummary(glm.fit)\n\n\nCall:\nglm(formula = default ~ income + balance, family = binomial, \n    data = Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4725  -0.1444  -0.0574  -0.0211   3.7245  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***\nincome       2.081e-05  4.985e-06   4.174 2.99e-05 ***\nbalance      5.647e-03  2.274e-04  24.836  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1579.0  on 9997  degrees of freedom\nAIC: 1585\n\nNumber of Fisher Scoring iterations: 8\n\n\nUsing the standard formula for the standard errors in logistic regression:\n\nEstimated standard error of the parameter estimator income: \\(4.985 \\cdot 10^{-06}\\)\nEstimated standard error of the parameter estimator balance: \\(2.274\\cdot 10^{-04}\\)\n\n6 b) Write a function, boot_fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nAnswer:\n\nboot_fn <- function(data, index){\n    return(coef(glm(default ~ income + balance, \n                    data   = data, \n                    family = binomial, \n                    subset = index)))\n}\n\n6 c) Use the boot() function together with your boot_fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\nAnswer:\n\n# install.packages(\"boot\")\n\n# Load the boot package\nlibrary(\"boot\")\n\n# Set seed\nset.seed(1)\n\n# The boot package provides extensive facilities for bootstrapping \n# and related resampling methods. \nboot(data = Default, statistic = boot_fn, R = 100)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Default, statistic = boot_fn, R = 100)\n\n\nBootstrap Statistics :\n         original        bias     std. error\nt1* -1.154047e+01  8.556378e-03 4.122015e-01\nt2*  2.080898e-05 -3.993598e-07 4.186088e-06\nt3*  5.647103e-03 -4.116657e-06 2.226242e-04\n\n\n6 d) Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\nAnswer:\n\nEstimated standard error (bootstrap version) of the parameter estimator income: \\(4.186 \\cdot 10^{-06}\\)\n\nThe bootstrap approach estimates slightly smaller std errors for income than the classic standard formula.\n\nEstimated standard error (bootstrap version) of the parameter estimator balance: \\(2.226\\cdot 10^{-04}\\)\n\nThe bootstrap approach and the standard formula yield essentially equal results.\n\n\n\n\nExercise 8\nWe will now perform cross-validation on a simulated data set.\n8 a) Generate a simulated data set as follows.\n\n# set seed for rnorm function\nset.seed(1)\n\n# rnorm draws pseudo-random variables from a \n# (standard) normal distribution\nx <- rnorm(100)\ny <- x - 2 * x^2 + rnorm(100)\n\nIn this data set, what is \\(n\\) and what is \\(p\\)? Write out the model used to generate the data in equation form.\nAnswer:\n\nThe sample size: \\(n=100\\)\nNumber of predictors: \\(p=2\\)\nModel: \\(Y_i= X_i -2 \\, X_i^2 + \\epsilon_i\\) with i.i.d. errors \\(\\epsilon_i \\sim \\mathcal{N}(0,1)\\), \\(i=1,...,n=100\\)\n\n8 b) Create a scatterplot of \\(X\\) against \\(Y\\). Comment on what you find.\nAnswer:\n\nplot(x,y)\n\n\n\n\nWe can observe a quadratic relationship between \\(Y\\) and \\(X.\\) Moreover, \\(X\\) ranges from about \\(-2\\) to \\(2,\\) while \\(Y\\) ranges from about \\(-12\\) to \\(2.\\) The largest \\(Y\\) values are observed for \\(X\\) values around \\(0.\\)\n8 c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\n\n\\(Y = \\beta_0 +\\beta_1 X + \\epsilon\\)\n\\(Y = \\beta_0 +\\beta_1 X + \\beta_2 X^2 + \\epsilon\\)\n\\(Y = \\beta_0 +\\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon\\)\n\\(Y = \\beta_0 +\\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\epsilon\\)\n\nAnswers:\n\n# call boot package\nlibrary(boot)\n\n# create data frame containing the simulated data for X and Y\nSim_Data <- data.frame(x, y)\n\n#set seed\nset.seed(1)\n\n8 c i)\n\n# i\n# Caution:\n# This performes linear regression (no 'family' argument specified)\nlm.fit_i <- glm(y ~ x, data = Sim_Data)\n\n# cv.glm calculates the estimated K-fold cross-validation prediction\n# error for generalized linear models (with K=n by default).\n# '$delta' selects a vector of length two: \n# The first component is the raw cross-validation estimate of the \n# prediction error. \n# The second component is the adjusted cross-validation estimate. \n# (The adjustment is designed to compensate for bias.)\ncv.glm(data   = Sim_Data, \n       glmfit = lm.fit_i, \n       K      = nrow(Sim_Data))$delta\n\n[1] 7.288162 7.284744\n\n\n8 c ii)\n\n# ii \n# see ?poly for infos on poly()\nlm.fit_ii <- glm(y ~ poly(x, 2, raw=TRUE), data = Sim_Data)\ncv.glm(Sim_Data, lm.fit_ii, K = nrow(Sim_Data))$delta\n\n[1] 0.9374236 0.9371789\n\n\n8 c iii)\n\n# iii\nlm.fit_iii <- glm(y ~ poly(x, 3, raw=TRUE), data = Sim_Data)\ncv.glm(Sim_Data, lm.fit_iii, K = nrow(Sim_Data))$delta\n\n[1] 0.9566218 0.9562538\n\n\n8 c iv)\n\n# iv\nlm.fit_iv <- glm(y ~ poly(x, 4, raw=TRUE), data = Sim_Data)\ncv.glm(Sim_Data, lm.fit_iv, K = nrow(Sim_Data))$delta\n\n[1] 0.9539049 0.9534453\n\n\n\n8. d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?\n\nset.seed(10)\n# i.\nlm.fit1 <- glm(y ~ x, data = Sim_Data)\ncv.glm(Sim_Data, lm.fit1)$delta\n\n[1] 7.288162 7.284744\n\n# ii.\nlm.fit2 <- glm(y ~ poly(x, 2, raw=TRUE), data = Sim_Data)\ncv.glm(Sim_Data, lm.fit2)$delta\n\n[1] 0.9374236 0.9371789\n\n# iii.\nlm.fit3 <- glm(y ~ poly(x, 3, raw=TRUE), data = Sim_Data)\ncv.glm(Sim_Data, lm.fit3)$delta\n\n[1] 0.9566218 0.9562538\n\n# iv.\nlm.fit4 <- glm(y ~ poly(x, 4, raw=TRUE), data = Sim_Data)\ncv.glm(Sim_Data, lm.fit4)$delta\n\n[1] 0.9539049 0.9534453\n\n\nAnswer:\nThe results are exactly the same, because LOOCV will be the same since it evaluates n folds of a single observation.\n8 e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.\nAnswer:\nThe quadratic polynomial had the lowest LOOCV test error rate. This was expected as it matches the true form of \\(Y\\).\n8 f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\n\nsummary(lm.fit1)\n\n\nCall:\nglm(formula = y ~ x, data = Sim_Data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-9.5161  -0.6800   0.6812   1.5491   3.8183  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.6254     0.2619  -6.205 1.31e-08 ***\nx             0.6925     0.2909   2.380   0.0192 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 6.760719)\n\n    Null deviance: 700.85  on 99  degrees of freedom\nResidual deviance: 662.55  on 98  degrees of freedom\nAIC: 478.88\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nsummary(lm.fit2)\n\n\nCall:\nglm(formula = y ~ poly(x, 2, raw = TRUE), data = Sim_Data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9650  -0.6254  -0.1288   0.5803   2.2700  \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              0.05672    0.11766   0.482    0.631    \npoly(x, 2, raw = TRUE)1  1.01716    0.10798   9.420  2.4e-15 ***\npoly(x, 2, raw = TRUE)2 -2.11892    0.08477 -24.997  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9178258)\n\n    Null deviance: 700.852  on 99  degrees of freedom\nResidual deviance:  89.029  on 97  degrees of freedom\nAIC: 280.17\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nsummary(lm.fit3)\n\n\nCall:\nglm(formula = y ~ poly(x, 3, raw = TRUE), data = Sim_Data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9765  -0.6302  -0.1227   0.5545   2.2843  \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              0.06151    0.11950   0.515    0.608    \npoly(x, 3, raw = TRUE)1  0.97528    0.18728   5.208 1.09e-06 ***\npoly(x, 3, raw = TRUE)2 -2.12379    0.08700 -24.411  < 2e-16 ***\npoly(x, 3, raw = TRUE)3  0.01764    0.06429   0.274    0.784    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9266599)\n\n    Null deviance: 700.852  on 99  degrees of freedom\nResidual deviance:  88.959  on 96  degrees of freedom\nAIC: 282.09\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nsummary(lm.fit4)\n\n\nCall:\nglm(formula = y ~ poly(x, 4, raw = TRUE), data = Sim_Data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0550  -0.6212  -0.1567   0.5952   2.2267  \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              0.156703   0.139462   1.124    0.264    \npoly(x, 4, raw = TRUE)1  1.030826   0.191337   5.387 5.17e-07 ***\npoly(x, 4, raw = TRUE)2 -2.409898   0.234855 -10.261  < 2e-16 ***\npoly(x, 4, raw = TRUE)3 -0.009133   0.067229  -0.136    0.892    \npoly(x, 4, raw = TRUE)4  0.069785   0.053240   1.311    0.193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9197797)\n\n    Null deviance: 700.852  on 99  degrees of freedom\nResidual deviance:  87.379  on 95  degrees of freedom\nAIC: 282.3\n\nNumber of Fisher Scoring iterations: 2\n\n\nAnswer:\nThe \\(p\\)-values only show statistical significance of the linear and the quadratic predictor, which agrees with the CV-results."
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html",
    "href": "Ch6_LinModSelectRegul.html",
    "title": "6  Linear Model Selection and Regularization",
    "section": "",
    "text": "Lecture videos for Chapter 6.3:\nIn this chapter, we revisit the linear regression model \\[\nY = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p + \\epsilon.\n\\] In Chapter Chapter 3, we focused on fitting a given linear regression model using least squares and completely ignored the model selection process. However, selecting a “good” model is itself a statistical problem which we need to solve using reliable statistical procedures. In this chapter, we consider fitting procedures that integrate the model selection process.\nWe discuss three important classes of methods:"
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#ch.-6.1-subset-selection",
    "href": "Ch6_LinModSelectRegul.html#ch.-6.1-subset-selection",
    "title": "6  Linear Model Selection and Regularization",
    "section": "(Ch. 6.1) Subset Selection",
    "text": "(Ch. 6.1) Subset Selection\nIn the following, we consider linear models with some large set of \\(p\\) many potentially relevant predictors \\(X=(X_1,\\dots,X_{p}),\\) \\[\n\\begin{align*}\nY\n&=\\mathcal{M}_{p}(X) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_{p} + \\epsilon,\n\\end{align*}\n\\tag{6.1}\\] where \\(Var(\\epsilon)=\\sigma^2,\\) and where \\(\\epsilon\\) is independent of the predictors \\(X.\\)\nLet’s assume that only \\(k\\leq p\\) many predictors in Equation 6.1 are actually relevant having non-zero slope coefficients \\(|\\beta_j|>0,\\) and let denote the index set of these relevant predictors by\n\\[\n\\mathcal{I}^*_{k^*}\\subset\\{1,2,\\dots,p\\}\\quad\\text{with}\\quad |\\mathcal{I}^*_{k^*}|=k^*\\leq p.\n\\]\nIn (linear) model selection, one often aims to select the sub-model \\[\n\\mathcal{M}_{\\mathcal{I}^*_{k^*}}(X)=\\beta_0 + \\sum_{j\\in\\mathcal{I}^*_{k^*}}\\beta_j X_j\n\\] containing all relevant predictors with coefficients \\(|\\beta_j|>0.\\)\n\n\nA first idea may be to select candidate models \\(\\mathcal{M}_{\\mathcal{I}_k}(X)\\) by maximizing the fit to the data; i.e. by maximizing the \\(R^2\\) or equivalently by minimizing the Residual Sum of Squares (RSS), \\[\n\\begin{align*}\n\\operatorname{RSS}_k&=\\sum_{i=1}^n\\left(y_i - \\widehat{\\mathcal{M}}_{\\mathcal{I}_k}(X)\\right)^2\\\\\n%R^2_p &=1-\\frac{\\operatorname{RSS}_p}{TSS}\\\\\n\\widehat{\\mathcal{M}}_{\\mathcal{I}_k}(X) & = \\hat\\beta_0 + \\sum_{j\\in\\mathcal{I}_k}\\hat\\beta_j X_j,  \n%TSS&=\\sum_{i=1}^n\\left(y_i - \\bar{y}\\right)^2.\n\\end{align*}\n\\] where \\(\\mathcal{I}_k\\) is some candidate sub-set \\(\\mathcal{I}_k\\subset\\{1,2,\\dots,p\\}\\) with \\(|\\mathcal{I}_k|=k\\leq p.\\)\nWhile this strategy can be used for selecting the best fitting model among all \\[\n\\binom{p}{k}=\\frac{p!}{k!(p-k)!}\n\\] many models with \\(k\\) parameters, it fails to select the best fitting model with different numbers of parameters. Indeed, one can show that the “fit” of the model can be always increased by adding more predictors; i.e.  \\[\n\\operatorname{RSS}_k\\geq \\operatorname{RSS}_{k'}\\quad\\text{for}\\quad k<k'.\n\\] Thus, we need alternative criteria to choose between models with different numbers of regressors \\(k\\neq k'\\).\nIn order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:\n\nWe can directly estimate the test error, using the validation set approach or cross-validation (Chapter 5).\nWe can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\n\nThe latter option is accomplished using information criteria such as, for instance, Mellow’s \\(C_p\\), Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and the adjusted \\(R^2.\\) Information criteria are particularly useful in cases, where cross-validation is computationally too expensive.\n\nMellow’s \\(C_p\\)\n\\[\nC_p \\equiv C_p(k) = \\frac{1}{n}\\left(\\operatorname{RSS}_k+2k\\hat\\sigma^2\\right),\n\\tag{6.2}\\] where \\(\\hat\\sigma^2=\\frac{1}{n-p}\\sum_{i=1}^n(y_i - \\widehat{\\mathcal{M}}_{p}(X))^2\\) is the sample variance of the residuals from the large model Equation 6.1 with all \\(p\\) predictors, and where \\(\\operatorname{RSS}_k\\) are the RSS of the estimated model \\(\\widehat{\\mathcal{M}}_{\\mathcal{I}_k}(X)\\) with \\(k\\leq p\\) predictors.\nUnder certain regularity assumptions, one can show that \\(C_p\\) is an unbiased estimate of the test MSE. The best model is the one which has the lowest \\(C_p\\) value.\n\n\nThe AIC\nThe AIC is defined for a large class of models fit by maximum likelihood. If the error terms in the linear regression model Equation 6.1 are Gaussian, maximum likelihood and least squares estimation are equivalent. In this case AIC is given by \\[\n\\operatorname{AIC} \\equiv \\operatorname{AIC}(k) = \\frac{1}{n}\\left(\\operatorname{RSS}_k + 2 k\\hat\\sigma^2\\right),\n\\] where \\(\\hat\\sigma^2\\) is defined as in Equation 6.2. The above formula for the AIC omits factors that are constant in \\(k\\) and thus irrelevant for model selection. Hence for least squares the AIC and Mellow’s \\(C_p\\) are proportional to each other, and thus lead to the same model choices.\n\n\nThe BIC\n\\[\n\\operatorname{BIC} \\equiv \\operatorname{BIC}(k) = \\frac{1}{n}\\left(\\operatorname{RSS}_k + \\log(n)2 k\\hat\\sigma^2\\right)\n\\] where \\(\\hat\\sigma^2\\) is defined as in Equation 6.2.\nSince \\(\\log(n)>2\\) for any sample size \\(n>7,\\) the BIC statistic generally places a heavier penalty on models with many predictors, and hence results in the selection of smaller models than Mellow’s \\(C_p\\) and AIC.\n\n\nThe adjusted \\(R^2\\)\n\\[\n\\operatorname{adjusted } R^2 \\equiv (\\operatorname{adjusted } R^2)(k) = 1-\\frac{\\operatorname{RSS}_k/(n-k-1)}{\\operatorname{TSS}/(n-1)},\n\\] where \\(\\operatorname{TSS}=\\sum_{i=1}^n\\left(y_i - \\bar{y}\\right)^2.\\)\nUnlike Mellow’s \\(C_p,\\) AIC, and BIC, large values of \\(\\operatorname{adjusted } R^2\\) indicate models with low test errors.\nNote that maximizing \\(\\operatorname{adjusted } R^2\\) is equivalent to minimizing \\(\\operatorname{RSS}_k/(n-k-1).\\) While \\(\\operatorname{RSS}_k\\) is a decreasing function of \\(k,\\) \\(\\operatorname{RSS}_k/(n-k-1)\\) may decrease or increase when increasing \\(k,\\) depending on the amount of RSS-reduction due to the added predictors.\nNote: Mellow’s \\(C_p,\\) AIC, and BIC have rigorous theoretical justifications. The \\(\\operatorname{adjusted } R^2\\) is not es well motivated by statistical theory.\n\n\n\n(Ch. 6.1.1) Best Subset Selection\nAlgorithm 6.1: Best Subset Selection:\n\nInitialization: Let \\(\\mathcal{M}_0\\) denote the null model \\(f(X)=\\beta_0\\) containing no predictors, except the intercept. (This model predicts each observed outcome by the total sample mean.)\nFor \\(k=1,2,\\dots,p:\\)\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\nPick the “best” among these \\(\\binom{p}{k}\\) models, and call it \\(\\widehat{\\mathcal{M}}_k\\). Here “best” is defined as having smallest \\(\\operatorname{RSS}_k\\) (or equivalently highest \\(R^2.\\))\n\nSelect a single best model from \\(\\widehat{\\mathcal{M}}_0,\\dots,\\widehat{\\mathcal{M}}_p\\) using CV, Mellow’s \\(C_p\\) (AIC), BIC, or \\(\\operatorname{adjusted } R^2.\\)\n\n\nStep 2 of Algorithm 6.1 identifies the best model (on training data) for each subset size \\(k\\), and thus reduces the model selection problem from \\(2^p\\) models to \\(p+1\\) models.\n\nBest subset selection (Algorithm 6.1) can be computationally expensive for largish \\(p\\).\n\n\n\n\\(p\\)\n\\(2^p\\)\n\n\n\n\n\\(10\\)\n\\(1024\\)\n\n\n\\(20\\)\n\\(1048576\\)\n\n\n\\(40\\)\n\\(1.1\\cdot 10^{12}\\)\n\n\n\nSummary:\n\nBest subset selection becomes computationally infeasible for values of \\(p\\) greater than about \\(40\\), even with extremely fast modern computers.\nMoreover, the larger the search space, the higher the chance of finding models that look good in the training data (Step 2), even tough they might not have any predictive power in test data.\n\n\n\n(Ch. 6.1.2) Stepwise Selection\n\nForward Stepwise Selection\nAlgorithm 6.2: Forward Stepwise Selection:\n\nInitialization: Let \\(\\mathcal{M}_0\\) denote the null model \\(f(X)=\\beta_0\\) containing no predictors, except the intercept. (This model predicts each observed outcome by the total sample mean.)\nFor \\(k=0,1,\\dots,p-1:\\)\n\nFit all \\(p-k\\) models that augment the predictors in \\(\\widehat{\\mathcal{M}}_k\\) with one additional predictor.\nPick the “best” among these \\(p-k\\) models, and call it \\(\\widehat{\\mathcal{M}}_{k+1}\\). Here “best” is defined as having smallest \\(\\operatorname{RSS}_{k+1}\\) (or equivalently highest \\(R^2.\\))\n\nSelect a single best model from \\(\\widehat{\\mathcal{M}}_0,\\dots,\\widehat{\\mathcal{M}}_p\\) using CV, Mellow’s \\(C_p\\) (AIC), BIC, or \\(\\operatorname{adjusted } R^2.\\)\n\n\nUnlike best subset selection, which involved fitting \\(2^p\\) models, forward stepwise selection considers a much smaller set of models: It begins with fitting the null \\((p=0)\\) model, and proceeds with fitting \\(p-k\\) models in the \\(k\\)th iteration, for \\(k=1,\\dots,p-1.\\) This amounts to a total of \\[\n1+\\sum_{k=0}^{p-1}(p-k) = 1 + p(p+1)/2\n\\] models.\n\n\n\n\\(p\\)\n\\(2^p\\)\n\\(1 + p(p+1)/2\\)\n\n\n\n\n\\(10\\)\n\\(1024\\)\n\\(56\\)\n\n\n\\(20\\)\n\\(1048576\\)\n\\(211\\)\n\n\n\\(40\\)\n\\(1.1\\cdot 10^{12}\\)\n\\(821\\)\n\n\n\nForward stepwise selection is a guided search strategy, that tends to do well in practice. However, it is not guaranteed to find the best possible model out of all \\(2^2p\\) models containing subsets of the \\(p\\) predictors.\nExample: Consider the case of \\(p=3\\) predictors \\(X_1,\\,X_2\\) and \\(X_3.\\) Let the best one-predictor model contain \\(X_1\\), and let the best two-predictor model contain \\(X_2\\) and \\(X_3.\\) Then forward stepwise selection will fail to select the best two-predictor model.\n\n\n\nBackward Stepwise Selection\nAlgorithm 6.3: Backward Stepwise Selection:\n\nInitialization: Let \\(\\mathcal{M}_p\\) denote the full model which contains all \\(p\\) predictors.\nFor \\(k=p,p-1,\\dots,1:\\)\n\nFit all \\(k\\) models that contain all but one of the predictors in \\(\\widehat{\\mathcal{M}}_k,\\) for a total of \\(k-1\\) predictors.\nPick the “best” among these \\(k\\) models, and call it \\(\\widehat{\\mathcal{M}}_{k-1}\\). Here “best” is defined as having smallest \\(\\operatorname{RSS}_{k+1}\\) (or equivalently highest \\(R^2.\\))\n\nSelect a single best model from \\(\\widehat{\\mathcal{M}}_0,\\dots,\\widehat{\\mathcal{M}}_p\\) using CV, Mellow’s \\(C_p\\) (AIC), BIC, or \\(\\operatorname{adjusted } R^2.\\)\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1 + p(p+1)/2\\) models, and thus can be applied when \\(p\\) is too large to apply best subset selection.\nLike forward stepwise selection, the backward selection approach is a guided search strategy, that tends to do well in practice. However, it is not guaranteed to find the best possible model out of all \\(2^2p\\) models containing subsets of the \\(p\\) predictors.\nBackward selection requires that the sample size \\(n\\) is larger than the number of predictors \\(p\\) (so that the full model can be fit).\n\n\nHybrid Approaches\nThe above selection algorithms often give similar, but not identical models. The literature knows many alternative selection algorithms such as hybrid approaches in which, for instance, variables are added to the model sequentially, in analogy to forward selection; however, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit."
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#ch.-6.2-shrinkage-methods",
    "href": "Ch6_LinModSelectRegul.html#ch.-6.2-shrinkage-methods",
    "title": "6  Linear Model Selection and Regularization",
    "section": "(Ch. 6.2) Shrinkage Methods",
    "text": "(Ch. 6.2) Shrinkage Methods\nThe discussed subset selection methods involve using least squares to fit the linear models. Alternatively, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates by shrinking the coefficient estimates towards zero.\nThe two best-known techniques for shrinking the regression coefficients towards zero are\n\nRidge Regression and\nLasso\n\n\n(Ch. 6.2.1) Ridge Regression\nRecall from Chapter 3 that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) using parameter values that minimize the RSS criterion \\[\n\\operatorname{RSS}=\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2.\n\\tag{6.3}\\]\nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. The ridge regression coefficient estimates \\(\\hat\\beta^R\\) are the values that minimize \\[\n\\begin{align*}\n\\underbrace{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2}_{=\\operatorname{RSS}}+\\lambda \\sum_{j=1}^p\\beta_j^2,\n%= &\\operatorname{RSS} +\\lambda \\sum_{j=1}^p\\beta_j^2,\n\\end{align*}\n\\tag{6.4}\\] where \\(\\lambda \\sum_{j=1}^p\\beta_j^2\\) is called shrinkage penalty, and where \\(\\lambda\\) is a tuning parameter that needs to be determined separately. Equation 6.4 trades of two different criteria:\n\nBy making RSS small, ridge regression tries to fit the data well.\nBy making \\(\\sum_{j=1}^p\\beta_j^2=||(\\beta_{1},\\dots,\\beta_{p})'||^2_2\\) small, ridge regression shrinks all coefficient estimates towards zero—except the intercept \\(\\beta_0.\\)\n\n\n\\(||\\beta||_2 = \\sqrt{\\sum_{j=1}^p\\beta_j^2}\\) denotes the \\(\\ell_2\\) (or Euclidean) norm of a \\(p\\)-dimensional vector \\(\\beta.\\) If \\(||\\beta||_2\\to 0\\) then all elements \\(\\beta_1,\\dots,\\beta_p\\) eventually approach \\(0.\\)\n\nThe tuning parameter \\(\\lambda\\) controls the relative impact of these two terms on the regression coefficient estimates:\n\nWhen \\(\\lambda=0\\), the penalty term has no effect, and ridge regression will produce the least squares estimates.\nAs \\(\\lambda\\to\\infty\\), the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.\n\nRidge regression will produce a different set of coefficient estimates, \\[\n\\hat\\beta_\\lambda^R=(\\hat\\beta_{1,\\lambda}^R,\\dots,\\hat\\beta_{p,\\lambda}^R)',\n\\] for each value of \\(\\lambda.\\) Selecting a good value for \\(\\lambda\\) is critical and can be accomplished using, for instance, cross-validation.\n\nStandardized Ridge Regression Coefficients\nThe RSS criterion of least squares (Equation 6.3) is scale equivariant. Scaling a predictor by a constant \\(c\\) from \\(x_{ij}\\) to \\(x^S_{ij}=x_{ij}c\\) will simply rescale the corresponding least squares estimate from \\(\\hat\\beta_j\\) to \\(\\hat\\beta_j^S=\\hat\\beta_j/c\\) such that \\[\nx_{ij}\\hat\\beta_j = x_{ij}^S\\hat\\beta_j^S,\\quad i=1,\\dots,n\n\\] which leaves the fitted values unchanged, and thus the RSS value unaffected.\n\nn         <- 100\nx         <- rnorm(n = n, mean = 50000, sd = 10000)\neps       <- rnorm(n = n, mean = 0,     sd = 10000)\ny         <- 2 + 5 * x + eps\n##\nlm_obj_1  <- lm(y ~ x)\nRSS_1     <- sum(resid(lm_obj_1)^2)\n##\nc         <- 1/1000 # scaling factor \nx_S       <- x * c\nlm_obj_2  <- lm(y ~ x_S)\nRSS_2     <- sum(resid(lm_obj_2)^2)\n##\n\n## comparing the estimates \nround(\nc(coef(lm_obj_1)[2] / c, \n  coef(lm_obj_2)[2]),\ndigits = 1)\n\n     x    x_S \n4842.2 4842.2 \n\n## comparing the RSS's:\nc(RSS_1, RSS_2)\n\n[1] 9637038969 9637038969\n\n\nBy contrast, the ridge regression criterion in Equation 6.4 is not scale invariant since scaling the predictors will lead (as seen above) to a rescaling of the coefficient estimates and thus affects the penalty term \\(\\lambda\\sum_{j=1}^p\\beta_j^2.\\)\nThus, scaling one or more predictors will generally affect all ridge regression estimates \\(\\hat\\beta_{0,\\lambda}^R,\\dots,\\hat\\beta_{p,\\lambda}^R,\\) since typically the regressors are correlated with each other.\nTherefore, it is best practice to apply ridge regression after standardizing the predictors, using the formula \\[\n\\tilde{x}_{ij}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}\n\\tag{6.5}\\] so that they are all on the same scale; i.e. all standardized predictors have a standard deviation of one.\nAfter standardizing the predictors, the ridge regression coefficient estimates do not depend on the different scales on which the predictors were measured.\nRidge regression estimates based on standardized predictors are called standardized ridge regression coefficients. The absolute values of the standardized coefficients allow us to rank the effects of the predictors on the depend variable: the predictor corresponding to the largest (in absolute values) standardized coefficient has the largest effect on the dependent variable.\nFigure 6.4 shows the standardized ridge regression coefficient estimates for the Credit dataset. Each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\n\nInterpretation of Figure 6.4:\n\nThe case, where \\(||\\hat\\beta^R_\\lambda||_2/||\\hat\\beta||_2=1\\), i.e. where \\(\\lambda\\approx 0\\), is equivalent to the least squares fit.\nThe case where \\(||\\hat\\beta^R_\\lambda||_2/||\\hat\\beta||_2=0\\), i.e. where \\(\\lambda\\) is very large, corresponds to the case where \\(\\hat\\beta^R_{\\lambda 1}\\approx\\dots\\approx \\hat\\beta^R_{\\lambda p}\\approx 0.\\)\n\n\n\n\nWhy and When Does Ridge Regression Improve Over Least Squares?\nRidge regression’s potential advantage over least squares is rooted in the bias-variance trade-off.\n\nA large value of \\(\\lambda\\) decreases the flexibility of the model, and thus decreases variance, but increases bias.\nA small value of \\(\\lambda\\) increases the flexibility of the model, and thus decreases bias, but increases variance.\n\n\nHigh variance means that a small change in the training data can cause a large change in the coefficient estimates\n\n\nLarge bias means that the coefficient estimates are on average not equal to the true coefficient values\n\nFigure 6.5 illustrates this, using a simulated data set containing \\(p = 45\\) predictors and \\(n = 50\\) observations. When the number of variables \\(p\\) is almost as large as the number of observations \\(n\\), as in the example in Figure 6.5, the least squares estimates will be extremely variable.\n\nIn comparison to the least squares fit (\\(\\lambda\\approx 0\\), \\(||\\hat\\beta^R_\\lambda||_2/||\\hat\\beta||_2=1\\)), the ridge regression shows a lower test MSE for an appropriate choice of \\(\\lambda.\\)\nGenerally, ridge regression outperforms least squares in situations where the least squares estimates have high variance—as in the example of Fig 6.5. In these high variance situations, ridge regression can trade off a small increase in bias for a large decrease in variance leading to an overall reduction in the test MSE.\nIf \\(p > n\\), then the least squares estimates do not have a unique solution, but ridge regression estimates are well defined.\nMoreover, ridge regression also has substantial computational advantages over best subset selection, which requires searching through \\(2^p\\) fitted models. In contrast, ridge regression models only need to be fitted once for each candidate tuning parameter \\(\\lambda.\\)"
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#ch.-6.2.2-the-lasso",
    "href": "Ch6_LinModSelectRegul.html#ch.-6.2.2-the-lasso",
    "title": "6  Linear Model Selection and Regularization",
    "section": "(Ch. 6.2.2) The Lasso",
    "text": "(Ch. 6.2.2) The Lasso\nWhile ridge regression is able to show which predictors are of major vs. minor relevance for predicting the outcome, none of the predictors is actually removed from the model equation since none of the coefficients is exactly set to zero—unless in the limit \\(\\lambda = \\infty,\\) but then coefficients are zero (no variable selection).\nThis may not be a problem with respect to prediction accuracy, but it can create a challenge in model interpretation—particularly in settings in which the number of variables \\(p\\) is quite large.\nFor example, in the Credit data set, it appears that the most important variables are income, limit, rating, and student. So we might wish to build a model including just these predictors. However, ridge regression will always generate a model involving all ten predictors.\nThe lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat\\beta_\\lambda^L\\), minimize \\[\n\\begin{align*}\n\\underbrace{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2}_{=\\operatorname{RSS}}+\\lambda \\sum_{j=1}^p|\\beta_j|.\n%= &\\operatorname{RSS} +\\lambda \\sum_{j=1}^p\\beta_j^2,\n\\end{align*}\n\\tag{6.6}\\]\nBy contrast to the ridge regression criterion (Equation 6.4), the lasso criterion uses an \\(\\ell_1\\) norm \\[\n||(\\beta_{1},\\dots,\\beta_p)'||_1 = \\sum_{j=1}^p|\\beta_j|\n\\] as a shrinkage penalty. The \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nLasso yields sparse models—that is, models that involve only a subset of the variables (see Figure 6.6). Sparse models are easier to interpret.\n\n\nComparing Lasso, Ridge Regression and Best Subset Selection\nOne can show that the lasso and ridge regression coefficient estimates solve constrained optimization (minimization) problems.\nLasso: \\[\n\\min_{\\beta}\\left\\{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right\\}\\;\\text{s.t.}\\; \\sum_{j=1}^p|\\beta_j|\\leq s\n\\tag{6.7}\\]\nRidge: \\[\n\\min_{\\beta}\\left\\{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right\\}\\;\\text{s.t.}\\; \\sum_{j=1}^p\\beta_j^2\\leq s\n\\tag{6.8}\\]\nThat is, for every \\(\\lambda\\) in Equation 6.4 there is some \\(s\\) such in Equation 6.8 that will lead to the same ridge coefficient estimates, and for every \\(\\lambda\\) in Equation 6.6 there is some \\(s\\) such in Equation 6.7 that will lead to the same lasso coefficient estimates.\nIn the case of two \\((p=2)\\) predictors, one can interpret Equation 6.7 and Equation 6.8 graphically (see Figure 6.7).\n\nthe lasso coefficient estimates have the smallest RSS out of all points \\((\\beta_1,\\beta_2)\\) that lie within the diamond defined by \\(|\\beta_1|+|\\beta_2|\\leq s.\\)\nthe ridge coefficient estimates have the smallest RSS out of all points \\((\\beta_1,\\beta_2)\\) that lie within the circle defined by \\(\\beta_1^2+\\beta_2^2\\leq s.\\)\n\n\nInterpretation of Figure 6.7:\n\nThe tuning parameter \\(s\\) acts like a budge constraint for how large the penalty term can be. This key idea also applies to cases \\(p>2,\\) although plotting then becomes difficult/infeasible.\nFigure 6.7 shows why lasso is able to perform variable selection, by contrast to ridge regression. The ellipses show coefficient estimates leading to equal RSS values. While the \\(\\ell_1\\) geometry of lasso’s “budge constraint” allows to set a coefficient value to zero (here \\(\\beta_1\\)), the \\(\\ell_2\\) geometry of ridge regression’s budget constraint only allows to shrink a coefficient value towards zero.\n\n\n\nConnection with Best Subset Selection\nEquation 6.7 and Equation 6.8 reveal a close connection between the lasso, ridge regression, and best subset selection. Consider the problem \\[\n\\min_{\\beta}\\left\\{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right\\}\\;\\text{s.t.}\\; \\sum_{j=1}^pI(\\beta_j\\neq 0)\\leq s,\n\\tag{6.9}\\] where \\(I(\\beta_j\\neq 0)=1\\) if \\(\\beta_j\\neq 0\\) and zero else.\nEquation 6.9 amounts to finding a set of at most \\(s\\) many coefficient estimates such that RSS is as small as possible. The problem of Equation 6.9 is equivalent to Step 2 in Best Subset Selection (Algorithm 6.1) for a given number of predictors \\(s\\) (or \\(k\\) in the notation of Algorithm 6.1). The choice of \\(s\\) (or equivalently \\(\\lambda\\)) is then critical and needs to be done using, for instance, cross-validation.\nThis insight allows us to interpret ridge regression and lasso (even more so) as computationally feasible versions of Best Subset Regression.\n\n\nComparing the Lasso and Ridge Regression\nFigure 6.8 displays the variance, squared bias, and test MSE of the lasso applied to the same simulated data as in Figure 6.5; i.e. \\(p=45\\) and \\(n=50.\\) Thus, in the case, where many predictors have an effect on the response, ridge regression can perform better than lasso.\n\nFigure 6.9 illustrates a similar situation, except that now the response is a function of only \\(p=2\\) out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.\n\nIn general, one might expect the lasso to perform better in sparse models, i.e. in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or equal zero. Ridge regression will perform better when the response is a function of many predictors.\n\n\n(Ch. 6.2.3) Selecting the Tuning Parameter\nEach value of the tuning parameters \\(\\lambda\\) in Equation 6.4 and Equation 6.6 or of the tuning parameters \\(s\\) in Equation 6.7 and Equation 6.8 represent a new more or less flexible model—similarly to the tuning parameter \\(k\\), with \\(k\\leq p\\), (i.e., number of predictors included) in Best Subset Selection. Thus, as in Best Subset Selection we need to be carful when comparing models of different size and flexibility.\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the coss-validation error for each value of \\(\\lambda\\), as described in Chapter 5.\nWe then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#ch.-6.3-dimension-reduction-methods",
    "href": "Ch6_LinModSelectRegul.html#ch.-6.3-dimension-reduction-methods",
    "title": "6  Linear Model Selection and Regularization",
    "section": "(Ch. 6.3) Dimension Reduction Methods",
    "text": "(Ch. 6.3) Dimension Reduction Methods\nThe methods discussed so far in this chapter are all defined using the total set of predictors \\(X_1,X_2,\\dots,X_p\\) in Equation 6.1. We now explore a class of approaches that transform the set of \\(p\\) predictors to a reduced set of \\(M\\leq p\\) transformed variables, and then fit a least squares model using only the \\(M\\) transformed variables as predictors.\nLet \\(Z_1,\\dots,Z_M\\) represent \\(M\\leq p\\) linear combinations of our original \\(p\\) predictors, i.e. \\[\nZ_m = \\sum_{j=1}^p\\phi_{jm}X_{j},\\quad\\text{for}\\quad m=1,\\dots,M.\n\\tag{6.10}\\]\nGiven the new predictors \\(Z_1,\\dots,Z_M\\), we can fit the linear regression model \\[\ny_i = \\theta_0 +  \\sum_{m=1}^M\\theta_m z_{im} + \\epsilon_i,\\quad i=1,\\dots,n,\n\\tag{6.11}\\] where \\(z_{im}\\), \\(i=1,\\dots,n\\), are observed measurements from \\(Z_m,\\) based on observations \\(x_{ij}\\) from \\(X_j\\), and where \\(\\theta_0,\\dots,\\theta_M\\) are the (unknown) regression coefficients that we estimate using least squares.\nIf the constants \\(\\phi_{jm}\\) in Equation 6.10 are chosen wisely, then such dimension reduction (from \\(p\\) to \\(M\\)) approaches can outperform (lower test MSE) least squares regression.\nNotice that the linear combination of the \\(M\\) transformed predictors, \\(z_{i1},\\dots,z_{iM}\\), in Equation 6.11 can be rewritten as a linear combination of the \\(p\\) original predictors, \\(x_{i1},\\dots,x_{ip}\\): \\[\n\\begin{align*}\n\\sum_{m=1}^M\\theta_m z_{im}\n&= \\sum_{m=1}^M\\theta_m \\overbrace{\\left(\\sum_{j=1}^p\\phi_{jm}x_{ij}\\right)}^{z_{im}}\\\\\n&= \\sum_{j=1}^p\\underbrace{\\left(\\sum_{m=1}^M\\theta_m \\phi_{jm}\\right)}_{=\\beta^M_j}x_{ij}.\n\\end{align*}\n\\] Hereby, \\[\n\\beta^M_j = \\sum_{m=1}^M\\theta_m \\phi_{jm}\n\\tag{6.12}\\] serves as a certain regularized version of the coefficients \\(\\beta_j\\) in the original linear model Equation 6.1.\n\nIf \\(M=p\\) (no dimension reduction), and if all \\(M\\) many \\((z_{1m},\\dots,z_{nm})',\\) \\(m=1,\\dots,M,\\) vectors are linearly independent from each other (i.e. no redundant vectors), then Equation 6.12 poses no constraints and \\(\\beta_j^M=\\beta_j.\\)\nIf \\(M<p\\) (dimension reduction), Equation 6.12 serves to constrain the estimated \\(\\beta_j\\) coefficients \\((\\beta_j^M\\neq \\beta_j).\\)\n\nAll dimension reduction methods work in two steps.\n\nThe transformed predictors \\(Z_1,Z_2,\\dots,Z_M\\) are obtained.\nThe model is fit using these \\(M\\leq p\\) predictors.\n\nHowever, the choice of \\(Z_1,Z_2,\\dots,Z_M,\\) or equivalently the selection of the \\(\\phi_m\\)’s, can be achieved in different ways.\nThe best known dimension reduction approach is principal components regression.\n\n\n(Ch. 6.3.1) Principal Components Regression\nPrincipal components regression regression uses principal components to derive new (low dimensional) predictors. Thus, in a first step we need to discuss principal components analysis (PCA) to construct principal components.\n\nPrincipal Component Analysis (PCA)\nReading: Chapter 12.2.1 of our textbook.\nPrincipal components analysis is a popular approach for deriving a low-dimensional \\((M<p)\\) set of features \\(Z_1,Z_2,\\dots,Z_M\\) from a large set of variables \\(X_1,X_2,\\dots,X_p.\\)\nThe first principal component (or better principal component score) of a set of variables \\(X_1,X_2,\\dots,X_p\\) is the normalized linear combination \\[\nZ_1 = \\phi_{11}X_{1} + \\dots + \\phi_{p1}X_{p},\n\\] that maximizes the variance of \\(Z_1.\\) By normalized, we mean that the \\(\\ell_2\\) norm of \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) must equal one, i.e. \\[\n\\sum_{j=1}^p\\phi_{j1}^2=||\\phi_1||_2^2=1;\n\\] otherwise we could make the variance of \\(Z_1\\) arbitrarily large by simply by choosing \\(\\phi_1\\) such that \\(||\\phi_1||_2^2\\) is large.\nThe vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) is called the principal component loadings vector. An in absolute values large loading parameter \\(\\phi_{1j}\\) means that the \\(j\\)th predictor \\(X_j\\) contributes much to the first principal component score \\(Z_1.\\)\nTo estimate the coefficients \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})',\\) we need observed measurements of the features \\(X_1,\\dots,X_p.\\) We collect these observed measurements in a \\(n\\times p\\) dimensional data matrix \\(\\mathbf{X},\\) such that \\[\n\\mathbf{X} = \\left[\n    \\begin{matrix}\n    x_{11}&x_{12}&\\dots&x_{1p}\\\\\n    \\vdots&\\vdots&\\vdots&\\vdots\\\\\n    x_{n1}&x_{n2}&\\dots&x_{np}\\\\\n    \\end{matrix}\n\\right],\n\\] where the columns in \\(\\mathbf{X}\\) are centered. That is, the \\(j\\)th data column in \\(X\\) consists of centered data points\n\\[x_{ij}=x^{orig}_{ij}-\\bar{x}^{orig}_j\n\\quad\\text{with}\\quad\n\\bar{x}^{orig}_j = \\frac{1}{n}\\sum_{i=1}^nx_{ij}^{orig}\n\\] such that the sample mean of each column in \\(X\\) is zero, i.e. \\[\n\\bar{x}_{j} = \\frac{1}{n}\\sum_{i=1}^nx_{ij}=0\n\\] for each \\(j=1,\\dots,p.\\)\nWith centered observations \\(x_{ij}\\), the observed linear combinations \\[\nz_{i1} = \\phi_{11}x_{i1} + \\dots + \\phi_{p1}x_{ip},\n\\] become centered too, since \\[\n\\begin{align*}\n\\bar{z}_1\n&=\\frac{1}{n}\\sum_{i=1}^nz_{i1} \\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\phi_{11}x_{i1} + \\dots + \\phi_{p1}x_{ip}\\right)\\\\\n&= \\phi_{11} \\frac{1}{n}\\sum_{i=1}^n x_{i1} + \\dots + \\phi_{p1} \\frac{1}{n}\\sum_{i=1}^nx_{ip}\\\\\n& = \\phi_{11}\\bar{x}_{1} + \\dots + \\phi_{p1}\\bar{x}_{p} = 0.\n\\end{align*}\n\\]\nTherefore, the formula for the sample variance of \\(z_{i1}\\), \\(i=1,\\dots,n\\), simplifies as following: \\[\n\\frac{1}{n}\\sum_{i=1}^n\\left(z_{i1}-\\bar{z}_1\\right)^2=\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2\n\\tag{6.13}\\]\nTo determine the first principal component scores \\[\nz_{11},\\dots,z_{n1},\n\\] we need to find that loading vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) that maximizes the sample variance \\[\n\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(\\phi_{11}x_{i1} + \\dots + \\phi_{p1}x_{ip}\\right)^2\n\\] subject to the side constraint that \\(||\\phi_1||_2^2=1.\\) In other words, the first principal component loading vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) is determined as the solution of the optimization problem \\[\n\\max_{\\phi_{11},\\dots,\\phi_{p1}}\\underbrace{\\left\\{\\frac{1}{n}\\sum_{i=1}^n\\left(\\sum_{j=1}^p\\phi_{j1}x_{ij}\\right)^2\\right\\}}_{=\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2}\\quad\\text{s.t.}\\quad\\sum_{j=1}^p\\phi_{j1}^2=1,\n\\] where \\(\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2\\) equals the sample variance of \\(z_{11},\\dots,z_{n1}\\) (see Equation 6.13).\nThere is a nice geometric interpretation for the first principal component (likewise for the further principal components). The first loading vector \\[\n\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\n\\] defines a direction vector in the feature space along which the data points vary the most. If we project each of the \\(n\\) many \\(p\\)-dimensional data points \\[\n\\begin{align*}\nx_1 &=(x_{11},\\dots,x_{1p})'\\\\\n    &\\; \\vdots \\\\\nx_n &=(x_{n1},\\dots,x_{np})'\n\\end{align*}\n\\] onto the direction vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\), the projected values are the principal component scores \\(z_{11},\\dots,z_{n1}\\) themselves, i.e \\[\n\\begin{align*}\nz_{11} & = \\phi_1'x_1 = \\sum_{j=1}^p \\phi_{j1}x_{1j}\\\\\n&\\;\\;\\;\\vdots \\\\\nz_{n1} & = \\phi_1'x_n = \\sum_{j=1}^p \\phi_{j1}x_{nj}.\n\\end{align*}\n\\]\nThe case \\(p=2\\) is simple to visualize. Figure 6.14 displays the direction of the first principal component loading vector \\(\\phi_1\\) (green solid line) on an advertising data set. \nThe left panel in Figure 6.15 displays the projection of the \\(i\\)th, \\(i=1,\\dots,n,\\) data vectors \\(x_i=(x_{i1},x_{i2})'\\) (purple circles 🟣) onto the first loading vector \\(\\phi_1=(\\phi_{11},\\phi_{21})'\\) leading to the \\(i\\)th principal component score \\(z_{i1},\\) \\(i=1,\\dots,n,\\) (black crosses \\(\\mathbf{\\times}\\).\n\n\n\nHigher order principal components\nAfter the first principal component of the features has been determined, we can find the second principal component. The second principal component is the linear combination of \\(X_1,X_2,\\dots,X_p\\) that has maximal variance out of all linear combinations that are uncorrelated with \\(Z_1.\\)\nThus, the second principal component scores \\(z_{12},\\dots,z_{np}\\) take the form \\[\nz_{i2} = \\phi_2'x_i = \\sum_{j=1}^p \\phi_{j2}x_{ij},\n\\] where \\(\\phi_2=(\\phi_{12},\\dots,\\phi_{p2})'\\) is the second principal component loading vector with \\(||\\phi_2||_2^2=1.\\)\nConstraining \\(Z_2\\) to be uncorrelated with \\(Z_1\\) is equivalent to constraining \\(\\phi_2\\) to be orthogonal to \\(\\phi_1\\), i.e. the inner product of \\(\\phi_1\\) and \\(\\phi_2\\) must be zero, \\[\n\\phi_2'\\phi_1=\\sum_{j=1}^p\\phi_{j2}\\phi_{j1}=0.\n\\] Therefore, the loading vector \\(\\phi_2=(\\phi_{12},\\dots,\\phi_{p2})'\\) is determined by the solution of \\[\n\\begin{align*}\n\\max_{\\phi_{12},\\dots,\\phi_{p2}}&\\underbrace{\\left\\{\\frac{1}{n}\\sum_{i=1}^n\\left(\\sum_{j=1}^p\\phi_{j2}x_{ij}\\right)^2\\right\\}}_{=\\frac{1}{n}\\sum_{i=1}^nz_{i2}^2}\\\\\n\\text{such that}& \\quad \\sum_{j=1}^p\\phi_{j2}^2=1\\quad\\\\\n\\text{and}& \\quad \\sum_{j=1}^p\\phi_{j1}\\phi_{j2}=0.\n\\end{align*}\n\\] Correspondingly, the \\(m\\)th \\((m=1,\\dots,M)\\) loading vector \\(\\phi_m=(\\phi_{1m},\\dots,\\phi_{pm})'\\) is determined by the solution of \\[\n\\begin{align*}\n\\max_{\\phi_{1m},\\dots,\\phi_{pm}}&\\underbrace{\\left\\{\\frac{1}{n}\\sum_{i=1}^n\\left(\\sum_{j=1}^p\\phi_{jm}x_{ij}\\right)^2\\right\\}}_{=\\frac{1}{n}\\sum_{i=1}^nz_{im}^2}\\\\\n\\text{such that}&\\quad\\sum_{j=1}^p\\phi_{jm}^2=1\\quad\\\\\n\\text{and}&\\quad \\sum_{j=1}^p\\phi_{j\\ell}\\phi_{jm}=0\\quad\\text{for all}\\quad 0\\leq \\ell<m,\n\\end{align*}\n\\] with \\(\\phi_0=(\\phi_{10},\\dots,\\phi_{p0})'=(0,\\dots,0)'.\\)\nAgain, the case \\(p=2\\) is simple to visualize. If \\(p=2\\) there are only \\(M=2\\) principal components. As shown in Figure 6.14, the second principal components loading vector \\(\\phi_2\\) (blue dashed line) is orthogonal to the first loading vector \\(\\phi_1\\) (green solid line). Thus when rotating the coordinate system such that the direction of the first loading vector becomes the x-axis, then the direction of the second loading vector becomes the the orthogonal y-axis (see right panel of Figure 6.15).\n\n\n\nThe Principal Components Regression Approach\nThe principal components regression (PCR) approach fits the model in Equation 6.11 which we repeat here for convenience: \\[\ny_i = \\theta_0 +  \\sum_{m=1}^M\\theta_m z_{im} + \\epsilon_i,\n\\] where \\(z_{im}\\) denotes the \\(i\\)th \\((i=1,\\dots,n)\\) observation of the \\(m\\)th \\((m=1,\\dots,M)\\) principal component score, and where the number of principal components \\(M\\) acts as a tuning parameter.\nThe key idea is that often a small number \\(M\\ll p\\) of principal components suffice to explain most of the variability in the data \\(\\mathbf{X}.\\) Therefore, often a relatively low number of principal components \\(M\\ll p\\) suffices to achieve very good predictions (low test MSE).\nGenerally, fitting a least squares model to \\(z_{i1},\\dots,z_{iM},\\) \\(i=1,\\dots,n,\\) leads to better results than fitting a least squares model to \\(x_{i1},\\dots,x_{ip},\\) particularly when \\(p\\approx n,\\) since typically most of the information in the data \\(\\mathbf{X}\\) that relates to the response is contained in \\(z_{i1},\\dots,z_{iM},\\) \\(i=1,\\dots,n,\\) and by estimating only \\(M\\ll p\\) coefficients we can mitigate overfitting (high variance).\nFigure 6.18 displays the PCR fits on the simulated data sets from Figures 6.8 and 6.9.\n\nRecall that both data sets were generated using \\(n = 50\\) observations and \\(p = 45\\) predictors. However, while the response in the first data set was a function of all the \\(p=45\\) predictors (Figure 6.8), the response in the second data set was generated using only \\(p=2\\) of the predictors (Figure 6.9).\n\n\nThe curves in Figure 6.18 are plotted as a function of \\(M,\\) the number of principal components used as predictors in the regression model.\n\nFor \\(M = p\\), PCR is equivalent to a classic least squares fit using all of the original predictors\nlarge \\(M\\) values lead to flexible models with low bias, but large variance\nsmall \\(M\\) values lead to inflexible models with large bias, but low variance\n\nThe latter two points lead to the typical U-shape of the test MSE.\nFigure 6.18 indicates that PCR based on an appropriate choice of \\(M\\) can substantially reduce the test MSE in comparison to classic least squares regression. In PCR, the number of principal components, \\(M,\\) is typically chosen by cross-validation.\nStandardize the predictor variables. When performing PCR, it is generally a good idea to standardize each predictor, using Equation 6.5, prior to generating the principal components. Standardization ensures that all variables are on the same scale. In the absence of standardization, the high-variance predictors will tend to play a larger role in the principal components obtained, and thus the scale on which the variables are measured will ultimately have an effect on the final PCR model."
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#r-lab-linear-model-selection-and-regularization",
    "href": "Ch6_LinModSelectRegul.html#r-lab-linear-model-selection-and-regularization",
    "title": "6  Linear Model Selection and Regularization",
    "section": "6.1 R-Lab: Linear Model Selection and Regularization",
    "text": "6.1 R-Lab: Linear Model Selection and Regularization\n\n6.1.1 Subset Selection Methods\n\n6.1.1.1 Best Subset Selection\nHere we apply the best subset selection approach to the Hitters data. We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.\nFirst of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements.\n\nlibrary(ISLR2) # load library 'ISLR2' (contains the data)\nnames(Hitters) # check variable names of the Hitters data set \n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)   # check sample size n and number of predictors p\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary)) # number of missing Salary observations\n\n[1] 59\n\n\nHence we see that Salary is missing for \\(59\\) players. The na.omit() function removes all of the rows that have missing values in any variable.\n\nHitters <- na.omit(Hitters) # remove all rows containing missing data points \ndim(Hitters)                # check sample size n and number of predictors p\n\n[1] 263  20\n\nsum(is.na(Hitters))         # no missing values anymore \n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.\n\nlibrary(leaps)\nregfit_full <- regsubsets(Salary ~ ., \n                          nvmax  = 8,           # largest number of predictors\n                          nbest  = 1,           # number of subsets of each size to record\n                          method = \"exhaustive\",# Best Subset Selection\n                          data   = Hitters)\nsummary(regfit_full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., nvmax = 8, nbest = 1, method = \"exhaustive\", \n    data = Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nAn asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Next, we fit up to a 19-variable model.\n\nregfit_full <- regsubsets(Salary ~ ., \n                          nvmax  = 19,          # largest number of predictors\n                          nbest  = 1,           # number of subsets of each size to record\n                          method = \"exhaustive\",# Best Subset Selection\n                          data   = Hitters)\nreg_summary <- summary(regfit_full)\n\nThe summary() function also returns \\(R^2\\), RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC. We can examine these to try to select the best overall model.\n\nnames(reg_summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nFor instance, we see that the \\(R^2\\) statistic increases from \\(32\\,\\%\\), when only one variable is included in the model, to almost \\(55\\,\\%\\), when all variables are included. That is, as expected, the \\(R^2\\) statistic increases monotonically as more variables are included. (Equivalently, RSS decreases monotonically as more variables are includes.)\n\nreg_summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nPlotting RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC for all of the models at once will help us decide which model to select. Note the type = \"l\" option tells R to connect the plotted points with lines.\n\npar(mfrow = c(1, 2))\nplot(reg_summary$rss, xlab = \"Number of Variables\",\n    ylab = \"RSS\", type = \"l\")\nplot(reg_summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\n\n\n\n\nThe points() command works like the plot() command, except that it puts points on a plot that has already been created, instead of creating a new plot. The which.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted \\(R^2\\) statistic.\n\nwhich.max(reg_summary$adjr2)\n\n[1] 11\n\nplot(reg_summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\npoints(11, reg_summary$adjr2[11], col = \"red\", cex = 2, \n    pch = 20)\n\n\n\n\nIn a similar fashion we can plot the \\(C_p\\) and BIC statistics, and indicate the models with the smallest statistic using which.min().\n\nplot(reg_summary$cp, xlab = \"Number of Variables\",\n    ylab = \"Cp\", type = \"l\")\nwhich.min(reg_summary$cp)\n\n[1] 10\n\npoints(10, reg_summary$cp[10], col = \"red\", cex = 2,\n    pch = 20)\n\n\n\nwhich.min(reg_summary$bic)\n\n[1] 6\n\nplot(reg_summary$bic, xlab = \"Number of Variables\",\n    ylab = \"BIC\", type = \"l\")\npoints(6, reg_summary$bic[6], col = \"red\", cex = 2,\n    pch = 20)\n\n\n\n\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, \\(C_p\\), adjusted \\(R^2\\), or AIC. To find out more about this function, type ?plot.regsubsets. The grey-shading represents a color-code for the selected information criterion.\n\nplot(regfit_full, scale = \"r2\")\n\n\n\nplot(regfit_full, scale = \"adjr2\")\n\n\n\nplot(regfit_full, scale = \"Cp\")\n\n\n\nplot(regfit_full, scale = \"bic\")\n\n\n\n\nThe top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to \\(-150\\). However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts.\nWe can use the coef() function to see the coefficient estimates associated with this model. The argument id selects the model with the id-many best predictors. \n\ncoef(regfit_full, id = 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n6.1.1.2 Forward and Backward Stepwise Selection\nWe can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method = \"forward\" or method = \"backward\".\n\nregfit.fwd <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\nregfit.bwd <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\nFor instance, we see that using forward stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.\n\ncoef(regfit_full, id = 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd,  id = 7)\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\ncoef(regfit.bwd,  id = 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n6.1.1.3 Choosing Among Models Using the Validation-Set Approach and Cross-Validation\n\nValidation-Set Approach\nWe just saw that it is possible to choose among a set of models of different sizes using \\(C_p\\), BIC, and adjusted \\(R^2\\). We will now consider how to do this using the validation set and cross-validation approaches.\nIn order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting—including variable selection. Therefore, the determination of which model of a given size is best must be made using only the training observations. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\nIn order to use the validation set approach, we begin by splitting the observations into a training set and a test set. We do this by creating a random vector, train, of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. The vector test has a TRUE if the observation is in the test set, and a FALSE otherwise. Note the ! in the command to create test causes TRUEs to be switched to FALSEs and vice versa. We also set a random seed so that the user will obtain the same training/test set split.\n\nset.seed(1)\ntrain <- sample(x       = c(TRUE, FALSE), \n                size    = nrow(Hitters),\n                replace = TRUE)\ntest  <- (!train)\n\nNow, we apply regsubsets() to the training set in order to perform best subset selection.\n\nregfit.best <- regsubsets(Salary ~ .,\n                          data  = Hitters[train, ], \n                          nvmax = 19)\n\nNotice that we subset the Hitters data frame directly in the call in order to access only the training subset of the data, using the expression Hitters[train, ]. We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.\n\ntest.mat <- model.matrix(Salary ~ ., data = Hitters[test, ])\n\nThe model.matrix() function is used in many regression packages for building an \\(X\\) matrix from data. Now we run a loop, and for each size i, we extract the coefficients from regfit.best for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.\n\nval.errors <- rep(NA, 19)\nfor (i in 1:19){\n coefi         <- coef(regfit.best, id = i)\n pred          <- test.mat[, names(coefi)] %*% coefi\n val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)\n}\n\nWe find that the best model is the one that contains seven variables.\n\nval.errors\n\n [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4\n [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4\n[17] 141767.4 142339.6 142238.2\n\nwhich.min(val.errors)\n\n[1] 7\n\ncoef(regfit.best, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\nThis was a little tedious, partly because there is no predict() method for regsubsets(). Since we will be using this function again, we can capture our steps above and write our own predict method.\n\n predict.regsubsets <- function(object, newdata, id, ...) {\n    ## extract regression formula used in regsubsets()\n    form  <- as.formula(object$call[[2]])\n    ## build up the model matrix X of all predictors\n    mat   <- model.matrix(form, newdata)\n    ## extract the coefficients of the best model with 'id' many predictors\n    coefi <- coef(object, id = id)\n    ## ... and the names of those coefficients\n    xvars <- names(coefi)\n    ## compute the predicted values \n    mat[, xvars] %*% coefi\n }\n\nOur function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to regsubsets(). We demonstrate how we use this function below, when we do cross-validation.\nFinally, we perform best subset selection on the full data set, and select the best seven-variable model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates. Note that we perform best subset selection on the full data set and select the best seven-variable model, rather than simply using the variables that were obtained from the training set, because the best seven-variable model on the full data set may differ from the corresponding model on the training set.\n\nregfit.best <- regsubsets(Salary ~ ., \n                          data  = Hitters,\n                          nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\nIn fact, we see that the best seven-variable model on the full data set has a different set of variables than the best seven-variable model on the training set.\n\n\nCross-Validation\nWe now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy. First, we create a vector that allocates each observation to one of \\(k=10\\) folds, and we create a matrix in which we will store the results.\n\nset.seed(1)\n\nk         <- 10\nn         <- nrow(Hitters)\nfolds     <- sample(rep(1:k, length = n))\ncv.errors <- matrix(data     = NA, \n                    nrow     = k, \n                    ncol     = 19,\n                    dimnames = list(NULL, paste(1:19)))\n\nNow we write a for loop that performs cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors. Note that in the following code R will automatically use our predict.regsubsets() function when we call predict() because the best.fit object has class regsubsets.\n\nfor (j in 1:k) {\n  best.fit <- regsubsets(Salary ~ .,\n                         data = Hitters[folds != j, ],\n                         nvmax = 19)\n  for (i in 1:19) {\n    pred            <- predict.regsubsets(best.fit, \n                                 Hitters[folds == j, ], \n                                 id = i)\n    cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2)\n   }\n }\n\nThis has given us a \\(10 \\times 19\\) matrix of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\nmean.cv.errors <- apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\npar(mfrow = c(1, 1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\nWe see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.\n\nreg.best <- regsubsets(Salary ~ ., \n                       data  = Hitters,\n                       nvmax = 19)\ncoef(reg.best, 10)\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 \n        CRBI       CWalks    DivisionW      PutOuts      Assists \n   0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680 \n\n\n\n\n\n\n6.1.2 Ridge Regression and the Lasso\nWe will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far in this book. In particular, we must pass in an x matrix as well as a y vector, and we do not use the {} syntax. We will now perform ridge regression and the lasso in order to predict Salary on the Hitters data. Before proceeding ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nx <- model.matrix(Salary ~ ., Hitters)[, -1]\ny <- Hitters$Salary\n\nThe model.matrix() function is particularly useful for creating x; not only does it produce a matrix corresponding to the \\(19\\) predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs.\n\n6.1.2.1 Ridge Regression\nThe glmnet() function has an alpha argument that determines what type of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit. We first fit a ridge regression model.\n\nlibrary(\"glmnet\")\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-4\n\ngrid      <- 10^seq(10, -2, length = 100)\nridge.mod <- glmnet(x, y, \n                    alpha  = 0, \n                    lambda = grid)\n\nBy default the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. However, here we have chosen to implement the function over a grid of values ranging from \\(\\lambda=10^{10}\\) to \\(\\lambda=10^{-2}\\), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of \\(\\lambda\\) that is not one of the original grid values. Note that by default, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize = FALSE.\nAssociated with each value of \\(\\lambda\\) is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a \\(20 \\times 100\\) matrix, with \\(20\\) rows (one for each predictor, plus an intercept) and \\(100\\) columns (one for each value of \\(\\lambda\\)).\n\ndim(coef(ridge.mod))\n\n[1]  20 100\n\n\nWe expect the coefficient estimates to be much smaller, in terms of \\(\\ell_2\\) norm, when a large value of \\(\\lambda\\) is used, as compared to when a small value of \\(\\lambda\\) is used. These are the coefficients when \\(\\lambda=11498\\), along with their \\(\\ell_2\\) norm:\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\nsqrt(sum(coef(ridge.mod)[-1, 50]^2))\n\n[1] 6.360612\n\n\nIn contrast, here are the coefficients when \\(\\lambda=705\\), along with their \\(\\ell_2\\) norm. Note the much larger \\(\\ell_2\\) norm of the coefficients associated with this smaller value of \\(\\lambda\\).\n\nridge.mod$lambda[60]\n\n[1] 705.4802\n\ncoef(ridge.mod)[, 60]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 \n      Errors   NewLeagueN \n -0.70358655   8.61181213 \n\nsqrt(sum(coef(ridge.mod)[-1, 60]^2))\n\n[1] 57.11001\n\n\nWe can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of \\(\\lambda\\), say \\(50\\):\n\npredict(ridge.mod, s = 50, type = \"coefficients\")[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n 4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 \n          RBI         Walks         Years        CAtBat         CHits \n 8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n 6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 \n\n\nWe now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a data set. The first is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to TRUE for the training data. The second is to randomly choose a subset of numbers between \\(1\\) and \\(n\\); these can then be used as the indices for the training observations. The two approaches work equally well. We used the former method in Section 6.5.1. Here we demonstrate the latter approach.\nWe first set a random seed so that the results obtained will be reproducible.\n\nset.seed(1)\ntrain  <- sample(1:nrow(x), nrow(x) / 2)\ntest   <- (-train)\ny.test <- y[test]\n\nNext we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using \\(\\lambda=4\\). Note the use of the predict() function again. This time we get predictions for a test set, by replacing type=\"coefficients\" with the newx argument.\n\nridge.mod <- glmnet(x[train, ], y[train], \n                    alpha  = 0,\n                    lambda = grid, \n                    thresh = 1e-12)\nridge.pred <- predict(ridge.mod, \n                      s    = 4, \n                      newx = x[test, ])\n## test MSE:                       \nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\nThe test MSE is \\(142{,}199\\). Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:\n\nmean((mean(y[train]) - y.test)^2)\n\n[1] 224669.9\n\n\nWe could also get the same (“just an intercept”) result by fitting a ridge regression model with a very large value of \\(\\lambda\\). Note that 1e10 means \\(10^{10}\\).\n\nridge.pred <- predict(ridge.mod, \n                      s    = 1e10, \n                      newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 224669.8\n\n\nSo fitting a ridge regression model with \\(\\lambda=4\\) leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with \\(\\lambda=4\\) instead of just performing least squares regression. Recall that least squares is simply ridge regression with \\(\\lambda=0\\).\nNote: In order for glmnet() to yield the exact least squares coefficients when \\(\\lambda=0\\), we use the argument exact = TRUE when calling the predict() function. Otherwise, the predict() function will interpolate over the grid of \\(\\lambda\\) values used in fitting the glmnet() model, yielding approximate results. When we use exact = TRUE, there remains a slight discrepancy in the third decimal place between the output of glmnet() when \\(\\lambda = 0\\) and the output of lm(); this is due to numerical approximation on the part of glmnet().\n\nridge.pred <- predict(ridge.mod, \n                      s     = 0, \n                      newx  = x[test, ],\n                      exact = TRUE, \n                      x     = x[train, ], # need to be supplied when exact = TRUE\n                      y     = y[train]    # need to be supplied when exact = TRUE\n                      )\nmean((ridge.pred - y.test)^2)\n\n[1] 168588.6\n\n## coefficient fits of lm\nlm(y ~ x, subset = train)\n\n\nCall:\nlm(formula = y ~ x, subset = train)\n\nCoefficients:\n(Intercept)       xAtBat        xHits       xHmRun        xRuns         xRBI  \n   274.0145      -0.3521      -1.6377       5.8145       1.5424       1.1243  \n     xWalks       xYears      xCAtBat       xCHits      xCHmRun       xCRuns  \n     3.7287     -16.3773      -0.6412       3.1632       3.4008      -0.9739  \n      xCRBI      xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists  \n    -0.6005       0.3379     119.1486    -144.0831       0.1976       0.6804  \n    xErrors  xNewLeagueN  \n    -4.7128     -71.0951  \n\n## Coefficient fits of glmnet with exact == TRUE\npredict(ridge.mod, \n        s     = 0, \n        exact = TRUE, \n        type  = \"coefficients\",\n        x     = x[train, ], # need to be supplied when exact = TRUE\n        y     = y[train]    # need to be supplied when exact = TRUE\n        )[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 274.0200994   -0.3521900   -1.6371383    5.8146692    1.5423361    1.1241837 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n   3.7288406  -16.3795195   -0.6411235    3.1629444    3.4005281   -0.9739405 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  -0.6003976    0.3378422  119.1434637 -144.0853061    0.1976300    0.6804200 \n      Errors   NewLeagueN \n  -4.7127879  -71.0898914 \n\n\nIn general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients.\nIn general, instead of arbitrarily choosing \\(\\lambda=4\\), it would be better to use cross-validation to choose the tuning parameter \\(\\lambda\\). We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs 10-fold cross-validation, though this can be changed using the argument nfolds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random.\n\nset.seed(1)\n\n## Only on the train data to have comparison \n## with the above chosen lambda values (i.e., 4 and 1e10)\ncv.out  <- cv.glmnet(x[train, ], \n                     y[train], \n                     alpha = 0)\nplot(cv.out)\n\n\n\nbestlam <- cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\nTherefore, we see that the value of \\(\\lambda\\) that results in the smallest cross-validation error is \\(326\\). What is the test MSE associated with this value of \\(\\lambda\\)?\n\nridge.pred <- predict(ridge.mod, \n                      s    = bestlam,\n                      newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\nThis represents a further improvement over the test MSE that we got using \\(\\lambda=4\\). Finally, we refit our ridge regression model on the full data set, using the value of \\(\\lambda\\) chosen by cross-validation, and examine the coefficient estimates.\n\nout <- glmnet(x, \n              y, \n              alpha = 0)\npredict(out, \n        type = \"coefficients\", \n        s    = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\nAs expected, none of the coefficients are zero—ridge regression does not perform variable selection!\n\n\n6.1.2.2 The Lasso\nWe saw that ridge regression with a wise choice of \\(\\lambda\\) can outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. Other than that change, we proceed just as we did in fitting a ridge model.\n\nlasso.mod <- glmnet(x[train, ], \n                    y[train], \n                    alpha  = 1,\n                    lambda = grid)\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\nWe can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.\n\nset.seed(1)\ncv.out     <- cv.glmnet(x[train, ], \n                        y[train], \n                        alpha = 1)\nplot(cv.out)\n\n\n\n## optimal lambda (according to CV)\nbestlam    <- cv.out$lambda.min\n\nlasso.pred <- predict(lasso.mod, \n                      s    = bestlam, \n                      newx = x[test, ])\n## test MSE                      \nmean((lasso.pred - y.test)^2)\n\n[1] 143673.6\n\n\nThis is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with \\(\\lambda\\) chosen by cross-validation.\nHowever, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with \\(\\lambda\\) chosen by cross-validation contains only eleven variables.\n\nout        <- glmnet(x, y, \n                     alpha  = 1, \n                     lambda = grid)\n\nlasso.coef <- predict(out, \n                      type = \"coefficients\", \n                      s    = bestlam)[1:20, ]\n\nlasso.coef\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\nlasso.coef[lasso.coef != 0]\n\n  (Intercept)         AtBat          Hits         Walks         Years \n   1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 \n       CHmRun         CRuns          CRBI       LeagueN     DivisionW \n   0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 \n      PutOuts        Errors \n   0.23752385   -0.85629148 \n\n\n\n\n\n6.1.3 Principal Components Regression\nPrincipal components regression (PCR) can be performed using the pcr() function, which is part of the pls library. We now apply PCR to the Hitters data, in order to predict Salary. Again, we ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nset.seed(2)\n\npcr.fit <- pcr(Salary ~ ., \n               data       = Hitters, \n               scale      = TRUE,\n               validation = \"CV\")\n\nThe syntax for the pcr() function is similar to that for lm(), with a few additional options. Setting scale = TRUE has the effect of standardizing each predictor, using ( 6.6), prior to generating the principal components, so that the scale on which each variable is measured will not have an effect. Setting validation = \"CV\" causes pcr() to compute the ten-fold cross-validation error for each possible value of \\(M\\), the number of principal components used. The resulting fit can be examined using summary().\n\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452    351.9    353.2    355.0    352.8    348.4    343.6\nadjCV          452    351.6    352.7    354.4    352.1    347.6    342.7\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       345.5    347.7    349.6     351.4     352.1     353.5     358.2\nadjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        349.7     349.4     339.9     341.6     339.2     339.6\nadjCV     348.0     347.7     338.2     339.7     337.2     337.6\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n\nThe CV score is provided for each possible number of components, ranging from \\(M=0\\) onwards.\nNote: pcr() reports the root mean squared error; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of \\(352.8\\) corresponds to an MSE of \\(352.8^2=124{,}468\\).\nOne can also plot the cross-validation scores using the validationplot() function. Using val.type = \"MSEP\" will cause the cross-validation MSE to be plotted.\n\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\nWe see that the smallest cross-validation error occurs when \\(M=18\\) components are used. This is barely fewer than \\(M=19\\), which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs.\nHowever, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.\nThe summary() function also provides the percentage of variance explained in the predictors and in the response using different numbers of components. This concept is discussed in greater detail in Chapter 12. Briefly, we can think of this as the amount of information about the predictors or the response that is captured using \\(M\\) principal components. For example, setting \\(M=1\\) only captures \\(38.31\\,\\%\\) of all the variance, or information, in the predictors. In contrast, using \\(M=5\\) increases the value to \\(84.29\\,\\%\\). If we were to use all \\(M=p=19\\) components, this would increase to \\(100\\,\\%\\).\nWe now perform PCR on the training data and evaluate its test set performance.\n\nset.seed(1)\npcr.fit <- pcr(Salary ~ ., \n               data       = Hitters, \n               subset     = train, \n               scale      = TRUE, \n               validation = \"CV\")\n##                \nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\nNow we find that the lowest cross-validation error occurs when \\(M=5\\) components are used. We compute the test MSE as follows.\n\npcr.pred <- predict(pcr.fit, \n                    x[test, ], \n                    ncomp = 5)\nmean((pcr.pred - y.test)^2)\n\n[1] 142811.8\n\n\nThis test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates.\nFinally, we fit PCR on the full data set, using \\(M=5\\), the number of components identified by cross-validation.\n\npcr.fit <- pcr(y ~ x, \n               scale = TRUE, \n               ncomp = 5)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 5\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps  5 comps\nX    38.31    60.16    70.84    79.03    84.29\ny    40.63    41.58    42.17    43.22    44.90"
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#exercises",
    "href": "Ch6_LinModSelectRegul.html#exercises",
    "title": "6  Linear Model Selection and Regularization",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\nPrepare the following exercises of Chapter 6 in our course textbook ISLR:\n\nExercise 2\nExercise 4\nExercise 8\nExercise 10"
  },
  {
    "objectID": "Ch6_LinModSelectRegul.html#solutions",
    "href": "Ch6_LinModSelectRegul.html#solutions",
    "title": "6  Linear Model Selection and Regularization",
    "section": "6.3 Solutions",
    "text": "6.3 Solutions\n\nExercise 2\nFor parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.\n2 a) The Lasso, relative to least squares, is:\n\nMore flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nMore flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\nLess flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nLess flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n\nAnswer: Claim iii is correct. Relative to least squares, the Lasso is less flexible and hence it will give us better predictions when its increase in (squared) bias is less than its decrease in variance.\n2 b) Repeat a) for ridge regression relative to least squares.\nAnswer: Claim iii is correct with the same justification as in a).\n2 c) Repeat a) for non-linear methods relative to least squares.\nAnswer: Claim ii is correct. Relative to least squares, non-linear methods are typically more flexible and hence, these methods will give us better predictions when the increase in variance is less than the decrease in bias.\n\n\nExercise 4\nSuppose we estimate the regression coefficients in a linear regression model by minimizing:\n\\[\\sum^n_{i=1} \\left(y_i - \\beta_0 - \\sum^p_{j=1} \\beta_j x_{ij}\\right)^2 + \\lambda \\sum^p_{j=1} \\beta_j^2\\]\nfor a particular value of \\(\\lambda\\). For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.\n4 a) As we increase \\(\\lambda\\) from 0, the training RSS will:\n\nIncrease initially, and then eventually start decreasing in an inverted U shape.\nDecrease initially, and then eventually start increasing in a U shape.\nSteadily increase.\nSteadily decrease.\nRemain constant.\n\nAnswer: Claim iii. (Steadily increase) is correct. As we increase \\(\\lambda\\) from \\(0\\), all \\(\\beta\\)’s are shrunk from their unrestricted least square estimates to \\(0\\). Training error is minimized for the most flexible model (i.e. \\(\\lambda=0\\)), and steadily increases as \\(\\beta\\)’s are shrunk twards \\(0\\), since this hinders the model to “follow” the training data.\n4 b) Repeat 4 a) for test RSS.\nAnswer: Claim ii. (Decrease initially, and then eventually start increasing in a U shape) is correct. When \\(\\lambda=0\\), all \\(\\beta\\)’s have their least square estimate values. In this case, the model is most flexible which typically results in a small bias, but (too) large variance and hence a high test RSS value. As we increase \\(\\lambda\\), the \\(\\beta\\) estimates are shrunk to zero which reduces variance. Thus, test RSS initially decreases; however, as \\(\\beta\\)’s approach infinity, the model becomes too simple (small variance, but (too) large bias) and test RSS increases.\n4 c) Repeat a) for variance.\nAnswer: Claim iv. (Steadily decrease) is correct.\nWhen \\(\\lambda =0\\), the \\(\\beta\\)’s have their least square estimate values. The actual estimates heavily depend on the training data and hence variance is high. As we increase \\(\\lambda\\), the estimates of \\(\\beta\\) are shrunk towards zero. In the limiting case of \\(\\lambda\\) approaching infinity, all \\(\\beta\\) ’s reduce to zero and model predicts a constant and has minimal variance.\n4 d) Repeat a) for (squared) bias.\nAnswer: iii. Steadily increase.\nWhen \\(\\lambda =0\\), the \\(\\beta\\) ’s have their least-square estimate values and hence have the least bias. As \\(\\lambda\\) increases, \\(\\beta\\) ’s start reducing towards zero, the model fits less accurately to training data and hence bias increases. In the limiting case of \\(\\lambda\\) approaching infinity, the model predicts a constant and hence bias is maximum.\n4 e) Repeat a) for the irreducible error.\nAnswer: Claim v. (Remain constant) is correct.\nBy definition, irreducible error is model independent and hence irrespective of the choice of \\(\\lambda\\), it remains constant.\n\n\nExercise 8\nIn this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n8 a) Use the rnorm() function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\epsilon\\) of length \\(n = 100\\).\nAnswer:\n\n## set seed for rnorm function\nset.seed(10)\n\n## rnorm simulates pseudo random variates from a  \n## (standard) normal distribution\nX    <- rnorm(100)\neps  <- rnorm(100)\n\n8 b) Generate a response vector \\(Y\\) of length \\(n = 100\\) according to the model:\n\\[Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon,\\]\nwhere \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\) are constants of your choice.\nAnswer:\n\n## beta parameters\nbeta0 <-  2\nbeta1 <-  2\nbeta2 <- -2\nbeta3 <-  4\n\n## Generate response vector Y\nY <- beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps\n\n8 c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X, X^2, ..., X^{10}\\). What is the best model obtained according to Cp, BIC, and adjusted \\(R^2\\)? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\n\n## Install package leaps if needed\n# install.packages(\"leaps\")\noptions(warn=-1)\n\n## Call the leaps package\nlibrary(leaps)\n\n## Use data.frame() function to create single dataset of X and Y\ndata.full <- data.frame(y = Y, x = X)\n\n## Use the regsubsets() function to perform best subset selection\nmod.full  <- regsubsets(y ~ poly(x, 10, raw = TRUE), \n                        data  = data.full, \n                        nvmax = 10)\n\n## Store results \"printed\" when using summary function\nmod.summary <- summary(mod.full)\n\n## Find the model size which minimizes Cp:\nwhich.min(mod.summary$cp)\n\n[1] 3\n\n## Plot the Cp-values\nplot(mod.summary$cp, \n     xlab = \"Subset Size\", \n     ylab = \"Cp\", \n     type = \"l\")\npoints(which.min(mod.summary$cp), \n       mod.summary$cp[3], \n       pch = 19, \n       col = \"red\", \n       lwd = 7)\n\n\n\n\n\n# BIC\nwhich.min(mod.summary$bic)\n\n[1] 3\n\nplot(mod.summary$bic, \n     xlab = \"Subset Size\", \n     ylab = \"BIC\", \n     type = \"l\")\npoints(which.min(mod.summary$bic), \n       mod.summary$bic[3], \n       pch = 19, \n       col = \"red\", \n       lwd = 7)\n\n\n\n\n\n# adjr2\nwhich.max(mod.summary$adjr2)\n\n[1] 3\n\nplot(mod.summary$adjr2, \n     xlab = \"Subset Size\", \n     ylab = \"Adjusted R2\", \n     type = \"l\")\npoints(which.max(mod.summary$adjr2), \n       mod.summary$adjr2[3], \n       pch = 19, \n       col = \"red\", \n       lwd = 7)\n\n\n\n\nAnswer: We find that all criteria (Cp, BIC and Adjusted R2) choose the 3 variables model.\n\n round(coefficients(mod.full, id = 3), digits=2)\n\n             (Intercept) poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 \n                    1.93                     1.88                    -2.04 \npoly(x, 10, raw = TRUE)3 \n                    4.02 \n\n\nAnswer: As expected, all the coefficients are quite close to our chosen parameter values: \\(\\beta_0=\\beta_1=2\\), \\(\\beta_2=-2\\), and \\(\\beta_3=4.\\)\n8 d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\n\n## Use the regsubsets() function to perform best subset selection \n\n## Forward Selection\nmod.fwd <- regsubsets(y ~ poly(x, 10, raw = TRUE), \n                      data   = data.full, \n                      nvmax  = 10, \n                      method = \"forward\")\n\n## Backward Selection\nmod.bwd <- regsubsets(y ~ poly(x, 10, raw = TRUE), \n                      data   = data.full, \n                      nvmax  = 10, \n                      method = \"backward\")\n\n## Store results\nfwd.summary <- summary(mod.fwd)\nbwd.summary <- summary(mod.bwd)\n\n## Find the model size for best:\n## Cp \nwhich.min(fwd.summary$cp) # forward\n\n[1] 3\n\nwhich.min(bwd.summary$cp) # backward\n\n[1] 5\n\n# BIC\nwhich.min(fwd.summary$bic) # forward\n\n[1] 3\n\nwhich.min(bwd.summary$bic) # backward\n\n[1] 5\n\n# adjr2\nwhich.max(fwd.summary$adjr2) # forward\n\n[1] 3\n\nwhich.max(bwd.summary$adjr2) # backward\n\n[1] 5\n\n\n\n# Plot the statistics\npar(mfrow = c(3, 2))\n#cp\nplot(fwd.summary$cp, xlab = \"Subset Size\", ylab = \"Forward Cp\", type = \"l\")\npoints(which.min(fwd.summary$cp), min(fwd.summary$cp), pch = 19, col = \"red\", lwd = 7)\nplot(bwd.summary$cp, xlab = \"Subset Size\", ylab = \"Backward Cp\", type = \"l\")\npoints(which.min(bwd.summary$cp), min(bwd.summary$cp), pch = 19, col = \"red\", lwd = 7)\n\n## BIC\nplot(fwd.summary$bic, xlab = \"Subset Size\", ylab = \"Forward BIC\", type = \"l\")\npoints(which.min(fwd.summary$bic), min(fwd.summary$bic), pch = 19, col = \"red\", lwd = 7)\nplot(bwd.summary$bic, xlab = \"Subset Size\", ylab = \"Backward BIC\", type = \"l\")\npoints(which.min(bwd.summary$bic), min(bwd.summary$bic), pch = 19, col = \"red\", lwd = 7)\n\n#adjr2\nplot(fwd.summary$adjr2, xlab = \"Subset Size\", ylab = \"Forward Adjusted R2\", type = \"l\")\npoints(which.max(fwd.summary$adjr2), max(fwd.summary$adjr2), pch = 19, col = \"red\", lwd = 7)\nplot(bwd.summary$adjr2, xlab = \"Subset Size\", ylab = \"Backward Adjusted R2\", type = \"l\")\npoints(which.max(bwd.summary$adjr2), max(bwd.summary$adjr2), pch = 19, col = \"red\", lwd = 7)\n\n\n\n\nAnswer: While all backward stepwise selection approaches with Cp, BIC and adjusted \\(R^2\\) pick the 5 variables model, the forward stepwise selection approaches with Cp, BIC and adjusted \\(R^2\\) pick the 3 variables model. Here are the coefficients:\n\n round(coefficients(mod.fwd, id = 3), digits=2)\n\n             (Intercept) poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 \n                    1.93                     1.88                    -2.04 \npoly(x, 10, raw = TRUE)3 \n                    4.02 \n\n\n\nround(coefficients(mod.bwd, id = 5), digits=2)\n\n             (Intercept) poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 \n                    1.98                     3.14                    -2.08 \npoly(x, 10, raw = TRUE)5 poly(x, 10, raw = TRUE)7 poly(x, 10, raw = TRUE)9 \n                    3.42                    -1.05                     0.11 \n\n\nAnswer: All the coefficient estimates from forward stepwise selection are quite close to our chosen \\(\\beta\\) ’s. However, for the backward stepwise with 5 variable picks \\(X^5, X^7, X^9\\) and drops \\(X^3\\).\n8 e) Now fit a lasso model to the simulated data, again using \\(X, X^2, ..., X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained.\n\n## install.packages(\"glmnet\")\n\n## Load the glmnet package\nlibrary(glmnet)\n\n## set seed\nset.seed(10)\n\n## Training Lasso on the data\nxmat <- model.matrix(y ~ poly(x, 10, raw = TRUE), \n                     data = data.full)[, -1]\n\n## Use cross-validation to select the optimal value of  lambda\nmod.lasso <- cv.glmnet(xmat, Y, alpha = 1)\n\n## Store and display lambda\nbest.lambda <- mod.lasso$lambda.min\nbest.lambda\n\n[1] 0.05988275\n\n## Plot\nplot(mod.lasso)\n\n\n\n\n\n## Next fit the model on entire data using the best lambda\nbest.model <- glmnet(xmat, Y, alpha = 1)\n##\npredict(best.model, \n        s    = best.lambda, \n        type = \"coefficients\") # print the coefficient estimates\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                                 s1\n(Intercept)                1.881373\npoly(x, 10, raw = TRUE)1   1.854449\npoly(x, 10, raw = TRUE)2  -1.991838\npoly(x, 10, raw = TRUE)3   4.010379\npoly(x, 10, raw = TRUE)4   .       \npoly(x, 10, raw = TRUE)5   .       \npoly(x, 10, raw = TRUE)6   .       \npoly(x, 10, raw = TRUE)7   .       \npoly(x, 10, raw = TRUE)8   .       \npoly(x, 10, raw = TRUE)9   .       \npoly(x, 10, raw = TRUE)10  .       \n\n\nAnswer: Lasso picks the correct specification.\n8 f) Now generate a response vector Y according to the model:\n\\[Y = \\beta_0 + \\beta_7 X^7 + \\epsilon\\]\nand perform best subset selection and the lasso. Discuss the results obtained.\n\n## Create new beta7 value \nbeta7     <- 2\n\n## Model\nY         <- beta0 + beta7 * X^7 + eps\n\n## Predict using regsubsets\ndata.full <- data.frame(y = Y, x = X)\nmod.full  <- regsubsets(y ~ poly(x, 10, raw = TRUE), \n                        data  = data.full, \n                        nvmax = 10)\n\nmod.summary <- summary(mod.full)\n\n## Find the model size for best Cp, BIC and adjr2\nwhich.min(mod.summary$cp)\n\n[1] 1\n\nwhich.min(mod.summary$bic)\n\n[1] 1\n\nwhich.max(mod.summary$adjr2)\n\n[1] 1\n\n\n\nround(coefficients(mod.full, id = 1), digits=2)\n\n             (Intercept) poly(x, 10, raw = TRUE)7 \n                     1.9                      2.0 \n\n\nAnswer: Best subset selection selects the correct 1 variable model—no matter which criterion (Cp, BIC, and adjusted \\(R^2\\)) is used.\n\nxmat <- model.matrix(y ~ poly(x, 10, raw = TRUE), \n                     data = data.full)[, -1]\n\nmod.lasso   <- cv.glmnet(xmat, Y, alpha = 1)\nbest.lambda <- mod.lasso$lambda.min\nbest.lambda\n\n[1] 2.717423\n\n\n\nbest.model  <- glmnet(xmat, Y, alpha = 1)\npredict(best.model, \n        s    = best.lambda, \n        type = \"coefficients\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                                s1\n(Intercept)               1.783453\npoly(x, 10, raw = TRUE)1  .       \npoly(x, 10, raw = TRUE)2  .       \npoly(x, 10, raw = TRUE)3  .       \npoly(x, 10, raw = TRUE)4  .       \npoly(x, 10, raw = TRUE)5  .       \npoly(x, 10, raw = TRUE)6  .       \npoly(x, 10, raw = TRUE)7  1.941570\npoly(x, 10, raw = TRUE)8  .       \npoly(x, 10, raw = TRUE)9  .       \npoly(x, 10, raw = TRUE)10 .       \n\n\nAnswer: Lasso also picks the correct 1-variable model.\n\n\nExercise 10\nWe have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n10 a) Generate a data set with \\(p = 20\\) features, \\(n = 1000\\) observations, and an associated quantitative response vector generated according to the model:\n\\[Y = X\\beta + \\epsilon,\\]\nwhere \\(\\beta\\) has some elements that are exactly equal to zero.\n\n## set seed for rnorm function\nset.seed(123)\n\n## select number of features\np  <- 20\n\n## select number of observations\nn   <- 1000\n\n## rnorm simulates realizations of \n## random variates having a specified \n## normal distribution (here standard normal)\n## x is a 1000x20 matrix\nx      <- matrix(rnorm(n * p), n, p)\n## B and eps are a 20x1 vector\nbeta   <- rep(3, p)\neps    <- rnorm(n)\n\n## Set some elements of B equal to zero\nbeta[1]  <- 0\nbeta[2]  <- 0\nbeta[3]  <- 0\nbeta[4]  <- 0\nbeta[5]  <- 0\n\n## Generate the dependent variable Y realizations\ny = cbind(1, x) %*% c(1, beta) + eps\n\n10 b) Split your data set into a training set containing 100 observations and a test set containing 900 observations.\n\n## set seed\nset.seed(123)\n\n## sample function takes a sample of the specified size (100) \n## from the elements of 1, 2, ..., 1000, without replacement\ntrain   <- sample(x       = 1:1000, \n                  size    = 100, \n                  replace = FALSE)\n\n## use the previously defined train vector to construct \n## training and testing data\ny.train <- y[ train, ]\ny.test  <- y[-train, ]\nx.train <- x[ train, ]\nx.test  <- x[-train, ]\n\n10 c) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\n\n## set seed\nset.seed(123)\n\n## call leaps package\nlibrary(leaps)\n\n## nvmax - maximum size of subsets to examine\nregfit.full <- regsubsets(y ~ ., \n                          data = data.frame(x = x.train, \n                                            y = y.train), \n                          nvmax = p)\n\n## create storage vector\nval.errors <- rep(NA, p)\n\n## store column names\nx_cols     <- colnames(x, do.NULL = FALSE, prefix = \"x.\")\n\n## loop over each subset p\nfor (i in 1:p) {\n    ## store coefficients\n    coefi         <- coef(regfit.full, id = i)\n    ## make prediction using training subset\n    pred          <- cbind(1, x.train[, x_cols %in% names(coefi)]) %*% coefi\n    ## store MSE of the training subset\n    val.errors[i] <- mean((y.train - pred)^2)\n}\n\n## Plot\nplot(val.errors, \n     ylab = \"Training MSE\", \n     type = \"b\")\npoints(y   = val.errors[which.min(val.errors)], \n       x   = which.min(val.errors), \n       pch = 19, \n       col = \"red\")     \n\n\n\n\n10 d) Plot the test set MSE associated with the best model of each size.\n\n## create storage vector\nval.errors <- rep(NA, p)\n\n# loop over each subset p\nfor (i in 1:p) {\n    ## store coefficients\n    coefi         <- coef(regfit.full, id = i)\n    ## make prediction using testing subset\n    pred          <- cbind(1, x.test[, x_cols %in% names(coefi)]) %*% coefi\n    ## store MSE of the testing subset\n    val.errors[i] <- mean((y.test - pred)^2)\n}\n\n## Plot\nplot(val.errors, \n     ylab = \"Test MSE\", \n     type = \"b\")\npoints(y   = val.errors[which.min(val.errors)], \n       x   = which.min(val.errors), \n       pch = 19, \n       col = \"red\")\n\n\n\n\n10 e) For which model size does the test set MSE take on its minimum value? Comment on your results.\n\nwhich.min(val.errors)\n\n[1] 15\n\n\nAnswer: A 15 parameter model has the smallest test MSE.\n10 f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\n\nround(coef(regfit.full, id = which.min(val.errors)), digits=2)\n\n(Intercept)         x.6         x.7         x.8         x.9        x.10 \n       0.92        3.04        2.88        3.05        3.05        3.13 \n       x.11        x.12        x.13        x.14        x.15        x.16 \n       3.09        3.12        3.03        2.95        3.04        2.99 \n       x.17        x.18        x.19        x.20 \n       3.00        3.15        2.94        2.79 \n\n\nAnswer: Best subset regression correctly selected out all irrelevant predictors (1, 2, 3,4, 5).\n10 g) Create a plot displaying \\(\\sqrt{\\sum_{j=1}^p (\\beta_j - \\hat{\\beta}_j^r)^2}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}_j^r\\) is the \\(j\\)th coefficient estimate for the best model containing \\(r\\) coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?\n\n## create storage vectors\nval.errors <- rep(NA, p)\na          <- rep(NA, p)\nb          <- rep(NA, p)\n\n## loop over each subset p\nfor (i in 1:p) {\n    ## store coefficients\n    coefi <- coef(regfit.full, id = i)\n    \n    ## store number of coefficients excluding the intercept\n    a[i] <- length(coefi) - 1\n    \n    ## compute what is asked in the question \n    b[i] <- sqrt(\n        ## Compare estimation results vs true values for the \n        ## selected predictors \n        sum((beta[x_cols %in% names(coefi)] - coefi[-1])^2) + \n        ## Compare estimation results (=0) vs true values for the \n        ## **not** selected predictors\n        sum(beta[!(x_cols %in% names(coefi))] - 0 )^2)\n}\n\n## Plot\nplot(x = a, y = b, \n     xlab = \"Number of coefficients\", \n     ylab = \"Error between estimated and true coefficients\", \n     type = \"b\")\npoints(y   = b[which.min(b)], \n       x   = which.min(b), \n       pch = 19, \n       col = \"red\")     \n\n\n\n\n\nwhich.min(b)\n\n[1] 15\n\n\nAnswer: Test error in the parameter estimates is minimized with a 15 parameter model."
  },
  {
    "objectID": "Ch2_StatLearning.html#what-is-statistical-learning",
    "href": "Ch2_StatLearning.html#what-is-statistical-learning",
    "title": "1  Statistical Learning",
    "section": "1.1 What is Statistical Learning?",
    "text": "1.1 What is Statistical Learning?\nSuppose that we observe a quantitative response \\(Y\\) and \\(p\\) different predictors, \\(X_1, X_2, \\dots, X_p.\\)\nWe assume that there is some relationship between \\(Y\\) and \\[\nX = (X_1, X_2, \\dots, X_p),\n\\] which can be written in the very general form \\[\nY = f(X) + \\epsilon.\n\\]\n\n\\(f\\) is some fixed but unknown function of \\(X = (X_1, X_2, \\dots, X_p).\\)\n\\(\\epsilon\\) a random error term, which is independent of \\(X\\) and has mean zero.\n\nIn this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y.\\)\nIn essence, statistical learning refers to a set of approaches for estimating \\(f.\\) In this chapter we outline some of the key theoretical concepts that arise in estimating \\(f,\\) as well as tools for evaluating the estimates obtained.\n\n1.1.1 Why Estimate \\(f\\)?\nThere are two main reasons that we may wish to estimate \\(f\\):\n\nprediction and\ninference.\n\nWe discuss each in turn."
  },
  {
    "objectID": "Ch2_StatLearning.html#prediction",
    "href": "Ch2_StatLearning.html#prediction",
    "title": "1  Statistical Learning",
    "section": "Prediction",
    "text": "Prediction\nIn many situations, a set of inputs \\(X\\) are readily available, but the output \\(Y\\) cannot be easily obtained. In this setting, since the error term averages to zero, we can predict \\(Y\\) using \\[\n\\hat{Y} = \\hat{f}(X),\n\\]\n\n\\(\\hat{f}\\) represents our estimate for \\(f\\)\n\\(\\hat{Y}\\) represents the resulting prediction for \\(Y\\)\n\nIn this setting, \\(\\hat{f}\\) is often treated as a black box, in the sense that one is not typically concerned with the exact form of \\(\\hat{f},\\) provided that it yields accurate predictions for \\(Y.\\)\nExample: As an example, suppose that \\((X_1, X_2, \\dots, X_p)\\) are characteristics of a patient’s blood sample that can be easily measured in a lab, and \\(Y\\) is a variable encoding the patient’s risk for a severe adverse reaction to a particular drug. It is natural to seek to predict \\(Y\\) using \\(X,\\) since we can then avoid giving the drug in question to patients who are at high risk of an adverse reaction–that is, patients for whom the estimate of \\(Y\\) is high.\n\nAccuracy of a Prediction\nThe accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on two quantities: * the reducible error and * the irreducible error.\nIn general, \\(\\hat{f}\\) will not be a perfect estimate for \\(f,\\) and this inaccuracy will introduce some error. This error is reducible, because we can potentially improve the accuracy of \\(\\hat{f}\\) by using the most appropriate statistical learning technique to estimate \\(f.\\)\nHowever, even if it were possible to form a perfect estimate for \\(f,\\) so that our estimated response took the form \\[\n\\hat{Y} = f (X),\n\\] our prediction would still have some error in it! This is because \\(Y\\) is also a function of \\(\\epsilon\\) which, by definition, cannot be predicted using \\(X.\\) Therefore, variability associated with \\(\\epsilon\\) also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate \\(f,\\) we cannot reduce the error introduced by \\(\\epsilon.\\)\nConsider a given estimate \\(\\hat{f}\\) and a given set of predictors \\(X,\\) which yields the prediction \\(\\hat{Y} = \\hat{f}(X).\\) Assume for a moment that both \\(\\hat{f}\\) and \\(X\\) are fixed, so that the only variability comes from \\(\\epsilon.\\) Then, it is easy to show that \\[\\begin{align*}\n&E\\left[(Y - \\hat{Y})^2\\right]\\\\\n&=E\\left[(f(X) + \\epsilon - \\hat{f}(X))^2\\right] \\\\\n&=E\\left[\\left(f(X) -\\hat{f}(X)\\right)^2 - \\left(f(X) -\\hat{f}(X)\\right)\\epsilon + \\epsilon^2\\right] \\\\\n% &=E\\left[\\left(f(X) -\\hat{f}(X)\\right)^2\\right] - E\\left[\\left(f(X) -\\hat{f}(X)\\right)\\epsilon\\right] + E\\left[\\epsilon^2\\right] \\\\\n&=\\left(f(X) -\\hat{f}(X)\\right)^2 - \\left(f(X) -\\hat{f}(X)\\right) E\\left[\\epsilon\\right] + E\\left[\\epsilon^2\\right] \\\\\n&=\\left(f(X) -\\hat{f}(X)\\right)^2 - \\left(f(X) -\\hat{f}(X)\\right) \\cdot 0 + Var\\left(\\epsilon\\right) \\\\\n&=\\underbrace{\\left(f(X) -\\hat{f}(X)\\right)^2}_{\\text{reducable}} + \\underbrace{Var\\left(\\epsilon\\right)}_{\\text{irreducable}},\n\\end{align*}\\] where \\(E\\left[(Y - \\hat{Y})^2\\right]\\) represents the expected value,of the squared difference between the predicted and actual value of \\(Y,\\) and where \\(Var(\\epsilon)\\) represents the variance associated with the error term \\(\\epsilon.\\)\nThe focus of this course is on techniques for estimating \\(f\\) with the aim of minimizing the reducible error.\nIt is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for \\(Y.\\) This bound is almost always unknown in practice."
  },
  {
    "objectID": "Ch2_StatLearning.html#r-lab-introduction-to-r",
    "href": "Ch2_StatLearning.html#r-lab-introduction-to-r",
    "title": "1  Statistical Learning",
    "section": "1.4 R-Lab: Introduction to R",
    "text": "1.4 R-Lab: Introduction to R\nThis tutorial aims to serve as an introduction to the software package R. Other very good and much more exhaustive tutorials and useful reference-cards can be found at the following links:\n\nReference card for R commands (always useful)\nThe official Introduction to R (very detailed)\nAnd many more at www.r-project.org (see “Documents”) \nAn R-package for learning R: www.swirl.com\nAn excellent book project which covers also advanced issues such as “writing performant code” and “package development”: adv-r.had.co.nz\n\nAnother excellent book: R for Data Science\n\nSome other tutorials:\n\nIntroduction to data science\nCreating dynamic graphics\n\nWhy R?\n\nR is free of charge from: www.r-project.org\nThe celebrated IDE RStudio for R is also free of charge: www.rstudio.com\nR is equipped with one of the most flexible and powerful graphics routines available anywhere. For instance, check out one of the following repositories:\n\nClean Graphs\nPublication Ready Plots\n\nToday, R is the de-facto standard for statistical science.\n\n\n1.4.1 Short Glossary\nLets start the tutorial with a (very) short glossary:\n\nConsole: The thing with the > sign at the beginning.\nScript file: An ordinary text file with suffix .R. For instance, yourFavoritFileName.R.\nWorking directory: The file-directory you are working in. Useful commands: with getwd() you get the location of your current working directory and setwd() allows you to set a new location for it.\nWorkspace: This is a hidden file (stored in the working directory), where all objects you use (e.g., data, matrices, vectors, variables, functions, etc.) are stored. Useful commands: ls() shows all elements in our current workspace and rm(list=ls()) deletes all elements in our current workspace.\n\n\n\n1.4.2 First Steps\nA good idea is to use a script file such as yourFavoritFileName.R in order to store your R commands. You can send single lines or marked regions of your R-code to the console by pressing the keys STRG+ENTER.\nTo begin with baby steps, do some simple computations:\n\n2+2 # and all the others: *,/,-,^2,^3,... \n\n[1] 4\n\n\nNote: Everything that is written after the #-sign is ignored by R, which is very useful to comment your code.\nThe assignment operator <- or = will be your most often used tool. Here an example to create a scalar variable:\n\nx <- 4 \nx\n\n[1] 4\n\n4 -> x # possible but unusual\nx\n\n[1] 4\n\n\nNote: The R community loves the <- assignment operator, which is a very unusual syntax. Alternatively, you can use the more common = operator which is also used in languages like python or matlab.\nAnd now a more interesting object - a vector:\n\ny <- c(2,7,4,1)\ny\n\n[1] 2 7 4 1\n\n\nThe command ls() shows the total content of your current workspace, and the command rm(list=ls()) deletes all elements of your current workspace:\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nNote: RStudio’s Environment pane also lists all the elements in your current workspace. That is, the command ls() becomes a bit obsolete when working with RStudio.\nLet’s try how we can compute with vectors and scalars in R.\n\nx <- 4\ny <- c(2,7,4,1)\n\nx*y # each element in the vector, y, is multiplied by the scalar, x.\n\n[1]  8 28 16  4\n\ny*y # this is a term by term product of the elements in y\n\n[1]  4 49 16  1\n\n\nPerforming vector multiplications as you might expect from your last math-course, e.g., an outer product: \\(y\\,y^\\top\\):\n\ny %*% t(y)\n\n     [,1] [,2] [,3] [,4]\n[1,]    4   14    8    2\n[2,]   14   49   28    7\n[3,]    8   28   16    4\n[4,]    2    7    4    1\n\n\nOr an inner product \\(y^\\top y\\):\n\nt(y) %*% y\n\n     [,1]\n[1,]   70\n\n\nNote: Sometimes, R’s treatment of vectors can be annoying. The product y %*% y is treated as the product t(y) %*% y.\nThe term-by-term execution as in the above example, y*y, is actually a central strength of R. We can conduct many operations vector-wisely:\n\ny^2\n\n[1]  4 49 16  1\n\nlog(y)\n\n[1] 0.6931472 1.9459101 1.3862944 0.0000000\n\nexp(y)\n\n[1]    7.389056 1096.633158   54.598150    2.718282\n\ny-mean(y)\n\n[1] -1.5  3.5  0.5 -2.5\n\n(y-mean(y))/sd(y) # standardization \n\n[1] -0.5669467  1.3228757  0.1889822 -0.9449112\n\n\nThis is a central characteristic of so called matrix based languages like R (or Matlab). Other programming languages often have to use loops instead:\n\nN <- length(y)\n1:N\n\ny.sq <- numeric(N)\ny.sq\n\nfor(i in 1:N){\n  y.sq[i] <- y[i]^2\n  if(i == N){\n    print(y.sq)\n  }\n}\n\nThe for()-loop is the most common loop. But there is also a while()-loop and a repeat()-loop. However, loops in R can be rather slow, therefore, try to avoid them!\n\nUseful commands to produce sequences of numbers:\n\n1:10\n-10:10\n?seq # Help for the seq()-function\nseq(from=1, to=100, by=7)\n\nUsing the sequence command 1:16, we can go for our first matrix:\n\n?matrix\nA <- matrix(data=1:16, nrow=4, ncol=4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA <- matrix(1:16, 4, 4)\n\nNote that a matrix has always two dimensions, but a vector has only one dimension:\n\ndim(A)    # Dimension of matrix A?\n\n[1] 4 4\n\ndim(y)    # dim() does not operate on vectors.\n\nNULL\n\nlength(y) # Length of vector y?\n\n[1] 4\n\n\nLets play a bit with the matrix A and the vector y. As we have seen in the loop above, the []-operator selects elements of vectors and matrices:\n\nA[,1]\nA[4,4]\ny[c(1,4)]\n\nThis can be done on a more logical basis, too. For example, if you want to know which elements in the first column of matrix A are strictly greater than 2:\n\nA[,1][A[,1]>2]\n\n[1] 3 4\n\n# Note that this give you a boolean vector:\nA[,1]>2\n\n[1] FALSE FALSE  TRUE  TRUE\n\n# And you can use it in a non-sense relation, too:\ny[A[,1]>2]\n\n[1] 4 1\n\n\nNote: Logical operations return so-called boolean objects, i.e., either a TRUE or a FALSE. For instance, if we ask R whether 1>2 we get the answer FALSE.\n\n\n1.4.3 Further Data Objects\nBesides classical data objects such as scalars, vectors, and matrices there are three further data objects in R:\n\nThe array: As a matrix but with more dimensions. Here is an example of a \\(2\\times 2\\times 2\\)-dimensional array:\n\n\nmyFirst.Array <- array(c(1:8), dim=c(2,2,2)) # Take a look at it!\n\n\nThe list: In lists you can organize different kinds of data. E.g., consider the following example:\n\n\nmyFirst.List <- list(\"Some_Numbers\" = c(66, 76, 55, 12, 4, 66, 8, 99), \n                     \"Animals\"      = c(\"Rabbit\", \"Cat\", \"Elefant\"),\n                     \"My_Series\"    = c(30:1)) \n\nA very useful function to find specific values and entries within lists is the str()-function:\n\nstr(myFirst.List)\n\nList of 3\n $ Some_Numbers: num [1:8] 66 76 55 12 4 66 8 99\n $ Animals     : chr [1:3] \"Rabbit\" \"Cat\" \"Elefant\"\n $ My_Series   : int [1:30] 30 29 28 27 26 25 24 23 22 21 ...\n\n\n\nThe data frame: A data.frame is a list-object but with some more formal restrictions (e.g., equal number of rows for all columns). As indicated by its name, a data.frame-object is designed to store data:\n\n\nmyFirst.Dataframe <- data.frame(\"Credit_Default\"   = c( 0, 0, 1, 0, 1, 1), \n                                \"Age\"              = c(35,41,55,36,44,26), \n                                \"Loan_in_1000_EUR\" = c(55,65,23,12,98,76)) \n# Take a look at it!\n\n\n\n1.4.4 Simple Regression Analysis using R\nAlright, let’s do some statistics with real data. You can download the data HERE. Save it on your computer, at a place where you can find it, and give the path (e.g. \"C:\\textbackslash path\\textbackslash auto.data.csv\", which references to the data, to the file-argument of the function read.csv():\n\n# ATTENTION! YOU HAVE TO CHANGE \"\\\" TO \"/\":\nauto.data <- read.csv(file=\"C:/your_path/autodata.txt\", header=TRUE)\nhead(auto.data)\n\nIf you have problems to read the data into R, go on with these commands. (For this you need a working internet connection!):\n\n# install.packages(\"readr\")\nlibrary(\"readr\")\nauto.data <- suppressMessages(read_csv(file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\",col_names = TRUE))\n# head(auto.data)\n\nYou can select specific variables of the auto.data using the $-operator:\n\ngasolin.consumption      <- auto.data$MPG.city\ncar.weight               <- auto.data$Weight\n## Take a look at the first elements of these vectors:\nhead(cbind(gasolin.consumption,car.weight))\n\n     gasolin.consumption car.weight\n[1,]                  25       2705\n[2,]                  18       3560\n[3,]                  20       3375\n[4,]                  19       3405\n[5,]                  22       3640\n[6,]                  22       2880\n\n\nThis is how you can produce your first plot:\n\n## Plot the data:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy Light-Weight Cars!\")\n\n\n\n\nFigure 1.1: Scatterplot of Gasoline consumption (mpg) vs. car weight.\n\n\n\n\nAs a first step, we might assume a simple kind of linear relationship between the variables gasolin.consumption and car.weight. Let us assume that the data was generated by the following simple regression model: \\[\ny_i=\\alpha+\\beta_1 x_i+\\varepsilon_i,\\quad i=1,\\dots,n\n\\] where \\(y_i\\) denotes the gasoline-consumption, \\(x_i\\) the weight of car \\(i\\), and \\(\\varepsilon_i\\) is a mean zero constant variance noise term. (This is clearly a non-sense model!)\nThe command lm() computes the estimates of this linear regression model. The command (in fact it’s a method) summary() computes further quantities of general interest from the object that was returned from the lm() function.\n\nlm.result   <- lm(gasolin.consumption~car.weight)\nlm.summary  <- summary(lm.result)\nlm.summary\n\n\nCall:\nlm(formula = gasolin.consumption ~ car.weight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7946 -1.9711  0.0249  1.1855 13.8278 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 47.048353   1.679912   28.01   <2e-16 ***\ncar.weight  -0.008032   0.000537  -14.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.038 on 91 degrees of freedom\nMultiple R-squared:  0.7109,    Adjusted R-squared:  0.7077 \nF-statistic: 223.8 on 1 and 91 DF,  p-value: < 2.2e-16\n\n\nOf course, we want to have a possibility to access all the quantities computed so far, e.g., in order to plot the results. This can be done as following:\n\n## Accessing the computed quantities\nnames(lm.summary) ## Alternatively: str(lm.summary)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nalpha <- lm.summary$coefficients[1]\nbeta  <- lm.summary$coefficients[2]\n\n## Plot all:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy light-weight Cars!\")\nabline(a=alpha, \n       b=beta, col=\"red\")\n\n\n\n\nScatterplot of Gasoline consumption (mpg) vs. car weight plus linear regression fit.\n\n\n\n\n\n\n1.4.5 Programming in R\nLet’s write, i.e., program our own R-function for estimating linear regression models. In order to be able to validate our function, we start with simulating data for which we then know all true parameters.\nSimulating data is like being the “Data-God”: For instance, we generate realizations of the error term \\(\\varepsilon_i\\), i.e., something which we never observe in real data.\nLet us consider the following multiple regression model:\n\\[y_i=\\beta_1 +\\beta_2 x_{2i}+\\beta_3 x_{3i}+\\varepsilon_{i},\\quad i=1,\\dots,n,\\] where \\(\\varepsilon_{i}\\) is a heteroscedastic error term \\[\\varepsilon_{i}\\sim N(0,\\sigma_i^2),\\quad \\sigma_i=|x_{3i}|,\\]\nand where for all \\(i=1,\\dots,n=50\\):\n\n\\(x_{2i}\\sim N(10,1.5^2)\\)\n\\(x_{3i}\\) comes from a t-distribution with 5 degrees of freedom and non-centrality parameter 2\n\n\nset.seed(109) # Sets the \"seed\" of the random number generators:\nn   <- 50     # Number of observations\n\n## Generate two explanatory variables plus an intercept-variable:\nX.1 <- rep(1, n)                 # Intercept\nX.2 <- rnorm(n, mean=10, sd=1.5) # Draw realizations form a normal distr.\nX.3 <- rt(n, df=5, ncp=2)        # Draw realizations form a t-distr.\nX   <- cbind(X.1, X.2, X.3)      # Save as a Nx3-dimensional data matrix.\n\nOK, we have regressors, i.e., data that we also have in real data sets.\nNow we define the elements of the \\(\\beta\\)-vector. Be aware of the difference: In real data sets we do not know the true \\(\\beta\\)-vector, but try to estimate it. However, when simulating data, we determine (as “Data-Gods”) the true \\(\\beta\\)-vector and can compare our estimate \\(\\hat{\\beta}\\) with the true \\(\\beta\\):\n\n## Define the slope-coefficients\nbeta.vec  <- c(1,-5,5)\n\nWe still need to simulate realizations of the dependent variable \\(y_i\\). Remember that \\(y_i=\\beta_1 x_{1i}+\\beta_1 x_{2i}+\\beta_3 x_{3i}+\\varepsilon_{i}\\). That is, we only need realizations from the error terms \\(\\varepsilon_i\\) in order to compute the realizations from \\(y_i\\). This is how you can simulate realizations from the heteroscedastic error terms \\(\\varepsilon_i\\):\n\n## Generate realizations from the heteroscadastic error term\neps       <- rnorm(n, mean=0, sd=abs(X.3))\n\nTake a look at the heteroscedasticity in the error term:\n\nplot(y=eps, x=X.3, \n     main=\"Realizations of the \\nHeteroscedastic Error Term\")\n\n\n\n\nScatterplot of error term realizations (usually unknown) versus the predictor values of X.3.\n\n\n\n\nWith the (pseudo-random) realizations from \\(\\varepsilon_i\\), we can finally generate realizations from the dependent variable \\(y_i\\):\n\n## Dependent variable:\ny   <- X %*% beta.vec + eps\n\nLet’s take a look at the data:\n\nmydata    <- data.frame(\"Y\"=y, \"X.1\"=X.1, \"X.2\"=X.2, \"X.3\"=X.3)\npairs(mydata[,-2]) # The '-2' removes the intercept variable \"X.1\"\n\n\n\n\nOnce we have data, we can compute the OLS estimate of the true \\(\\beta\\) vector. Remember the formula: \\[\\hat{\\beta}=(X^\\top X)^{-1}X^\\top y\\] In R-Code this is: \\((X^\\top X)^{-1}=\\)solve(t(X) %*% X), i.e.:\n\n## Computation of the beta-Vector:\nbeta.hat <- solve(t(X) %*% X) %*% t(X) %*% y\nbeta.hat\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\nWell done. Using the above lines of code we can easily program our own myOLSFun() function!\n\nmyOLSFun <- function(y, x, add.intercept=FALSE){\n  \n  ## Number of Observations:\n  n         <- length(y)\n  \n  ## Add an intercept to x:\n  if(add.intercept){\n    Intercept <- rep(1, n)\n    x         <- cbind(Intercept, x)\n  }\n  \n  ## Estimation of the slope-parameters:\n  beta.hat.vec <- solve(t(x) %*% x) %*% t(x) %*% y\n  \n  ## Return the result:\n  return(beta.hat.vec)\n}\n\n## Run the function:\nmyOLSFun(y=y, x=X)\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\nCan you extend the function for the computation of the covariance matrix of the slope-estimates, several measures of fits (R\\(^2\\), adj.-R\\(^2\\), etc.), t-tests, …?\n\n\n1.4.6 R-packages\nOne of the best features in R are its contributed packages. The list of all packages on CRAN is impressive! Take a look at it HERE\nFor instance, nice plots can be produced using the R-package is ggplot2. You can find an intro do this package HERE.\n\n# install.packages(\"ggplot2\")\nlibrary(\"ggplot2\")\n\nqplot(Sepal.Length, Petal.Length, data = iris, color = Species)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nOf course, ggplot2 concerns “only” plotting, but you’ll find R-packages for almost any statistical method out there.\n\n\n1.4.7 Tidyverse\nThe tidyverse package is a collection of packages that lets you import, manipulate, explore, visualize and model data in a harmonized and consistent way which helps you to be more productive.\nInstalling the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nTo use the tidyverse package load it using the library() function:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ stringr   1.5.0\n✔ forcats   1.0.0     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nChick Weight Data\nR comes with many datasets installed. We will use the ChickWeight dataset to learn (a little) about the tidyverse. The help system gives a basic summary of the experiment from which the data was collect:\n\n“The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups of chicks on different protein diets.”\n\nYou can get more information, including references by typing:\n\nhelp(\"ChickWeight\")\n\nThe Data:  There are 578 observations (rows) and 4 variables:\n\nChick – unique ID for each chick.\nDiet – one of four protein diets.\nTime – number of days since birth.\nweight – body weight of chick in grams.\n\nNote: weight has a lower case w (recall R is case sensitive).\nStore the data locally:\n\nChickWeight %>%\n  dplyr::select(Chick, Diet, Time, weight) %>% \n  dplyr::arrange(Chick, Diet, Time) %>% \n  write_csv(\"DATA/ChickWeight.csv\")\n\nFirst we will import the data from a file called ChickWeight.csv using the read_csv() function from the readr package (part of the tidyverse). The first thing to do, outside of R, is to open the file ChickWeight.csv to check what it contains and that it makes sense. Now we can import the data as follows:\n\nCW <- readr::read_csv(\"DATA/ChickWeight.csv\")\n\nRows: 578 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Chick, Diet, Time, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf all goes well then the data is now stored in an R object called CW. If you get the following error message then you need to change the working directory to where the data is stored:\n\nError: ‘ChickWeight.csv’ does not exist in current working directory …\n\nChanging the working directory: In RStudio you can use the menu bar (“Session - Set Working Directory - Choose Directory…”). Alternatively, you can use the function setwd(). Last but not least, to avoid issues with brocken paths to files and data sets, use RStudios’ “Project” tools.\nLooking at the Dataset: To look at the data type just type the object (dataset) name:\n\nCW\n\n# A tibble: 578 × 4\n   Chick  Diet  Time weight\n   <dbl> <dbl> <dbl>  <dbl>\n 1    18     1     0     39\n 2    18     1     2     35\n 3    16     1     0     41\n 4    16     1     2     45\n 5    16     1     4     49\n 6    16     1     6     51\n 7    16     1     8     57\n 8    16     1    10     51\n 9    16     1    12     54\n10    15     1     0     41\n# … with 568 more rows\n\n\nIf there are too many variables then not all them may be printed. To overcome this issue we can use the glimpse() function which makes it possible to see every column in your dataset (called a “data frame” in R speak).\n\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  <dbl> 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   <dbl> 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight <dbl> 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe function View() allows for a spread-sheet type of view on the data:\n\nView(CW)\n\n\n1.4.7.1 Tidyverse: Plotting Basics\nTo visualize the chick weight data, we will use the ggplot2 package (part of the tidyverse). Our interest is in seeing how the weight changes over time for the chicks by diet. For the moment don’t worry too much about the details just try to build your own understanding and logic. To learn more try different things even if you get an error messages.\nLet’s plot the weight data (vertical axis) over time (horizontal axis). Generally, ggplot2 works in layers. The following codes generates an empty plot:\n\n# An empty plot\nggplot(CW, aes(Time, weight))  \n\n\n\n\nEmpty ggplot layer.\n\n\n\n\nTo the empty plot, one can add fuhrer layers:\n\n# Adding a scatter plot \nggplot(CW, aes(Time, weight)) + geom_point() \n\n\n\n\nAdding a scatter plot layer to the empty ggplot layer.\n\n\n\n\nAdd color for Diet. The graph above does not differentiate between the diets. Let’s use a different color for each diet.\n\n# Adding colour for diet\nggplot(CW,aes(Time,weight,colour=factor(Diet))) +\n  geom_point() \n\n\n\n\nAdding a further layer for shown the effect of the Diet.\n\n\n\n\nIt is difficult to conclude anything from this graph as the points are printed on top of one another (with diet 1 underneath and diet 4 at the top).\nTo improve the plot, it will be handy to store Diet and Time as a factor variables.\nFactor Variables: Before we continue, we have to make an important change to the CW dataset by making Diet and Time factor variables. This means that R will treat them as categorical variables (see the <fct> variables below) instead of continuous variables. It will simplify our coding. The next section will explain the mutate() function.\n\nCW <- mutate(CW, Diet = factor(Diet))\nCW <- mutate(CW, Time = factor(Time))\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  <dbl> 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   <fct> 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight <dbl> 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe facet_wrap() function: To plot each diet separately in a grid using facet_wrap():\n\n# Adding jitter to the points\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  geom_point() +\n  facet_wrap(~Diet) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Diet 4 has the least variability but we can’t really say anything about the mean effect of each diet although diet 3 seems to have the highest.\nNext we will plot the mean changes over time for each diet using the stat_summary() function:\n\nggplot(CW, aes(Time, weight, \n               group=Diet, colour=Diet)) +\n  stat_summary(fun=\"mean\", geom=\"line\") \n\n\n\n\nInterpretation: We can see that diet 3 has the highest mean weight gains by the end of the experiment. However, we don’t have any information about the variation (uncertainty) in the data.\nTo see variation between the different diets we use geom_boxplot to plot a box-whisker plot. A note of caution is that the number of chicks per diet is relatively low to produce this plot.\n\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\")\n\n\n\n\nInterpretation: Diet 3 seems to have the highest “average” weight gain but it has more variation than diet 4 which is consistent with our findings so far.\nLet’s finish with a plot that you might include in a publication.\n\nggplot(CW, aes(Time, weight, group=Diet, \n                             colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_point() +\n  # geom_jitter() +\n  stat_summary(fun=\"mean\", geom=\"line\",\n               colour=\"black\") +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\") + \n  xlab(\"Time (days)\") +\n  ylab(\"Weight (grams)\")\n\n\n\n\n\n\n1.4.7.2 Tidyverse: Data Wrangling Basics\n\n\n\nIn this section we will learn how to wrangle (manipulate) datasets using the tidyverse package. Let’s start with the mutate(), select(), rename(), filter() and arrange() functions.\nmutate(): Adds a new variable (column) or modifies an existing one. We already used this above to create factor variables.\n\n# Added a column\nCWm1 <- mutate(CW, weightKg = weight/1000)\nCWm1\n\n# A tibble: 578 × 5\n  Chick Diet  Time  weight weightKg\n  <dbl> <fct> <fct>  <dbl>    <dbl>\n1    18 1     0         39    0.039\n2    18 1     2         35    0.035\n3    16 1     0         41    0.041\n# … with 575 more rows\n\n# Modify an existing column\nCWm2 <- mutate(CW, Diet = str_c(\"Diet \", Diet))\nCWm2\n\n# A tibble: 578 × 4\n  Chick Diet   Time  weight\n  <dbl> <chr>  <fct>  <dbl>\n1    18 Diet 1 0         39\n2    18 Diet 1 2         35\n3    16 Diet 1 0         41\n# … with 575 more rows\n\n\nselect(): Keeps, drops or reorders variables.\n\n# Drop the weight variable from CWm1 using minus\ndplyr::select(CWm1, -weight)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weightKg\n  <dbl> <fct> <fct>    <dbl>\n1    18 1     0        0.039\n2    18 1     2        0.035\n3    16 1     0        0.041\n# … with 575 more rows\n\n# Keep variables Time, Diet and weightKg\ndplyr::select(CWm1, Chick, Time, Diet, weightKg)\n\n# A tibble: 578 × 4\n  Chick Time  Diet  weightKg\n  <dbl> <fct> <fct>    <dbl>\n1    18 0     1        0.039\n2    18 2     1        0.035\n3    16 0     1        0.041\n# … with 575 more rows\n\n\nrename(): Renames variables whilst keeping all variables.\n\ndplyr::rename(CW, Group = Diet, Weight = weight)\n\n# A tibble: 578 × 4\n  Chick Group Time  Weight\n  <dbl> <fct> <fct>  <dbl>\n1    18 1     0         39\n2    18 1     2         35\n3    16 1     0         41\n# … with 575 more rows\n\n\nfilter(): Keeps or drops observations (rows).\n\ndplyr::filter(CW, Time==21 & weight>300)\n\n# A tibble: 8 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1     7 1     21       305\n2    29 2     21       309\n3    21 2     21       331\n# … with 5 more rows\n\n\nFor comparing values in vectors use: < (less than), > (greater than), <= (less than and equal to), >= (greater than and equal to), == (equal to) and != (not equal to). These can be combined logically using & (and) and | (or).\narrange(): Changes the order of the observations.\n\ndplyr::arrange(CW, Chick, Time)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1     1 1     0         42\n2     1 1     2         51\n3     1 1     4         59\n# … with 575 more rows\n\ndplyr::arrange(CW, desc(weight))\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1    35 3     21       373\n2    35 3     20       361\n3    34 3     21       341\n# … with 575 more rows\n\n\nWhat does the desc() do? Try using desc(Time).\n\n\n1.4.7.3 The pipe operator %>%\nIn reality you will end up doing multiple data wrangling steps that you want to save. The pipe operator %>% makes your code nice and readable:\n\nCW21 <- CW %>% \n  dplyr::filter(Time %in% c(0, 21)) %>% \n  dplyr::rename(Weight = weight) %>% \n  dplyr::mutate(Group = factor(str_c(\"Diet \", Diet))) %>% \n  dplyr::select(Chick, Group, Time, Weight) %>% \n  dplyr::arrange(Chick, Time) \nCW21\n\n# A tibble: 95 × 4\n  Chick Group  Time  Weight\n  <dbl> <fct>  <fct>  <dbl>\n1     1 Diet 1 0         42\n2     1 Diet 1 21       205\n3     2 Diet 1 0         40\n# … with 92 more rows\n\n\nHint: To understand the code above we should read the pipe operator %>% as “then”.\n\nCreate a new dataset (object) called CW21 using dataset CW then keep the data for days 0 and 21 then rename variable weight to Weight then create a variable called Group then keep variables Chick, Group, Time and Weight and then finally arrange the data by variables Chick and Time.\n\nThis is the same code:\n\nCW21 <- CW %>% \n  dplyr::filter(., Time %in% c(0, 21)) %>% \n  dplyr::rename(., Weight = weight) %>% \n  dplyr::mutate(., Group=factor(str_c(\"Diet \",Diet))) %>% \n  dplyr::select(., Chick, Group, Time, Weight) %>% \n  dplyr::arrange(., Chick, Time) \n\nThe pipe operator, %>%, replaces the dots (.) with whatever is returned from code preceding it. For example, the dot in filter(., Time %in% c(0, 21)) is replaced by CW. The output of the filter(...) then replaces the dot in rename(., Weight = weight) and so on. Think of it as a data assembly line with each function doing its thing and passing it to the next.\n\n\n1.4.7.4 The group_by() function\nFrom the data visualizations above we concluded that the diet 3 has the highest mean and diet 4 the least variation. In this section, we will quantify the effects of the diets using summmary statistics. We start by looking at the number of observations and the mean by diet and time.\n\nmnsdCW <- CW %>% \n  dplyr::group_by(Diet, Time) %>% \n  dplyr::summarise(N = n(), Mean = mean(weight)) %>% \n  dplyr::arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nmnsdCW\n\n# A tibble: 48 × 4\n# Groups:   Diet [4]\n  Diet  Time      N  Mean\n  <fct> <fct> <int> <dbl>\n1 1     0        20  41.4\n2 1     2        20  47.2\n3 1     4        19  56.5\n# … with 45 more rows\n\n\nFor each distinct combination of Diet and Time, the chick weight data is summarized into the number of observations (N) and the mean (Mean) of weight.\nFurther summaries: Let’s also calculate the standard deviation, median, minimum and maximum values but only at days 0 and 21.\n\nsumCW <-  CW %>% \n  dplyr::filter(Time %in% c(0, 21)) %>% \n  dplyr::group_by(Diet, Time) %>% \n  dplyr::summarise(N = n(),\n            Mean = mean(weight),\n            SD = sd(weight),\n            Median = median(weight),\n            Min = min(weight),\n            Max = max(weight)) %>% \n  dplyr::arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nsumCW\n\n# A tibble: 8 × 8\n# Groups:   Diet [4]\n  Diet  Time      N  Mean     SD Median   Min   Max\n  <fct> <fct> <int> <dbl>  <dbl>  <dbl> <dbl> <dbl>\n1 1     0        20  41.4  0.995   41      39    43\n2 1     21       16 178.  58.7    166      96   305\n3 2     0        10  40.7  1.49    40.5    39    43\n# … with 5 more rows\n\n\nLet’s make the summaries “prettier”, say, for a report or publication.\n\nlibrary(\"knitr\") # to use the kable() function\nprettySumCW <- sumCW %>% \n dplyr::mutate(`Mean (SD)` = str_c(format(Mean, digits=1),\n           \" (\", format(SD, digits=2), \")\")) %>% \n dplyr::mutate(Range = str_c(Min, \" - \", Max)) %>% \n dplyr::select(Diet, Time, N, `Mean (SD)`, Median, Range) %>%\n dplyr::arrange(Diet, Time) %>% \n kable(format = \"latex\")\nprettySumCW\n\n\n\n\n\n \n  \n    Diet \n    Time \n    N \n    Mean (SD) \n    Median \n    Range \n  \n \n\n  \n    1 \n    0 \n    20 \n    41 ( 0.99) \n    41.0 \n    39 - 43 \n  \n  \n    1 \n    21 \n    16 \n    178 (58.70) \n    166.0 \n    96 - 305 \n  \n  \n    2 \n    0 \n    10 \n    41 ( 1.5) \n    40.5 \n    39 - 43 \n  \n  \n    2 \n    21 \n    10 \n    215 (78.1) \n    212.5 \n    74 - 331 \n  \n  \n    3 \n    0 \n    10 \n    41 ( 1) \n    41.0 \n    39 - 42 \n  \n  \n    3 \n    21 \n    10 \n    270 (72) \n    281.0 \n    147 - 373 \n  \n  \n    4 \n    0 \n    10 \n    41 ( 1.1) \n    41.0 \n    39 - 42 \n  \n  \n    4 \n    21 \n    9 \n    239 (43.3) \n    237.0 \n    196 - 322 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: This summary table offers the same interpretation as before, namely that diet 3 has the highest mean and median weights at day 21 but a higher variation than group 4. However it should be noted that at day 21, diet 1 lost 4 chicks from 20 that started and diet 4 lost 1 from 10. This could be a sign of some health related issues.\n\n\n\n\n\n\n1.4.8 Further Links\n\nFurther R-Intros\n\nhttps://eddelbuettel.github.io/gsir-te/Getting-Started-in-R.pdf\nhttps://www.datacamp.com/courses/free-introduction-to-r\nhttps://swcarpentry.github.io/r-novice-gapminder/\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\n\n\n\nVersion Control (Git/GitHub)\n\nhttps://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN\nhttp://happygitwithr.com/\nhttps://www.gitkraken.com/\n\n\n\nR-Ladies\n\nhttps://rladies.org/"
  },
  {
    "objectID": "Ch2_StatLearning.html#parameter-inference",
    "href": "Ch2_StatLearning.html#parameter-inference",
    "title": "1  Statistical Learning",
    "section": "(Parameter) Inference",
    "text": "(Parameter) Inference\nWe are often interested in understanding the association between \\(Y\\) and \\(X_1,\\dots,X_p.\\) In this situation we wish to estimate \\(f,\\) but our goal is not necessarily to make predictions for \\(Y.\\) Now \\(\\hat{f}\\) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:\n\nWhich predictors are associated with the response?\nWhat is the relationship between the response and each predictor?\nCan the relationship between \\(Y\\) and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\nIn this course, we will see a number of examples that fall into the prediction setting, the inference setting, or a combination of the two."
  },
  {
    "objectID": "Ch2_StatLearning.html#how-do-we-estimate-f",
    "href": "Ch2_StatLearning.html#how-do-we-estimate-f",
    "title": "1  Statistical Learning",
    "section": "1.2 How Do We Estimate \\(f\\)?",
    "text": "1.2 How Do We Estimate \\(f\\)?\nWe will always assume that we have observed a set of \\(n\\) different data points \\[\n\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\},\n\\] where \\[\nx_i=(x_{i1},x_{i2},\\dots,x_{ip})^T\n\\] for all \\(i=1,2,\\dots,n.\\)\nThese \\(n\\) observations are called training data because we will use these training observations to train, or teach, our method how to estimate \\(f.\\)\nOur goal is to find a function \\(hat{f}\\) such that \\[\nY \\approx \\hat{f}(X)\n\\] for any observation \\((X, Y)\\).\nBroadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric.\n\nParametric Methods\nParametric methods involve a two-step model-based approach.\n\nFirst, we make an assumption about the functional form, or shape, of \\(f.\\) For example, a very simple, but often used assumption is that \\(f\\) is linear in \\(X\\): \\[\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\tag{1.1}\\]\nAfter a model has been selected, we need a procedure that uses the training data to fit or train the model. For example, in the case of the linear model Equation 1.1, we need to estimate the parameters \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\) such that \\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nMost common estimation technique: (Ordinary) Least Squares\nThe parametric model-based approach reduces the problem of estimating \\(f\\) down to one of estimating a set of parameters.\n\nPro: Simple to estimate\nCon: Possible model misspecification (Why shall we know that the shape/form of the true \\(f\\)?)\n\nWe can try to address the problem of model misspecification by choosing flexible models that can fit many different possible functional forms for \\(f.\\)\nBut: Fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely.\nThese issues are discussed through-out this course.\n\n\nNon-Parametric Methods\nNon-parametric methods do not make explicit assumptions about the functional form of \\(f.\\) Instead, they make qualitative assumptions on \\(f\\) being a smooth function (i.e. continuously differentiable).\nBy avoiding the assumption of a particular, parametric functional form for \\(f,\\) non-parametric methods have the potential to accurately fit a wider range of possible shapes for \\(f.\\)\nMajor disadvantage: Non-parametric methods require a large number of observations to obtain an accurate estimate for \\(f;\\) far more than is typically needed for a parametric approach if the parametric model assumption is correct.\n\n\n1.2.1 The Trade-Off Between Prediction Accuracy and Model Interpretability\nOne might reasonably ask the following question:\n\nWhy would we ever choose to use a more restrictive method instead of a very flexible approach?\n\nIf we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between \\(Y\\) and \\(X_1,\\dots,X_p.\\)\nBy contrast, very flexible approaches, such as purely non-parametric methods, can lead to such complicated estimates of \\(f\\) that it is difficult to understand how any individual predictor \\(X_j\\) is associated with the response \\(Y.\\)\nIn some settings, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the algorithm is that it predict accurately–interpretability is not a concern. In such settings, we might expect that it will be best to use the most flexible model available. Right?\nSurprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.\n\n\n1.2.2 Supervised Versus Unsupervised Learning\nMost statistical learning problems fall into one of two categories:\n\nsupervised or\nunsupervised.\n\nIn supervised learning problems, we observe for each predictor \\(x_i,\\) \\(i=1,\\dots,n,\\) also a response \\(y_i.\\)\nIn unsupervised learning problems, we only the predictor \\(x_i,\\) \\(i=1,\\dots,n,\\) but not the associated responses \\(y_i.\\)\nSupervised learning methods:\n\nregression analysis\nlogistic regression\nlasso\nridge regression\n\nUnsupervised learning methods:\n\ncluster analysis (clustering)\n\\(K\\)-means\n\n\n\n1.2.3 Regression Versus Classification Problems\nVariables can be characterized as either quantitative or qualitative (also known as categorical).\nQuantitative: Quantitative variables take on numerical values. Examples include a person’s age, height, or income, the value of a house, and categorical the price of a stock.\nQualitative/Categorial: Examples of qualitative variables include a person’s marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia).\nWe tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems.\nHowever, the distinction (regression vs. classification) is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or binary). Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K-nearest neighbors can be used in the case of either quantitative or qualitative responses."
  },
  {
    "objectID": "Ch2_StatLearning.html#assessing-model-accuracy",
    "href": "Ch2_StatLearning.html#assessing-model-accuracy",
    "title": "1  Statistical Learning",
    "section": "1.3 Assessing Model Accuracy",
    "text": "1.3 Assessing Model Accuracy\nThere is no free lunch in statistics: no one method dominates all others over all possible data sets. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set.\nTherefore, it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.\n\n1.3.1 Measuring the Quality of Fit\nIn the regression setting, the most commonly-used measure is the mean squared (prediction) error (MSE).\nThe training (data) MSE is given by \\[\\begin{align*}\n\\operatorname{MSE}_{\\text{train}}=\\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat{f}(x_i)\\right)^2,\n\\end{align*}\\] where * \\(\\hat{f}\\) is computed from the training data * \\(\\hat{f}(x_i)\\) is the prediction that \\(\\hat{f}\\) gives for the \\(i\\)th training data observation.\nIn general, however, we do not really care how well the method works on the training data. In fact, a very flexible (e.g. non-parametric) estimation method will tend to overfit the training data such that \\(y_i\\approx \\hat{f}(x_i)\\) for all \\(i=1,\\dots,n\\) resulting in a training MSE that is close to zero since \\(\\hat{f}(x_i)\\) fits also the errors \\(\\epsilon_i.\\)\nWe are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.\nExample: Suppose that we are interested in developing an algorithm to predict a stock’s price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don’t really care how well our method predicts last week’s stock price. We instead care about how well it will predict tomorrow’s price or next month’s price.\nExample: Suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements.\nThus, we want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE.\nLet \\(\\hat{f}\\) be computed from the training data \\(\\{(x_1,y_1),\\dots,(x_n,y_n)\\}.\\) And let \\[\n\\{(x_{01},y_{01}),(x_{02},y_{02})\\dots,(x_{0m},y_{0m})\\}\n\\] denote the set of \\(m\\) test data points. Then, the test MSE is given by, \\[\\begin{align*}\n\\operatorname{MSE}_{\\text{test}}=\\frac{1}{m}\\sum_{i=1}^m\\left(y_{0i} - \\hat{f}(x_{0i})\\right)^2.\n\\end{align*}\\]\nNote that if \\(\\hat{f}\\approx f,\\) then \\[\n\\operatorname{MSE}_{\\text{test}}\\approx \\frac{1}{m}\\sum_{i=1}^m\\epsilon_i^2\n\\] estimates the variance of the error term, i.e., equals the irreducible error component of the mean squared (prediction) error."
  },
  {
    "objectID": "Ch1_StatLearning.html",
    "href": "Ch1_StatLearning.html",
    "title": "1  Statistical Learning",
    "section": "",
    "text": "Reading: Chapter 2 of our course textbook An Introduction to Statistical Learning"
  },
  {
    "objectID": "Ch1_StatLearning.html#what-is-statistical-learning",
    "href": "Ch1_StatLearning.html#what-is-statistical-learning",
    "title": "1  Statistical Learning",
    "section": "1.1 What is Statistical Learning?",
    "text": "1.1 What is Statistical Learning?\nSuppose that we observe a quantitative response \\(Y\\) and \\(p\\) different predictors, \\(X_1, X_2, \\dots, X_p.\\)\nWe assume that there is some relationship between \\(Y\\) and \\[\nX = (X_1, X_2, \\dots, X_p),\n\\] which can be written in the very general form \\[\nY = f(X) + \\epsilon.\n\\]\n\n\\(f\\) is some fixed but unknown function of \\(X = (X_1, X_2, \\dots, X_p):\\)\n\\(\\epsilon\\) a random error term fulfilling the following two assumptions:\n\n\\(\\epsilon\\) and \\(X\\) are independent of each other\n\\(\\epsilon\\) has mean zero \\(E(\\epsilon)=0.\\)\n\n\nIn this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y.\\)\nIn essence, statistical learning refers to a set of approaches for estimating \\(f.\\) In this chapter we outline some of the key theoretical concepts that arise in estimating \\(f,\\) as well as tools for evaluating the estimates obtained.\n\n1.1.1 Why Estimate \\(f\\)?\nThere are two main reasons that we may wish to estimate \\(f\\):\n\nprediction and\ninference.\n\nWe discuss each in turn."
  },
  {
    "objectID": "Ch1_StatLearning.html#prediction",
    "href": "Ch1_StatLearning.html#prediction",
    "title": "1  Statistical Learning",
    "section": "Prediction",
    "text": "Prediction\nIn many situations, a set of inputs \\(X\\) are readily available, but the output \\(Y\\) cannot be easily obtained. In this setting, since the error term averages to zero, we can predict \\(Y\\) using \\[\n\\hat{Y} = \\hat{f}(X),\n\\]\n\n\\(\\hat{f}\\) represents our estimate for \\(f\\)\n\\(\\hat{Y}\\) represents the resulting prediction for \\(Y\\)\n\nIn this setting, \\(\\hat{f}\\) is often treated as a black box, in the sense that one is not typically concerned with the exact form of \\(\\hat{f},\\) provided that it yields accurate predictions for \\(Y.\\)\nExample: As an example, suppose that \\((X_1, X_2, \\dots, X_p)\\) are characteristics of a patient’s blood sample that can be easily measured in a lab, and \\(Y\\) is a variable encoding the patient’s risk for a severe adverse reaction to a particular drug. It is natural to seek to predict \\(Y\\) using \\(X,\\) since we can then avoid giving the drug in question to patients who are at high risk of an adverse reaction–that is, patients for whom the estimate of \\(Y\\) is high.\n\nAccuracy of a Prediction\nThe accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on two quantities:\n\nthe reducible error and\nthe irreducible error.\n\nIn general, \\(\\hat{f}\\) will not be a perfect estimate for \\(f,\\) and this inaccuracy will introduce some error. This error is reducible, because we can potentially improve the accuracy of \\(\\hat{f}\\) by using the most appropriate statistical learning technique to estimate \\(f.\\)\nHowever, even if it were possible to form a perfect estimate for \\(f,\\) so that our estimated response took the form \\[\n\\hat{Y} = f (X),\n\\] our prediction would still have some error in it! This is because \\(Y\\) is also a function of \\(\\epsilon\\) which, by definition, cannot be predicted using \\(X.\\) Therefore, variability associated with \\(\\epsilon\\) also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate \\(f,\\) we cannot reduce the error introduced by \\(\\epsilon.\\)\nConsider a given estimate \\(\\hat{f}\\) and a given set of predictors \\(X,\\) which yields the prediction \\(\\hat{Y} = \\hat{f}(X).\\) Assume for a moment that both \\(\\hat{f}\\) and \\(X\\) are fixed, so that the only variability comes from \\(\\epsilon.\\) Then, it is easy to show that \\[\\begin{align*}\n\\overbrace{E\\left[(Y - \\hat{Y})^2\\right]}^{\\text{Mean Squared (Prediction) Error}}\n=\\underbrace{\\left(f(X) -\\hat{f}(X)\\right)^2}_{\\text{reducable}} + \\underbrace{Var\\left(\\epsilon\\right)}_{\\text{irreducable}},\n\\end{align*}\\] where\n\n\\(E\\left[(Y - \\hat{Y})^2\\right]\\) represents the expected value, of the squared difference between the predicted \\(\\hat{Y}=\\hat{f}(X)\\) and actual value of \\(Y,\\)\nand \\(Var(\\epsilon)\\) represents the variance associated with the error term \\(\\epsilon.\\)\n\nDerivation (for a given \\(\\hat{f}\\) and a given \\(X;\\) i.e. only \\(\\epsilon\\) is random):\n\\[\\begin{align*}\n&E\\left[(Y - \\hat{Y})^2\\right]\\\\\n&=E\\left[(f(X) + \\epsilon - \\hat{f}(X))^2\\right] \\\\\n&=E\\left[\\left(f(X) -\\hat{f}(X)\\right)^2 - 2\\left(f(X) -\\hat{f}(X)\\right)\\epsilon + \\epsilon^2\\right] \\\\\n% &=E\\left[\\left(f(X) -\\hat{f}(X)\\right)^2\\right] - 2E\\left[\\left(f(X) -\\hat{f}(X)\\right)\\epsilon\\right] + E\\left[\\epsilon^2\\right] \\\\\n&=\\left(f(X) -\\hat{f}(X)\\right)^2 - 2\\left(f(X) -\\hat{f}(X)\\right) E\\left[\\epsilon\\right] + E\\left[\\epsilon^2\\right] \\\\\n&=\\left(f(X) -\\hat{f}(X)\\right)^2 - 2\\left(f(X) -\\hat{f}(X)\\right) \\cdot 0 + Var\\left(\\epsilon\\right) \\\\\n&=\\underbrace{\\left(f(X) -\\hat{f}(X)\\right)^2}_{\\text{reducable}} + \\underbrace{Var\\left(\\epsilon\\right)}_{\\text{irreducable}}\n\\end{align*}\\]\nIt is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for \\(Y,\\) i.e.  \\[\nE\\left[(Y - \\hat{Y})^2\\right] \\geq Var\\left(\\epsilon\\right)\n\\] This bound is almost always unknown in practice.\n\n\n\n\n\n\nTip\n\n\n\nThe focus of this course is on techniques for estimating \\(f\\) with the aim of minimizing the reducible error."
  },
  {
    "objectID": "Ch1_StatLearning.html#parameter-inference",
    "href": "Ch1_StatLearning.html#parameter-inference",
    "title": "1  Statistical Learning",
    "section": "(Parameter) Inference",
    "text": "(Parameter) Inference\nWe are often interested in understanding the association between \\(Y\\) and \\(X_1,\\dots,X_p.\\) In this situation we wish to estimate \\(f,\\) but our goal is not necessarily to make predictions for \\(Y.\\) Now \\(\\hat{f}\\) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:\n\nWhich predictors are associated with the response?\nWhat is the relationship between the response and each predictor?\nCan the relationship between \\(Y\\) and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\n\n\n\n\n\n\nTip\n\n\n\nIn this course, we will see a number of examples that fall into the prediction setting, the inference setting, or a combination of the two."
  },
  {
    "objectID": "Ch1_StatLearning.html#how-do-we-estimate-f",
    "href": "Ch1_StatLearning.html#how-do-we-estimate-f",
    "title": "1  Statistical Learning",
    "section": "1.2 How Do We Estimate \\(f\\)?",
    "text": "1.2 How Do We Estimate \\(f\\)?\nSetup: Consider the general regression model \\[\nY=f(X)+\\epsilon,\n\\tag{1.1}\\] where \\[\nX=(X_{1}, \\dots,X_{p})\n\\] is a multivariate (\\(p\\)-dimensional) predictor.\nLet \\[\n\\{(X_1,Y_1),(X_2,Y_2),\\dots,(X_n,Y_n)\\},\n\\tag{1.2}\\] be a random sample from Equation 1.1, i.e.\n\nThe multivariate, \\(p+1\\) dimensional, random vectors \\[\n(X_i,Y_i)\\quad\\text{and}\\quad (X_j,Y_j)\n\\] are independent of each other for all \\(i=1,\\dots,n\\) and \\(j=1,\\dots,n\\) with \\(i\\neq j.\\)\nThe multivariate, \\(p+1\\) dimensional, random vectors \\((X_i,Y_i),\\) \\(i=1,\\dots,n,\\) have all the same distribution as \\((X,Y),\\) i.e. \\[\n(X_i,Y_i)\\sim(X,Y)\n\\] for all \\(i=1,\\dots,n.\\) \n\nThe random sample Equation 1.2 is thus a set of \\(n\\) many independent and identically distributed (iid) multivariate random variables \\[\n\\{(X_1,Y_1),(X_2,Y_2),\\dots,(X_n,Y_n)\\}\n\\] with \\[\n(X_i,Y_i)\\overset{\\text{iid}}{\\sim} (X,Y),\\quad i=1,\\dots,n.\n\\]\nAn observed realization of the random sample will be denoted using lowercase letters \\[\n\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\}.\n\\] These \\(n\\) observations are called training data because we will use these observations to train/learn \\(\\hat{f}\\), i.e., to compute the estimate \\(\\hat{f}\\) of the unknown \\(f.\\)\n\n\n\n\n\n\nGoal of Statistical Learning\n\n\n\nOur goal is to find (i.e. learn from training data) a function \\(\\hat{f}\\) such that \\[\nY \\approx \\hat{f}(X)\n\\] for any observed realization of \\((X, Y).\\)\nThat is, the estimate \\(\\hat{f}\\) needs to provide a good approximation \\[\ny_{i} \\approx \\hat{f}(x_{i})\n\\] not only for the observed training data points \\[\n(x_i,y_i),\\quad i=1,\\dots,n,\n\\] but also for any possible new realization \\((x_{new},y_{new})\\) of \\((X,Y)\\) \\[\ny_{new} \\approx \\hat{f}(x_{new}).\n\\]\nFitting the noise (irreducible component) in the training data will typically lead to bad approximations of new observations of a test data set.\n \n\n\n\n\n\nBroadly speaking, most statistical learning methods for this task can be characterized as either\n\nparametric or\nnon-parametric.\n\nWe discuss each in turn.\n\nParametric Methods\nParametric methods involve a two-step model-based estimation approach:\n\nFirst, we make an assumption about the functional form, or shape, of \\(f.\\) For example, a very simple, but often used assumption is that \\(f\\) is a linear function, i.e. \\[\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p.\n\\tag{1.3}\\]\nAfter a model has been selected, we need a procedure that uses the training data to fit or train the model. For example, in the case of the linear model Equation 1.3, we need to estimate the parameters \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\) such that \\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nMost common estimation technique: (Ordinary) Least Squares (OLS)\nThe parametric model-based approach reduces the problem of estimating \\(f\\) down to one of estimating a finite set of parameters \\(\\beta_0,\\beta_1,\\dots,\\beta_p.\\)\n\nPro: Simple to estimate\nCon: Possible model misspecification (Why should we know the true shape/form of \\(f\\)?)\n\nWe can try to address the problem of model misspecification by choosing flexible models that can fit many different possible functional forms for \\(f.\\)\nBut: Fitting a more flexible model requires estimating a greater number of parameters (large \\(p\\)). These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors/noise too closely.\n\n\n\n\n\n\nTip\n\n\n\nThese issues (model-flexibility and overfitting) are discussed through-out this course.\n\n\n\n\nNon-Parametric Methods\nNon-parametric methods (e.g. \\(K\\) nearest neighbor regression) do not make explicit assumptions about the functional form of \\(f.\\) Instead, they make qualitative assumptions on \\(f\\) such as, for instance, requiring that \\(f\\) is a smooth (e.g. two times continuously differentiable) function.\n\nPro: By avoiding the assumption of a particular parametric functional form for \\(f,\\) non-parametric methods have the potential to accurately fit a wider range of possible shapes for \\(f.\\)\nCon: Non-parametric methods require a large number of observations to obtain an accurate estimate for \\(f;\\) far more observations than is typically needed for a parametric approach if the parametric model assumption is correct. (Non-parametric methods are “data-hungry”.)\n\n\n\n1.2.1 The Trade-Off Between Prediction Accuracy and Model Interpretability\nOne might reasonably ask the following question:\n\nWhy would we ever choose to use a more restrictive method instead of a very flexible approach?\n\nIf we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between \\(Y\\) and \\(X_1,\\dots,X_p.\\)  By contrast, very flexible approaches, such as purely non-parametric methods, can lead to such complicated estimates of \\(f\\) that it is difficult to understand how any individual predictor \\(X_j\\) is associated with the response \\(Y.\\)\nIn some settings, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the algorithm is that it predict accurately. In such settings, we might expect that it will be best to use the most flexible model available. Right? Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.\n\n\n1.2.2 Supervised versus Unsupervised Learning\nMost statistical learning problems fall into one of two categories:\n\nsupervised\nunsupervised\n\nIn supervised learning problems, we observe for each predictor \\(x_i,\\) \\(i=1,\\dots,n,\\) also a response \\(y_i.\\)\nIn unsupervised learning problems, we only observe the predictor \\(x_i,\\) \\(i=1,\\dots,n,\\) but not the associated responses \\(y_i.\\)\nSupervised learning methods:\n\nregression analysis\nlogistic regression\nlasso\nridge regression\n\nUnsupervised learning methods:\n\ncluster analysis (clustering)\n\\(K\\)-means\n\n\n\n1.2.3 Regression Versus Classification Problems\nVariables can be characterized as either quantitative or qualitative (also known as categorical).\nQuantitative: Quantitative variables take on numerical values. Examples include a person’s age, height, or income, the value of a house, and categorical the price of a stock.\nQualitative/Categorial: Examples of qualitative variables include a person’s marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia).\nWe tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems.\nHowever, the distinction (regression vs. classification) is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or binary). Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K-nearest neighbors can be used in the case of either quantitative or qualitative responses."
  },
  {
    "objectID": "Ch1_StatLearning.html#assessing-model-accuracy",
    "href": "Ch1_StatLearning.html#assessing-model-accuracy",
    "title": "1  Statistical Learning",
    "section": "1.3 Assessing Model Accuracy",
    "text": "1.3 Assessing Model Accuracy\nIt is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.\nLet \\[\n\\{(X_{01},Y_{01}),(X_{02},Y_{02}),\\dots,(X_{0m},Y_{0m})\\},\n\\tag{1.4}\\] denote the test data random sample, where \\[\n(X_{0i},Y_{0i})\\overset{\\text{iid}}{\\sim}(X,Y),\\quad i=1,\\dots,m.\n\\] with \\((X,Y)\\) being defined by the general regression model \\(Y=f(X)+\\epsilon\\) in Equation 1.1.\nThat is, the new test data random sample Equation 1.4\n\nis independent of the training data random sample Equation 1.2\nhas the same distribution as the training data random sample Equation 1.2\n\nThe observed realization \\[\n\\{(x_{01},y_{01}),(x_{02},y_{02}),\\dots,(x_{0m},y_{0m})\\},\n\\] of the test data random sample is used to check the accuracy of the estimate \\(\\hat{f}.\\)\n\n1.3.1 Measuring the Quality of Fit\nIn the regression setting, the most commonly-used measure is the mean squared (prediction) error (MSE).\nThe global training (data) MSE is given by \\[\\begin{align*}\n\\operatorname{MSE}_{\\text{train}}=\\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat{f}(x_i)\\right)^2,\n\\end{align*}\\] where\n\n\\(\\hat{f}\\) is computed from the training data\n\\(\\hat{f}(x_i)\\) is the prediction that \\(\\hat{f}\\) gives for the \\(i\\)th training data observation.\n\nIn general, however, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.\n\n\n\nThus, we want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE.\n\n\nLocal (i.e. Point-Wise) Test MSE\nLet \\(\\hat{f}\\) be computed from the training data \\(\\{(x_1,y_1),\\dots,(x_n,y_n)\\}.\\) And let \\[\n\\{(x_{0},y_{01}),(x_{0},y_{02})\\dots,(x_{0},y_{0m})\\}\n\\] denote the set of \\(m\\) test data points \\(y_{01},\\dots,y_{0m}\\) for one given predictor value \\(x_0\\).\nThis type of \\(x_0\\)-specific test data is a realization of a conditional random sample given \\(X=x_0,\\) \\[\n(x_{0},Y_{0i})\\overset{\\text{iid}}{\\sim}(X,Y)|X=x_0,\\quad i=1,\\dots,m.\n\\] This test data random sample is independent of the training data random sample whose realization was used to compute \\(\\hat{f}.\\)\nThen, the point-wise test MSE at \\(X=x_0\\) is given by, \\[\\begin{align*}\n\\operatorname{MSE}_{\\text{test}}(x_0)= \\frac{1}{m}\\sum_{i=1}^m\\left(y_{0i} - \\hat{f}(x_{0})\\right)^2.\n\\end{align*}\\]\n\n\nGlobal Test MSE\nTypically, however, we want that a method has globally, i.e. for all predictor values in the range of \\(X\\), a low test MSE (not only at a certain given value \\(x_0\\)). Let \\[\n\\{(x_{01},y_{01}),(x_{02},y_{02})\\dots,(x_{0m},y_{0m})\\}\n\\] denote the set of \\(m\\) test data points with different predictor values \\(x_{01},\\dots,x_{0m}\\) in the range of \\(X\\). This type of test data is a realization of a random sample \\[\n(X_{0i},Y_{0i})\\overset{\\text{iid}}{\\sim}(X,Y),\\quad i=1,\\dots,m.\n\\] This test data random sample is independent of the training data random sample whose realization was used to compute \\(\\hat{f}.\\)\nThen, the global test MSE is given by, \\[\\begin{align*}\n\\operatorname{MSE}_{\\text{test}}=\\frac{1}{m}\\sum_{i=1}^m\\left(y_{0i} - \\hat{f}(x_{0i})\\right)^2.\n\\end{align*}\\]\nNote that if \\(\\hat{f}\\) is a really good estimate of \\(f,\\) i.e. if \\(\\hat{f}\\approx f,\\) then \\[\n\\operatorname{MSE}_{\\text{test}}\\approx \\frac{1}{m}\\sum_{i=1}^m\\epsilon_{0i}^2\n\\] estimates the variance of the error term \\(Var(\\epsilon)\\), i.e., the irreducible error component.\n\nFigure 2.9 shows training and test MSEs for smoothing spline (R command smooth.spline()) estimates \\(\\hat{f}\\) in the case of\n\na moderately complex \\(f\\)\na moderate signal-to-noise ratio \\(\\frac{Var(f(X))}{Var(\\epsilon)}\\)\n\n\n\nFigure 2.10 shows training and test MSEs for smoothing spline estimates \\(\\hat{f}\\) in the case of\n\na very simple \\(f\\)\na moderate signal-to-noise ratio \\(\\frac{Var(f(X))}{Var(\\epsilon)}\\)\n\n\n\nFigure 2.11 shows training and test MSEs for smoothing spline estimates \\(\\hat{f}\\) in the case of\n\na moderately complex \\(f\\)\na very large signal-to-noise ratio \\(\\frac{Var(f(X))}{Var(\\epsilon)}\\)\n\n\n\n\n\n\n\n\n\nCoding Challenge:\n\n\n\nGenerate MSE-results similar to those shown in Figure 2.9.\n\n\nIn practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available.\nAs the three examples in Figures 2.9, 2.10, and 2.11 of our textbook illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably.\nThroughout this book, we discuss a variety of approaches that can be used in practice to estimate the minimum point of the test MSE.\nOne important method is cross-validation, which is a method for estimating the test MSE using the training data.\n\n\n1.3.2 The Bias-Variance Trade-Off\nThe U-shape observed in the test MSE curves (Figures 2.9–2.11) turns out to be the result of two competing properties of statistical learning methods.\n\nLet \\(\\hat{f}\\) be estimated from the training data random sample \\[\n\\{(X_1,Y_1),\\dots,(X_n,Y_n)\\},\n\\] \\[\n(X_i,Y_i)\\overset{\\text{iid}}{\\sim}(X,Y),\\quad i=1,\\dots,n.\n\\] I.e., since \\(\\hat{f}\\) is based on the random variables in the random sample, \\(\\hat{f}\\) is it self a random variable. (A realized observation of the training data random sample yields a realized observation of \\(\\hat{f}\\).)\nLet \\(x_0\\) denote a given value of the predictor \\(X\\)\nLet \\[\n\\{(x_{0},Y_{01}),\\dots,(x_0,Y_{0m})\\}\n\\] \\[\n(x_{0},Y_{0i})\\overset{\\text{iid}}{\\sim}(X,Y)|X=x_0,\\quad i=1,\\dots,m.\n\\] be the conditional test data random sample given \\(X=x_0.\\)\n\n\nOne can show that the expected test MSE at a given predictor value \\(x_0\\) can be decomposed as following: \\[\\begin{align*}\nE\\left[\\operatorname{MSE}_{test}(x_0)\\right]\n& =E\\left[\\frac{1}{m}\\sum_{i=1}^m\\left(Y_{0i}- \\hat{f}(x_0)\\right)^2\\right]\\\\[2ex]\n& =\\frac{1}{m}\\sum_{i=1}^mE\\left[\\left(Y_{0i}- \\hat{f}(x_0)\\right)^2\\right]\\quad(\\text{linearity of $E$})\\\\[2ex]\n& =\\frac{1}{m}\\,m\\,E\\left[\\left(Y_{0}- \\hat{f}(x_0)\\right)^2\\right]\\quad(\\text{since $Y_{0i}$ are iid})\\\\[2ex]\n& =E\\left[\\left(Y_0- \\hat{f}(x_0)\\right)^2\\right]\\\\[2ex]\n& =E\\left[\\left(f(x_0) + \\epsilon_0 - \\hat{f}(x_0)\\right)^2\\right]\\quad(Y_0=f(x_0)+\\epsilon_0)\\\\[2ex]\n& =E\\left[\\left(f(x_0)- \\hat{f}(x_0)\\right)^2 +2\\left(f(x_0)- \\hat{f}(x_0)\\right)\\epsilon_0 + \\epsilon_0^2 \\right]\\\\[2ex]\n& =E\\left[\\left(f(x_0)- \\hat{f}(x_0)\\right)^2\\right]\\\\[2ex]\n&+ \\underbrace{2E\\left[\\left(f(x_0)- \\hat{f}(x_0)\\right)\\right]\\overbrace{E\\left[\\epsilon_0\\right]}^{=0}}_{\\text{using independence between training (in $\\hat{f}$) and testing data}}\\\\[2ex]\n&+ E\\left[\\epsilon_0^2 \\right]\\\\[2ex]\n& =\\underbrace{E\\left[\\left(f(x_0)- \\hat{f}(x_0)\\right)^2\\right]}_{\\text{MSE of $\\hat{f}(x_0)$}}+0+Var(\\epsilon_0)\\\\[2ex]\n%& =E\\left[\\left(Y_0- \\hat{f}(x_0) \\underbrace{+f(x_0)-f(x_0)}_{=0}\\right)^2\\right]\\\\[2ex]\n%& =E\\left[\\left(\\left(f(x_0)-\\hat{f}(x_0)\\right)+\\epsilon\\right)^2\\right]\\\\[2ex]\n%& =E\\left[\\left(f(x_0)-\\hat{f}(x_0)\\right)^2+2\\left(f(x_0)-\\hat{f}(x_0)\\right)\\epsilon+\\epsilon^2\\right]\\\\[2ex]\n%&\\quad \\text{Since $\\epsilon$ (train) and $\\hat{f}$ (test) are independent:}\\\\[2ex]\n%& =E\\left[\\left(f(x_0)-\\hat{f}(x_0)\\right)^2\\right]+0+Var(\\epsilon_0)\\\\[2ex]\n%& =E\\left[\\left(f(x_0)-\\hat{f}(x_0)\\underbrace{+E(\\hat{f}(x_0))-E(\\hat{f}(x_0))}_{=0}\\right)^2\\right]+Var(\\epsilon_0)\\\\[2ex]\n%& =E\\left[\\left(-\\left\\{E(\\hat{f}(x_0)) - f(x_0)\\right\\} - \\left\\{\\hat{f}(x_0)-E(\\hat{f}(x_0))\\right\\}\\right)^2\\right]+Var(\\epsilon_0)\\\\[2ex]\n%&\\quad\\text{(steps skipped since beyond scope)}\\\\[2ex]\n& = Var\\left(\\hat{f}(x_0)\\right) + \\left[\\operatorname{Bias}\\left(\\hat{f}(x_0)\\right)\\right]^2 + Var\\left(\\epsilon_0\\right)\n\\end{align*}\\]\nThe expected MSE at \\(x_0,\\) \\(E\\left[\\operatorname{MSE}_{test}(x_0)\\right],\\) refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using training data set, and evaluated each at \\(x_0.\\)\n\n\n\n\n\n\nNote\n\n\n\nA computed value of \\(\\operatorname{MSE}_{test}(x_0)\\) (as done in the coding challenge) is not able to consistently approximate \\(E\\left[\\operatorname{MSE}_{test}(x_0)\\right].\\)\nHowever, to get information about Bias and Variance of a method, we need to approximate \\(E\\left[\\operatorname{MSE}_{test}(x_0)\\right].\\) This will be (among others) the topic of Chapter 4.\n\n\nTo minimize the expected test MSE, we need to select a statistical learning method that simultaneously achieves low variance and low bias.\nNote that \\[\nVar\\left(\\hat{f}(x_0)\\right)\\geq 0\n\\] and that \\[\n\\left[\\operatorname{Bias}\\left(\\hat{f}(x_0)\\right)\\right]^2\\geq 0.\n\\] Thus, the expected test MSE can never lie below of \\(Var(\\epsilon),\\) i.e. \\[\n\\begin{align*}\nE\\left[\\operatorname{MSE}_{test}(x_0)\\right]\n& =E\\left[\\left(Y_0- \\hat{f}(x_0)\\right)^2\\right]\n\\geq Var\\left(\\epsilon\\right).\n\\end{align*}\n\\]\n➡️ The overall, i.e., global expected test MSE can be computed by averaging \\(E[(Y_0- \\hat{f}(x_0))^2]\\) over all possible values of \\(x_0\\) in the test set. \n\nVariance of \\(\\hat{f}\\) at \\(x_0\\)\n\\[\nVar(\\hat{f}(x_0))=E\\left[\\left(\\hat{f}(x_0) - E\\left[\\hat{f}(x_0)\\right]\\right)^2\\right]\n\\] Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different \\(\\hat{f}.\\) But ideally the estimate for \\(f\\) should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in \\(\\hat{f}.\\) In general, more flexible statistical methods have higher variance.\n➡️ The overall, i.e., global variance can be computed by averaging \\(Var(\\hat{f}(x_0))\\) over all possible values of \\(x_0\\) in the test set.\n\n\nBias of \\(\\hat{f}\\) at \\(x_0\\)\n\\[\n\\operatorname{Bias}(\\hat{f}(x_0))=E\\left[\\hat{f}(x_0)\\right] - f(x_0)\n\\] Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease—and vice versa.\n➡️ The overall, i.e., global bias can be computed by averaging \\(\\operatorname{Bias}(\\hat{f}(x_0))\\) over all possible values of \\(x_0\\) in the test set.\n\n\n\n\n1.3.3 The Classification Setting\nSetup:\n\nObserved training data \\[\n\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\}\n\\]\nQualitative response \\(y_i\\) with categorical class labels. E.g.\n\n\\(y_i\\in\\{\\text{red},\\text{blue}\\}\\)\n\\(y_i\\in\\{\\text{positive returns},\\text{negative returns}\\}\\)\n\nThe classifier \\(\\hat{f}\\) is computed from the training data.\nPredicted training data class labels: \\[\n\\hat{y}_i = \\hat{f}(x_i)\n\\]\n\nThe alternative to the training MSE is here the training error rate \\[\n\\frac{1}{n}\\sum_{i=1}^nI(y_i\\neq \\hat{y}_i)\n\\] which gives the relative frequency of false categorical predictions.\nHere, \\[\nI(\\cdot)\n\\] is an indicator function with \\(I(\\text{true})=1\\) and \\(I(\\text{false})=0.\\)\nLet \\[\n\\{(y_{01},x_{01}), (y_{02},x_{02}),\\dots, (y_{0m},x_{0m})\\}\n\\] denote \\(m\\) test data observations.\nThe alternative to the test MSE is here the test error rate \\[\n\\frac{1}{m}\\sum_{i=1}^mI(y_{0i}\\neq \\hat{y}_{0i}),\n\\] where \\(\\hat{y}_{0i}\\) is the predicted class label that results from applying the classifier \\(\\hat{f}\\) (computed from the training data) to the test observation with predictor value \\(x_{i0}.\\)\nA good classifier is one for which the test error rate is smallest.\n\n\nThe Bayes Classifier\nIt is possible to show (proof is outside of the scope of this course) that the test error rate is minimized, on average, by the classifier that assigns an observation to the most likely class, given its predictor value \\(x_{0}.\\) This classifier is called the Bayes classifier.\nIn other words, the Bayes classifier assigns a test observation with predictor vector \\(x_{0}\\) to the class \\(j\\) for which \\[\nP(Y = j | X = x_{0})\n\\] is largest among all possible class labels \\(j\\) (e.g. \\(j\\in\\{1,2\\}\\)).\nIn a two-class problem where there are only two possible response values, say class \\(1\\) or class \\(2,\\) the Bayes classifier corresponds to predicting class \\(1\\) if \\[\nP(Y = 1| X = x_0 ) \\geq 0.5,\n\\] and class \\(2\\) if \\[\\begin{align*}\n& P(Y = 1| X = x_0 ) < 0.5 \\\\[2ex]\n\\Leftrightarrow\\; & P(Y = 2| X = x_0 ) > 0.5  \n\\end{align*}\\]\n\n\n\n\n\n\nClassification threshold \\(0.5\\)?\n\n\n\nIf no further information is given, one uses usually a threshold of \\(0.5\\) in a two-class classification problem; or, more generally, a threshold of \\(\\frac{1}{G}\\) in a \\(G\\)-classes classification problem.\nHowever, in certain applications, different thresholds are used. If, for instance, a certain classification error is very costly, we want to take this into account when choosing the classification threshold in order to the reduce the costs due to miss-classifications.\nExample: \\[\ny\\in\\{\\text{Person pays back}, \\text{Person does not pay back}\\}\n\\] The classification error \\[\n\\hat{y}=\\text{Person pays back} \\neq y = \\text{Person does not pay back}\n\\] can be very costly for a bank. So, it makes sense to classify a person with a certain predictor value \\(x_0\\) to the “Person pays back”-class only if, for instance, \\[\n\\hat{P}(Y = \\text{Person pays back}|X=x_0) \\geq 0.9,\n\\] and otherwise classify this person to the “Person does not pay back”-class. This will reduce the frequency of miss-classifications when classifying into the “Person pays back”-class, and thus reduce the costs.\n\n\nThose values of \\(x_0\\) for which \\[\\begin{align*}\nP(Y = 1| X = x_0 ) = 0.5 = P(Y = 2| X = x_0 )\n\\end{align*}\\] are called the Bayes decision boundary. An example of a Bayes decision boundary is shown as the purple dashed line in Fig. 2.13.\n\n\nNote to Fig. 2.13: Here, a perfect classification (i.e. zero error rate) is impossible, since the Bayes decision boundary does not partition the two groups (yellow, blue) in to complete separate groups.\n\nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. The point-wise Bayes error rate at \\(x_0\\) is given by \\[\n1 - \\max_{j}P(Y = j| X = x_0 ),\n\\] where the maximization is over all class labels \\(j\\) (e.g. \\(j\\in\\{1,2\\}\\)).\nThe global overall Bayes error rate is given by \\[\n1 - E\\left(\\max_{j}P(Y = 1| X )\\right),\n\\] where the expectation averages the probability over all possible values of \\(X.\\)\n\n\n\\(K\\)-Nearest Neighbors Classification\nIn theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of \\(Y\\) given \\(X,\\) and so computing the Bayes classifier is impossible.\nMany approaches attempt to estimate the conditional distribution of \\(Y\\) given \\(X,\\) and then classify a given observation to the class with highest estimated probability. One such method is the \\(K\\)-nearest neighbors (KNN) classifier.\nGiven a positive integer \\(K\\) and a test observation \\(x_0,\\) the KNN classifier first identifies the \\(K\\) points in the training data \\[\n\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\},\n\\] that are closest to \\(x_0.\\)\nThis set of \\(K\\) nearest points (near to \\(x_0\\)) can be represented by the \\(x_0\\)-specfic index set \\[\n\\mathcal{N}_0=\\{i=1,\\dots,n \\;|\\; x_i \\text{ is the $K$th closest point to }x_0 \\text{ or closter}\\}.\n\\] I.e. \\(\\mathcal{N}_0\\) is an index set that allows to select the \\(K\\) nearest neighbors in the training data.\nFrom the definition of \\(\\mathcal{N}_0\\) is follows that:\n\n\\(\\mathcal{N}_0\\subset\\{1,2,\\dots,n\\}\\)\n\\(|\\mathcal{N}_0|=K\\)\n\nKNN estimates the conditional probability for class \\(j\\) as the fraction of the \\(K\\) points \\((x_i,y_i)\\) selected by the index-set \\(\\mathcal{N}_0\\) whose response value \\(y_i\\) equals \\(j:\\) \\[\n\\begin{align}\nP(Y = j | X = x_{0})\n&\\approx \\hat{P}(Y = j | X = x_{0})\\\\[2ex]\n&= \\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}I(y_i = j),\n\\end{align}\n\\tag{1.5}\\] where \\(I(\\texttt{TRUE})=1\\) and \\(I(\\texttt{FALSE})=0.\\)\nFinally, KNN classifies the test observation \\(x_0\\) to the class \\(j\\) with the largest probability from Equation 1.5.\nFigure 2.14 provides an illustrative example of the KNN approach.\n\nTwo-dimensional predictor \\(X=(X_1,X_2),\\) where \\(X_1\\) is shown on the x-axis and \\(X_2\\) on the y-axis.\nTwo class labels \\(Y\\in\\{\\text{yellow}, \\text{blue}\\}.\\)\nTraining data consists of six data points \\[\n\\{(y_1,x_{11},x_{12}),\\dots,(y_6,x_{61},x_{62})\\}\n\\] (See the left panel of Figure 2.14.)\nClass-label prediction (“classification”) are computed for a regular grid of predictor values \\(x_{0}=(x_{01},x_{02}).\\) (See the regular grid of points in the right panel of Figure 2.14.)\n\\(K=3\\) nearest neighbors are used to compute the class-label predictions.\n\n\n\n\nIn the left-hand panel of Figure 2.14, a small training data set is shown consisting of six blue and six orange observations. Our goal is to make a prediction for the point labeled by the black cross.\nIn the right-hand panel of Figure 2.14, we have applied the KNN approach with \\(K = 3\\) at all of the possible values for \\(X_1\\) and \\(X_2,\\) and have drawn in the corresponding KNN decision boundary.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Challenge:\n\n\n\nThe following R-code generates training data for the two-class classification problem with\n\nClass 1: Circle with center \\((x_1,x_2)=(0.5,0.5)\\) and radius \\(r=0.2,\\) i.e. \\[\n\\text{Cl1} = \\{(x_1,x_2)\\in[0,1]^2:(x_1-0.5)^2+(x_2-0.5)^2\\leq 0.2^2\\}\n\\]\nClass 2: All data points in the square \\([0,1]^2\\) that do not belong to Class 1, i.e. \\[\n\\text{Cl2} = \\{(x_1,x_2)\\in[0,1]^2: (x_1,x_2)\\not\\in\\text{Cl1}\\}\n\\]\n\n\n# install.packages(\"plotrix\") # Install plotrix package\nlibrary(\"plotrix\")   # Load plotrix package (draw.circle())\n\n## Class 1: Circle  \nradius <- 0.2\ncenter <- c(0.5,0.5)\n## Class 2: [0,1] x [0,1] \\ Class 1\n\n## function to map data points to class labels \"Cl1\" and \"Cl2\"\nmy_class_fun <- function(x1, x2, cent = center, rad = radius, error = 0){\n  tmp    <- (x1 - cent[1])^2 + (x2 - cent[2])^2\n  rad    <- rad + sample(c(-1,1), 1) * error\n  ifelse(tmp <= rad^2, \"Cl1\", \"Cl2\")\n}\nmy_class_fun <- Vectorize(my_class_fun, c(\"x1\", \"x2\"))\n\n## ##################################\n## Generate training data (with error)\n## ##################################\nset.seed(321)\n\n## number of training data points\nn_train    <- 500\n\n## error\nerror  <- 0.05\n\ntrain_X1 <- runif(n_train, min = 0, max = 1)\ntrain_X2 <- runif(n_train, min = 0, max = 1)\ntrain_Cl <- my_class_fun(x1 = train_X1, \n                         x2 = train_X2, error = error)\ntrain_Cl <- as.factor(train_Cl)\nsummary(train_Cl)\n\nCl1 Cl2 \n 46 454 \n\ndata_train <- data.frame(\n  \"Cl\" = train_Cl,\n  \"X1\" = train_X1, \n  \"X2\" = train_X2\n)\n\nhead(data_train)\n\n   Cl        X1        X2\n1 Cl2 0.9558938 0.5858416\n2 Cl2 0.9372855 0.3106846\n3 Cl2 0.2382205 0.7639562\n4 Cl2 0.2550736 0.1329140\n5 Cl2 0.3905120 0.2018027\n6 Cl1 0.3411799 0.6347841\n\npar(mfrow = c(1,2))\nplot(x = 0, y = 0, \n     type = \"n\", \n     xlim = c(0,1),\n     ylim = c(0,1), \n     xlab = expression(X[1]),\n     ylab = expression(X[2]), asp=1)\nabline(h = c(0,1))\ndraw.circle(x = center[1], y = center[2], radius = radius)\ntext(x = 0.5, y = 0.5, label = \"Class 1\")\ntext(x = 0.1, y = 0.9, label = \"Class 2\")\n\nplot(x = 0, y = 0, \n     type = \"n\", \n     xlim = c(0,1),\n     ylim = c(0,1), \n     xlab = expression(X[1]),\n     ylab = expression(X[2]), asp=1,\n     main = \"Training data with errors\")\nabline(h = c(0,1))\ndraw.circle(x = center[1], y = center[2], radius = radius)\n\npoints(x   = data_train$X1, \n       y   = data_train$X2, \n       col = data_train$Cl,  pch = 19)\n\n\n\n\nChallenge:\n\nWrite a KNN-function that uses the training data to classify a grid of test data in \\([0,1]^2\\) into the two classes."
  },
  {
    "objectID": "Ch1_StatLearning.html#r-lab-introduction-to-r",
    "href": "Ch1_StatLearning.html#r-lab-introduction-to-r",
    "title": "1  Statistical Learning",
    "section": "1.4 R-Lab: Introduction to R",
    "text": "1.4 R-Lab: Introduction to R\nThis tutorial aims to serve as an introduction to the software package R. Other very good and much more exhaustive tutorials and useful reference-cards can be found at the following links:\n\nReference card for R commands (always useful)\nThe official Introduction to R (very detailed)\nAnd many more at www.r-project.org (see “Documents”) \nAn R-package for learning R: www.swirl.com\nAn excellent book project which covers also advanced issues such as “writing performant code” and “package development”: adv-r.had.co.nz\n\nAnother excellent book: R for Data Science\n\nSome other tutorials:\n\nIntroduction to data science\nCreating dynamic graphics\n\nWhy R?\n\nR is free of charge from: www.r-project.org\nThe celebrated IDE RStudio for R is also free of charge: www.rstudio.com\nR is equipped with one of the most flexible and powerful graphics routines available anywhere. For instance, check out one of the following repositories:\n\nClean Graphs\nPublication Ready Plots\n\nToday, R is the de-facto standard for statistical science.\n\n\n1.4.1 Short Glossary\nLets start the tutorial with a (very) short glossary:\n\nConsole: The thing with the > sign at the beginning.\nScript file: An ordinary text file with suffix .R. For instance, yourFavoritFileName.R.\nWorking directory: The file-directory you are working in. Useful commands: with getwd() you get the location of your current working directory and setwd() allows you to set a new location for it.\nWorkspace: This is a hidden file (stored in the working directory), where all objects you use (e.g., data, matrices, vectors, variables, functions, etc.) are stored. Useful commands: ls() shows all elements in our current workspace and rm(list=ls()) deletes all elements in our current workspace.\n\n\n\n1.4.2 First Steps\nA good idea is to use a script file such as yourFavoritFileName.R in order to store your R commands. You can send single lines or marked regions of your R-code to the console by pressing the keys STRG+ENTER.\nTo begin with baby steps, do some simple computations:\n\n2+2 # and all the others: *,/,-,^2,^3,... \n\n[1] 4\n\n\nNote: Everything that is written after the #-sign is ignored by R, which is very useful to comment your code.\nThe assignment operator <- or = will be your most often used tool. Here an example to create a scalar variable:\n\nx <- 4 \nx\n\n[1] 4\n\n4 -> x # possible but unusual\nx\n\n[1] 4\n\n\nNote: The R community loves the <- assignment operator, which is a very unusual syntax. Alternatively, you can use the more common = operator which is also used in languages like python or matlab.\nAnd now a more interesting object - a vector:\n\ny <- c(2,7,4,1)\ny\n\n[1] 2 7 4 1\n\n\nThe command ls() shows the total content of your current workspace, and the command rm(list=ls()) deletes all elements of your current workspace:\n\nls()\n\n [1] \"center\"       \"data_train\"   \"error\"        \"my_class_fun\" \"n_train\"     \n [6] \"radius\"       \"train_Cl\"     \"train_X1\"     \"train_X2\"     \"x\"           \n[11] \"y\"           \n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nNote: RStudio’s Environment pane also lists all the elements in your current workspace. That is, the command ls() becomes a bit obsolete when working with RStudio.\nLet’s try how we can compute with vectors and scalars in R.\n\nx <- 4\ny <- c(2,7,4,1)\n\nx*y # each element in the vector, y, is multiplied by the scalar, x.\n\n[1]  8 28 16  4\n\ny*y # this is a term by term product of the elements in y\n\n[1]  4 49 16  1\n\n\nPerforming vector multiplications as you might expect from your last math-course, e.g., an outer product: \\(y\\,y^\\top\\):\n\ny %*% t(y)\n\n     [,1] [,2] [,3] [,4]\n[1,]    4   14    8    2\n[2,]   14   49   28    7\n[3,]    8   28   16    4\n[4,]    2    7    4    1\n\n\nOr an inner product \\(y^\\top y\\):\n\nt(y) %*% y\n\n     [,1]\n[1,]   70\n\n\nNote: Sometimes, R’s treatment of vectors can be annoying. The product y %*% y is treated as the product t(y) %*% y.\nThe term-by-term execution as in the above example, y*y, is actually a central strength of R. We can conduct many operations vector-wisely:\n\ny^2\n\n[1]  4 49 16  1\n\nlog(y)\n\n[1] 0.6931472 1.9459101 1.3862944 0.0000000\n\nexp(y)\n\n[1]    7.389056 1096.633158   54.598150    2.718282\n\ny-mean(y)\n\n[1] -1.5  3.5  0.5 -2.5\n\n(y-mean(y))/sd(y) # standardization \n\n[1] -0.5669467  1.3228757  0.1889822 -0.9449112\n\n\nThis is a central characteristic of so called matrix based languages like R (or Matlab). Other programming languages often have to use loops instead:\n\nN <- length(y)\n1:N\n\ny.sq <- numeric(N)\ny.sq\n\nfor(i in 1:N){\n  y.sq[i] <- y[i]^2\n  if(i == N){\n    print(y.sq)\n  }\n}\n\nThe for()-loop is the most common loop. But there is also a while()-loop and a repeat()-loop. However, loops in R can be rather slow, therefore, try to avoid them!\n\nUseful commands to produce sequences of numbers:\n\n1:10\n-10:10\n?seq # Help for the seq()-function\nseq(from=1, to=100, by=7)\n\nUsing the sequence command 1:16, we can go for our first matrix:\n\n?matrix\nA <- matrix(data=1:16, nrow=4, ncol=4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA <- matrix(1:16, 4, 4)\n\nNote that a matrix has always two dimensions, but a vector has only one dimension:\n\ndim(A)    # Dimension of matrix A?\n\n[1] 4 4\n\ndim(y)    # dim() does not operate on vectors.\n\nNULL\n\nlength(y) # Length of vector y?\n\n[1] 4\n\n\nLets play a bit with the matrix A and the vector y. As we have seen in the loop above, the []-operator selects elements of vectors and matrices:\n\nA[,1]\nA[4,4]\ny[c(1,4)]\n\nThis can be done on a more logical basis, too. For example, if you want to know which elements in the first column of matrix A are strictly greater than 2:\n\nA[,1][A[,1]>2]\n\n[1] 3 4\n\n# Note that this give you a boolean vector:\nA[,1]>2\n\n[1] FALSE FALSE  TRUE  TRUE\n\n# And you can use it in a non-sense relation, too:\ny[A[,1]>2]\n\n[1] 4 1\n\n\nNote: Logical operations return so-called boolean objects, i.e., either a TRUE or a FALSE. For instance, if we ask R whether 1>2 we get the answer FALSE.\n\n\n1.4.3 Further Data Objects\nBesides classical data objects such as scalars, vectors, and matrices there are three further data objects in R:\n\nThe array: As a matrix but with more dimensions. Here is an example of a \\(2\\times 2\\times 2\\)-dimensional array:\n\n\nmyFirst.Array <- array(c(1:8), dim=c(2,2,2)) # Take a look at it!\n\n\nThe list: In lists you can organize different kinds of data. E.g., consider the following example:\n\n\nmyFirst.List <- list(\"Some_Numbers\" = c(66, 76, 55, 12, 4, 66, 8, 99), \n                     \"Animals\"      = c(\"Rabbit\", \"Cat\", \"Elefant\"),\n                     \"My_Series\"    = c(30:1)) \n\nA very useful function to find specific values and entries within lists is the str()-function:\n\nstr(myFirst.List)\n\nList of 3\n $ Some_Numbers: num [1:8] 66 76 55 12 4 66 8 99\n $ Animals     : chr [1:3] \"Rabbit\" \"Cat\" \"Elefant\"\n $ My_Series   : int [1:30] 30 29 28 27 26 25 24 23 22 21 ...\n\n\n\nThe data frame: A data.frame is a list-object but with some more formal restrictions (e.g., equal number of rows for all columns). As indicated by its name, a data.frame-object is designed to store data:\n\n\nmyFirst.Dataframe <- data.frame(\"Credit_Default\"   = c( 0, 0, 1, 0, 1, 1), \n                                \"Age\"              = c(35,41,55,36,44,26), \n                                \"Loan_in_1000_EUR\" = c(55,65,23,12,98,76)) \n# Take a look at it!\n\n\n\n1.4.4 Simple Regression Analysis using R\nAlright, let’s do some statistics with real data. You can download the data HERE. Save it on your computer, at a place where you can find it, and give the path (e.g. \"C:\\textbackslash path\\textbackslash auto.data.csv\", which references to the data, to the file-argument of the function read.csv():\n\n# ATTENTION! YOU HAVE TO CHANGE \"\\\" TO \"/\":\nauto.data <- read.csv(file=\"C:/your_path/autodata.txt\", header=TRUE)\nhead(auto.data)\n\nIf you have problems to read the data into R, go on with these commands. (For this you need a working internet connection!):\n\n# install.packages(\"readr\")\nlibrary(\"readr\")\nauto.data <- suppressMessages(read_csv(file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\",col_names = TRUE))\n# head(auto.data)\n\nYou can select specific variables of the auto.data using the $-operator:\n\ngasolin.consumption      <- auto.data$MPG.city\ncar.weight               <- auto.data$Weight\n## Take a look at the first elements of these vectors:\nhead(cbind(gasolin.consumption,car.weight))\n\n     gasolin.consumption car.weight\n[1,]                  25       2705\n[2,]                  18       3560\n[3,]                  20       3375\n[4,]                  19       3405\n[5,]                  22       3640\n[6,]                  22       2880\n\n\nThis is how you can produce your first plot:\n\n## Plot the data:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy Light-Weight Cars!\")\n\n\n\n\nFigure 1.1: Scatterplot of Gasoline consumption (mpg) vs. car weight.\n\n\n\n\nAs a first step, we might assume a simple kind of linear relationship between the variables gasolin.consumption and car.weight. Let us assume that the data was generated by the following simple regression model: \\[\ny_i=\\alpha+\\beta_1 x_i+\\varepsilon_i,\\quad i=1,\\dots,n\n\\] where \\(y_i\\) denotes the gasoline-consumption, \\(x_i\\) the weight of car \\(i\\), and \\(\\varepsilon_i\\) is a mean zero constant variance noise term. (This is clearly a non-sense model!)\nThe command lm() computes the estimates of this linear regression model. The command (in fact it’s a method) summary() computes further quantities of general interest from the object that was returned from the lm() function.\n\nlm.result   <- lm(gasolin.consumption~car.weight)\nlm.summary  <- summary(lm.result)\nlm.summary\n\n\nCall:\nlm(formula = gasolin.consumption ~ car.weight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7946 -1.9711  0.0249  1.1855 13.8278 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 47.048353   1.679912   28.01   <2e-16 ***\ncar.weight  -0.008032   0.000537  -14.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.038 on 91 degrees of freedom\nMultiple R-squared:  0.7109,    Adjusted R-squared:  0.7077 \nF-statistic: 223.8 on 1 and 91 DF,  p-value: < 2.2e-16\n\n\nOf course, we want to have a possibility to access all the quantities computed so far, e.g., in order to plot the results. This can be done as following:\n\n## Accessing the computed quantities\nnames(lm.summary) ## Alternatively: str(lm.summary)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nalpha <- lm.summary$coefficients[1]\nbeta  <- lm.summary$coefficients[2]\n\n## Plot all:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy light-weight Cars!\")\nabline(a=alpha, \n       b=beta, col=\"red\")\n\n\n\n\nScatterplot of Gasoline consumption (mpg) vs. car weight plus linear regression fit.\n\n\n\n\n\n\n1.4.5 Programming in R\nLet’s write, i.e., program our own R-function for estimating linear regression models. In order to be able to validate our function, we start with simulating data for which we then know all true parameters.\nSimulating data is like being the “Data-God”: For instance, we generate realizations of the error term \\(\\varepsilon_i\\), i.e., something which we never observe in real data.\nLet us consider the following multiple regression model:\n\\[y_i=\\beta_1 +\\beta_2 x_{2i}+\\beta_3 x_{3i}+\\varepsilon_{i},\\quad i=1,\\dots,n,\\] where \\(\\varepsilon_{i}\\) is a heteroscedastic error term \\[\\varepsilon_{i}\\sim N(0,\\sigma_i^2),\\quad \\sigma_i=|x_{3i}|,\\]\nand where for all \\(i=1,\\dots,n=50\\):\n\n\\(x_{2i}\\sim N(10,1.5^2)\\)\n\\(x_{3i}\\) comes from a t-distribution with 5 degrees of freedom and non-centrality parameter 2\n\n\nset.seed(109) # Sets the \"seed\" of the random number generators:\nn   <- 50     # Number of observations\n\n## Generate two explanatory variables plus an intercept-variable:\nX.1 <- rep(1, n)                 # Intercept\nX.2 <- rnorm(n, mean=10, sd=1.5) # Draw realizations form a normal distr.\nX.3 <- rt(n, df=5, ncp=2)        # Draw realizations form a t-distr.\nX   <- cbind(X.1, X.2, X.3)      # Save as a Nx3-dimensional data matrix.\n\nOK, we have regressors, i.e., data that we also have in real data sets.\nNow we define the elements of the \\(\\beta\\)-vector. Be aware of the difference: In real data sets we do not know the true \\(\\beta\\)-vector, but try to estimate it. However, when simulating data, we determine (as “Data-Gods”) the true \\(\\beta\\)-vector and can compare our estimate \\(\\hat{\\beta}\\) with the true \\(\\beta\\):\n\n## Define the slope-coefficients\nbeta.vec  <- c(1,-5,5)\n\nWe still need to simulate realizations of the dependent variable \\(y_i\\). Remember that \\(y_i=\\beta_1 x_{1i}+\\beta_1 x_{2i}+\\beta_3 x_{3i}+\\varepsilon_{i}\\). That is, we only need realizations from the error terms \\(\\varepsilon_i\\) in order to compute the realizations from \\(y_i\\). This is how you can simulate realizations from the heteroscedastic error terms \\(\\varepsilon_i\\):\n\n## Generate realizations from the heteroscadastic error term\neps       <- rnorm(n, mean=0, sd=abs(X.3))\n\nTake a look at the heteroscedasticity in the error term:\n\nplot(y=eps, x=X.3, \n     main=\"Realizations of the \\nHeteroscedastic Error Term\")\n\n\n\n\nScatterplot of error term realizations (usually unknown) versus the predictor values of X.3.\n\n\n\n\nWith the (pseudo-random) realizations from \\(\\varepsilon_i\\), we can finally generate realizations from the dependent variable \\(y_i\\):\n\n## Dependent variable:\ny   <- X %*% beta.vec + eps\n\nLet’s take a look at the data:\n\nmydata    <- data.frame(\"Y\"=y, \"X.1\"=X.1, \"X.2\"=X.2, \"X.3\"=X.3)\npairs(mydata[,-2]) # The '-2' removes the intercept variable \"X.1\"\n\n\n\n\nOnce we have data, we can compute the OLS estimate of the true \\(\\beta\\) vector. Remember the formula: \\[\\hat{\\beta}=(X^\\top X)^{-1}X^\\top y\\] In R-Code this is: \\((X^\\top X)^{-1}=\\)solve(t(X) %*% X), i.e.:\n\n## Computation of the beta-Vector:\nbeta.hat <- solve(t(X) %*% X) %*% t(X) %*% y\nbeta.hat\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\nWell done. Using the above lines of code we can easily program our own myOLSFun() function!\n\nmyOLSFun <- function(y, x, add.intercept=FALSE){\n  \n  ## Number of Observations:\n  n         <- length(y)\n  \n  ## Add an intercept to x:\n  if(add.intercept){\n    Intercept <- rep(1, n)\n    x         <- cbind(Intercept, x)\n  }\n  \n  ## Estimation of the slope-parameters:\n  beta.hat.vec <- solve(t(x) %*% x) %*% t(x) %*% y\n  \n  ## Return the result:\n  return(beta.hat.vec)\n}\n\n## Run the function:\nmyOLSFun(y=y, x=X)\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\nCan you extend the function for the computation of the covariance matrix of the slope-estimates, several measures of fits (R\\(^2\\), adj.-R\\(^2\\), etc.), t-tests, …?\n\n\n1.4.6 R-packages\nOne of the best features in R are its contributed packages. The list of all packages on CRAN is impressive! Take a look at it HERE\nFor instance, nice plots can be produced using the R-package is ggplot2. You can find an intro do this package HERE.\n\n# install.packages(\"ggplot2\")\nlibrary(\"ggplot2\")\n\nqplot(Sepal.Length, Petal.Length, data = iris, color = Species)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nOf course, ggplot2 concerns “only” plotting, but you’ll find R-packages for almost any statistical method out there.\n\n\n1.4.7 Tidyverse\nThe tidyverse package is a collection of packages that lets you import, manipulate, explore, visualize and model data in a harmonized and consistent way which helps you to be more productive.\nInstalling the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nTo use the tidyverse package load it using the library() function:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ stringr   1.5.0\n✔ forcats   1.0.0     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nChick Weight Data\nR comes with many datasets installed. We will use the ChickWeight dataset to learn (a little) about the tidyverse. The help system gives a basic summary of the experiment from which the data was collect:\n\n“The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups of chicks on different protein diets.”\n\nYou can get more information, including references by typing:\n\nhelp(\"ChickWeight\")\n\nThe Data:  There are 578 observations (rows) and 4 variables:\n\nChick – unique ID for each chick.\nDiet – one of four protein diets.\nTime – number of days since birth.\nweight – body weight of chick in grams.\n\nNote: weight has a lower case w (recall R is case sensitive).\nStore the data locally:\n\nChickWeight %>%\n  dplyr::select(Chick, Diet, Time, weight) %>% \n  dplyr::arrange(Chick, Diet, Time) %>% \n  write_csv(\"DATA/ChickWeight.csv\")\n\nFirst we will import the data from a file called ChickWeight.csv using the read_csv() function from the readr package (part of the tidyverse). The first thing to do, outside of R, is to open the file ChickWeight.csv to check what it contains and that it makes sense. Now we can import the data as follows:\n\nCW <- readr::read_csv(\"DATA/ChickWeight.csv\")\n\nRows: 578 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Chick, Diet, Time, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf all goes well then the data is now stored in an R object called CW. If you get the following error message then you need to change the working directory to where the data is stored:\n\nError: ‘ChickWeight.csv’ does not exist in current working directory …\n\nChanging the working directory: In RStudio you can use the menu bar (“Session - Set Working Directory - Choose Directory…”). Alternatively, you can use the function setwd(). Last but not least, to avoid issues with brocken paths to files and data sets, use RStudios’ “Project” tools.\nLooking at the Dataset: To look at the data type just type the object (dataset) name:\n\nCW\n\n# A tibble: 578 × 4\n   Chick  Diet  Time weight\n   <dbl> <dbl> <dbl>  <dbl>\n 1    18     1     0     39\n 2    18     1     2     35\n 3    16     1     0     41\n 4    16     1     2     45\n 5    16     1     4     49\n 6    16     1     6     51\n 7    16     1     8     57\n 8    16     1    10     51\n 9    16     1    12     54\n10    15     1     0     41\n# … with 568 more rows\n\n\nIf there are too many variables then not all them may be printed. To overcome this issue we can use the glimpse() function which makes it possible to see every column in your dataset (called a “data frame” in R speak).\n\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  <dbl> 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   <dbl> 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight <dbl> 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe function View() allows for a spread-sheet type of view on the data:\n\nView(CW)\n\n\n1.4.7.1 Tidyverse: Plotting Basics\nTo visualize the chick weight data, we will use the ggplot2 package (part of the tidyverse). Our interest is in seeing how the weight changes over time for the chicks by diet. For the moment don’t worry too much about the details just try to build your own understanding and logic. To learn more try different things even if you get an error messages.\nLet’s plot the weight data (vertical axis) over time (horizontal axis). Generally, ggplot2 works in layers. The following codes generates an empty plot:\n\n# An empty plot\nggplot(CW, aes(Time, weight))  \n\n\n\n\nEmpty ggplot layer.\n\n\n\n\nTo the empty plot, one can add fuhrer layers:\n\n# Adding a scatter plot \nggplot(CW, aes(Time, weight)) + geom_point() \n\n\n\n\nAdding a scatter plot layer to the empty ggplot layer.\n\n\n\n\nAdd color for Diet. The graph above does not differentiate between the diets. Let’s use a different color for each diet.\n\n# Adding colour for diet\nggplot(CW,aes(Time,weight,colour=factor(Diet))) +\n  geom_point() \n\n\n\n\nAdding a further layer for shown the effect of the Diet.\n\n\n\n\nIt is difficult to conclude anything from this graph as the points are printed on top of one another (with diet 1 underneath and diet 4 at the top).\nTo improve the plot, it will be handy to store Diet and Time as a factor variables.\nFactor Variables: Before we continue, we have to make an important change to the CW dataset by making Diet and Time factor variables. This means that R will treat them as categorical variables (see the <fct> variables below) instead of continuous variables. It will simplify our coding. The next section will explain the mutate() function.\n\nCW <- mutate(CW, Diet = factor(Diet))\nCW <- mutate(CW, Time = factor(Time))\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  <dbl> 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   <fct> 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight <dbl> 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe facet_wrap() function: To plot each diet separately in a grid using facet_wrap():\n\n# Adding jitter to the points\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  geom_point() +\n  facet_wrap(~Diet) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Diet 4 has the least variability but we can’t really say anything about the mean effect of each diet although diet 3 seems to have the highest.\nNext we will plot the mean changes over time for each diet using the stat_summary() function:\n\nggplot(CW, aes(Time, weight, \n               group=Diet, colour=Diet)) +\n  stat_summary(fun=\"mean\", geom=\"line\") \n\n\n\n\nInterpretation: We can see that diet 3 has the highest mean weight gains by the end of the experiment. However, we don’t have any information about the variation (uncertainty) in the data.\nTo see variation between the different diets we use geom_boxplot to plot a box-whisker plot. A note of caution is that the number of chicks per diet is relatively low to produce this plot.\n\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\")\n\n\n\n\nInterpretation: Diet 3 seems to have the highest “average” weight gain but it has more variation than diet 4 which is consistent with our findings so far.\nLet’s finish with a plot that you might include in a publication.\n\nggplot(CW, aes(Time, weight, group=Diet, \n                             colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_point() +\n  # geom_jitter() +\n  stat_summary(fun=\"mean\", geom=\"line\",\n               colour=\"black\") +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\") + \n  xlab(\"Time (days)\") +\n  ylab(\"Weight (grams)\")\n\n\n\n\n\n\n1.4.7.2 Tidyverse: Data Wrangling Basics\n\n\n\nIn this section we will learn how to wrangle (manipulate) datasets using the tidyverse package. Let’s start with the mutate(), select(), rename(), filter() and arrange() functions.\nmutate(): Adds a new variable (column) or modifies an existing one. We already used this above to create factor variables.\n\n# Added a column\nCWm1 <- mutate(CW, weightKg = weight/1000)\nCWm1\n\n# A tibble: 578 × 5\n  Chick Diet  Time  weight weightKg\n  <dbl> <fct> <fct>  <dbl>    <dbl>\n1    18 1     0         39    0.039\n2    18 1     2         35    0.035\n3    16 1     0         41    0.041\n# … with 575 more rows\n\n# Modify an existing column\nCWm2 <- mutate(CW, Diet = str_c(\"Diet \", Diet))\nCWm2\n\n# A tibble: 578 × 4\n  Chick Diet   Time  weight\n  <dbl> <chr>  <fct>  <dbl>\n1    18 Diet 1 0         39\n2    18 Diet 1 2         35\n3    16 Diet 1 0         41\n# … with 575 more rows\n\n\nselect(): Keeps, drops or reorders variables.\n\n# Drop the weight variable from CWm1 using minus\ndplyr::select(CWm1, -weight)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weightKg\n  <dbl> <fct> <fct>    <dbl>\n1    18 1     0        0.039\n2    18 1     2        0.035\n3    16 1     0        0.041\n# … with 575 more rows\n\n# Keep variables Time, Diet and weightKg\ndplyr::select(CWm1, Chick, Time, Diet, weightKg)\n\n# A tibble: 578 × 4\n  Chick Time  Diet  weightKg\n  <dbl> <fct> <fct>    <dbl>\n1    18 0     1        0.039\n2    18 2     1        0.035\n3    16 0     1        0.041\n# … with 575 more rows\n\n\nrename(): Renames variables whilst keeping all variables.\n\ndplyr::rename(CW, Group = Diet, Weight = weight)\n\n# A tibble: 578 × 4\n  Chick Group Time  Weight\n  <dbl> <fct> <fct>  <dbl>\n1    18 1     0         39\n2    18 1     2         35\n3    16 1     0         41\n# … with 575 more rows\n\n\nfilter(): Keeps or drops observations (rows).\n\ndplyr::filter(CW, Time==21 & weight>300)\n\n# A tibble: 8 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1     7 1     21       305\n2    29 2     21       309\n3    21 2     21       331\n# … with 5 more rows\n\n\nFor comparing values in vectors use: < (less than), > (greater than), <= (less than and equal to), >= (greater than and equal to), == (equal to) and != (not equal to). These can be combined logically using & (and) and | (or).\narrange(): Changes the order of the observations.\n\ndplyr::arrange(CW, Chick, Time)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1     1 1     0         42\n2     1 1     2         51\n3     1 1     4         59\n# … with 575 more rows\n\ndplyr::arrange(CW, desc(weight))\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  <dbl> <fct> <fct>  <dbl>\n1    35 3     21       373\n2    35 3     20       361\n3    34 3     21       341\n# … with 575 more rows\n\n\nWhat does the desc() do? Try using desc(Time).\n\n\n1.4.7.3 The pipe operator %>%\nIn reality you will end up doing multiple data wrangling steps that you want to save. The pipe operator %>% makes your code nice and readable:\n\nCW21 <- CW %>% \n  dplyr::filter(Time %in% c(0, 21)) %>% \n  dplyr::rename(Weight = weight) %>% \n  dplyr::mutate(Group = factor(str_c(\"Diet \", Diet))) %>% \n  dplyr::select(Chick, Group, Time, Weight) %>% \n  dplyr::arrange(Chick, Time) \nCW21\n\n# A tibble: 95 × 4\n  Chick Group  Time  Weight\n  <dbl> <fct>  <fct>  <dbl>\n1     1 Diet 1 0         42\n2     1 Diet 1 21       205\n3     2 Diet 1 0         40\n# … with 92 more rows\n\n\nHint: To understand the code above we should read the pipe operator %>% as “then”.\n\nCreate a new dataset (object) called CW21 using dataset CW then keep the data for days 0 and 21 then rename variable weight to Weight then create a variable called Group then keep variables Chick, Group, Time and Weight and then finally arrange the data by variables Chick and Time.\n\nThis is the same code:\n\nCW21 <- CW %>% \n  dplyr::filter(., Time %in% c(0, 21)) %>% \n  dplyr::rename(., Weight = weight) %>% \n  dplyr::mutate(., Group=factor(str_c(\"Diet \",Diet))) %>% \n  dplyr::select(., Chick, Group, Time, Weight) %>% \n  dplyr::arrange(., Chick, Time) \n\nThe pipe operator, %>%, replaces the dots (.) with whatever is returned from code preceding it. For example, the dot in filter(., Time %in% c(0, 21)) is replaced by CW. The output of the filter(...) then replaces the dot in rename(., Weight = weight) and so on. Think of it as a data assembly line with each function doing its thing and passing it to the next.\n\n\n1.4.7.4 The group_by() function\nFrom the data visualizations above we concluded that the diet 3 has the highest mean and diet 4 the least variation. In this section, we will quantify the effects of the diets using summmary statistics. We start by looking at the number of observations and the mean by diet and time.\n\nmnsdCW <- CW %>% \n  dplyr::group_by(Diet, Time) %>% \n  dplyr::summarise(N = n(), Mean = mean(weight)) %>% \n  dplyr::arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nmnsdCW\n\n# A tibble: 48 × 4\n# Groups:   Diet [4]\n  Diet  Time      N  Mean\n  <fct> <fct> <int> <dbl>\n1 1     0        20  41.4\n2 1     2        20  47.2\n3 1     4        19  56.5\n# … with 45 more rows\n\n\nFor each distinct combination of Diet and Time, the chick weight data is summarized into the number of observations (N) and the mean (Mean) of weight.\nFurther summaries: Let’s also calculate the standard deviation, median, minimum and maximum values but only at days 0 and 21.\n\nsumCW <-  CW %>% \n  dplyr::filter(Time %in% c(0, 21)) %>% \n  dplyr::group_by(Diet, Time) %>% \n  dplyr::summarise(N = n(),\n            Mean = mean(weight),\n            SD = sd(weight),\n            Median = median(weight),\n            Min = min(weight),\n            Max = max(weight)) %>% \n  dplyr::arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nsumCW\n\n# A tibble: 8 × 8\n# Groups:   Diet [4]\n  Diet  Time      N  Mean     SD Median   Min   Max\n  <fct> <fct> <int> <dbl>  <dbl>  <dbl> <dbl> <dbl>\n1 1     0        20  41.4  0.995   41      39    43\n2 1     21       16 178.  58.7    166      96   305\n3 2     0        10  40.7  1.49    40.5    39    43\n# … with 5 more rows\n\n\nLet’s make the summaries “prettier”, say, for a report or publication.\n\nlibrary(\"knitr\") # to use the kable() function\nprettySumCW <- sumCW %>% \n dplyr::mutate(`Mean (SD)` = str_c(format(Mean, digits=1),\n           \" (\", format(SD, digits=2), \")\")) %>% \n dplyr::mutate(Range = str_c(Min, \" - \", Max)) %>% \n dplyr::select(Diet, Time, N, `Mean (SD)`, Median, Range) %>%\n dplyr::arrange(Diet, Time) %>% \n kable(format = \"latex\")\nprettySumCW\n\n\n\n\n\n \n  \n    Diet \n    Time \n    N \n    Mean (SD) \n    Median \n    Range \n  \n \n\n  \n    1 \n    0 \n    20 \n    41 ( 0.99) \n    41.0 \n    39 - 43 \n  \n  \n    1 \n    21 \n    16 \n    178 (58.70) \n    166.0 \n    96 - 305 \n  \n  \n    2 \n    0 \n    10 \n    41 ( 1.5) \n    40.5 \n    39 - 43 \n  \n  \n    2 \n    21 \n    10 \n    215 (78.1) \n    212.5 \n    74 - 331 \n  \n  \n    3 \n    0 \n    10 \n    41 ( 1) \n    41.0 \n    39 - 42 \n  \n  \n    3 \n    21 \n    10 \n    270 (72) \n    281.0 \n    147 - 373 \n  \n  \n    4 \n    0 \n    10 \n    41 ( 1.1) \n    41.0 \n    39 - 42 \n  \n  \n    4 \n    21 \n    9 \n    239 (43.3) \n    237.0 \n    196 - 322 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: This summary table offers the same interpretation as before, namely that diet 3 has the highest mean and median weights at day 21 but a higher variation than group 4. However it should be noted that at day 21, diet 1 lost 4 chicks from 20 that started and diet 4 lost 1 from 10. This could be a sign of some health related issues.\n\n\n\n\n\n\n1.4.8 Further Links\n\nFurther R-Intros\n\nhttps://eddelbuettel.github.io/gsir-te/Getting-Started-in-R.pdf\nhttps://www.datacamp.com/courses/free-introduction-to-r\nhttps://swcarpentry.github.io/r-novice-gapminder/\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\n\n\n\nVersion Control (Git/GitHub)\n\nhttps://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN\nhttp://happygitwithr.com/\nhttps://www.gitkraken.com/\n\n\n\nR-Ladies\n\nhttps://rladies.org/"
  },
  {
    "objectID": "Ch1_StatLearning.html#exercises",
    "href": "Ch1_StatLearning.html#exercises",
    "title": "1  Statistical Learning",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nPrepare the following exercises of Chapter 2 in our course textbook ISLR:\n\nExercise 7\nExercise 8\nExercise 9"
  },
  {
    "objectID": "Ch2_LinearRegression.html",
    "href": "Ch2_LinearRegression.html",
    "title": "2  Linear Regression",
    "section": "",
    "text": "Reading: Chapter 3 of our course textbook An Introduction to Statistical Learning"
  },
  {
    "objectID": "Ch2_LinearRegression.html#ch.-3.1-simple-linear-regression",
    "href": "Ch2_LinearRegression.html#ch.-3.1-simple-linear-regression",
    "title": "2  Linear Regression",
    "section": "(Ch. 3.1) Simple Linear Regression",
    "text": "(Ch. 3.1) Simple Linear Regression\nThe linear regression model assumes a linear relationship between \\(Y\\) and the predictor(s) \\(X\\).\nThe simple (only one predictor) linear regression model: \\[\nY\\approx \\beta_0 + \\beta_1 X\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV\n\n\n(Ch. 3.1.1) Estimating the Coefficients\nWe choose \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) such that the Residual Sum of Squares criterion is minimized: \\[\n\\begin{align*}\n\\operatorname{RSS}\\equiv \\operatorname{RSS}(\\hat{\\beta}_0,\\hat{\\beta_1})\n& = e_1^2 + \\dots + e_n^2\\\\\n&=)\\sum_{i=1}^n\\left(y_i - \\left(\\hat\\beta_0 + \\hat\\beta_1x_i\\right)\\right)^2\n\\end{align*}\n\\] The minimizers are \\[\n\\hat\\beta_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\] and \\[\n\\hat\\beta_0=\\bar{y} - \\hat\\beta_1\\bar{x},\n\\] where \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i\\) and \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i\\).\n\n\n\n\n(Ch. 3.1.2) Assessing the Accuracy of the Coefficient Estimates\nTrue unknown model \\[\nY=f(X)+\\epsilon\n\\]\nIn in linear regression analysis, we assume1 that \\[\nf(X) = \\beta_0 + \\beta_1 X\n\\]\nOrdinary least squares estimators \\[\n\\hat\\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\n\\] are unbiased, that is \\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat\\beta_0)&=E(\\hat\\beta_0)-\\beta_0=0\\\\\n\\operatorname{Bias}(\\hat\\beta_1)&=E(\\hat\\beta_1)-\\beta_1=0\n\\end{align*}\n\\] I.e., on average, the estimation results equal the true (unknown) parameters. However, in an actual data analysis, we only have one realization of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) computed from one give dataset and thus we cannot compute averages of estimation results. Each single estimation result will have estimation errors, i.e., \\[\n\\hat\\beta_0\\neq \\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\\neq \\beta_1.\n\\]\nThe following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook ISLR.\n\n## ###############################\n## A function to generate data \n## similar to that shown in Fig 3.3\n## ##############################\n\nbeta_0 <- 0.1                          # intercept parameter\nbeta_1 <- 5  \n\n## A Function to simulate data\nmyDataGenerator <- function(){\n  n      <- 50                           # sample size\n  beta_0 <- 0.1                          # intercept parameter\n  beta_1 <- 5                            # slope parameter\n  X      <- runif(n, min = -2, max = 2)  # predictor\n  error  <- rnorm(n, mean = 0, sd = 8.5) # error term\n  Y      <- beta_0 + beta_1 * X + error  # outcome \n  ##\n  return(data.frame(\"Y\" = Y, \"X\" = X))\n}\n\n## Generate a first realization of the data\nset.seed(123)\ndata_sim <- myDataGenerator()\nhead(data_sim)\n\n            Y          X\n1 -18.4853427 -0.8496899\n2  12.9872926  1.1532205\n3  -0.4167901 -0.3640923\n4  -1.9138159  1.5320696\n5  19.5667725  1.7618691\n6  -5.3639241 -1.8177740\n\n\nUsing repeated samples form the data generating process defined in myDataGenerator(), we can generate multiple estimation results of the unknown simple linear regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) and plot the corresponding empirical regression lines:\n\n## Estimation\nlm_obj <- lm(Y ~ X, data = data_sim)\n\n## Plotting the results\npar(mfrow=c(1,2)) # Two plots side by side\n\n## First Plot (fit for the first realization of the data)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\")\nabline(a = beta_0, b = beta_1, col = \"red\")\nabline(lm_obj, col = \"blue\")\n\n## Second Plot (fits for multiple data realizations)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\", type = \"n\") # type = \"n\": empty plot\n##\nfor(r in 1:10){\n  data_sim_new <- myDataGenerator()\n  lm_obj_new   <- lm(Y ~ X, data=data_sim_new)\n  abline(lm_obj_new, col = \"lightskyblue\")\n}\n## Adding the first fit\nabline(a = beta_0, b = beta_1, col = \"red\", lwd = 2)\nabline(lm_obj, col = \"blue\", lwd = 2)\n\n\n\n\n\nCoding-Questions: Can you do this animated? https://gganimate.com/articles/gganimate.html\n\nThe magnitude of the estimation errors is expressed in unites of standard errors: \\[\n\\operatorname{SE}(\\hat\\beta_0)=\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right]\n\\] and \\[\n\\operatorname{SE}(\\hat\\beta_1)=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2},\n\\] where \\(Var(\\epsilon)=\\sigma^2\\Leftrightarrow \\operatorname{SD}(\\epsilon)=\\sqrt{Var(\\epsilon)}=\\sigma\\).\nTypically, \\(\\sigma\\) is unknown, but can be estimated by \\[\n\\sigma\\approx\\hat{\\sigma}=\\operatorname{RSE}=\\sqrt{\\frac{\\operatorname(RSS)}{n-2}},\n\\] where we subtract \\(2\\) from the sampel size \\(n\\) since \\(n-2\\) are the remaining degrees of freedom in the data after estimating two parameters \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\nKnowing \\(\\operatorname{SE}(\\hat\\beta_0)\\) and \\(\\operatorname{SE}(\\hat\\beta_1)\\) allows us to construct Confidence Intervals: \\[\n\\begin{align*}\n\\operatorname{CI}_{\\beta_1}\n&=\\left[\\hat{\\beta_1}-2\\operatorname{SE}(\\hat\\beta_1),\\;\n        \\hat{\\beta_1}+2\\operatorname{SE}(\\hat\\beta_1)\\right]\\\\\n&=\\hat\\beta_1\\pm 2\\operatorname{SE}(\\hat\\beta_1)\n\\end{align*}\n\\] likewise for \\(\\operatorname{CI}_{\\beta_1}\\).\nInterpretation: There is approximately a 95% change (in infinite resamplings) that the (random) confidence interval \\(\\operatorname{CI}_{\\beta_1}\\) contains the true (fix) parameter value \\(\\beta_1\\).\nThus, a given confidence interval either contains the true parameter value or not and we usually do not know it. To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:\n\nInteractive visualization for interpreting confidence intervals\n\nStandard errors can also be used to do hypothesis testing:\n\\[\n\\begin{align*}\nH_0:&\\;\\text{There is no relationship between $Y$ and $X$; i.e. $\\beta_1=0$}\\\\\nH_1:&\\;\\text{There is a relationship between $Y$ and $X$; i.e. $\\beta_1\\neq 0$}\n\\end{align*}\n\\]\n\\(t\\)-test statistic\n\\[\nt=\\frac{\\hat\\beta_1 - 0}{\\operatorname{SE}(\\hat\\beta_1)}\\overset{H_0}{\\sim}t_{(n-1)}\n\\]\n\\(p\\)-value\n\\[\n\\begin{align*}\np\n&=P_{H_0}\\left(|t|\\geq|t_{obs}|\\right)\n&=2\\cdot\\min\\{P_{H_0}\\left(t\\geq t_{obs} \\right),\\; P_{H_0}\\left(t\\leq t_{obs} \\right)\\},\n\\end{align*}\n\\] where \\(t_{obs}\\) denotes the observed value of the \\(t\\)-test statistic and where \\(t\\) is \\(t\\)-distributed with \\((n-2)\\) degrees of freedom.\nSelect a significance level \\(\\alpha\\) (e.g. \\(\\alpha=0.01\\) or \\(\\alpha=0.05\\)) and reject \\(H_0\\) if \\[\np<\\alpha\n\\]\n\n\n\n(Ch. 3.1.3) Assessing the Accuracy of the Model\nIn tendency an accurate model has …\n\na low \\(\\operatorname{RSE}\\) \\[\n\\operatorname{RSE}=\\hat\\sigma=\\sqrt{\\frac{\\operatorname{RSS}}{n-2}}\n\\]\na high \\(R^2\\)\n\n\\[\nR^2=\\frac{\\operatorname{TSS}-\\operatorname{RSS}}{\\operatorname{TSS}}=1-\\frac{\\operatorname{RSS}}{\\operatorname{TSS}},\n\\]\nwhere \\(0\\leq R^2\\leq 1\\) and\n\\[\n\\begin{align*}\n\\operatorname{TSS}&=\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2\\\\\n\\operatorname{RSS}&=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2\\\\\n\\hat{y}_i&=\\hat\\beta_0+\\hat\\beta_1x_i\n\\end{align*}\n\\]\nCaution: Do not forget that there is a irreducible error \\(Var(\\epsilon)=\\sigma^2>0\\). Thus\n\nvery low \\(\\operatorname{RSE}\\) values, \\(\\operatorname{RSE}\\approx 0\\), and\nvery high \\(R^2\\) values, \\(R^2\\approx 1\\),\n\ncan be warning signals indicating overfitting.\nIn the case of the simple linear regression model, \\(R^2\\) equals the squared sample correlation coefficient between \\(Y\\) and \\(X\\), \\[\nR^2 = r^2,\n\\] where \\[\nr=\\widehat{cor}(Y,X)=\\frac{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}.\n\\]"
  },
  {
    "objectID": "Ch2_LinearRegression.html#multiple-linear-regression-ch.-3.2",
    "href": "Ch2_LinearRegression.html#multiple-linear-regression-ch.-3.2",
    "title": "2  Linear Regression",
    "section": "Multiple Linear Regression (Ch. 3.2)",
    "text": "Multiple Linear Regression (Ch. 3.2)\nThe multiple linear regression model allows for more than only one predictor:\n\\[\nY\\approx \\beta_0 + \\beta_1 X_1 +  \\dots + \\beta_p X_p + \\epsilon\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV \\(+\\beta_2\\) radio \\(+\\beta_3\\) newspaper \\(+\\epsilon\\)\n\n\n(Ch. 3.2.1) Estimating the Regression Coefficients\nSelect\n\\[\n\\hat\\beta_0,\\dots,\\hat\\beta_p\n\\] by minimizing \\[\n\\operatorname{RSS}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2,\n\\] where \\[\n\\hat{y}_i=\\hat\\beta_0 + \\hat\\beta_1 x_{i1} \\dots + \\hat\\beta_p x_{ip}\n\\]\n\nMultiple linear regression is more than mere composition of single simple linear regression models.\nTake a look at the following two simple linear regression results:\n\nThus in separate simple linear regressions, the effects of radio and the effect of newspaper on sales are both (but separately) statistically.\nBy contrast, when looking at the multiple linear regression when regressing sales onto both radio and newspaper, only the effect of radio remains statistically significant:\n\nReason: Omitted Variable Bias\n\nradio has an effect on sales\nnewspaper has actually no effect on sales\nBut, newspaper is “strongly” correlated with radio (cor(newspaper,radio)=0.3541); see Table 3.5\n\n\n\nThus, when omitting radio from the multiple regression model, newspaper becomes a surrogate for radio. This is called a Omitted Variable Bias.\n\nConclusion: Simple linear regression can be dangerous. We need to control for all possibly relevant variables if we want to interpret the estimation results (“Inference”).\nInterpretation of the Coefficients in Table 3.5\nFor fixed values of TV and newspaper, spending additionally 1000 USD for radio, increases on average sales by approximately 189 units.\n\n\n(Ch. 3.2.2) Some Important Questions\n1. Is There a Relationship Between the Response and Predictors?\n\\[\n\\begin{align*}\nH_0:&\\;\\beta_1=\\beta_2=\\dots=\\beta_p=0\\\\\nH_1:&\\;\\text{at least one $\\beta_j\\neq 0$; $j=1,\\dots,p$}\n\\end{align*}\n\\]\n\\(F\\)-test statistic \\[\nF=\\frac{(\\operatorname{TSS}-\\operatorname{RSS})/p}{\\operatorname{\n  RSS}/(n-p-1)}\n\\]\nIf \\(H_0\\) is correct \\[\n\\begin{align*}\nE(\\operatorname{RSS}/(n-p-1))&=\\sigma^2\\\\\nE((\\operatorname{TSS}-\\operatorname{RSS})/p)&=\\sigma^2\\\\\n\\end{align*}\n\\]\n\nThus, if \\(H_0\\) is correct, we expect values of \\(F\\approx 1\\).\nBut if \\(H_1\\) is correct, we expect values of \\(F\\gg 1\\).\n\nCaution: Cannot be computed if \\(p>n\\). (Chapter 6 on “high dimensional problems”)"
  },
  {
    "objectID": "Ch2_LinearRegression.html#ch.-3.3-other-considerations-in-the-regression-model",
    "href": "Ch2_LinearRegression.html#ch.-3.3-other-considerations-in-the-regression-model",
    "title": "2  Linear Regression",
    "section": "(Ch. 3.3) Other Considerations in the Regression Model",
    "text": "(Ch. 3.3) Other Considerations in the Regression Model\n\n(Ch. 3.3.1) Qualitative Predictors\nOften some predictors are qualitative variables (also known as a factor variables). For instance, the Credit dataset contains the following qualitative predictors:\n\nown (house ownership)\nstudent (student status)\nstatus (marital status)\nregion (East, West or South)\n\n\nPredictors with Only Two Levels\nIf a qualitative predictor (factor) only has two levels (i.e. possible values), then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values; for instance, \\[\nx_{i} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person owns a house}\\\\\n  0&\\quad \\text{if the $i$th person does not own a house.}\n  \\end{array}\\right.\n\\] Using this dummy variable as a predictor in the regression equation results in the following regression model: \\[\ny_{i}=\\beta_0 + \\beta_1 x_i + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1 + \\epsilon_i &\\quad \\text{if the $i$th person owns a house}\\\\\n  \\beta_0 + \\epsilon_i           &\\quad \\text{if the $i$th person does not own a house}\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The average credit card balance among those who do not own a house\n\\(\\beta_0+\\beta_1\\): The average credit card balance among those who do own a house\n\\(\\beta_1\\): The average difference in credit card balance between owners and non-owners\n\n\nAlternatively, instead of a 0/1 coding scheme, we could create a dummy variable \\[\nx_{i} = \\left\\{\n  \\begin{array}{ll}\n  1 &\\quad \\text{if the $i$th person owns a house}\\\\\n-1 &\\quad \\text{if the $i$th person does not own a house.}\n  \\end{array}\\right.\n\\] \\[\ny_{i}=\\beta_0 + \\beta_1 x_i + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1 + \\epsilon_i&\\quad \\text{if the $i$th person owns a house}\\\\\n  \\beta_0 - \\beta_1 + \\epsilon_i&\\quad \\text{if the $i$th person does not own a house}\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The overall average credit card balance (ignoring the house ownership effect)\n\\(\\beta_1\\): The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.\n\n\n\nQualitative Predictors with More than Two Levels\nWhen a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the region \\(\\in\\{\\)South, West, East\\(\\}\\) variable we create two dummy variables. The first could be \\[\nx_{i1} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person is from the South}\\\\\n  0&\\quad \\text{if the $i$th person is not from the South,}\n  \\end{array}\\right.\n\\] and the second could be \\[\nx_{i2} = \\left\\{\n  \\begin{array}{ll}\n  1&\\quad \\text{if the $i$th person is from the West}\\\\\n  0&\\quad \\text{if the $i$th person is not from the West.}\n  \\end{array}\\right.\n\\] Using both of these dummy variables results in the following regression model: order to obtain the model \\[\ny_{i}=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i = \\left\\{\n  \\begin{array}{ll}\n  \\beta_0 + \\beta_1  + \\epsilon_i& \\quad \\text{if the $i$th person is from the South}\\\\\n  \\beta_0 + \\beta_2  + \\epsilon_i& \\quad \\text{if the $i$th person is from the West}\\\\\n  \\beta_0            + \\epsilon_i& \\quad \\text{if the $i$th person is from the East.}\\\\\n  \\end{array}\\right.\n\\]\nInterpretation:\n\n\\(\\beta_0\\): The average credit card balance for individuals from the East\n\\(\\beta_1\\): The difference in the average balance between people from the South versus the East\n\\(\\beta_2\\): The difference in the average balance between people from the West versus the East\n\n\nThere are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular contrasts. (A detailed discussion of contrasts is beyond the scope of this lecture.)\n\n\n\n(Ch. 3.3.2) Extensions of the Linear Model\n\nInteraction Effects: Removing the Additive Assumption using Interaction Effects\nPreviously, we used the following model\n\nsales \\(= \\beta_0 + \\beta_1\\) TV \\(+ \\beta_2\\) radio \\(+ \\beta_3\\) newspaper \\(+\\epsilon\\)\n\nwhich states that the average increase in sales associated with a one-unit increase in TV is always \\(\\beta_1,\\) regardless of the amount spent on radio.\nHowever, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on radio advertising actually increases the effectiveness of TV advertising.\nFigure 3.5 suggests that such an effect may be present in the advertising data:\n\nWhen levels of either TV or radio are low, then the true sales are lower than predicted by the linear model.\nBut when advertising is split between the two media, then the model tends to underestimate sales. \n\nSolution: Interaction Effects:\nConsider the standard linear regression model with two variables, \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon.\n\\] Here each predictor \\(X_1\\) and \\(X_2\\) has a given effect, \\(\\beta_1\\) and \\(\\beta_2\\), on \\(Y\\) and this effect does not depend on the value of the other predictor. (Additive Assumption)\nOne way of extending this model is to include a third predictor, called an interaction term, which is constructed by computing the product of \\(X_1\\) and \\(X_2.\\) This results in the model \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2 + \\epsilon.\n\\] This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as \\[\n\\begin{align*}\nY &= \\beta_0 + \\underbrace{(\\beta_1 + \\beta_3 X_2)}_{=\\tilde{\\beta}_1} X_1 + \\beta_2 X_2 + \\epsilon,\n\\end{align*}\n\\] where the new slope parameter \\(\\tilde{\\beta}_2\\) is a linear function of \\(X_2\\) \\[\n\\tilde{\\beta}_1\\equiv\\tilde{\\beta}_1(X_2)=\\beta_1 + \\beta_3 X_2.\n\\]\nThus, a change in the value of \\(X_2\\) will change the association between \\(X_1\\) and \\(Y.\\)\nA similar argument shows that a change in the value of \\(X_1\\) changes the association between \\(X_2\\) and \\(Y.\\)\nLet us return to the Advertising example. A linear model that uses radio, TV, and an interaction, radio\\(\\times\\)radio, between the two to predict sales takes the form\n\nsales \\(= \\beta_0 + \\beta_1\\times\\) TV \\(+ \\beta_2\\times\\) radio \\(+ \\beta_3\\times(\\) radio\\(\\times\\) TV\\()+\\epsilon\\)\n\nwhich can be rewritten as\n\nsales \\(=\\beta_0 + (\\beta_1+ \\beta_3\\times\\) radio \\()\\times\\) TV \\(+ \\beta_2\\times\\) radio \\(+\\epsilon\\)\n\n\nInterpretation:\n\n\\(\\beta_3\\) denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising (or vice-versa).\n\n\nInterpretation of Table 3.9:\n\nBoth (separate) main effects, TV and radio, are statistically significant (\\(p\\)-values smaller than 0.01).\nAdditionally, the \\(p\\)-value for the interaction term, TV\\(\\times\\)radio, is extremely low, indicating that there is strong evidence for \\(H_1: \\beta_3\\neq 0.\\) In other words, it is clear that the true relationship is not additive.\n\nHierarchical Principle:\nIf we include an interaction in a model, we should also include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant.\nInteractions with Qualitative Variables:\nAn interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.\nConsider the Credit data set and suppose that we wish to predict balance using the predictors:\n\nincome (quantitative) and\nstudent (qualitative) using a dummy variable with \\(x_{i2}=1\\) if \\(i\\)th person is a student and \\(x_{i2}=0\\) if not.\n\nIn the absence of an interaction term, the model takes the form \nThus, the regression lines for students and non-students have different intercepts, \\(\\beta_0+\\beta_2\\) versus \\(\\beta_0\\), but the same slope \\(\\beta_1\\).\nThis represents a potentially serious limitation of the model, since in fact a change in income may have a very different effect on the credit card balance of a student versus a non-student.\nThis limitation can be addressed by adding an interaction variable, created by multiplying income with the dummy variable for student. Our model now becomes \nNow we have different intercepts for students and non-students but also different slopes for these groups. \n\n\nPolynomial Regression: Non-linear Relationships\nPolynomial regression allows to accommodate non-linear relationships between the predictors \\(X\\) and the outcome \\(Y.\\) \nFor example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form\n\nmpg \\(=\\beta_0 + \\beta_1\\times\\) horsepower \\(+ \\beta_2\\times(\\)horsepower\\()^2+\\epsilon\\)\n\nThis regression model involves predicting mpg using a non-linear function of horsepower. But it is still a linear model! It’s simply a multiple linear regression model with \\(X_1=\\)horsepower and \\(X_2 =(\\)horsepower\\()^2.\\)\nSo we can use standard linear regression software to estimate \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) in order to produce a non-linear fit.\n\n\n\n\n(Ch. 3.3.3) Potential Problems\n1. Non-linearity of the response-predictor relationships.\nDiagnostic residual plots are most useful to detect possible non-linear response-predictor relationships.\n\nlibrary(\"ISLR2\")\ndata(Auto) \n\n## Gives the variable names in the Auto dataset\n# names(Auto)\n\n## Simple linear regression\nlmobj_1 <- lm(mpg ~ horsepower, data = Auto)\n\n## Quadratic regression \nlmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n\n## Diagnostic Plot\npar(mfrow = c(1,2))\nplot(lmobj_1, which = 1)\nplot(lmobj_2, which = 1)\n\n\n\n\nResidual plots are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals, \\[\ne_i = y_i - \\hat{y}_i,\n\\] versus the predictor \\(x_i.\\)\nIn the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values \\(\\hat{y}_i.\\) Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\nIf the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as \\[\n\\log(X),\\; \\sqrt{X},\\; \\text{or}\\; X^2\n\\] in the regression model. In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue.\n2. Correlation of Error Terms\nAn important assumption of the linear regression model is that the error terms, \\(\\epsilon_1, \\epsilon_2, \\dots , \\epsilon_n\\), are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that \\(\\epsilon_i\\) is positive provides little or no information about the sign of \\(\\epsilon_{i+1}.\\) The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors.\nCorrelations among the error terms typically occur in time series data (see Fig. 3.10).\n\n3. Non-Constant Variance of Error Terms\nAnother important assumption of the linear regression model is that the error terms have a constant variance, \\[\nVar(\\epsilon_i) = \\sigma^2.\n\\] The standard formulas for standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.\nOne can identify non-constant variances “heteroscedasticity” in the errors, using diagnostic residual plots.\nOften one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values which indicates. When faced with this problem, one possible solution is to transform the response \\(Y\\) using a concave function such as \\[\n\\log(Y)\\;\\text{ or }\\; \\sqrt{Y}.\n\\] Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.\n\n## Quadratic regression \nlmobj_2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n\n## Quadratic regression with transformed response log(Y)\nlmobj_3 <- lm(I(log(mpg)) ~ horsepower + I(horsepower^2), data = Auto)\n\n## Diagnostic Plot\npar(mfrow = c(1,2))\nplot(lmobj_2, which = 1)\nplot(lmobj_3, which = 1)\n\n\n\n\n4. Outliers\nAn outlier is a point for which \\(y_i\\) is far from the value predicted by the outlier model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.\nOutliers typically have a strong effect on the \\(R^2\\) value since they add a very large residual to its computation.\nFigure 3.12 in the textbook ISLR shows a clear outlier (observation 20) which, however, has a typical predictor value \\(x_i\\). Such outliers have little effect on the regression fit. \nFigure 3.13 in the textbook ISLR shows again a clear outlier (observation 41) which has a predictor value \\(x_i\\) that is very atypical. Such outliers are said to have large leverage giving them power to affect the regression fit considerably. \nSummary: Critical outliers have both, large residuals and large leverage.\n5. High Leverage Points\nIn order to quantify an observation’s leverage, we compute the leverage statistic \\(h_i\\) for each observation \\(i=1,\\dots,n.\\) A large value of this statistic indicates an observation with high leverage. For a simple linear regression, \\[\nh_i = \\frac{1}{n} + \\frac{(x_i-\\bar{x})^2}{\\sum_{j=1}^n(x_j-\\bar{x})^2}\n\\] There is a simple extension of \\(h_i\\) to the case of multiple predictors, though we do not provide the formula here.\n\nThe leverage statistic \\(h_i\\) is always between \\(1/n\\) and \\(1\\)\nThe average leverage for all the observations is equal to \\(\\bar{h}=\\frac{1}{n}\\sum_{i=1}^n h_i=(p + 1)/n.\\)\nIf a given observation has a leverage statistic \\(h_i\\) that greatly exceeds \\((p+1)/n,\\) then we may suspect that the corresponding point has high leverage.\n\n6. Collinearity\nCollinearity refers to the situation in which two or more predictor variables are closely related to one another.\n\nlibrary(\"ISLR2\")\ndata(Credit) # names(Credit)\n\npar(mfrow=c(1,2))\nplot(y = Credit$Age,    x = Credit$Limit, main = \"No Collinearity\", ylab = \"Age\", xlab = \"Limit\")\nplot(y = Credit$Rating, x = Credit$Limit, main = \"Strong Collinearity\", ylab = \"Rating\", xlab = \"Limit\")\n\n\n\n\n\n\nWe call this situation multicollinearity.\nTo detect multicollinearity issues, one can use the variance inflation factor (VIF) \\[\n\\operatorname{VIF}(\\hat{\\beta}_j)=\\frac{1}{1-R^2_{X_j|X_-j}},\n\\] where \\(R^2_{X_j|X_-j}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors.\n\nIf \\(R^2_{X_j|X_-j}\\) is close to one, then multicollinearity is present, and \\(\\operatorname{VIF}(\\hat{\\beta}_j)\\) will be large.\n\nIn the Credit data, a regression of balance on age, rating, and limit indicates that the predictors have VIF values of 1.01 (age), 160.67 (rating), and 160.59 (limit). Thus, as we suspected, there is considerable collinearity in the data!\nPossible solutions:\n\nDrop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables.  Caution: In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.\nCombine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.\nUse a different estimation procedure like ridge regression.\nLive with it. Sometimes you’re not allowed to drop or combine variables (e.g. important control variables) and also no other estimation procedure can be used. Then you have to live with large standard errors due to multicollinearity. But at least you know where the large stand errors are coming from."
  },
  {
    "objectID": "Ch2_LinearRegression.html#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors",
    "href": "Ch2_LinearRegression.html#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors",
    "title": "2  Linear Regression",
    "section": "(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors",
    "text": "(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors\nLinear regression is an example of a parametric approach because it assumes a linear model form for \\(f(X).\\)\nAdvantages of parametric approaches:\n\nTypically easy to fit\nSimple interpretation\nSimple inference\n\nDisadvantages of parametric approaches:\n\nThe parametric model assumption can be far from true; i.e. \\[\nf(X) \\neq \\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\n\\]\n\nAlternative: Non-parametric methods such as K-nearest neighbors regression since non-parametric approaches do not explicitly assume a parametric form for \\(f(X).\\)\n\nK-nearest neighbors regression (KNN regression)\nGiven a value for \\(K\\) and a prediction point \\(x_0,\\) KNN regression regression …\n\nidentifies the \\(K\\) training observations that are closest to \\(x_0\\), represented by the index set \\(\\mathcal{N}_0\\subset\\{1,2,\\dots,n_{Train}\\}.\\)\nestimates \\(f(x_0)\\) using the average of all the training responses \\(y_i\\) with \\(i\\in\\mathcal{N}_0.\\)\n\nIn other words, \\[\n\\hat{f}(x_0)=\\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}y_i.\n\\]\n\nIn general, the optimal value for \\(K\\) will depend on the bias-variance tradeoff, which we introduced in Chapter 2.\n\nA small value for \\(K\\) provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation of \\(K=1\\).\nA large value of \\(K\\) provide a smoother and less wiggly fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in \\(f(X).\\)\n\nIn Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of \\(K\\) in KNN regression.\nGenerally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of \\(f\\) and vice versa.\nFigure 3.17 of our textbook ISLR provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent the true \\(f(X)\\), while the blue curves correspond to the KNN fits using \\(K = 1\\) (left plot) and \\(K = 9\\) (right plot). In this case, the \\(K = 1\\) predictions are far too variable, while the smoother \\(K = 9\\) fit is much closer to the true \\(f(X).\\) However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. \nThe blue dashed line in the left-hand panel of Figure 3.18 represents the linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. \nFigure 3.19 displays a non-linear situations in which KNN performs much better than linear regression. \nCurse of dimensionality:\nUnfortunately, in higher dimensions, KNN often performs worse than linear regression, since non-parametric approaches suffer from the curse of dimensionality. Figure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise (i.e. redundant) predictors that are not associated with the response.\n\nWhen \\(p = 1\\) or \\(p = 2\\), KNN outperforms linear regression.\nBut for \\(p = 3\\) the results are mixed, and for \\(p\\geq 4\\) linear regression is superior to KNN. \n\nWhen \\(p=1\\), \\(50\\) data points can provide enough information to estimate \\(f(X)\\) accurately using non-parametric methods since the \\(K\\) nearest neighbors can actually be close to a given test observation \\(x_0.\\) However, when spreading the \\(50\\) data points over a large number of, for instance, \\(p=20\\) dimensions, even the \\(K\\) nearest neighbors tend to become far away from \\(x_0.\\)"
  },
  {
    "objectID": "Ch2_LinearRegression.html#r-lab-linear-regression",
    "href": "Ch2_LinearRegression.html#r-lab-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.3 R-Lab: Linear Regression",
    "text": "2.3 R-Lab: Linear Regression\n\n2.3.1 Libraries\nThe library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with this book.\n\nsuppressPackageStartupMessages(library(MASS))\nsuppressPackageStartupMessages(library(ISLR2))\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session.\n\n\n2.3.2 Simple Linear Regression\nThe ISLR2 library contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (average age of houses), and lstat (percent of households with low socioeconomic status).\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm.fit <- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat.\nThe next line tells R that the variables are in Boston:\n\nlm.fit <- lm(medv ~ lstat, data = Boston)\n\nAlternatively, we can attach the Boston object:\n\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\nWe can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\nType confint(lm.fit) at the command line to obtain the confidence intervals for the linear regression coefficients.\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), \n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), \n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nFor instance, the 95% confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95% prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nThe abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a, b). Below we experiment with some additional settings for plotting lines and points. The lwd = 3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\n\nplot(lstat, medv)\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\nplot(lstat, medv, col = \"red\")\n\n\n\nplot(lstat, medv, pch = 20)\n\n\n\nplot(lstat, medv, pch = \"+\")\n\n\n\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity.\nLeverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nplot(hatvalues(lm.fit))\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\nsort(hatvalues(lm.fit), decreasing = TRUE)[1:3]\n\n       375        415        374 \n0.02686517 0.02495670 0.02097101 \n\n\nThe sort() function can be used to sort and print values of a vector like hatvalues(lm.fit).\n\n\n2.3.3 Multiple Linear Regression\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nsuppressPackageStartupMessages(library(car)) # contains the vif() function\nsort(vif(lm.fit)) # computes the VIF statistics and sorts them\n\n    chas    black     crim  ptratio       rm       zn    lstat      age \n1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 \n     dis    indus      nox      rad      tax \n3.955945 3.991596 4.393720 7.484496 9.008554 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 <- update(lm.fit, ~ . - age)\n\n\n\n2.3.4 Interaction Terms\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells R to include an interaction term between lstat and black. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\n\n\n\n\n2.3.5 Non-linear Transformations of the Predictors\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n2.3.6 Qualitative Predictors\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n2.3.7 Writing Functions\nAs we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError in eval(expr, envir, enclos): object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function.\n\nLoadLibraries <- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries\n\nfunction() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "Ch2_LinearRegression.html#exercises",
    "href": "Ch2_LinearRegression.html#exercises",
    "title": "2  Linear Regression",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\nPrepare the following exercises of Chapter 3 in our course textbook ISLR:\n\nExercise 1\nExercise 2\nExercise 3\nExercise 8\nExercise 9"
  },
  {
    "objectID": "Ch3_Classification.html",
    "href": "Ch3_Classification.html",
    "title": "3  Classification",
    "section": "",
    "text": "Reading: Chapter 4 of our course textbook An Introduction to Statistical Learning"
  },
  {
    "objectID": "Ch3_Classification.html#ch.-4.1-an-overview-of-classification",
    "href": "Ch3_Classification.html#ch.-4.1-an-overview-of-classification",
    "title": "3  Classification",
    "section": "(Ch. 4.1) An Overview of Classification",
    "text": "(Ch. 4.1) An Overview of Classification\nClassification problems occur often, perhaps even more so than regression problems.\nSome examples include:\n\nA person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\nAn online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.\nOn the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.\n\nOne of the running example for this chapter: The (simulated) Default data set which is part of the online resources of our textbook ISLR, but also contained in the R package ISLR2. \nLet’s take a first look at the a priori default rate in this dataset:\n\nsuppressPackageStartupMessages(library(\"ISLR2\"))\ndata(Default)\n\nn <- nrow(Default)       # sample size\n\ntable(Default$default)/n # Overall no-default and default-rate\n\n\n    No    Yes \n0.9667 0.0333 \n\n\n\nOverall default rate: approx. \\(3\\%.\\) (Figure 4.1 shows only a small fraction of the individuals who did not default.)"
  },
  {
    "objectID": "Ch3_Classification.html#ch.-4.2-why-not-linear-regression",
    "href": "Ch3_Classification.html#ch.-4.2-why-not-linear-regression",
    "title": "3  Classification",
    "section": "(Ch. 4.2) Why Not Linear Regression?",
    "text": "(Ch. 4.2) Why Not Linear Regression?\nLinear regression is often not appropriate in the case of a qualitative response \\(Y.\\)\nSuppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses:\n\nstroke\ndrug overdose, and\nepileptic seizure\n\nWe can encoding these values as a quantitative response variable, \\[\nY=\\left\\{\n    \\begin{array}{ll}\n    1&\\quad\\text{if }\\texttt{stroke}\\\\\n    2&\\quad\\text{if }\\texttt{drug overdose}\\\\\n    3&\\quad\\text{if }\\texttt{epileptic seizure}\\\\\n    \\end{array}\n\\right.\n\\] Using this coding, least squares could be used to fit a linear regression model to predict \\(Y,\\) but:\n\nThe results would then depend on the numeric ordering \\(1<2<3\\), even though the ordering was completely arbitrary and could have been made differently.\nThe results would then depend on the assumption the gap \\((2-1=1)\\) between stroke and drug overdose is comparable to the gap \\((3-2=1)\\) between drug overdose and epileptic seizure.\n\nGenerally, both points are quite a lot of nonsense for most applications.\nOnly if the response variable’s values did take on a natural ordering, such as “mild”, “moderate”, and “severe”, and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable.\nFor a binary (two level) qualitative response, the situation is better. For instance, if there are only two conditions that we need to predict (e.g. either default\\(=\\)Yes or default\\(=\\)No), we can use a dummy variable coding \\[\nY=\\left\\{\n\\begin{array}{ll}\n    0&\\quad\\text{if }\\texttt{default}=\\texttt{Yes}\\\\\n    1&\\quad\\text{if }\\texttt{default}=\\texttt{No}\\\\\n\\end{array}\n\\right.\n\\] We could then fit a linear regression to this binary response, and predict drug overdose if \\(\\hat{Y}> 0.5\\) and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.\nFor binary responses with a \\(0/1\\) coding, linear regression is not completely unreasonable. It can be shown that \\[\nPr(\\texttt{default}=\\texttt{Yes}|X)\\approx \\hat{Y}=\\beta_0+ \\beta_1 X_1+\\dots +\\beta_p X_p.\n\\]\nHowever, if we use linear regression, some of our estimates might be outside the \\([0, 1]\\) interval (see Figure 4.2), which doesn’t make sense when predicting probabilities. \nSummary:\n\nA classic regression method cannot accommodate a qualitative response with more than two classes\nA classic regression method may not provide meaningful estimates of \\(Pr(Y |X),\\) even with just two classes.\n\nThus, it is often preferable to use a classification method that is truly suited for qualitative response values."
  },
  {
    "objectID": "Ch3_Classification.html#ch.-4.3-logistic-regression",
    "href": "Ch3_Classification.html#ch.-4.3-logistic-regression",
    "title": "3  Classification",
    "section": "(Ch. 4.3) Logistic Regression",
    "text": "(Ch. 4.3) Logistic Regression\nLogistic regression models the probability that \\(Y\\) belongs to a particular category.\nFor the Default data, logistic regression models the conditional probability of default given values for the predictor(s). For example, the probability of default given balance: \\[\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{balance}) = p(\\texttt{balance}),\n\\] where \\(p(\\texttt{balance})\\in[0,1]\\) is used as a short hand notation.\nOne might predict default\\(=\\)Yes for any individual for whom \\(p(\\texttt{balance}) > 0.5.\\)\nHowever, \\(0.5\\) this is not the only reasonable classification threshold!\nFor instance, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as \\(p(\\texttt{balance}) > 0.1.\\)\n\n(Ch. 4.3.1) The Logistic Model\nFor a binary coded dependen variable \\(Y\\in\\{0,1\\}\\) we aim to model the relationship between \\[\np(X)=Pr(Y=1|X)\\quad\\text{and}\\quad X.\n\\]\nAs discussed above, a linear regression model, e.g., \\[\np(X)=\\beta_0+\\beta_1 X\n\\] can produce nonsense predictions \\(p(X)<0\\) or \\(p(X)>1\\); see the left-hand panel of Figure 4.2.\nLogistic regression avoids this problem by using the logistic function, \\[\np(X)=\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}.\n\\] To fit the parameters \\(\\beta_0\\) and \\(\\beta_1\\) we use an estimation method called maximum likelihood.\nThe right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data.\nNote that \\[\n\\begin{align*}\np(X) & = \\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{1-p(X)} \\\\\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{1-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n%\\frac{p(X)}{1-p(X)} & = \\frac{\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}}{\\frac{1+e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}-\\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}} \\\\\n\\quad \\Leftrightarrow\\quad  \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1 X}\n\\end{align*}\n\\]\nThe quantity \\[\n\\frac{p(X)}{1 − p(X)}\n\\] is called the odds, and can take any value between 0 and plus infinity.\n\nA small odds value (close to zero) indicates a low probability of default.\nA large odds value indicates a high probability of default.\n\nFor instance\n\nAn odds value of \\(\\frac{1}{4}\\) means that \\(0.2=20\\%\\) of the people will default \\[\n\\frac{0.2}{1-0.2}=\\frac{1}{4}\n\\]\nAn odds value of \\(9\\) means that \\(0.9=90\\%\\) of the people will default \\[\n\\frac{0.9}{1-0.9}=9\n\\]\n\nBy taking the logarithm, we arrive at log odds or logit \\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0+\\beta_1 X\n\\]\nThus increasing \\(X\\) by one unit …\n\n… changes the log odds by \\(\\beta_1\\)\n… multiplies the odds by \\(e^{\\beta_1}\\)\n\nCaution: The amount that \\(p(X)\\) changes due to a one-unit change in \\(X\\) depends on the current value of \\(X.\\) (The logistic regression model is a non-linear model.)\nBut regardless of the value of \\(X\\), if \\(\\beta_1\\) is positive then increasing \\(X\\) will be associated with increasing \\(p(X)\\), and if \\(\\beta_1\\) is negative then increasing \\(X\\) will be associated with decreasing \\(p(X).\\)\n\n\n(Ch. 4.3.2) Estimating the Regression Coefficients\nIn logistic regression analysis, the unknown model parameters are estimated using maximum likelihood.\nBasic intuition:\nFind estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) such that the predicted probability \\(\\hat{p}(x_i)\\) for each person \\(i\\) corresponds as close as possible to its default status. (I.e. \\(\\hat{p}(x_i)\\approx 1\\) if person \\(i\\) defaulted and \\(\\hat{p}(x_i)\\approx 0\\) if not.)\nThis intuition can be formalized using a mathematical equation called a likelihood function: \\[\n\\ell(\\beta_0,\\beta_1)=\\prod_{i:y_{i}=1} p(x_i)\\prod_{i:y_{i}=0} (1-p(x_i))\n\\] The estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are chosen to maximize this likelihood function.\nMaximum likelihood is a very general estimation method that allows to estimate also non-linear models (like the logistic regression model).\nTable 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default\\(=\\)Yes using balance as the only predictor. \nInterpretation:\n\nWe see that \\(\\hat\\beta_1=0.0055\\); this indicates that an increase in balance is associated with an increase in the probability of default.\n\nTo be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0.0055 units.\n\nThe \\(z\\)-statistic in Table 4.1 plays the same role as the \\(t\\)-statistic in the linear regression output: a large (absolute) value of the \\(z\\)-statistic indicates evidence against the null hypothesis \\(H_0: \\beta_1= 0.\\)\n\n\n\n(Ch. 4.3.3) Making Predictions\nOnce the coefficients have been estimated, we can compute the probability of default\\(=1\\) for any given credit card balance.\nFor example, using the coefficient estimates given in Table 4.1, we predict that the default probability for an individual with a balance-value of 1,000 [USD] is \\[\n\\begin{align*}\n\\hat{p}(\\texttt{balance})\n&=Pr(\\texttt{default}=\\texttt{Yes}|\\texttt{balance})\\\\\n&=\\frac{e^{\\hat\\beta_0+\\hat\\beta_1 \\texttt{balance}}}{1+e^{\\hat\\beta_0+\\hat\\beta_1 \\texttt{balance}}}\\\\\n&=\\frac{e^{-10.6513+ 0.0055\\times 1000}}{1+e^{-10.6513+ 0.0055\\times 1000}} = 0.00576 < 1\\%\n\\end{align*}\n\\]\nBy contrast, the default probability for an individual with a balance-value of 2,000 [USD] equals \\(0.586\\) (or \\(58,6\\%\\)) and is thus much higher.\n\nQualitative Predictors:\n\n\\[\n\\begin{align*}\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{student}=\\texttt{Yes})\n&=\\frac{e^{-3.5041+0.4049\\times 1}}{1+e^{-3.5041+0.4049\\times 1}}\\\\\n&= 0.0431\\\\\nPr(\\texttt{default}=\\texttt{Yes}|\\texttt{student}=\\texttt{No})\n&=\\frac{e^{-3.5041+0.4049\\times 0}}{1+e^{-3.5041+0.4049\\times 0}}\\\\\n&= 0.0292\\\\\n\\end{align*}\n\\]\nThis may indicate that students tend to have higher default probabilities than non-students. But we may missed further factors here since we only use one predictor variable.\n\n\n\n(Ch. 4.3.4) Multiple Logistic Regression\nBy analogy with the extension from simple to multiple linear, we can generalize the (simple) logistic regression model as following \\[\n\\begin{align*}\np(X)\n&=\\frac{e^{\\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p}}{1+e^{\\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p}}\\\\\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right)\n&= \\beta_0+\\beta_1 X_1+\\dots+\\beta_p X_p,\\\\\n\\end{align*}\n\\] where \\(X=(X_1,\\dots,X_p)\\), and where the unknown parameters \\(\\beta_0,\\dots,\\beta_p\\) are estimated by maximum likelihood. \nThe negative coefficient for student in the multiple logistic regression indicates that for a fixed value of balance and income, a student is less likely to default than a non-student.\n\nInterestingly, the effect of the dummy variable student[Yes] is now negative, in contrast to the estimation results of the (simple) logistic regression in Table 4.2 where it was positive.\nThe left-hand panel of Figure 4.3 provides a graphical illustration of this apparent paradox:\n\nWithout considering balance, the (overall) default rates of students are higher than those of non-students (horizontal broken lines). This overall effect was shown in Table 4.2.\nHowever, for given balance-values, the default rate for students is lower than for non-students (solid lines).\n\n\nThe right-hand panel of Figure 4.3 provides an explanation for this discrepancy: Students tend to hold higher levels of debt, which is in turn associated with higher probability of default.\nSummary:\n\nAn individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance\nHowever, overall, students tend to default at a higher rate than non-students since, overall, they tend to have higher credit card balances.\n\nIn other words: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, a student is less risky than a non-student if both have the same credit card balance.\n\n\n(Ch. 4.3.5) Multinomial Logistic Regression\nIt is possible to extend the two-class logistic regression approach to the setting of \\(K > 2\\) classes. This extension is sometimes known as multinomial logistic regression.\nTo do this, we first select a single class to serve as the baseline; without loss of generality, we select the \\(K\\)th class for this role.\nWe model the probabilities that \\(Y=k\\), for \\(k=1,\\dots,K\\), using \\(k\\)-specific parameters \\(\\beta_{k0},\\dots,\\beta_{kp}\\) with \\[\np_k(x)=Pr(Y=k|X=x)=\\frac{e^{\\beta_{k0} + \\beta_{k1} x_1 + \\dots +  \\beta_{kp} x_p}}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots +  \\beta_{lp} x_l}}\n\\] for \\(k=1,\\dots,K-1\\), and \\[\np_K(x)=Pr(Y=K|X=x)=\\frac{1}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots +  \\beta_{lp} x_l}}\n\\]\nFor \\(k=1,\\dots,K-1\\) it holds that \\[\n\\log\\left(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\\right)=\\beta_{k0} + \\beta_{k1} x_1 + \\dots +  \\beta_{kp} x_p\n\\] which is the counterpart to the log odds equation for \\(K=2.\\)\nNote that:\n\nThe predictions \\(\\hat{p}_k(x)\\), for \\(k=1,\\dots,K\\) do not depend on the choice of the baseline class.\nHowever, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline."
  },
  {
    "objectID": "Ch3_Classification.html#ch.-4.4-discriminant-analysis-generative-models-for-classification",
    "href": "Ch3_Classification.html#ch.-4.4-discriminant-analysis-generative-models-for-classification",
    "title": "3  Classification",
    "section": "(Ch. 4.4) Discriminant Analysis: Generative Models for Classification",
    "text": "(Ch. 4.4) Discriminant Analysis: Generative Models for Classification\nWe now consider an alternative and less direct approach to estimating the probabilities \\(Pr(Y=K|X=x).\\)\nSuppose that we wish to classify an observation into one of \\(K\\geq 2\\) classes.\nPrior probability: Let \\[\n\\pi_k=Pr(Y=k)\n\\] represent the overall prior probability that a randomly chosen observation comes from class \\(k.\\) We have that \\[\n\\pi_1+\\dots+\\pi_K=1.\n\\]\nDensity function of \\(X\\): Let \\[\nf_k(x)=Pr(X=x|Y=k)\n\\] denote the (conditional) density of \\(X\\) for an observation that comes from class \\(k.\\)1\nPosterior probability: Then Bayes’ theorem states that the probability that a observation with predictor values \\(X=x\\) comes from class \\(k\\) (i.e. the posterior probability), is given by \\[\np_k(x) = Pr(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_l f_l(x)}.\n\\tag{3.1}\\]\nWhile logistic regression aims at estimating the posterior probability \\(p_k(x)\\) directly, Bayes’s theorem gives us a way to estimate \\(p_k(x)\\) indirectly simply by plugging in estimates of \\(\\pi_k\\) and \\(f_k(x)\\).\nHowever, estimating the densities \\(f_k(x)\\) can be very challenging and we therefore have to make some simplifying assumptions; namely, assuming that the data is normal distributed.\nWe know from Chapter 2 that the Bayes classifier, which classifies an observation \\(x\\) to the class \\(k\\) for which \\(p_k(x)\\) is largest, has the lowest possible error rate out of all classifiers.\nIn the following sections, we discuss three classifiers that use different estimates of \\(f_k(x)\\) to approximate the Bayes classifier:\n\nlinear discriminant analysis\nquadratic discriminant analysis\nNaive Bayes\n\n\n(Ch. 4.4.1) Linear Discriminant Analysis for \\(p = 1\\)\nFor the beginning, let us assume that we have only one predictor, i.e., \\(p=1.\\)\nTo estimate \\(f_k(x)\\), we will assume that \\(f_k\\) is normal (or Gaussian). In the simple \\(p=1\\) dimensional setting, the normal distribution is \\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\left(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2\\right),\n\\] where\n\n\\(\\mu_k\\) is the mean of the \\(k\\)th class and\n\\(\\sigma_k^2\\) is the variance of the \\(k\\)th class\n\\(\\pi\\approx 3.14159\\) is the mathematical constant \\(\\pi\\). (Do not confuse it with the prior probabilities \\(\\pi_k.\\))\n\nFor now, let us further assume the simplifying case of equal variances across all classes, i.e.  \\[\n\\sigma_1^2=\\dots = \\sigma_K^2\\equiv \\sigma^2.\n\\] Plugging this assumed version of \\(f_k\\) into Equation 3.1, leads to \\[\np_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\right)}{\\sum_{l=1}^K\\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\right)}.\n\\tag{3.2}\\]\nThe Bayes classifier involves assigning an observation \\(X = x\\) to the class \\(k\\) for which \\(p_k(x)\\) is largest. Taking the \\(\\log\\) of Equation 3.2 (i.e. a monotonic transformation) and rearranging terms shows that this is equivalent to assigning an observation \\(X=x\\) to the class \\(k\\) for which \\[\n\\delta_k(x)=x\\cdot\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\] is largest. The function \\(\\delta_k(x)\\) is called the discriminant function.\nExample:\nIn the case of only two classes, \\(K=2\\), with equal a priori probabilities \\(\\pi_1=\\pi_2\\equiv \\pi^*\\), the Bayes classifier assigns an observation \\(X=x\\) to class \\(1\\) if \\[\n\\begin{align*}\n\\delta_1(x) & > \\delta_2(x)\\\\\n%%%\nx\\cdot\\frac{\\mu_1}{\\sigma^2} - \\frac{\\mu_1^2}{2\\sigma^2} + \\log(\\pi^*)\n& >\nx\\cdot\\frac{\\mu_2}{\\sigma^2} - \\frac{\\mu_2^2}{2\\sigma^2} + \\log(\\pi^*)\\\\\n%%%\nx\\cdot\\frac{\\mu_1}{\\sigma^2} - \\frac{\\mu_1^2}{2\\sigma^2}  \n& >\nx\\cdot\\frac{\\mu_2}{\\sigma^2} - \\frac{\\mu_2^2}{2\\sigma^2}\\\\\n%%%\nx\\cdot\\mu_1 - \\frac{\\mu_1^2}{2}  \n& >\nx\\cdot\\mu_2 - \\frac{\\mu_2^2}{2}\\\\\n%%%\n2x\\cdot(\\mu_1-\\mu_2)   \n& >\n\\mu_1^2 - \\mu_2^2\\\\\n%%%\nx   \n& > \\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1-\\mu_2)}\n\\end{align*}\n\\] The Bayes decision boundary is the point for which \\(\\delta_1(x)=\\delta_2(x)\\) \\[\n\\begin{align*}\nx   \n=\\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1-\\mu_2)}\n&=\\frac{(\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2)}{2(\\mu_1-\\mu_2)}\\\\\n&=\\frac{(\\mu_1 + \\mu_2)}{2}\n\\end{align*}\n\\] Figure 4.4 shows a specific example with\n\n\\(\\pi_1=\\pi_2\\)\n\\(\\mu_1=-1.25\\) and \\(\\mu_2=1.25\\) and\n\\(\\sigma_1=\\sigma_2\\equiv\\sigma =1.\\)\n\nThe two densities \\(f_1\\) and \\(f_2\\) overlap such that for given \\(X=x\\) there is some uncertainty about the class to which the observation belongs to. In this example, the Bayes classifier assigns an observation \\(X=x\\) …\n\n… to class 1 if \\(x<0\\)\n… to class 2 if \\(x>0\\)\n\n\nIn the above example (Figure 4.4) we know all parameters \\(\\pi_k\\), \\(\\mu_k\\), \\(k=1,\\dots,K\\), and \\(\\sigma\\). In practice, however, these parameters are usually unknown, and thus need to be estimated from the data.\nThe Linear Discriminant Analysis (LDA) method approximates the Bayes classifier by using the normality assumption and by plugging in estimates for the unknown parameters \\(\\pi_k\\), \\(\\mu_k\\), \\(k=1,\\dots,K\\), and \\(\\sigma\\). The following estimates are used: \\[\n\\begin{align*}\n\\hat\\pi_k    & = \\frac{n_k}{n}\\\\\n\\hat\\mu_k    & = \\frac{1}{n_k}\\sum_{i:y_i=k}x_i\\\\\n\\hat\\sigma   & = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat\\mu_k)^2\\\\\n\\end{align*}\n\\] The LDA classifier assigns an observation \\(X=x\\) to the class \\(k\\) for which \\[\n\\hat\\delta_k(x)=x\\cdot\\frac{\\hat\\mu_k}{\\hat\\sigma^2} - \\frac{\\hat\\mu_k^2}{2\\hat\\sigma^2} + \\log(\\hat\\pi_k)\n\\] is largest.\nLDA is named linear since the discriminant functions \\(\\hat\\delta_k(x)\\), \\(k=1,\\dots,K\\) are linear functions of \\(x.\\)\nAssumptions made by the LDA method:\n\nObservation for each class come from a normal distribution\nThe class-specific normal distributions have class-specific means, \\(\\mu_k\\), but equal variances \\(\\sigma\\)\n\n\n\n(Ch. 4.4.2) Linear Discriminant Analysis for \\(p > 1\\)\nIn the case of multiple predictors \\[\nX=(X_1,\\dots,X_p)\\quad \\text{with}\\quad p>1\n\\] we need to assume that the observations for each class \\(k\\) come from a multivariate Gaussian distribution with\n\nClass-specific \\((p\\times 1)\\) mean vectors \\(\\mu_k\\)\nEqual \\((p\\times p)\\) covariance matrix \\(\\Sigma\\) for all classes \\(k=1,\\dots,K\\)\n\nThat is, a \\((p\\times 1)\\) random vector \\(X\\) of class \\(k\\) is distributed as \\[\nX\\sim N\\left(\\mu_k,\\Sigma\\right)\n\\] with class-specific \\((p\\times 1)\\) mean vector \\(\\mu_k\\), \\[\nE(X)=\\mu_k,\n\\] and \\((p\\times p)\\) covariance matrix \\[\nCov(X)=\\Sigma\n\\] that is common for all classes \\(k=1,\\dots,K.\\)\nThe multivariate Gaussian density for class \\(k=1,\\dots,K\\) is defined as \\[\nf_k(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)\\right)\n\\]\nFigure 4.5 shows the case of bivariate (\\(p=2\\)) Gaussian distribution predictors.\n\nUnder the multivariate normality assumption (with common covariance matrix and group-specific mean vectors) the Bayes classifier assigns an multivariate observation \\(X=x\\) to the class for which the discriminant function \\[\n\\delta_k(x)=x^T \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log(\\pi_k)\n\\tag{3.3}\\] is largest.\nAn example for \\(p=2\\) and \\(K=3\\) with three equally sized (i.e. \\(\\pi_1=\\pi_2=\\pi_3\\)) Gaussian classes is shown in the left-hand panel of Figure 4.6. The three ellipses represent regions that contain \\(95\\%\\) of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which \\(\\delta_k(x) = \\delta_l(x)\\), \\(k\\neq l\\).\n\nAgain, the unknown parameters \\(\\mu_k\\), \\(\\pi_k\\), and \\(\\Sigma\\) must be estimated from the data with formulas similar to those for the \\(p=1\\) case.\n\nThe Default data example\nWe can perform LDA on the Default data in order to predict whether or not an individual will default on the basis of\n\ncredit card balance (real variable) and\nstudent status (qualitative variable).\n\nNote: Student status is qualitative, and thus the normality assumption made by LDA is clearly violated in this example! However, LDA is often remarkably robust to model violations, as this example shows.\nThe LDA model fit to the \\(10,000\\) training samples results in an overall training error rate of \\(2.75\\%:\\)\n\n# MASS package contains the lda() function\nsuppressPackageStartupMessages(library(MASS)) \n\n## Sample size \nn              <- nrow(Default)\n\nlda_obj        <- lda(default ~ balance + student, \n                      data = Default)\nldaPredict_obj <- predict(lda_obj)\n\n## Training error rate:\nsum(Default$default != ldaPredict_obj$class)/n \n\n[1] 0.0275\n\n\nThis sounds like a low error rate, but two caveats must be noted:\n\nTraining error rates are usually smaller than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a new set of individuals will default.\n\nThe reason is that we specifically adjust the parameters of our model to do well on the training data. (We may “overfit” the training data.)\nGenerally, the higher the ratio of parameters \\(p\\) to number of samples \\(n\\), the more we expect overfitting to play a role. However, for these data we don’t expect this to be a problem, since \\(p = 2\\) and \\(n = 10,000.\\)\n\nSince only \\(3.33\\%\\) of the individuals in the training sample defaulted, a simple naive classifier that always predicts “not default” for every person, regardless of his or her credit card balance and student status, will result in an error rate of \\(3.33\\%.\\) In other words, the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate.\n\n\n## Training error rate of the \"null classifier\":\nsum(Default$default != rep(\"No\", n))/n \n\n[1] 0.0333\n\n\n\n\n\nClassification Errors of Binary Classifiers\nAny binary classifier can make two types of errors:\n\nFalse Negatives (FN): Predicting “no default” for a person who actually defaults\nFalse Positives (FP): Predicing “default” for a person who actually does not default\n\nDepending on the context, both error can have different consequential damages.\nA confusion matrix shows both types of errors. The upper off-diagonal entry shows the FNs; the lower off-diagonal entry shows the FPs. A perfect confusion matrix has zeros in the off-diagonal entries.\n\n## Confusion matrix\ntable(ldaPredict_obj$class, Default$default, \n      dnn = c(\"Predicted default status\", \n              \"True default status\"))\n\n                        True default status\nPredicted default status   No  Yes\n                     No  9644  252\n                     Yes   23   81\n\n\n\nTrue/False Positive Predictions:\n\nLDA predicts that P\\(^*=104\\) people default (Predicted Positives, P\\(^*\\)).\n\n\\(81\\) of these actually default (True Positives, TP).\n\\(23\\) of these actually do not default (False Positives, FP).\n\n\nTrue/False Negative Predictions:\n\nLDA predicts that N\\(^*=9896\\) people do not default (Predicted Negatives, N\\(^*\\)).\n\n\\(9644\\) of these actually do not default (True Negatives, TN)\n\\(252\\) of these actually default (FALSE Negatives, FN)\n\n\nTrue Positive/Negative Rates:\n\nTrue Positive Rate (TP/P): \\((81/333)\\cdot 100\\%\\approx 24.3\\%\\)  \nTrue Negative Rate (TN/N): \\((9644/9667)\\cdot 100\\%\\approx 99.8\\%\\) \n\nFalse Positive/Negative Rates:\n\nFalse Positive Rate (FP/N): \\((23/9667)\\cdot 100\\%\\approx 0.2\\%\\)\n \nFalse Negative Rate (FN/P): \\((252/333)\\cdot 100\\%\\approx 75.7\\%\\)\n\n\nThe small False Positive Rate of \\(0.2\\%\\) is good since this means that trustworthy “no default” people are rarely labeled as non-trustworthy “default” people.\nHowever, the high False Negative Rate of \\(75.7\\%\\) is problematic for a bank since this means that the bank often does not detect the non-trustworthy “default” people.\n\nChoosing the Classification Threshold\nIf a “False Negative Event” costs the bank more than a “False Positive Event” (or vice versa), then the bank will want to adjust the classifying threshold of the classifier.\n\nThe Bayes classifier, and thus also all classifiers which try to approximate the Bayes classifier, use a threshold of \\(50\\%\\). I.e., a person \\(i\\) with predictor value \\(X=x_i\\) is assigned to the default\\(=\\)Yes class if \\[\nPr(\\texttt{default}_i=\\texttt{Yes}|X=x_i)>0.5.\n\\]\nHowever, if the costs of False Negatives (FN) are higher than the costs of False Positives (FP), one can consider lowering the classifying threshold. This decreases the predicted negatives (N\\(^*\\)) and thus also decreases the False Negatives (FN) since N\\(^*=\\)TN\\(+\\)FN.\nFor instance, we might classify any person \\(i\\) with a posterior probability of default above \\(20\\%,\\) \\[\nPr(\\texttt{default}=\\texttt{Yes}|X=x_i)>0.2,\n\\] into the default\\(=\\)Yes class.\n\n## Predicted posterior probabilities \n## head(ldaPredict_obj$posterior)\n\n## Container for the predicted classes\nPredictedDefault <- rep(\"No\", n)\n## Fill in the \"Yes\" classifications \nPredictedDefault[ldaPredict_obj$posterior[,2] > 0.2] <- \"Yes\"\n\n## Confusion matrix for 0.2 threshold:\ntable(PredictedDefault, Default$default, \n      dnn = c(\"Predicted default status\", \n              \"True default status\"))\n\n                        True default status\nPredicted default status   No  Yes\n                     No  9432  138\n                     Yes  235  195\n\n\n\nChanging the threshold from \\(0.5\\) to \\(0.2\\) …\n\n… lowered the False Negative Rate (FN/P) 😎\n\nfrom \\(252/333\\approx 75.7\\%\\)\nto \\(138/333\\approx 41.4\\%\\)\n\n\n… increased the False Positive Rate (FP/N) 😕\n\nfrom \\(23/9667\\approx 0.2\\%\\)\nto \\(235/9667\\approx 2.4\\%\\)\n\n… increased the Overall Error Rate ((FN+FP)/n) 😕\n\nfrom \\((23 + 252)/10000\\approx 2.75\\%\\)\nto \\((235 + 138)/10000\\approx 3.73\\%\\)\n\n\nFor a credit card company, this slight increase in the overall error rate can be a small price to pay for a more accurate identification of people who indeed default.\n\n\n\nClassification Performance Measures\nTable 4.6 and 4.7 summarize the different classification performance measures.\n\n\n\n\nThe ROC curve\nThe (Receiver Operating Characteristics) ROC curve is a popular graphic for displaying the \\[\n\\text{TP Rate}=\\text{TP/P}\\quad \\text{(``sensitivity'' or ``power'')}\n\\] against the \\[\n\\text{FP Rate}=\\text{FP/N} \\quad \\text{(``}1-\\text{specificity'' or ``type I error'')}\n\\] for all possible threshold values (from zero to one).\n\n## Considered thresholds (exclude the trivial 0/1 values)\nthresholds       <- seq(from = 0, to = 1, length.out = 250)\nn_thresholds     <- length(thresholds)\n\n## Container for the predicted classes\nPredictedDefault_mat <- matrix(\"No\", n, n_thresholds)\n\n## Fill in the \"Yes\" classifications \nfor(j in 1:n_thresholds){\n  PredicedYes  <- ldaPredict_obj$posterior[,2] > thresholds[j]\n  PredictedDefault_mat[PredicedYes, j] <- \"Yes\"\n}\n\n## Number of actual positives\nP <- length(Default$default[Default$default == \"Yes\"])\n## Number of actual negatives\nN <- length(Default$default[Default$default == \"No\"])\n\nTP_Rate <- numeric(n_thresholds)\nFP_Rate <- numeric(n_thresholds)\n\nfor(j in 1:n_thresholds){\n  ## Classification results among the actually positives\n  Classifications_among_trueYes <- PredictedDefault_mat[,j][Default$default == \"Yes\"]\n  ## Classification results among the actually negatives\n  Classifications_among_trueNo <- PredictedDefault_mat[,j][Default$default == \"No\"]\n  ## Number of True Positives\n  TP <- length(Classifications_among_trueYes[Classifications_among_trueYes == \"Yes\"])\n  ## Number of False Positives\n  FP <- length(Classifications_among_trueNo[Classifications_among_trueNo == \"Yes\"])\n\n  ## TP error rate:\n  TP_Rate[j] <- TP/P\n  ## FP error rate:\n  FP_Rate[j] <- FP/N \n}\n\n## Layout for the plotting region (two plots side-by-side)\nlayout(matrix(1:2, ncol=2), width = c(2,1),height = c(1,1))\n\ncol_range <- hcl.colors(n       = n_thresholds, \n                       palette = \"viridis\")\n\n## Plot the ROC curve \nplot(x    = FP_Rate, \n     y    = TP_Rate, \n     col  = col_range, \n     xlab = \"False Positive Rate\",\n     ylab = \"True Positive Rate\",\n     main = \"ROC Curve\", \n     type = \"o\", lwd=1.5, pch = 19)\nabline(a = 0, b = 1, lty = 3, col = \"gray\")\n\n## Color-Legend for threshold values\nlegend_image <- as.raster(matrix(rev(col_range), ncol=1))\nplot(c(0,2),c(0,1), type = \"n\", axes = FALSE, xlab = \"\", ylab = \"\", main = 'Threshold-Value')\ntext(x      = 1.5, \n     y      = seq(from = 0, to = 1, l = 5), \n     labels = seq(from = 0, to = 1, l=5))\nrasterImage(legend_image, xleft = 0, ybottom = 0, xright = 1, ytop = 1)\n\n\n\n\nFigure 3.1: The ROC curve traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are often not shown. Here, however, we show them using a color code from yellow [threshold = 1] to purple [threshold = 0]. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default.\n\n\n\n\n\nThe commonly reported summary statistic for the ROC curve is the “Area Under the (ROC) Curve” AUC\n\nIdeal AUC-value is AUC\\(=1.\\)\nA “no information” classifier has AUC\\(=0.5.\\)\n\nThe R package ROCR contains function for computing and plotting the ROC curve and the AUC value.\n\n## install.packages(\"ROCR\")\nlibrary(\"ROCR\")\n\n## Caution: Choose the posterior probability column carefully, \n## it may be lda.pred$posterior[,1] or lda.pred$posterior[,2], \n## depending on your factor levels\n\n## Predicted and actual classes:\npredict <- prediction(ldaPredict_obj$posterior[,2], Default$default) \n## compute TP-Ratios and FP-Ratios:\nperform <- performance(predict, \"tpr\", \"fpr\")\n## ROC curve\nplot(perform, colorize = TRUE)\n\n\n\n## AUC\nAUC     <- performance(predict, measure = \"auc\")\nprint(AUC@y.values)\n\n[[1]]\n[1] 0.9495584\n\n\n\n\n(Ch. 4.4.3) Quadratic Discriminant Analysis (QDA)\nUnlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form \\[\nX|Y=k \\sim N(\\mu_k,\\Sigma_k),\n\\] where \\(\\Sigma_k\\) is a \\((p\\times p)\\) covariance matrix for the \\(k\\)th class.\nUnder this assumption, the Bayes classifier assigns an observation \\(X=x\\) to the class for which \\[\n\\begin{align*}\n&\\delta_k(x)=\\\\\n&= - \\frac{1}{2}\\left(x-\\mu_k\\right)^T\\Sigma_k^{-1}\\left(x-\\mu_k\\right)\n   - \\frac{1}{2}\\log\\left(|\\Sigma_k|\\right)\n   + \\log(\\pi_k)\\\\\n&= - \\frac{1}{2}x^T    \\Sigma_k^{-1}x\n   + \\frac{1}{2}x^T    \\Sigma_k^{-1}\\mu_k\n   - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k\n   - \\frac{1}{2}\\log\\left(|\\Sigma_k|\\right)\n   + \\log(\\pi_k)\n\\end{align*}\n\\] is largest.\nThis classifier is called quadratic discriminant analysis since the quantity \\(x\\) appears as a quadratic function.\nQDA has much more parameters than LDA and is thus a much more flexible classifier than LDA:\n\nLDA requires\n\n\\(K\\) a-priori probability estimates\n\\(K\\cdot p\\) mean estimations\n\\((p+1)/2\\) covariance estimates\n\nQDA requires\n\n\\(K\\) a-priori probability estimates\n\\(K\\cdot p\\) mean estimations\n\\({\\color{red}K}\\cdot(p+1)/2\\) covariance estimates\n\n\nThe difference in flexibility needs to be considered under the bias variance trade-off:\n\nLow flexibility generally means low variance, but large bias\nHigh flexibility generally means high variance, but low bias\n\nRoughly:\n\nLDA tends to be better than QDA if there are relatively few training observations (small \\(n_{Train}\\)) and so reducing variance is crucial.\nQDA is recommended\n\nif the training set \\(n_{Train}\\) is large, so that the variance of the classifier is not a major concern, or\nif the assumption of a common covariance matrix for the \\(K\\) classes is clearly unrealistic.\n\n\n\n\n\n(Ch. 4.4.4) Naive Bayes\nRemember, that Bayes’ theorem states that the probability that a observation with predictor values \\(X=x\\) comes from class \\(k\\) (i.e. the posterior probability), is given by \\[\np_k(x) = Pr(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_l f_l(x)}.\n\\]\nWe have seen above, that estimating the prior probabilities \\(\\pi_1,\\dots,\\pi_K\\) is typically straight forward using the relative frequencies \\[\n\\hat\\pi_k=\\frac{n_k}{n},\\quad k=1,\\dots,K.\n\\] However, estimating the \\(p\\) dimensional density functions \\[\nf_1(x),\\dots,f_K(x)\n\\] was and is substantially more difficult.\nTo make estimation tractable and useful, LDA and QDA assume parametric families for these density functions; namely, multivariate normals with and without common covariances.\nBy contrast, Naive Bayes only makes the assumption that the \\(p\\) dimensional components of the predictor are independent from each other. I.e. that the joint density \\(f_k(x)\\) can be written as the product of the \\(p\\) marginal densities  \\[\nf_k(x) = f_{k1}(x_1)\\times f_{k2}(x_2)\\times \\dots \\times f_{kp}(x_p)\n\\]\nThis simplifying assumption may not be completely correct, but it often leads to very good classification results - particularly, in classification problems with smallish sample sizes.\nUnder the naive Bayes assumption, we have that \\[\nPr(Y=k|X=x)=\\frac{\\pi_k f_{k1}(x_1)\\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K\\pi_l f_{l1}(x_1)\\times \\dots \\times f_{lp}(x_p)}.\n\\]\nFor estimating the one dimensional marginal density functions \\[\nf_{kj}(x),\\quad j=1,\\dots,p\\quad\\text{and}\\quad k=1,\\dots,K\n\\] we can use …\n\n(univariate) normal distribution assumptions, i.e. \\(X_j|Y=k \\sim N(\\mu_{jk},\\sigma_{jk})\\)\nnon-parametric kernel density estimation (for quantitative predictors)\nsimple relative frequencies (for qualitative predictors)"
  },
  {
    "objectID": "Ch3_Classification.html#r-lab-classification",
    "href": "Ch3_Classification.html#r-lab-classification",
    "title": "3  Classification",
    "section": "3.1 R-Lab: Classification",
    "text": "3.1 R-Lab: Classification\n\n3.1.1 The Stock Market Data\nWe will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over \\(1,250\\) days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days: Lag1, Lag2, … Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date). Our goal is to predict Direction (a qualitative response) using the other predictors/features.\n\nlibrary(ISLR2)   # package contains the data\nattach(Smarket)  # attach(Smarket) allows to use the variables \n                 # contained Smarket directly \n                 # (i.e. 'Direction' instead of 'Smarket$Direction')\nnames(Smarket)   # names of the variables in the Smarket data\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)     # total sample size n, number of variables\n\n[1] 1250    9\n\nsummary(Smarket) # descriptive statistics (mean, median, ...)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\npairs(Smarket)   # pairwise scatter plots\n\n\n\n\nThe cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the Direction variable is qualitative (Up and Down).\n\ncor(Smarket)\n\nError in cor(Smarket): 'x' must be numeric\n\nround(cor(Smarket[, names(Smarket) != \"Direction\"]), 2)\n\n       Year  Lag1  Lag2  Lag3  Lag4  Lag5 Volume Today\nYear   1.00  0.03  0.03  0.03  0.04  0.03   0.54  0.03\nLag1   0.03  1.00 -0.03 -0.01  0.00 -0.01   0.04 -0.03\nLag2   0.03 -0.03  1.00 -0.03 -0.01  0.00  -0.04 -0.01\nLag3   0.03 -0.01 -0.03  1.00 -0.02 -0.02  -0.04  0.00\nLag4   0.04  0.00 -0.01 -0.02  1.00 -0.03  -0.05 -0.01\nLag5   0.03 -0.01  0.00 -0.02 -0.03  1.00  -0.02 -0.03\nVolume 0.54  0.04 -0.04 -0.04 -0.05 -0.02   1.00  0.01\nToday  0.03 -0.03 -0.01  0.00 -0.01 -0.03   0.01  1.00\n\n\nAs one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and Volume.\n\n\n\n3.1.2 Logistic Regression\nNext, we will fit a logistic regression model in order to predict Direction using Lag1 through Lag5 and Volume. The glm() function can be used to fit many types of generalized linear models , including logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\n\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.446  -1.203   1.065   1.145   1.326  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe smallest \\(p\\)-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of \\(0.145\\), the \\(p\\)-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction.\nWe use the coef() function in order to access just the coefficients for this fitted model. We can also use the summary() function to access particular aspects of the fitted model, such as the \\(p\\)-values for the coefficients.\n\ncoef(glm.fits)\n\n (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 \n-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 \n      Volume \n 0.135440659 \n\nsummary(glm.fits)$coef\n\n                Estimate Std. Error    z value  Pr(>|z|)\n(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\nLag1        -0.073073746 0.05016739 -1.4565986 0.1452272\nLag2        -0.042301344 0.05008605 -0.8445733 0.3983491\nLag3         0.011085108 0.04993854  0.2219750 0.8243333\nLag4         0.009358938 0.04997413  0.1872757 0.8514445\nLag5         0.010313068 0.04951146  0.2082966 0.8349974\nVolume       0.135440659 0.15835970  0.8552723 0.3924004\n\nsummary(glm.fits)$coef[, 4]\n\n(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 \n  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 \n     Volume \n  0.3924004 \n\n\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form \\(P(Y=1|X)\\), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities.\n\n## Posterior probabilities\nglm.probs <- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\n\nWe know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\n\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than \\(0.5\\).\n\nglm.pred                 <- rep(\"Down\", 1250) # sample size: 1250\nglm.pred[glm.probs > .5] <- \"Up\"\n\nThe first command creates a vector of 1,250 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds \\(0.5\\). Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified. \n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on \\(507\\) days and that it would go down on \\(145\\) days, for a total of \\(507+145 = 652\\) correct predictions. The overall training error rate is\n\n## Overall training error rate \n(507 + 145) / 1250\n\n[1] 0.5216\n\n\nAlternatively, the mean() function can be used to compute the overall training error rate; i.e., the fraction of days for which the prediction was correct (within the training data). In this case, logistic regression correctly predicted the movement of the market \\(52.2\\%\\) of the time.\n\n## Alternative way to compute the \n## overall training error rate \nmean(glm.pred == Direction)\n\n[1] 0.5216\n\n\nAt first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of \\(1,250\\) observations. In other words, \\(100\\%-52.2\\%=47.8\\%\\), is the training error rate.\nThe training error rate is often overly optimistic. It tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data.\nThis will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.\nTo implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.\n\ntrain          <- (Year < 2005)\n## Test data \nSmarket_2005   <- Smarket[!train, ] \n## Test sample size \ndim(Smarket_2005)[1] \n\n[1] 252\n\n## Dependent variable of the test data \nDirection_2005 <- Direction[!train]\n\nThe object train is a vector of \\(1250\\) elements, corresponding to the observations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to TRUE, whereas those that correspond to observations in 2005 are set to FALSE.\nThe object train is a Boolean vector, since its elements are TRUE and FALSE. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command Smarket[train, ] would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of train are TRUE.\nThe ! symbol can be used to reverse all of the elements of a Boolean vector. That is, !train is a vector similar to train, except that the elements that are TRUE in train get swapped to FALSE in !train, and the elements that are FALSE in train get swapped to TRUE in !train. Therefore, Smarket[!train, ] yields a submatrix of the stock market data containing only the observations for which train is FALSE—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.\n\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\n## Using the trained model to predict the held out test data  \nglm.probs <- predict(glm.fits, Smarket_2005,\n    type = \"response\")\n\nNotice that we have trained and tested our model on two completely separate data sets:\n\ntraining (estimation) was performed using only the dates before 2005\ntesting was performed using only the dates in 2005.\n\nFinally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\nglm.pred                 <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\n## Test data confusion matrix \ntable(glm.pred, Direction_2005)\n\n        Direction_2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n## Rate of correct predictions in the test data\nmean(glm.pred == Direction_2005)\n\n[1] 0.4801587\n\n## Rate of prediction errors in the test data \nmean(glm.pred != Direction_2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is \\(52\\%\\), which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)\nWe recall that the logistic regression model had very underwhelming \\(p\\)-values associated with all of the predictors, and that the smallest \\(p\\)-value, though not very small, corresponded to Lag1. Perhaps by removing the variables that appear not to be helpful in predicting Direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. Below we have refit the logistic regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model.\n\n## Train model using the training data\nglm.fits  <- glm(Direction ~ Lag1 + Lag2, \n                 family = binomial, \n                 data   = Smarket, \n                 subset = train)\n## Predict the test data                  \nglm.probs <- predict(glm.fits, Smarket_2005, type = \"response\")\n##\nglm.pred                 <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\n## Test data confusion matrix\ntable(glm.pred, Direction_2005)\n\n        Direction_2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\n## Test data rate of correct predictions\nmean(glm.pred == Direction_2005)\n\n[1] 0.5595238\n\n## Test data positive predictive value TP/P*\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nNow the results appear to be a little better: \\(56\\%\\) of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct \\(56\\%\\) of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a \\(58\\%\\) accuracy rate (Positive predictive value TP/P*).\nThis suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction on a day when Lag1 and Lag2 equal \\(1.2\\) and \\(1.1\\), respectively, and on a day when they equal \\(1.5\\) and \\(-0.8.\\) We do this using the predict() function.\n\npredict(glm.fits,\n        newdata = data.frame(Lag1 = c(1.2,  1.5),  \n                         Lag2 = c(1.1, -0.8)),\n        type    = \"response\")\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\n3.1.3 Linear Discriminant Analysis\nNow we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. We fit the model using only the observations before 2005.\n\nlibrary(MASS)\nlda.fit <- lda(Direction ~ Lag1 + Lag2, \n               data   = Smarket,\n               subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit)\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\%\\) of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The “coefficients of linear discriminants” output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in Equation 3.3. If \\(-0.642\\times\\)Lag1\\(- 0.514 \\times\\) Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe plot() function produces plots of the linear discriminants, obtained by computing \\(-0.642\\times\\)Lag1\\(- 0.514 \\times\\) Lag2 for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns a list with three elements. The first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15). Finally, x contains the linear discriminants, described earlier.\n\nlda.pred <- predict(lda.fit, Smarket_2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nThe LDA and logistic regression predictions are almost identical. This observation holds true also general since LDA and logistic regression are effectively very similar classifiers; see Section 4.5 in our textbook ISLR2.\n\nlda.class <- lda.pred$class\ntable(lda.class, Direction_2005)\n\n         Direction_2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction_2005)\n\n[1] 0.5595238\n\n\nApplying a \\(50\\%\\) threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.\n\nsum(lda.pred$posterior[, 1] >= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] < .5)\n\n[1] 182\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will decrease:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\%\\) in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability for a decrease is at least \\(90\\%.\\)\n\nsum(lda.pred$posterior[, 1] > .9)\n\n[1] 0\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\%.\\)\n\n\n3.1.4 Quadratic Discriminant Analysis\nWe will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. The syntax is identical to that of lda().\n\nqda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\nqda.class <- predict(qda.fit, Smarket_2005)$class\ntable(qda.class, Direction_2005)\n\n         Direction_2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction_2005)\n\n[1] 0.5992063\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\%\\) of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!\n\n\n3.1.5 Naive Bayes\nNext, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a univariate Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\n\n## install.packages(\"e1071\") \nlibrary(\"e1071\")\nnb_fit <- naiveBayes(Direction ~ Lag1 + Lag2, \n                     data   = Smarket,\n                     subset = train)\nnb_fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for Lag1 is \\(0.0428\\) for Direction=Down, and the standard deviation is \\(1.23\\). We can easily verify this:\n\nmean(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nThe predict() function is straightforward to use:\n\nnb.class <- predict(nb_fit, Smarket_2005)\n## Test data confusion matrix\ntable(nb.class, Direction_2005)\n\n        Direction_2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\n## Test data overall rate of correct classifications\nmean(nb.class == Direction_2005)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class.\n\nnb.preds <- predict(nb_fit, Smarket_2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110\n\n\n\n\n3.1.6 \\(K\\)-Nearest Neighbors\nWe will now perform KNN using the knn() function, which is part of the class library. This function works rather differently from the other model-fitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, knn() forms predictions using a single command. The function requires four inputs.\n\nA matrix containing the predictors associated with the training data, labeled train.X below.\nA matrix containing the predictors associated with the data for which we wish to make predictions, labeled test.X below.\nA vector containing the class labels for the training observations, labeled train.Direction below.\nA value for \\(K\\), the number of nearest neighbors to be used by the classifier.\n\nWe use the cbind() function, short for column bind, to bind the Lag1 and Lag2 variables together into two matrices, one for the training set and the other for the test set.\n\n## install.packages(\"class\")\nlibrary(\"class\")\ntrain.X         <- cbind(Lag1, Lag2)[train, ]\ntest.X          <- cbind(Lag1, Lag2)[!train, ]\ntrain.Direction <- Direction[train]\n\nNow the knn() function can be used to predict the market’s movement for the dates in 2005. We set a random seed before we apply knn() because if several observations are tied as nearest neighbors, then R will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.\n\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Direction, k = 1)\n## Test data confusion matrix\ntable(knn.pred, Direction_2005)\n\n        Direction_2005\nknn.pred Down Up\n    Down   43 58\n    Up     68 83\n\n## Test data overall rate of correct classifications\nmean(knn.pred == Direction_2005)\n\n[1] 0.5\n\n\nThe results using \\(K=1\\) are not very good, since only \\(50\\%\\) of the observations are correctly predicted. Of course, it may be that \\(K=1\\) results in an overly flexible fit to the data. Below, we repeat the analysis using \\(K=3\\).\n\nknn.pred <- knn(train.X, test.X, train.Direction, k = 3)\n## Test data confusion matrix\ntable(knn.pred, Direction_2005)\n\n        Direction_2005\nknn.pred Down Up\n    Down   48 54\n    Up     63 87\n\n## Test data overall rate of correct classifications\nmean(knn.pred == Direction_2005)\n\n[1] 0.5357143\n\n\nThe results have improved slightly. But increasing \\(K\\) further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.\nKNN does not perform well on the Smarket data, but generally, KNN does often provide impressive results. As an example we will apply the KNN approach to the Insurance data set, which is part of the ISLR2 library. This data set includes \\(85\\) predictors that measure demographic characteristics for \\(5822\\) individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only \\(6\\%\\) of people purchased caravan insurance.\n\ndim(Caravan)     # sample size and number of variables in the Caravan dataset \n\n[1] 5822   86\n\nattach(Caravan)  # makes the variables in Caravan (e.g. Purchase) directly usable\n\nsummary(Purchase)\n\n  No  Yes \n5474  348 \n\n## Empirical prior probability of 'Purchase = Yes' \n348 / 5822\n\n[1] 0.05977327\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of \\(1000\\) in salary is enormous compared to a difference of \\(50\\) years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of \\(1000\\) is quite small compared to an age difference of \\(50\\) years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column \\(86\\), because that is the qualitative Purchase variable.\n\nstandardized.X <- scale(Caravan[, -86])\nvar(Caravan[, 1])\n\n[1] 165.0378\n\nvar(Caravan[, 2])\n\n[1] 0.1647078\n\nvar(standardized.X[, 1]) \n\n[1] 1\n\nvar(standardized.X[, 2])\n\n[1] 1\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first \\(1000\\) observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using \\(K=1\\), and evaluate its performance on the test data.\n\ntest     <- 1:1000\n## Training data \ntrain.X  <- standardized.X[-test, ]\ntrain.Y  <- Purchase[-test]\n## Testing data \ntest.X   <- standardized.X[test, ]\ntest.Y   <- Purchase[test]\n\nset.seed(1)\n## KNN (K=1)\nknn.pred <- knn(train.X, test.X, train.Y, k = 1)\n## Test data overall classification error \nmean(test.Y != knn.pred)\n\n[1] 0.118\n\n## Test data overall classification error \n## of the no information classifier (\"always No\")\nmean(test.Y != \"No\")\n\n[1] 0.059\n\n\nThe vector test is numeric, with values from \\(1\\) through \\(1000\\). Typing standardized.X[test, ] yields the submatrix of the data containing the observations whose indices range from \\(1\\) to \\(1000\\), whereas typing standardized.X[-test, ] yields the submatrix containing the observations whose indices do not range from \\(1\\) to \\(1,000\\).\nThe KNN overall test error rate on the \\(1000\\) test observations is just under \\(12\\%.\\) At first glance, this may appear to be fairly good. However, since only \\(6\\%\\) of customers purchased insurance, we could get the error rate down to \\(6\\%\\) by always predicting No regardless of the values of the predictors!\nSuppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, perhaps a salesperson must visit each potential customer.\nIf the company tries to sell insurance to a random selection of customers, then the success rate will be only \\(6\\%\\) (the empirical prior probability of Purchase = Yes), which may be far too low given the costs involved. Instead, the company would like to try to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest. Instead, the fraction of individuals that are correctly predicted to buy insurance is of interest.\nIt turns out that KNN with \\(K=1\\) does far better than random guessing among the customers that are predicted to buy insurance. Among the P* \\(=77\\) Purchase=Yes predictions, \\(9\\) (TP), or \\(11.7\\%\\) (TP/P*), actually do purchase the insurance. This is double the rate that one would obtain from random guessing.\n\n## Test data confusion matrix (KNN, K=1)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n## Test data positive predictive value TP/P*\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nUsing \\(K=3\\), the success rate increases to \\(19\\%,\\) and with \\(K=5\\) the rate is \\(26.7\\%.\\) This is over four times the rate that results from random guessing. It appears that KNN is finding some real patterns in a difficult data set!\n\n## K = 3\nknn.pred <- knn(train.X, test.X, train.Y, k = 3)\n\n## Test data confusion matrix (KNN, K=3)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  920  54\n     Yes  21   5\n\n## Test data positive predictive value TP/P*\n5 / 26\n\n[1] 0.1923077\n\n## K = 5\nknn.pred <- knn(train.X, test.X, train.Y, k = 5)\n\n## Test data confusion matrix (KNN, K=5)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  930  55\n     Yes  11   4\n\n## Test data positive predictive value TP/P*\n4 / 15\n\n[1] 0.2666667\n\n\nHowever, while this strategy is cost-effective, it is worth noting that only 15 customers are predicted to purchase insurance using KNN with \\(K=5\\). In practice, the insurance company may wish to expend resources on convincing more than just 15 potential customers to buy insurance.\nAs a comparison, we can also fit a logistic regression model to the data. If we use \\(0.5\\) as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of \\(0.5\\). If we instead predict a purchase any time the predicted probability of purchase exceeds \\(0.25\\), we get much better results: we predict that 33 people will purchase insurance, and we are correct for about \\(33\\%\\) of these people. This is over five times better than random guessing!\n\nglm.fits  <- glm(Purchase ~ ., data = Caravan,\n                 family = binomial, \n                 subset = -test)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm.probs <- predict(glm.fits, Caravan[test, ], type = \"response\")\n\n## Classifications (Bayes threshold)\nglm.pred                 <- rep(\"No\", 1000)\nglm.pred[glm.probs > .5] <- \"Yes\"\n\n## confusion matrix (test set)\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  934  59\n     Yes   7   0\n\n## Classifications (adjusted threshold)\nglm.pred                  <- rep(\"No\", 1000)\nglm.pred[glm.probs > .25] <- \"Yes\"\n\n## confusion matrix (test set)\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  919  48\n     Yes  22  11\n\n11 / (22 + 11)\n\n[1] 0.3333333"
  },
  {
    "objectID": "Ch3_Classification.html#exercises",
    "href": "Ch3_Classification.html#exercises",
    "title": "3  Classification",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\nPrepare the following exercises of Chapter 4 in our course textbook ISLR:\n\nExercise 1\nExercise 6\nExercise 13\nExercise 15\nExercise 16"
  },
  {
    "objectID": "Ch4_ResamplingMethods.html",
    "href": "Ch4_ResamplingMethods.html",
    "title": "4  Resampling Methods",
    "section": "",
    "text": "Reading: Chapter 5 of our course textbook An Introduction to Statistical Learning\nResampling methods involve repeatedly drawing samples from a training data set and refitting a model of interest on each of these samples. The different estimation results across resamples can be used, for instance, to estimate the variability of a linear regression fit.\nIn the following, we consider the resampling methods:"
  },
  {
    "objectID": "Ch4_ResamplingMethods.html#ch.-5.1-cross-validation",
    "href": "Ch4_ResamplingMethods.html#ch.-5.1-cross-validation",
    "title": "4  Resampling Methods",
    "section": "(Ch. 5.1) Cross-Validation",
    "text": "(Ch. 5.1) Cross-Validation\nIn this section, we consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.\n\n(Ch. 5.1.1) Validation Set Approach\nThe validation set approach randomly divides the available set of observations into two parts:\n\na training set and\na validation set (or hold-out set)\n\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\n\n\nIllustration\nReconsider the Auto data set. In Chapter 3, we found that a model that predicts mpg using horsepower and horsepower\\(^2\\) predicts better than a model that uses only the linear term. But maybe a cubic or a higher order polynomial regression model predicts even better? The validation set approach can be used to select the degree \\(p\\) of the polynomial regression model \\[\n\\texttt{mpg}=\\beta_0 + \\sum_{j=1}^p\\beta_j \\texttt{horsepower}^j + \\epsilon.\n\\]\nStep 1: Randomly split the total data set into mutually exclusive training and test (validation) sets of roughly equal subsample sizes:\n\nTraining set: \\(\\{(x_i,y_i), i\\in\\mathcal{I}_{Train}\\},\\) where \\(n_{Train}=|\\mathcal{I}_{Train}|<n\\)\nTest set: \\(\\{(x_i,y_i), i\\in\\mathcal{I}_{Test}\\},\\) where \\(n_{Test}=|\\mathcal{I}_{Test}|<n\\)\n\nsuch that \\(n_{Train}\\approx n_{Test}\\) with \\(n=n_{Train} + n_{Test}\\) and \\[\n\\mathcal{I}_{Train}\\cap \\mathcal{I}_{Test}=\\emptyset.\n\\] Code for splitting data randomly into training and validation sets:\n\nlibrary(\"ISLR2\")\ndata(\"Auto\")\n\nn        <- nrow(Auto)    # Sample size\nn_Train  <- 200           # Sample size of training set \nn_Valid  <- n - n_Train   # Sample size of test/validation set \n\nset.seed(1234) \n\n## Index-Sets for selecting the training and validation sets\nI_Train  <- sample(x = 1:n, size = n_Train, replace = FALSE)\nI_Valid  <- c(1:n)[-I_Train]\n\n## Trainingsdaten \nAuto_Train_df <- Auto[I_Train, ]\n## Validierungsdaten \nAuto_Valid_df <- Auto[I_Valid, ]\n\nStep 2: Estimation of the polynomial regression model, e.g., for \\(p=2\\) using the training set:\n\np            <- 2\nTrain_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw=TRUE), \n                   data = Auto_Train_df)\n\nStep 3: Validation of the polynomial regression model by computing the test mean squared (prediction) error using the validation set: \\[\n\\operatorname{MSE}_{Test}^{ValidationSetApproach}=\\frac{1}{n_{Test}}\\sum_{i\\in\\mathcal{I}_{Test}}(y_i - \\hat{y}_i)^2,\n\\] where \\(\\hat{f}\\) in \\(\\hat{y}_i=\\hat{f}(x_i)\\) is computed from the training data, but evaluated at the test data \\(x_i,\\) \\(i\\in\\mathcal{I}_{Test}.\\)\n\ny_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\nRSS_Valid     <- sum((Auto_Valid_df$mpg - y_fit_Valid)^2)\nMSE           <- RSS_Valid / n_Valid\n\nRepeating Steps 1-3 for a series of polynomial degrees \\(p=1,\\dots,10\\) allows us to search for the polynomial degree with lowest test MSE.\n\np_max         <- 10\nMSE           <- numeric(p_max)\nfor(p in 1:p_max){\n  ## Step 1\n  Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw=TRUE), \n                     data = Auto_Train_df)\n  ## Step 2\n  y_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\n  ## Step 3\n  RSS_Valid     <- sum( (Auto_Valid_df$mpg - y_fit_Valid)^2 )\n  MSE[p]        <- RSS_Valid / n_Valid\n}\n\nplot(x = 1:p_max, y = MSE, type = \"b\", \n     col = \"black\", bg = \"black\", pch = 21,  \n     xlab = \"Degree of Polynomial\", ylab = \"MSE\")\npoints(y = MSE[which.min(MSE)], \n       x = c(1:p_max)[which.min(MSE)], \n       col = \"red\", bg = \"red\", pch = 21)     \n\n\n\n\nFigure 4.1: Validation error estimates for a single split into training and validation data sets. This result suggests that \\(p=9\\) minimizes the test MSE; however, the test MSE values for polynomial degrees from \\(p=2\\) to \\(p=10\\) are all of comparable order of magnitude.\n\n\n\n\nFigure 4.1 shows the test MSE values based on one random split of the dataset. The result that \\(p=9\\) minimizes the test MSE, however, may depend on the random split. Different random splits may lead to different model selection (choices of \\(p\\)).\nThe following code repeats the above computations for multiple random splits of the dataset into training and validation sets:\n\n## R = 10 random splits\nR        <- 10\n## Container for the MSE results\nMSE      <- matrix(NA, R, p_max)\n\nfor(r in 1:R){\n  ## Index sets for training and validation sets\n  I_Train  <- sample(x = 1:n, size = n_Train, replace = FALSE)\n  I_Valid  <- c(1:n)[-I_Train]\n\n  ## Training set \n  Auto_Train_df <- Auto[I_Train, ]\n  ## Validation set\n  Auto_Valid_df <- Auto[I_Valid, ]\n\n  for(p in 1:p_max){\n    ## Step 1\n    Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                       data = Auto_Train_df)\n    ## Step 2\n    y_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\n    ## Step 3\n    RSS_Valid     <- sum( (Auto_Valid_df$mpg - y_fit_Valid)^2 )\n    MSE[r,p]      <- RSS_Valid / n_Valid\n  }\n}\n\nmatplot(y = t(MSE), type=\"b\", ylab=\"MSE\", xlab=\"Degree of Polynomial\", \n        pch=21, col=\"black\", bg=\"black\", lty = 1, main=\"\")\nfor(r in 1:R){\n  points(y = MSE[r,][which.min(MSE[r,])], \n       x = c(1:p_max)[which.min(MSE[r,])], \n       col = \"red\", bg = \"red\", pch = 21)\n}\n\n\n\n\nFigure 4.2: Validation error estimates for ten different random splits into training and validation data sets. The polynomial degrees that minimize the test MSE strongly vary across the different random splits.\n\n\n\n\nFigure 4.2 shows that the validation set approach can be highly variable. The selected polynomial degrees (minimal test MSE) strongly varies across the different random splits and thus depend on the data included in the test and validation sets.\nA further serious problem with the validation set approach is that the evaluated predictions \\(\\hat{y}_i=\\hat{f}(x_i)\\) are based on estimates \\(\\hat{f}\\) computed from the training set, where, however, the training set sample size \\(n_{Train}\\) is typically substantially smaller than the actual sample size \\(n.\\) This leads to increased (i.e. biased) test MSE values which do not reflect the actual test MSE values for the total sample size \\(n.\\)\nLeave-One-Out and \\(k\\)-fold Cross-validation are refinements of the validation set approach that addresses these issues.\n\n\n\n(Ch. 5.1.2) Leave-One-Out Cross-Validation (LOOCV)\nLike the validation set approach, LOOCV involves splitting the set of validation observations into two parts.\nHowever, instead of creating two subsets of comparable size, a single observation is used for the validation set, and the remaining observations are used for the training set. , i.e.\n\nTraining set: \\(\\{(x_1,y_1),\\dots,(x_{i-1},y_{i-1}),(x_{i+1},y_{i+1}),\\dots,(x_{n},y_{n})\\}\\) with \\(n_{Train}=n-1\\)\nTest set: \\(\\{(x_i,y_i)\\}\\) with \\(n_{Test}=1\\)\n\nThe \\(i\\)th estimate for the test MSE is thus \\[\n\\operatorname{MSE}_i = \\left(y_i - \\hat{y}_i\\right)^2,\n\\] which is an (approximately) unbiased estimate for the test MSE, although a poor estimate with a high variance as it is based on only one observation in the test set.\nRepeating this leave-one-out splitting approach for each \\(i=1,\\dots,n,\\) produces \\(n\\) many estimates of the test MSE: \\[\n\\operatorname{MSE}_1, \\operatorname{MSE}_2,\\dots,\\operatorname{MSE}_n\n\\]\nThe LOOCV estimate is then formed by the average of the \\(n\\) MSE estimates: \\[\n\\operatorname{LOOCV}=\\operatorname{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n\\operatorname{MSE}_i.\n\\tag{4.1}\\]\nFigure 5.3 shows schematically the leave-one-out data splitting approach.\n\nAdvantages of CV over the Validation Set approach:\n\nLower bias. Since the test MSE estimates are based on training sets with sample sizes \\(n_{Train}=n-1 \\approx n,\\), LOOCV does not overestimate the test error rate as much the validation set approach does.\nPerforming LOOCV multiple times, always yields the same result. I.e., there is no randomness due to the training/validation set splits as seen for the validation set approach.\n\nCodes to implement the LOOCV approach for the Auto data example:\n\nMSE_i      <- matrix(NA, n, p_max)\n\n## Save starting time of the loop\nstart_time <- Sys.time()\n\nfor(r in 1:n){\n  ## Training set \n  Auto_Train_df <- Auto[-r, ]\n  ## Validation set\n  Auto_Valid_df <- Auto[r, ]\n\n  for(p in 1:p_max){\n    ## Step 1\n    Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                       data = Auto_Train_df)\n    ## Step 2\n    y_fit_Valid   <- predict(Train_polreg, newdata = Auto_Valid_df)\n    ## Step 3\n    MSE_i[r,p]    <- (Auto_Valid_df$mpg - y_fit_Valid)^2  \n  }\n}\n## Save end time of the loop\nend_time <- Sys.time()\n\nLOOCV  <- colMeans(MSE_i)\n\nplot(x = 1:p_max, y = LOOCV, type = \"b\", \n     col = \"black\", bg = \"black\", pch = 21,  \n     xlab = \"Degree of Polynomial\", ylab = \"LOOCV\")\npoints(y = LOOCV[which.min(LOOCV)], \n       x = c(1:p_max)[which.min(LOOCV)], \n       col = \"red\", bg = \"red\", pch = 21)     \n\n\n\n\nFigure 4.3: LOOCV error estimates for different polynomial degrees \\(p.\\)\n\n\n\n\nLOOCV has the potential to be computationally expensive, since the model has to be fit \\(n\\) times. Indeed the above code, which represents a rather simple implementation of LOOCV for least squares fits of linear/polynomial regression models, takes\n\n\nend_time\\(-\\)start_time \\(=\\) 5.137 seconds\n\nfor the computations which is quite long.\nLuckily, for least squares fits of linear/polynomial regression models one can use the following short-cut formula \\[\n\\operatorname{LOOCV}=\\operatorname{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n\\left(\\frac{y_i - \\hat{y}_i}{1-h_i}\\right)^2,\n\\tag{4.2}\\] where\n\n\\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the original least squares fit, based on the total sample size \\(n,\\) and\n\\(h_i\\) is the leverage statistic for the \\(i\\)th observation, i.e.  \\[\nh_i=\\left[X(X'X)^{-1}X'\\right]_{ii}.\n\\]\n\nThe following codes implement this fast LOOCV version:\n\nLOOCV_fast <- numeric(p_max)\n\n## Save starting time \nstart_time2 <- Sys.time()\n\nfor(p in 1:p_max){\n  PolyReg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                           data = Auto)\n  h             <- lm.influence(PolyReg)$hat\n  LOOCV_fast[p] <- mean(((Auto$mpg - fitted.values(PolyReg))/(1 - h))^2)\n}\n## Save end time of the loop\nend_time2 <- Sys.time()\n\nIndeed, both approaches yield the same LOOCV values\n\n## Minimal absolute difference between \n## the naive and the fast implementation: \nround(max(abs(LOOCV - LOOCV_fast)), 3)\n\n[1] 0\n\n\nHowever, the fast version takes only\n\nend_time2\\(-\\)start_time2 \\(=\\) 0.027 seconds\n\nfor the computations.\nLOOCV is a very general method, and can be used with any kind of predictive modeling; e.g.\n\nLogistic regression\nLinear discriminant analysis\nQuadratic discriminant analysis\netc.\n\nand any statistical prediction method discussed in the lecture or in our textbook ISLR2.\nCaution: The fast LOOCV Equation 4.2 does not hold in general, but only for least squares fits of linear regression models, which includes, for instance, polynomial regressions, but, for instance, not logistic regression models."
  },
  {
    "objectID": "Ch4_ResamplingMethods.html#ch.-5.1.3-k-fold-cross-validation",
    "href": "Ch4_ResamplingMethods.html#ch.-5.1.3-k-fold-cross-validation",
    "title": "4  Resampling Methods",
    "section": "(Ch. 5.1.3) \\(k\\)-Fold Cross-Validation",
    "text": "(Ch. 5.1.3) \\(k\\)-Fold Cross-Validation\nAn alternative to LOOCV is \\(k\\)-fold CV.\nThis approach divides the total index set \\(\\mathcal{I}=\\{1,2,\\dots,n\\}\\) of the original data data set into \\(k\\) mutually exclusive subsets (folds) of roughly equal sizes \\[\n\\mathcal{I}_1,\\,\\mathcal{I}_2,\\dots,\\mathcal{I}_k\n\\] with \\(|\\mathcal{I}_1|\\approx |\\mathcal{I}_k|\\approx n/k.\\)\nThese \\(k\\) index sets allow us construct different training and test sets for each \\(j=1,2,\\dots,k\\)\n\nTraining set: \\(\\{(x_i,y_i),\\; i\\in\\mathcal{I}\\setminus \\mathcal{I}_j\\}\\) with sample size of \\(n_{Train}\\approx n - n/k\\)\nTest set: \\(\\{(x_i,y_i),\\;i\\in\\mathcal{I}_j\\}\\) with sample size of \\(n_{Test}\\approx n/k\\)\n\nEach pair of training and test set allow to compute a estimate of the test error \\[\n\\operatorname{MSE}_1, \\operatorname{MSE}_2,\\dots,\\operatorname{MSE}_k.\n\\] The \\(k\\)-fold CV estimate is computed by averaging these values \\[\n\\operatorname{CV}_{(k)}=\\frac{1}{k}\\sum_{j=1}^k\\operatorname{MSE}_j\n\\tag{4.3}\\]\nFigure 5.5 illustrates the data splitting for \\(k\\)-fold CV.\n\n\nLOOCV is a special case of \\(k\\)-fold CV with \\(k=n\\).\nMost often used \\(k\\)-values in practice are \\(k=5\\) or \\(k=10\\).\n\nWhy \\(k=5\\) or \\(k=10\\) instead of \\(k=n\\)?\n\nFaster computation times (\\(k=5\\) instead of \\(k=n\\) model fits)\nImproved estimates of the test MSE (see next section)\n\nThe following codes illustrate \\(k\\)-fold CV:\n\nset.seed(123)\n\n## number of folds for k-fold CV \nk              <- 5\n\n## container for storing the MSE results\nMSE_folds      <- matrix(NA, k, p_max)\n\n## selector for the folds \nfolds          <- sample(rep(1:k, length = n))\n\n## Save starting time of the loop\nstart_time     <- Sys.time()\n\nfor(j in 1:k){\n  ## Training set \n  Auto_Train_df <- Auto[folds != j, ]\n  ## Validation set\n  Auto_Valid_df <- Auto[folds == j, ]\n\n  for(p in 1:p_max){\n    ## Step 1\n    Train_polreg <- lm(mpg ~ poly(horsepower, degree = p, raw = TRUE), \n                       data = Auto_Train_df)\n    ## Step 2\n    y_fit_Valid    <- predict(Train_polreg, newdata = Auto_Valid_df)\n    ## Step 3\n    MSE_folds[j,p] <- mean((Auto_Valid_df$mpg - y_fit_Valid)^2)\n  }\n}\n## Save end time of the loop\nend_time  <- Sys.time()\n\nCV_kfold  <- colMeans(MSE_folds)\n\nplot(x = 1:p_max, y = CV_kfold, type = \"b\", \n     col = \"black\", bg = \"black\", pch = 21,  main=\"k-fold CV\", \n     xlab = \"Degree of Polynomial\", ylab = expression(\"CV\"[k]))\npoints(y = CV_kfold[which.min(CV_kfold)], \n       x = c(1:p_max)[which.min(CV_kfold)], \n       col = \"red\", bg = \"red\", pch = 21)     \n\n\n\n\n\n(Ch. 5.1.4) Bias-Variance Trade-Off for \\(k\\)-Fold Cross-Validation\nThere is a bias-variance trade-off associated with the choice of \\(k\\) in \\(k\\)-fold CV.\n\nBias:\n\nSmall \\(k\\) lead to test MSE estimates with large bias\nLarge \\(k\\) lead to test MSE estimates with small bias\n\nExplanation:\n\nA small \\(k\\) leads to trainings sets with samples sizes \\(n_{Train} \\ll n\\) substantially smaller than the actual sample size \\(n.\\) Thus, we estimate the MSE for a sample size that is substantially smaller than the sample size \\(n\\) we are actually interested in. This leads to systematic overestimations of the actual test MSE for sample size \\(n.\\)\nA large \\(k\\) reduces this bias since \\(n_{Train}\\approx n.\\) Thus we estimate essentially the actual test MSE for sample size \\(n.\\)\n\nVariance:\n\nSmall \\(k\\) lead to test MSE estimates with small variance\nLarge \\(k\\) lead to test MSE estimates with large variance\n\nExplanation:\n\nIn \\(k\\)-fold CV, the training sets overlap pairwise by roughly \\(((k-2)/k)\\times 100 \\%\\).\n\nFor \\(k=2\\) there is no overlap.\nFor \\(k=5\\) (\\(k\\)-fold CV) approximately \\((k-2)/k=(3/5)=60\\%\\) of the training data points are equal in each pair of training sets.\nFor \\(k=n\\) (LOOCV) approximately \\((n-2)/n=98\\%\\) of the training data points are equal in each pair of trainings sets.\n\n\nThus, the larger \\(k\\) the more similar the training data sets become. However, very similar training sets lead to highly correlated test MSE estimates. Since the mean of highly correlated quantities has higher variance than does the mean of quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from \\(k\\)-fold CV with \\(k<n.\\)\n\\(k\\)-fold CV with \\(k=5\\) or \\(k=10\\) is often considered a good compromise balancing these bias and variance issues.\n\n\n(Ch. 5.1.5) Cross-Validation on Classification Problems\nCross-validation can also be a very useful approach in the classification setting when \\(Y\\) is qualitative.\nIn the classification setting, the LOOCV error rate takes the form \\[\n\\operatorname{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^n\\operatorname{Err}_i,\n\\] where \\[\n\\operatorname{Err}_i=I(y_i\\neq \\hat{y}_i)\n\\] with \\(I(\\texttt{TRUE})=1\\) and \\(I(\\texttt{FALSE})=0.\\)\nAnalogously for the \\(k\\)-fold CV error rate and the validation set error rate.\nWe can, for instance, determine the degree \\(d\\) in logistic regression models \\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0 +\\sum_{j=1}^d X_j^d\n\\] by selecting that polynomial degree \\(d\\) that minimizes the CV error rate.\nLikewise, one can select the tuning parameter \\(K\\) in KNN classification by minimizing the CV error rate across different candidate values for \\(K.\\)"
  },
  {
    "objectID": "Ch4_ResamplingMethods.html#ch.-5.2-the-bootstrap",
    "href": "Ch4_ResamplingMethods.html#ch.-5.2-the-bootstrap",
    "title": "4  Resampling Methods",
    "section": "(Ch. 5.2) The Bootstrap",
    "text": "(Ch. 5.2) The Bootstrap\nThe bootstrap is a widely applicable and powerful statistical tool to quantify the uncertainty associated with a given estimator or statistical learning method.\n\nIllustration\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y.\\) These returns \\(X\\) and \\(Y\\) are random with\n\n\\(Var(X)=\\sigma^2_X\\)\n\\(Var(Y)=\\sigma^2_Y\\)\n\\(Cov(X,Y)=\\sigma_{XY}\\)\n\nWe want to invest a fraction \\(\\alpha\\in(0,1)\\) in \\(X\\) and invest the remaining \\(1-\\alpha\\) in \\(Y.\\)\nOur aim is to minimize the variance (risk) of our investment, i.e., we want to minimize \\[\nVar\\left(\\alpha X + (1-\\alpha)Y\\right).\n\\] One can show that the value \\(\\alpha\\) that minimizes this variance is \\[\n\\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}}.\n\\tag{4.4}\\] Using a data set that contains past measurements \\[\n((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] for \\(X\\) and \\(Y,\\) we can estimate the unknown \\(\\alpha\\) by plugging in estimates of the variances and covariances \\[\n\\hat\\alpha = \\frac{\\hat\\sigma^2_Y - \\hat\\sigma_{XY}}{\\hat\\sigma^2_X + \\hat\\sigma^2_Y - 2\\hat\\sigma_{XY}}\n\\tag{4.5}\\] with \\[\n\\begin{align*}\n\\hat{\\sigma}^2_X&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2\\\\\n\\hat{\\sigma}^2_Y&=\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2\\\\\n\\hat{\\sigma}_{XY}&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)\\left(Y_i-\\bar{Y}\\right),\n\\end{align*}\n\\] where \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i,\\) and likewise for \\(\\bar{Y}.\\)\nIt is natural to wish to quantify the accuracy of our estimate \\(\\hat\\alpha\\approx \\alpha.\\) I.e., we wish to know the standard error of the estimator \\(\\hat\\alpha\\), \\[\n\\sqrt{Var(\\hat\\alpha)} = \\operatorname{SE}(\\hat\\alpha)=?\n\\] Computing \\(\\operatorname{SE}(\\hat\\alpha)\\) is here difficult due to the definition of \\(\\hat\\alpha\\) in Equation 4.5 which contains variance estimates also in the denominator.\n\n\nThe Infeasible Bootstrap: A Monte Carlo Simulation\nLet us, for a moment, assume that we know the distributions of \\(X\\) and \\(Y.\\) For simplicity, let’s say \\[\n\\left(\\begin{matrix}X\\\\ Y\\end{matrix}\\right) \\sim F_{(X,Y)},\n\\] where \\(F_{(X,Y)}\\) is the distribution function of the bi-variate normal distribution \\[\n\\mathcal{N}\\left(\\left(\\begin{matrix}0\\\\0\\end{matrix}\\right),\\left[\\begin{matrix}\\sigma_X^2&\\sigma_{XY}\\\\\\sigma_{XY}&\\sigma_{Y}^2\\end{matrix}\\right]\\right).\n\\tag{4.6}\\] If this were true, i.e., if we would know the true population distribution of \\(X\\) and \\(Y,\\) we could simply generate a new dataset containing new observations for \\(X\\) and \\(Y\\) that allows us to compute a new estimate \\(\\hat\\alpha.\\)\nRepeatedly generating new datasets \\(((X_1,Y_1),\\dots,(X_n,Y_n))\\) by sampling new observations from the (here assumed) true population distribution Equation 4.6, for instance, \\(B=1000\\) many times, would allow us to compute \\(B=1000\\) estimates\n\\[\n\\hat\\alpha_1,\\;\\hat\\alpha_2,\\dots,\\hat\\alpha_{B}.\n\\] The empirical standard deviation \\[\n\\sqrt{\\frac{1}{B}\\sum_{b=1}^B\\left(\\hat\\alpha_b - \\bar{\\alpha}\\right)^2},\\quad\\text{with}\\quad \\bar{\\alpha} = \\frac{1}{B}\\sum_{b=1}^B\\hat\\alpha_b,\n\\] is then a very good estimate of the (unknown) true \\(\\operatorname{SE}(\\hat\\alpha).\\)\nIndeed, by the law of large numbers this sample standard deviation consistently estimates the true \\(\\operatorname{SE}(\\hat\\alpha)\\) as \\(B\\to\\infty,\\) provided that we sample from the true population distribution \\(F_{(X,Y)}.\\)\nR code for doing this Monte Carlo simulation:\n\nsuppressPackageStartupMessages(library(\"MASS\")) # for mvrnorm()\n\nn        <- 100 # sample size\n\n## Next: Defining the (usually unknown) population \n## distribution of (X,Y) ~ F_XY, where F_XY is \n## assumed to be a Bi-variate normal distribution \n## with the following parameters: \nmu_X     <- 0\nmu_Y     <- 0\n\nsigma2_X <- 3\nsigma2_Y <- 4\nsigma_XY <- 1\n\nSigma    <- rbind(c(sigma2_X, sigma_XY), \n                  c(sigma_XY, sigma2_Y))\n\n\n## The true (usually unknown) alpha value: \nalpha_true  <- (sigma2_Y - sigma_XY) / (sigma2_X + sigma2_X - 2 * sigma_XY)                  \n\n\n## Infeasible Bootstrap (i.e. a Monte Carlo (MC) Simulation)\nset.seed(333)\n\nB            <- 1000\nalpha_hat_MC <- numeric(B)\n\nfor(b in 1:B){\n  dat             <- mvrnorm(n = n, mu = c(mu_X, mu_Y), Sigma = Sigma)\n  X               <- dat[,1]\n  Y               <- dat[,2]\n  ##\n  sigma2_X_hat    <- var(X)\n  sigma2_Y_hat    <- var(Y)\n  sigma_XY_hat    <- cov(X,Y)\n  ##\n  alpha_hat_MC[b] <- (sigma2_Y_hat - sigma_XY_hat) / (sigma2_X_hat + sigma2_X_hat - 2 * sigma_XY_hat)\n}\n\n## Estimate of the standard error of the estimates for alpha:\nsd(alpha_hat_MC)\n\n[1] 0.2301389\n\n\nThus, this Monte Carlo simulation estimates that the true standard error equals 0.2301389, i.e. \n\n\\(\\operatorname{SE}(\\hat\\alpha) \\approx\\) sd(alpha_hat_MC) \\(=\\) 0.2301389,\n\nand by the law of large number (large B), we can expect this estimation to be really good and reliable.\nBut, unfortunately, this result depends on our **completely unrealistic assumption that we know the true population distribution \\(F_{(X,Y)}\\) of \\((X,Y),\\) which makes this simple resampling approach infeasible in practice. 😭\n\n\nThe Actual (Feasible) Bootstrap\nFortunately, we can use the empirical cumulative distribution function \\(F_{n,(X,Y)}\\) from the originally observed dataset of past measurements for \\(X\\) and \\(Y,\\) as an approximation to the true (unknown) population distribution \\(F_{(X,Y)}\\), \\[\nF_{n,(X,Y)}\\approx F_{(X,Y)}.\n\\]\nSo, instead of resampling from an unknown population distribution \\(F_{(X,Y)},\\) which is not possible in practice, we resample from the empirical distribution \\(F_{n,(X,Y)},\\) which is easily possible in practice. 🥳\nThis idea will work well, as long as \\(F_{n,(X,Y)}\\) serves as a good approximation of \\(F_{(X,Y)}\\) which will always be the case if the sample size \\(n\\) is sufficiently large since, by the famous Glivenko-Cantelli Theorem, \\(F_{n,(X,Y)}\\) is uniformly consistent for \\(F_{(X,Y)}.\\)\nSampling from an empirical cdf \\(F_{n}\\) simply means sampling from the observed dataset \\((X_i,Y_i)\\), \\(i=1,\\dots,n\\), with replacement, for instance like this:\n\nbootstrap_sample <- sample(x = 1:n, n, replace = TRUE)\nbootstrap_data   <- data_frame[bootstrap_sample, ]\n\nIn order to illustrate the bootstrap, let us generate some artificial data. We use again the bi-variate normal distribution as in the “infeasible bootstrap” illustration.\n\n## Generate some artificial data\nobserved_data  <- mvrnorm(n = n, mu = c(mu_X, mu_Y), Sigma = Sigma)\n\nIf the bootstrap works, then the bootstrap estimate of the standard error \\(\\operatorname{SE}(\\hat\\alpha)\\) should be close to the infeasible Monte Carlo estimate, even though the bootstrap method does not explicitly use the true data generating process, but only the observed data.\nThe following code implements the bootstrap:\n\nset.seed(123)\n## Bootstrap \nB              <- 1000\nalpha_hat_boot <- numeric(B)\n\nfor(b in 1:B){\n  bootstrap_sample  <- sample(x = 1:n, n, replace = TRUE)\n  bootstrap_data    <- observed_data[bootstrap_sample, ]\n  ##\n  X                 <- bootstrap_data[,1]\n  Y                 <- bootstrap_data[,2]\n  ##\n  sigma2_X_hat      <- var(X)\n  sigma2_Y_hat      <- var(Y)\n  sigma_XY_hat      <- cov(X,Y)\n  ##\n  alpha_hat_boot[b] <- (sigma2_Y_hat - sigma_XY_hat) / (sigma2_X_hat + sigma2_X_hat - 2 * sigma_XY_hat)\n}\n\n## Estimate of the standard error of the estimates for alpha:\nsd(alpha_hat_boot)\n\n[1] 0.2523776\n\n\nThe bootstrap estimate of the true standard error equals 0.2523776, i.e. \n\n\\(\\operatorname{SE}(\\hat\\alpha) \\approx\\) sd(alpha_hat_boot) \\(=\\) 0.2523776.\n\nThis is really close to the infeasible Monte Carlo simulation based estimate sd(alpha_hat_MC) \\(=\\) 0.2301389, but without making use of the unknown data generating process.\n\nThe bootstrap method is attributed to Bradley Efron, who received the International Prize in Statistics (the Nobel price of statistics) for his seminal works on the bootstrap method."
  },
  {
    "objectID": "Ch4_ResamplingMethods.html#r-lab-resampling-methods",
    "href": "Ch4_ResamplingMethods.html#r-lab-resampling-methods",
    "title": "4  Resampling Methods",
    "section": "4.1 R-Lab: Resampling Methods",
    "text": "4.1 R-Lab: Resampling Methods\nIn this lab, we explore the resampling techniques covered in this chapter. Some of the commands in this lab may take a while to run on your computer.\n\n4.1.1 The Validation Set Approach\nWe explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the Auto data set.\nBefore we begin, we use the set.seed() function in order to set a for R’s random number generator, so that the (pseudo) random data splits are reproducible.\nWe begin by using the sample() function to split the set of observations into two halves, by selecting a random subset of \\(196\\) observations out of the original \\(392\\) observations. We refer to these observations as the training set.\n\nlibrary(\"ISLR2\")\nattach(Auto)\n\n## One half of the sample size\nnrow(Auto)/2\n\n[1] 196\n\nset.seed(1)\ntrain <- sample(x = 1:392, size = 196)\n\nWe then use the subset option in lm() to fit a linear regression using only the observations corresponding to the training set.\n\nlm.fit <- lm(mpg ~ horsepower, \n             data   = Auto, \n             subset = train)\n\nWe now use the predict() function to estimate the response for all \\(392\\) observations, and we use the mean() function to calculate the MSE of the \\(196\\) observations in the validation set. Note that the -train index below selects only the observations that are not in the training set.\n\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 23.26601\n\n\nTherefore, the estimated test MSE for the linear regression fit is \\(23.27\\). We can use the poly() function to estimate the test error for the quadratic and cubic regressions.\n\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), \n             data   = Auto, \n             subset = train)\n\n## Test MSE \nmean((mpg - predict(lm.fit2, Auto))[-train]^2)\n\n[1] 18.71646\n\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), \n             data   = Auto, \n             subset = train)\n## Test MSE             \nmean((mpg - predict(lm.fit3, Auto))[-train]^2)\n\n[1] 18.79401\n\n\nThese error rates are \\(18.72\\) and \\(18.79\\), respectively. If we choose a different training set instead, then we will obtain somewhat different errors on the validation set.\n\nset.seed(2)\n\n## Polynomial degree 1\ntrain  <- sample(x = 1:392, size = 196)\nlm.fit <- lm(mpg ~ horsepower, subset = train)\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 25.72651\n\n## Polynomial degree 2\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2)\n\n[1] 20.43036\n\n## Polynomial degree 3\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, \n    subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2)\n\n[1] 20.38533\n\n\nUsing this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are \\(25.73\\), \\(20.43\\), and \\(20.39\\), respectively.\nThese results are consistent with our previous findings: a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence that a model that uses a cubic function of horsepower performance substantially better.\n\n\n4.1.2 Leave-One-Out Cross-Validation\nThe LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions. In the lab for Chapter 4, we used the glm() function to perform logistic regression by passing in the family = \"binomial\" argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function. So for instance,\n\nglm_fit <- glm(mpg ~ horsepower, data = Auto)\ncoef(glm_fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nand\n\nlm.fit <- lm(mpg ~ horsepower, data = Auto)\ncoef(lm.fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nyield identical linear regression models. In this lab, we will perform linear regression using the glm() function rather than the lm() function because the former can be used together with cv.glm(). The cv.glm() function is part of the boot library.\n\n## install.packages(\"boot\")\nlibrary(\"boot\")\n\nglm_fit <- glm(mpg ~ horsepower, data = Auto)\ncv.err  <- cv.glm(Auto, glm_fit)\ncv.err$delta\n\n[1] 24.23151 24.23114\n\n\nThe cv.glm() function produces a list with several components. The two numbers in the delta vector contain the cross-validation results. In this case the numbers are identical (up to two decimal places) and correspond to the LOOCV statistic given in Equation 4.1. Below, we discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately \\(24.23\\).\nWe can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for() function to initiate a which iteratively fits polynomial regressions for polynomials of order \\(i=1\\) to \\(i=10\\), computes the associated cross-validation error, and stores it in the \\(i\\)th element of the vector cv_error. We begin by initializing the vector.\n\ncv_error <- rep(0, 10)\nfor (i in 1:10) {\n  glm_fit     <- glm(mpg ~ poly(horsepower, i), \n                     data = Auto)\n  cv_error[i] <- cv.glm(Auto, glm_fit)$delta[1]\n}\nplot(cv_error, type=\"b\", ylab=\"Test MSE\", xlab=\"Polynomial Degree\",\n     col = \"black\", bg = \"black\", pch = 21, main=\"LOOCV\")\n\n\n\n\nWe see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.\n\n\n4.1.3 \\(k\\)-Fold Cross-Validation\nThe cv.glm() function can also be used to implement \\(k\\)-fold CV. Below we use \\(k=10\\), a common choice for \\(k\\), on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.\n\nset.seed(17)\ncv_error_10 <- rep(0, 10)\nfor (i in 1:10) {\n  glm_fit        <- glm(mpg ~ poly(horsepower, i), \n                        data = Auto)\n  cv_error_10[i] <- cv.glm(Auto, glm_fit, K = 10)$delta[1]\n}\n##\nplot(cv_error_10, type=\"b\", ylab=\"Test MSE\", xlab=\"Polynomial Degree\",\n     col = \"black\", bg = \"black\", pch = 21, main=\"10-fold CV\")\n\n\n\n\nWe still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.\nNotice that the computation time is shorter than that of LOOCV.\n(In principle, the computation time for LOOCV for a least squares linear model should be faster than for \\(k\\)-fold CV, due to the availability of the formula Equation 4.2 for LOOCV; however, unfortunately the cv.glm() function does not make use of this formula.)\nWe saw in Section 4.1.2 that the two numbers associated with delta are essentially the same when LOOCV is performed. When we instead perform \\(k\\)-fold CV, then the two numbers associated with delta differ slightly. The first number is the standard \\(k\\)-fold CV estimate, as in Equation 4.3. The second is a bias-corrected version. On this data set, however, the two estimates are very similar to each other.\n\n\n4.1.4 The Bootstrap\nWe illustrate the use of the bootstrap revisiting the portfolio choice example from above, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\n\n4.1.4.1 Estimating the Accuracy of a Statistic of Interest\nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in R entails only two steps:\n\nFirst, we must create a function that computes the statistic of interest.\nSecond, we use the boot() function, which is part of the boot library, to perform the bootstrap by repeatedly sampling observations from the data set with replacement.\n\nThe Portfolio data set in the ISLR2 package is simulated data of \\(100\\) pairs of returns, generated in the fashion described above, where we introduced the portfolio example.\nTo illustrate the use of the bootstrap on this data, we must first create a function, alpha_fn(), which takes as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations.\n\nalpha_fn <- function(data, index) {\n  X         <- data$X[index]\n  Y         <- data$Y[index]\n  alpha_hat <- (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n  return(alpha_hat)\n}\n\nThis function returns an estimate for \\(\\alpha\\) based on applying Equation 4.4 to the observations indexed by the argument index. For instance, the following command tells R to estimate \\(\\alpha\\) using all of the \\(100\\) observations.\n\nalpha_fn(data  = Portfolio, \n         index = 1:100) # complete original dataset\n\n[1] 0.5758321\n\n\nThe next command uses the sample() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing \\(\\hat{\\alpha}\\) based on the new data set.\n\nset.seed(7)\nalpha_fn(data  = Portfolio, \n         index = sample(x=1:100, size=100, replace = TRUE))\n\n[1] 0.5385326\n\n\nWe can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. (We use this approach above.)\nHowever, the boot() function automates this approach. Below we produce \\(R=1,000\\) bootstrap estimates for \\(\\alpha\\).\n\nboot(data      = Portfolio, \n     statistic = alpha_fn, \n     R         = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha_fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 0.0007959475  0.08969074\n\n\nThe final output shows that using the original data, \\(\\hat{\\alpha}=0.5758\\), and that the bootstrap estimate for \\(\\operatorname{SE}(\\hat{\\alpha})\\) is \\(0.0897\\).\n\n\n4.1.4.2 Estimating the Accuracy of a Linear Regression Model\nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\) described in Chapter 3.1.2.\nWe first create a simple function, boot_fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3.\n\n## Function to compute coefficient estimates using lm()\nboot_fn <- function(data, index){\n  coef(lm(mpg ~ horsepower, \n          data   = data, \n          subset = index))\n}\n\n## Sample size\nn <- nrow(Auto)\n\n## Coeficient estimates using the total sample\nboot_fn(data  = Auto, \n        index = 1:n)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nThe boot_fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. Here we give two examples.\n\nset.seed(1)\n\nboot_fn(data  = Auto, \n        index = sample(1:n, n, replace = TRUE))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\nboot_fn(data  = Auto, \n        index = sample(1:n, n, replace = TRUE))\n\n(Intercept)  horsepower \n 40.1186906  -0.1577063 \n\n\nNext, we use the boot() function to compute the standard errors of \\(R=1,000\\) bootstrap estimates for the intercept and slope terms.\n\nboot_obj <- boot(data      = Auto, \n                 statistic = boot_fn, \n                 R         = 1000)\nboot_obj                 \n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot_fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0544513229 0.841289790\nt2* -0.1578447 -0.0006170901 0.007343073\n\n\nThis indicates that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.84\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\).\nThe reported bias equals the difference between the sample means of the bootstrap realizations and the full sample estimates:\n\n## estimated biases: \ncolMeans(boot_obj$t) - boot_obj$t0\n\n  (Intercept)    horsepower \n 0.0544513229 -0.0006170901 \n\n\nAs discussed in Chapter 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.\n\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\nThe standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained using the formulas from Chapter 3.1.2 are \\(0.717\\) for the intercept and \\(0.0064\\) for the slope.\nInterestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 (Chapter 3.1.2) rely on certain assumptions. For example, they depend on the unknown parameter \\(\\sigma^2\\), the noise variance. We then estimate \\(\\sigma^2\\) by \\(\\operatorname{RSS}/(n-p-1).\\) Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does. We see in Figure 3.8 of our textbook that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\).\nMoreover, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function.\nBelow we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data.\n\nboot_fn <- function(data, index)\n  coef(lm(mpg ~ horsepower + I(horsepower^2), \n        data = data, subset = index))\n\nset.seed(1)\nboot(data      = Auto, \n     statistic = boot_fn, \n     R         = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot_fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\nsummary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$coef\n\n                    Estimate   Std. Error   t value      Pr(>|t|)\n(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\nhorsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\nI(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21\n\n\nSince the quadratic model provides a good fit to the data (Figure 3.8 of our textbook), there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\)."
  },
  {
    "objectID": "Ch4_ResamplingMethods.html#exercises",
    "href": "Ch4_ResamplingMethods.html#exercises",
    "title": "4  Resampling Methods",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nPrepare the following exercises of Chapter 5 in our course textbook:\n\nExercise 3\nExercise 4\nExercise 5\nExercise 6\nExercise 8"
  },
  {
    "objectID": "Ch5_LinModSelectRegul.html",
    "href": "Ch5_LinModSelectRegul.html",
    "title": "5  Linear Model Selection and Regularization",
    "section": "",
    "text": "Reading: Chapter 6 of our course textbook An Introduction to Statistical Learning\nIn this chapter, we revisit the linear regression model \\[\nY = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p + \\epsilon.\n\\] In Chapter Chapter 2, we focused on fitting a given linear regression model using least squares and completely ignored the model selection process. However, selecting a “good” model is itself a statistical problem which we need to solve using reliable statistical procedures. In this chapter, we consider fitting procedures that integrate the model selection process.\nWe discuss three important classes of methods:"
  },
  {
    "objectID": "Ch5_LinModSelectRegul.html#ch.-6.1-subset-selection",
    "href": "Ch5_LinModSelectRegul.html#ch.-6.1-subset-selection",
    "title": "5  Linear Model Selection and Regularization",
    "section": "(Ch. 6.1) Subset Selection",
    "text": "(Ch. 6.1) Subset Selection\nIn the following, we consider linear models with some large set of \\(p\\) many potentially relevant predictors \\(X=(X_1,\\dots,X_{p}),\\) \\[\n\\begin{align*}\nY\n&=\\mathcal{M}_{p}(X) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_{p} + \\epsilon,\n\\end{align*}\n\\tag{5.1}\\] where \\(Var(\\epsilon)=\\sigma^2,\\) and where \\(\\epsilon\\) is independent of the predictors \\(X.\\)\nLet’s assume that only \\(k\\leq p\\) many predictors in Equation 5.1 are actually relevant having non-zero slope coefficients \\(|\\beta_j|>0,\\) and let denote the index set of these relevant predictors by\n\\[\n\\mathcal{I}^*_{k^*}\\subset\\{1,2,\\dots,p\\}\\quad\\text{with}\\quad |\\mathcal{I}^*_{k^*}|=k^*\\leq p.\n\\]\nIn (linear) model selection, one often aims to select the sub-model \\[\n\\mathcal{M}_{\\mathcal{I}^*_{k^*}}(X)=\\beta_0 + \\sum_{j\\in\\mathcal{I}^*_{k^*}}\\beta_j X_j\n\\] containing all relevant predictors with coefficients \\(|\\beta_j|>0.\\)\n\n\nA first idea may be to select candidate models \\(\\mathcal{M}_{\\mathcal{I}_k}(X)\\) by maximizing the fit to the data; i.e. by maximizing the \\(R^2\\) or equivalently by minimizing the Residual Sum of Squares (RSS), \\[\n\\begin{align*}\n\\operatorname{RSS}_k&=\\sum_{i=1}^n\\left(y_i - \\widehat{\\mathcal{M}}_{\\mathcal{I}_k}(X)\\right)^2\\\\\n%R^2_p &=1-\\frac{\\operatorname{RSS}_p}{TSS}\\\\\n\\widehat{\\mathcal{M}}_{\\mathcal{I}_k}(X) & = \\hat\\beta_0 + \\sum_{j\\in\\mathcal{I}_k}\\hat\\beta_j X_j,  \n%TSS&=\\sum_{i=1}^n\\left(y_i - \\bar{y}\\right)^2.\n\\end{align*}\n\\] where \\(\\mathcal{I}_k\\) is some candidate sub-set \\(\\mathcal{I}_k\\subset\\{1,2,\\dots,p\\}\\) with \\(|\\mathcal{I}_k|=k\\leq p.\\)\nWhile this strategy can be used for selecting the best fitting model among all \\[\n\\binom{p}{k}=\\frac{p!}{k!(p-k)!}\n\\] many models with \\(k\\) parameters, it fails to select the best fitting model with different numbers of parameters. Indeed, one can show that the “fit” of the model can be always increased by adding more predictors; i.e.  \\[\n\\operatorname{RSS}_k\\geq \\operatorname{RSS}_{k'}\\quad\\text{for}\\quad k<k'.\n\\] Thus, we need alternative criteria to choose between models with different numbers of regressors \\(k\\neq k'\\).\nIn order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:\n\nWe can directly estimate the test error, using the validation set approach or cross-validation (Chapter 4).\nWe can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\n\nThe latter option is accomplished using information criteria such as, for instance, Mellow’s \\(C_p\\), Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and the adjusted \\(R^2.\\) Information criteria are particularly useful in cases, where cross-validation is computationally too expensive.\n\nMellow’s \\(C_p\\)\n\\[\nC_p \\equiv C_p(k) = \\frac{1}{n}\\left(\\operatorname{RSS}_k+2k\\hat\\sigma^2\\right),\n\\tag{5.2}\\] where \\(\\hat\\sigma^2=\\frac{1}{n-p}\\sum_{i=1}^n(y_i - \\widehat{\\mathcal{M}}_{p}(X))^2\\) is the sample variance of the residuals from the large model Equation 5.1 with all \\(p\\) predictors, and where \\(\\operatorname{RSS}_k\\) are the RSS of the estimated model \\(\\widehat{\\mathcal{M}}_{\\mathcal{I}_k}(X)\\) with \\(k\\leq p\\) predictors.\nUnder certain regularity assumptions, one can show that \\(C_p\\) is an unbiased estimate of the test MSE. The best model is the one which has the lowest \\(C_p\\) value.\n\n\nThe AIC\nThe AIC is defined for a large class of models fit by maximum likelihood. If the error terms in the linear regression model Equation 5.1 are Gaussian, maximum likelihood and least squares estimation are equivalent. In this case AIC is given by \\[\n\\operatorname{AIC} \\equiv \\operatorname{AIC}(k) = \\frac{1}{n}\\left(\\operatorname{RSS}_k + 2 k\\hat\\sigma^2\\right),\n\\] where \\(\\hat\\sigma^2\\) is defined as in Equation 5.2. The above formula for the AIC omits factors that are constant in \\(k\\) and thus irrelevant for model selection. Hence for least squares the AIC and Mellow’s \\(C_p\\) are proportional to each other, and thus lead to the same model choices.\n\n\nThe BIC\n\\[\n\\operatorname{BIC} \\equiv \\operatorname{BIC}(k) = \\frac{1}{n}\\left(\\operatorname{RSS}_k + \\log(n)2 k\\hat\\sigma^2\\right)\n\\] where \\(\\hat\\sigma^2\\) is defined as in Equation 5.2.\nSince \\(\\log(n)>2\\) for any sample size \\(n>7,\\) the BIC statistic generally places a heavier penalty on models with many predictors, and hence results in the selection of smaller models than Mellow’s \\(C_p\\) and AIC.\n\n\nThe adjusted \\(R^2\\)\n\\[\n\\operatorname{adjusted } R^2 \\equiv (\\operatorname{adjusted } R^2)(k) = 1-\\frac{\\operatorname{RSS}_k/(n-k-1)}{\\operatorname{TSS}/(n-1)},\n\\] where \\(\\operatorname{TSS}=\\sum_{i=1}^n\\left(y_i - \\bar{y}\\right)^2.\\)\nUnlike Mellow’s \\(C_p,\\) AIC, and BIC, large values of \\(\\operatorname{adjusted } R^2\\) indicate models with low test errors.\nNote that maximizing \\(\\operatorname{adjusted } R^2\\) is equivalent to minimizing \\(\\operatorname{RSS}_k/(n-k-1).\\) While \\(\\operatorname{RSS}_k\\) is a decreasing function of \\(k,\\) \\(\\operatorname{RSS}_k/(n-k-1)\\) may decrease or increase when increasing \\(k,\\) depending on the amount of RSS-reduction due to the added predictors.\nNote: Mellow’s \\(C_p,\\) AIC, and BIC have rigorous theoretical justifications. The \\(\\operatorname{adjusted } R^2\\) is not es well motivated by statistical theory.\n\n\n\n(Ch. 6.1.1) Best Subset Selection\nAlgorithm 6.1: Best Subset Selection:\n\nInitialization: Let \\(\\mathcal{M}_0\\) denote the null model \\(f(X)=\\beta_0\\) containing no predictors, except the intercept. (This model predicts each observed outcome by the total sample mean.)\nFor \\(k=1,2,\\dots,p:\\)\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\nPick the “best” among these \\(\\binom{p}{k}\\) models, and call it \\(\\widehat{\\mathcal{M}}_k\\). Here “best” is defined as having smallest \\(\\operatorname{RSS}_k\\) (or equivalently highest \\(R^2.\\))\n\nSelect a single best model from \\(\\widehat{\\mathcal{M}}_0,\\dots,\\widehat{\\mathcal{M}}_p\\) using CV, Mellow’s \\(C_p\\) (AIC), BIC, or \\(\\operatorname{adjusted } R^2.\\)\n\n\nStep 2 of Algorithm 6.1 identifies the best model (on training data) for each subset size \\(k\\), and thus reduces the model selection problem from \\(2^p\\) models to \\(p+1\\) models.\n\nBest subset selection (Algorithm 6.1) can be computationally expensive for largish \\(p\\).\n\n\n\n\\(p\\)\n\\(2^p\\)\n\n\n\n\n\\(10\\)\n\\(1024\\)\n\n\n\\(20\\)\n\\(1048576\\)\n\n\n\\(40\\)\n\\(1.1\\cdot 10^{12}\\)\n\n\n\nSummary:\n\nBest subset selection becomes computationally infeasible for values of \\(p\\) greater than about \\(40\\), even with extremely fast modern computers.\nMoreover, the larger the search space, the higher the chance of finding models that look good in the training data (Step 2), even tough they might not have any predictive power in test data.\n\n\n\n(Ch. 6.1.2) Stepwise Selection\n\nForward Stepwise Selection\nAlgorithm 6.2: Forward Stepwise Selection:\n\nInitialization: Let \\(\\mathcal{M}_0\\) denote the null model \\(f(X)=\\beta_0\\) containing no predictors, except the intercept. (This model predicts each observed outcome by the total sample mean.)\nFor \\(k=0,1,\\dots,p-1:\\)\n\nFit all \\(p-k\\) models that augment the predictors in \\(\\widehat{\\mathcal{M}}_k\\) with one additional predictor.\nPick the “best” among these \\(p-k\\) models, and call it \\(\\widehat{\\mathcal{M}}_{k+1}\\). Here “best” is defined as having smallest \\(\\operatorname{RSS}_{k+1}\\) (or equivalently highest \\(R^2.\\))\n\nSelect a single best model from \\(\\widehat{\\mathcal{M}}_0,\\dots,\\widehat{\\mathcal{M}}_p\\) using CV, Mellow’s \\(C_p\\) (AIC), BIC, or \\(\\operatorname{adjusted } R^2.\\)\n\n\nUnlike best subset selection, which involved fitting \\(2^p\\) models, forward stepwise selection considers a much smaller set of models: It begins with fitting the null \\((p=0)\\) model, and proceeds with fitting \\(p-k\\) models in the \\(k\\)th iteration, for \\(k=1,\\dots,p-1.\\) This amounts to a total of \\[\n1+\\sum_{k=0}^{p-1}(p-k) = 1 + p(p+1)/2\n\\] models.\n\n\n\n\\(p\\)\n\\(2^p\\)\n\\(1 + p(p+1)/2\\)\n\n\n\n\n\\(10\\)\n\\(1024\\)\n\\(56\\)\n\n\n\\(20\\)\n\\(1048576\\)\n\\(211\\)\n\n\n\\(40\\)\n\\(1.1\\cdot 10^{12}\\)\n\\(821\\)\n\n\n\nForward stepwise selection is a guided search strategy, that tends to do well in practice. However, it is not guaranteed to find the best possible model out of all \\(2^2p\\) models containing subsets of the \\(p\\) predictors.\nExample: Consider the case of \\(p=3\\) predictors \\(X_1,\\,X_2\\) and \\(X_3.\\) Let the best one-predictor model contain \\(X_1\\), and let the best two-predictor model contain \\(X_2\\) and \\(X_3.\\) Then forward stepwise selection will fail to select the best two-predictor model.\n\n\n\nBackward Stepwise Selection\nAlgorithm 6.3: Backward Stepwise Selection:\n\nInitialization: Let \\(\\mathcal{M}_p\\) denote the full model which contains all \\(p\\) predictors.\nFor \\(k=p,p-1,\\dots,1:\\)\n\nFit all \\(k\\) models that contain all but one of the predictors in \\(\\widehat{\\mathcal{M}}_k,\\) for a total of \\(k-1\\) predictors.\nPick the “best” among these \\(k\\) models, and call it \\(\\widehat{\\mathcal{M}}_{k-1}\\). Here “best” is defined as having smallest \\(\\operatorname{RSS}_{k+1}\\) (or equivalently highest \\(R^2.\\))\n\nSelect a single best model from \\(\\widehat{\\mathcal{M}}_0,\\dots,\\widehat{\\mathcal{M}}_p\\) using CV, Mellow’s \\(C_p\\) (AIC), BIC, or \\(\\operatorname{adjusted } R^2.\\)\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1 + p(p+1)/2\\) models, and thus can be applied when \\(p\\) is too large to apply best subset selection.\nLike forward stepwise selection, the backward selection approach is a guided search strategy, that tends to do well in practice. However, it is not guaranteed to find the best possible model out of all \\(2^2p\\) models containing subsets of the \\(p\\) predictors.\nBackward selection requires that the sample size \\(n\\) is larger than the number of predictors \\(p\\) (so that the full model can be fit).\n\n\nHybrid Approaches\nThe above selection algorithms often give similar, but not identical models. The literature knows many alternative selection algorithms such as hybrid approaches in which, for instance, variables are added to the model sequentially, in analogy to forward selection; however, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit."
  },
  {
    "objectID": "Ch5_LinModSelectRegul.html#ch.-6.2-shrinkage-methods",
    "href": "Ch5_LinModSelectRegul.html#ch.-6.2-shrinkage-methods",
    "title": "5  Linear Model Selection and Regularization",
    "section": "(Ch. 6.2) Shrinkage Methods",
    "text": "(Ch. 6.2) Shrinkage Methods\nThe discussed subset selection methods involve using least squares to fit the linear models. Alternatively, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates by shrinking the coefficient estimates towards zero.\nThe two best-known techniques for shrinking the regression coefficients towards zero are\n\nRidge Regression and\nLasso\n\n\n(Ch. 6.2.1) Ridge Regression\nRecall from Chapter 3 that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) using parameter values that minimize the RSS criterion \\[\n\\operatorname{RSS}=\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2.\n\\tag{5.3}\\]\nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. The ridge regression coefficient estimates \\(\\hat\\beta^R\\) are the values that minimize \\[\n\\begin{align*}\n\\underbrace{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2}_{=\\operatorname{RSS}}+\\lambda \\sum_{j=1}^p\\beta_j^2,\n%= &\\operatorname{RSS} +\\lambda \\sum_{j=1}^p\\beta_j^2,\n\\end{align*}\n\\tag{5.4}\\] where \\(\\lambda \\sum_{j=1}^p\\beta_j^2\\) is called shrinkage penalty, and where \\(\\lambda\\) is a tuning parameter that needs to be determined separately. Equation 5.4 trades of two different criteria:\n\nBy making RSS small, ridge regression tries to fit the data well.\nBy making \\(\\sum_{j=1}^p\\beta_j^2=||(\\beta_{1},\\dots,\\beta_{p})'||^2_2\\) small, ridge regression shrinks all coefficient estimates towards zero—except the intercept \\(\\beta_0.\\)\n\n\n\\(||\\beta||_2 = \\sqrt{\\sum_{j=1}^p\\beta_j^2}\\) denotes the \\(\\ell_2\\) (or Euclidean) norm of a \\(p\\)-dimensional vector \\(\\beta.\\) If \\(||\\beta||_2\\to 0\\) then all elements \\(\\beta_1,\\dots,\\beta_p\\) eventually approach \\(0.\\)\n\nThe tuning parameter \\(\\lambda\\) controls the relative impact of these two terms on the regression coefficient estimates:\n\nWhen \\(\\lambda=0\\), the penalty term has no effect, and ridge regression will produce the least squares estimates.\nAs \\(\\lambda\\to\\infty\\), the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.\n\nRidge regression will produce a different set of coefficient estimates, \\[\n\\hat\\beta_\\lambda^R=(\\hat\\beta_{1,\\lambda}^R,\\dots,\\hat\\beta_{p,\\lambda}^R)',\n\\] for each value of \\(\\lambda.\\) Selecting a good value for \\(\\lambda\\) is critical and can be accomplished using, for instance, cross-validation.\n\nStandardized Ridge Regression Coefficients\nThe RSS criterion of least squares (Equation 5.3) is scale equivariant. Scaling a predictor by a constant \\(c\\) from \\(x_{ij}\\) to \\(x^S_{ij}=x_{ij}c\\) will simply rescale the corresponding least squares estimate from \\(\\hat\\beta_j\\) to \\(\\hat\\beta_j^S=\\hat\\beta_j/c\\) such that \\[\nx_{ij}\\hat\\beta_j = x_{ij}^S\\hat\\beta_j^S,\\quad i=1,\\dots,n\n\\] which leaves the fitted values unchanged, and thus the RSS value unaffected.\n\nn         <- 100\nx         <- rnorm(n = n, mean = 50000, sd = 10000)\neps       <- rnorm(n = n, mean = 0,     sd = 10000)\ny         <- 2 + 5 * x + eps\n##\nlm_obj_1  <- lm(y ~ x)\nRSS_1     <- sum(resid(lm_obj_1)^2)\n##\nc         <- 1/1000 # scaling factor \nx_S       <- x * c\nlm_obj_2  <- lm(y ~ x_S)\nRSS_2     <- sum(resid(lm_obj_2)^2)\n##\n\n## comparing the estimates \nround(\nc(coef(lm_obj_1)[2] / c, \n  coef(lm_obj_2)[2]),\ndigits = 1)\n\n   x  x_S \n4931 4931 \n\n## comparing the RSS's:\nc(RSS_1, RSS_2)\n\n[1] 11592034997 11592034997\n\n\nBy contrast, the ridge regression criterion in Equation 5.4 is not scale invariant since scaling the predictors will lead (as seen above) to a rescaling of the coefficient estimates and thus affects the penalty term \\(\\lambda\\sum_{j=1}^p\\beta_j^2.\\)\nThus, scaling one or more predictors will generally affect all ridge regression estimates \\(\\hat\\beta_{0,\\lambda}^R,\\dots,\\hat\\beta_{p,\\lambda}^R,\\) since typically the regressors are correlated with each other.\nTherefore, it is best practice to apply ridge regression after standardizing the predictors, using the formula \\[\n\\tilde{x}_{ij}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}\n\\tag{5.5}\\] so that they are all on the same scale; i.e. all standardized predictors have a standard deviation of one.\nAfter standardizing the predictors, the ridge regression coefficient estimates do not depend on the different scales on which the predictors were measured.\nRidge regression estimates based on standardized predictors are called standardized ridge regression coefficients. The absolute values of the standardized coefficients allow us to rank the effects of the predictors on the depend variable: the predictor corresponding to the largest (in absolute values) standardized coefficient has the largest effect on the dependent variable.\nFigure 6.4 shows the standardized ridge regression coefficient estimates for the Credit dataset. Each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\n\nInterpretation of Figure 6.4:\n\nThe case, where \\(||\\hat\\beta^R_\\lambda||_2/||\\hat\\beta||_2=1\\), i.e. where \\(\\lambda\\approx 0\\), is equivalent to the least squares fit.\nThe case where \\(||\\hat\\beta^R_\\lambda||_2/||\\hat\\beta||_2=0\\), i.e. where \\(\\lambda\\) is very large, corresponds to the case where \\(\\hat\\beta^R_{\\lambda 1}\\approx\\dots\\approx \\hat\\beta^R_{\\lambda p}\\approx 0.\\)\n\n\n\n\nWhy and When Does Ridge Regression Improve Over Least Squares?\nRidge regression’s potential advantage over least squares is rooted in the bias-variance trade-off.\n\nA large value of \\(\\lambda\\) decreases the flexibility of the model, and thus decreases variance, but increases bias.\nA small value of \\(\\lambda\\) increases the flexibility of the model, and thus decreases bias, but increases variance.\n\n\nHigh variance means that a small change in the training data can cause a large change in the coefficient estimates\n\n\nLarge bias means that the coefficient estimates are on average not equal to the true coefficient values\n\nFigure 6.5 illustrates this, using a simulated data set containing \\(p = 45\\) predictors and \\(n = 50\\) observations. When the number of variables \\(p\\) is almost as large as the number of observations \\(n\\), as in the example in Figure 6.5, the least squares estimates will be extremely variable.\n\nIn comparison to the least squares fit (\\(\\lambda\\approx 0\\), \\(||\\hat\\beta^R_\\lambda||_2/||\\hat\\beta||_2=1\\)), the ridge regression shows a lower test MSE for an appropriate choice of \\(\\lambda.\\)\nGenerally, ridge regression outperforms least squares in situations where the least squares estimates have high variance—as in the example of Fig 6.5. In these high variance situations, ridge regression can trade off a small increase in bias for a large decrease in variance leading to an overall reduction in the test MSE.\nIf \\(p > n\\), then the least squares estimates do not have a unique solution, but ridge regression estimates are well defined.\nMoreover, ridge regression also has substantial computational advantages over best subset selection, which requires searching through \\(2^p\\) fitted models. In contrast, ridge regression models only need to be fitted once for each candidate tuning parameter \\(\\lambda.\\)"
  },
  {
    "objectID": "Ch5_LinModSelectRegul.html#ch.-6.2.2-the-lasso",
    "href": "Ch5_LinModSelectRegul.html#ch.-6.2.2-the-lasso",
    "title": "5  Linear Model Selection and Regularization",
    "section": "(Ch. 6.2.2) The Lasso",
    "text": "(Ch. 6.2.2) The Lasso\nWhile ridge regression is able to show which predictors are of major vs. minor relevance for predicting the outcome, none of the predictors is actually removed from the model equation since none of the coefficients is exactly set to zero—unless in the limit \\(\\lambda = \\infty,\\) but then coefficients are zero (no variable selection).\nThis may not be a problem with respect to prediction accuracy, but it can create a challenge in model interpretation—particularly in settings in which the number of variables \\(p\\) is quite large.\nFor example, in the Credit data set, it appears that the most important variables are income, limit, rating, and student. So we might wish to build a model including just these predictors. However, ridge regression will always generate a model involving all ten predictors.\nThe lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat\\beta_\\lambda^L\\), minimize \\[\n\\begin{align*}\n\\underbrace{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2}_{=\\operatorname{RSS}}+\\lambda \\sum_{j=1}^p|\\beta_j|.\n%= &\\operatorname{RSS} +\\lambda \\sum_{j=1}^p\\beta_j^2,\n\\end{align*}\n\\tag{5.6}\\]\nBy contrast to the ridge regression criterion (Equation 5.4), the lasso criterion uses an \\(\\ell_1\\) norm \\[\n||(\\beta_{1},\\dots,\\beta_p)'||_1 = \\sum_{j=1}^p|\\beta_j|\n\\] as a shrinkage penalty. The \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nLasso yields sparse models—that is, models that involve only a subset of the variables (see Figure 6.6). Sparse models are easier to interpret.\n\n\nComparing Lasso, Ridge Regression and Best Subset Selection\nOne can show that the lasso and ridge regression coefficient estimates solve constrained optimization (minimization) problems.\nLasso: \\[\n\\min_{\\beta}\\left\\{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right\\}\\;\\text{s.t.}\\; \\sum_{j=1}^p|\\beta_j|\\leq s\n\\tag{5.7}\\]\nRidge: \\[\n\\min_{\\beta}\\left\\{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right\\}\\;\\text{s.t.}\\; \\sum_{j=1}^p\\beta_j^2\\leq s\n\\tag{5.8}\\]\nThat is, for every \\(\\lambda\\) in Equation 5.4 there is some \\(s\\) such in Equation 5.8 that will lead to the same ridge coefficient estimates, and for every \\(\\lambda\\) in Equation 5.6 there is some \\(s\\) such in Equation 5.7 that will lead to the same lasso coefficient estimates.\nIn the case of two \\((p=2)\\) predictors, one can interpret Equation 5.7 and Equation 5.8 graphically (see Figure 6.7).\n\nthe lasso coefficient estimates have the smallest RSS out of all points \\((\\beta_1,\\beta_2)\\) that lie within the diamond defined by \\(|\\beta_1|+|\\beta_2|\\leq s.\\)\nthe ridge coefficient estimates have the smallest RSS out of all points \\((\\beta_1,\\beta_2)\\) that lie within the circle defined by \\(\\beta_1^2+\\beta_2^2\\leq s.\\)\n\n\nInterpretation of Figure 6.7:\n\nThe tuning parameter \\(s\\) acts like a budge constraint for how large the penalty term can be. This key idea also applies to cases \\(p>2,\\) although plotting then becomes difficult/infeasible.\nFigure 6.7 shows why lasso is able to perform variable selection, by contrast to ridge regression. The ellipses show coefficient estimates leading to equal RSS values. While the \\(\\ell_1\\) geometry of lasso’s “budge constraint” allows to set a coefficient value to zero (here \\(\\beta_1\\)), the \\(\\ell_2\\) geometry of ridge regression’s budget constraint only allows to shrink a coefficient value towards zero.\n\n\n\nConnection with Best Subset Selection\nEquation 5.7 and Equation 5.8 reveal a close connection between the lasso, ridge regression, and best subset selection. Consider the problem \\[\n\\min_{\\beta}\\left\\{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right\\}\\;\\text{s.t.}\\; \\sum_{j=1}^pI(\\beta_j\\neq 0)\\leq s,\n\\tag{5.9}\\] where \\(I(\\beta_j\\neq 0)=1\\) if \\(\\beta_j\\neq 0\\) and zero else.\nEquation 5.9 amounts to finding a set of at most \\(s\\) many coefficient estimates such that RSS is as small as possible. The problem of Equation 5.9 is equivalent to Step 2 in Best Subset Selection (Algorithm 6.1) for a given number of predictors \\(s\\) (or \\(k\\) in the notation of Algorithm 6.1). The choice of \\(s\\) (or equivalently \\(\\lambda\\)) is then critical and needs to be done using, for instance, cross-validation.\nThis insight allows us to interpret ridge regression and lasso (even more so) as computationally feasible versions of Best Subset Regression.\n\n\nComparing the Lasso and Ridge Regression\nFigure 6.8 displays the variance, squared bias, and test MSE of the lasso applied to the same simulated data as in Figure 6.5; i.e. \\(p=45\\) and \\(n=50.\\) Thus, in the case, where many predictors have an effect on the response, ridge regression can perform better than lasso.\n\nFigure 6.9 illustrates a similar situation, except that now the response is a function of only \\(p=2\\) out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.\n\nIn general, one might expect the lasso to perform better in sparse models, i.e. in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or equal zero. Ridge regression will perform better when the response is a function of many predictors.\n\n\n(Ch. 6.2.3) Selecting the Tuning Parameter\nEach value of the tuning parameters \\(\\lambda\\) in Equation 5.4 and Equation 5.6 or of the tuning parameters \\(s\\) in Equation 5.7 and Equation 5.8 represent a new more or less flexible model—similarly to the tuning parameter \\(k\\), with \\(k\\leq p\\), (i.e., number of predictors included) in Best Subset Selection. Thus, as in Best Subset Selection we need to be carful when comparing models of different size and flexibility.\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the coss-validation error for each value of \\(\\lambda\\), as described in Chapter 4.\nWe then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "Ch5_LinModSelectRegul.html#ch.-6.3-dimension-reduction-methods",
    "href": "Ch5_LinModSelectRegul.html#ch.-6.3-dimension-reduction-methods",
    "title": "5  Linear Model Selection and Regularization",
    "section": "(Ch. 6.3) Dimension Reduction Methods",
    "text": "(Ch. 6.3) Dimension Reduction Methods\nThe methods discussed so far in this chapter are all defined using the total set of predictors \\(X_1,X_2,\\dots,X_p\\) in Equation 5.1. We now explore a class of approaches that transform the set of \\(p\\) predictors to a reduced set of \\(M\\leq p\\) transformed variables, and then fit a least squares model using only the \\(M\\) transformed variables as predictors.\nLet \\(Z_1,\\dots,Z_M\\) represent \\(M\\leq p\\) linear combinations of our original \\(p\\) predictors, i.e. \\[\nZ_m = \\sum_{j=1}^p\\phi_{jm}X_{j},\\quad\\text{for}\\quad m=1,\\dots,M.\n\\tag{5.10}\\]\nGiven the new predictors \\(Z_1,\\dots,Z_M\\), we can fit the linear regression model \\[\ny_i = \\theta_0 +  \\sum_{m=1}^M\\theta_m z_{im} + \\epsilon_i,\\quad i=1,\\dots,n,\n\\tag{5.11}\\] where \\(z_{im}\\), \\(i=1,\\dots,n\\), are observed measurements from \\(Z_m,\\) based on observations \\(x_{ij}\\) from \\(X_j\\), and where \\(\\theta_0,\\dots,\\theta_M\\) are the (unknown) regression coefficients that we estimate using least squares.\nIf the constants \\(\\phi_{jm}\\) in Equation 5.10 are chosen wisely, then such dimension reduction (from \\(p\\) to \\(M\\)) approaches can outperform (lower test MSE) least squares regression.\nNotice that the linear combination of the \\(M\\) transformed predictors, \\(z_{i1},\\dots,z_{iM}\\), in Equation 5.11 can be rewritten as a linear combination of the \\(p\\) original predictors, \\(x_{i1},\\dots,x_{ip}\\): \\[\n\\begin{align*}\n\\sum_{m=1}^M\\theta_m z_{im}\n&= \\sum_{m=1}^M\\theta_m \\overbrace{\\left(\\sum_{j=1}^p\\phi_{jm}x_{ij}\\right)}^{z_{im}}\\\\\n&= \\sum_{j=1}^p\\underbrace{\\left(\\sum_{m=1}^M\\theta_m \\phi_{jm}\\right)}_{=\\beta^M_j}x_{ij}.\n\\end{align*}\n\\] Hereby, \\[\n\\beta^M_j = \\sum_{m=1}^M\\theta_m \\phi_{jm}\n\\tag{5.12}\\] serves as a certain regularized version of the coefficients \\(\\beta_j\\) in the original linear model Equation 5.1.\n\nIf \\(M=p\\) (no dimension reduction), and if all \\(M\\) many \\((z_{1m},\\dots,z_{nm})',\\) \\(m=1,\\dots,M,\\) vectors are linearly independent from each other (i.e. no redundant vectors), then Equation 5.12 poses no constraints and \\(\\beta_j^M=\\beta_j.\\)\nIf \\(M<p\\) (dimension reduction), Equation 5.12 serves to constrain the estimated \\(\\beta_j\\) coefficients \\((\\beta_j^M\\neq \\beta_j).\\)\n\nAll dimension reduction methods work in two steps.\n\nThe transformed predictors \\(Z_1,Z_2,\\dots,Z_M\\) are obtained.\nThe model is fit using these \\(M\\leq p\\) predictors.\n\nHowever, the choice of \\(Z_1,Z_2,\\dots,Z_M,\\) or equivalently the selection of the \\(\\phi_m\\)’s, can be achieved in different ways.\nThe best known dimension reduction approach is principal components regression.\n\n\n(Ch. 6.3.1) Principal Components Regression\nPrincipal components regression regression uses principal components to derive new (low dimensional) predictors. Thus, in a first step we need to discuss principal components analysis (PCA) to construct principal components.\n\nPrincipal Component Analysis (PCA)\nReading: Chapter 12.2.1 of our textbook.\nPrincipal components analysis is a popular approach for deriving a low-dimensional \\((M<p)\\) set of features \\(Z_1,Z_2,\\dots,Z_M\\) from a large set of variables \\(X_1,X_2,\\dots,X_p.\\)\nThe first principal component (or better principal component score) of a set of variables \\(X_1,X_2,\\dots,X_p\\) is the normalized linear combination \\[\nZ_1 = \\phi_{11}X_{1} + \\dots + \\phi_{p1}X_{p},\n\\] that maximizes the variance of \\(Z_1.\\) By normalized, we mean that the \\(\\ell_2\\) norm of \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) must equal one, i.e. \\[\n\\sum_{j=1}^p\\phi_{j1}^2=||\\phi_1||_2^2=1;\n\\] otherwise we could make the variance of \\(Z_1\\) arbitrarily large by simply by choosing \\(\\phi_1\\) such that \\(||\\phi_1||_2^2\\) is large.\nThe vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) is called the principal component loadings vector. An in absolute values large loading parameter \\(\\phi_{1j}\\) means that the \\(j\\)th predictor \\(X_j\\) contributes much to the first principal component score \\(Z_1.\\)\nTo estimate the coefficients \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})',\\) we need observed measurements of the features \\(X_1,\\dots,X_p.\\) We collect these observed measurements in a \\(n\\times p\\) dimensional data matrix \\(\\mathbf{X},\\) such that \\[\n\\mathbf{X} = \\left[\n    \\begin{matrix}\n    x_{11}&x_{12}&\\dots&x_{1p}\\\\\n    \\vdots&\\vdots&\\vdots&\\vdots\\\\\n    x_{n1}&x_{n2}&\\dots&x_{np}\\\\\n    \\end{matrix}\n\\right],\n\\] where the columns in \\(\\mathbf{X}\\) are centered. That is, the \\(j\\)th data column in \\(X\\) consists of centered data points\n\\[x_{ij}=x^{orig}_{ij}-\\bar{x}^{orig}_j\n\\quad\\text{with}\\quad\n\\bar{x}^{orig}_j = \\frac{1}{n}\\sum_{i=1}^nx_{ij}^{orig}\n\\] such that the sample mean of each column in \\(X\\) is zero, i.e. \\[\n\\bar{x}_{j} = \\frac{1}{n}\\sum_{i=1}^nx_{ij}=0\n\\] for each \\(j=1,\\dots,p.\\)\nWith centered observations \\(x_{ij}\\), the observed linear combinations \\[\nz_{i1} = \\phi_{11}x_{i1} + \\dots + \\phi_{p1}x_{ip},\n\\] become centered too, since \\[\n\\begin{align*}\n\\bar{z}_1\n&=\\frac{1}{n}\\sum_{i=1}^nz_{i1} \\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\phi_{11}x_{i1} + \\dots + \\phi_{p1}x_{ip}\\right)\\\\\n&= \\phi_{11} \\frac{1}{n}\\sum_{i=1}^n x_{i1} + \\dots + \\phi_{p1} \\frac{1}{n}\\sum_{i=1}^nx_{ip}\\\\\n& = \\phi_{11}\\bar{x}_{1} + \\dots + \\phi_{p1}\\bar{x}_{p} = 0.\n\\end{align*}\n\\]\nTherefore, the formula for the sample variance of \\(z_{i1}\\), \\(i=1,\\dots,n\\), simplifies as following: \\[\n\\frac{1}{n}\\sum_{i=1}^n\\left(z_{i1}-\\bar{z}_1\\right)^2=\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2\n\\tag{5.13}\\]\nTo determine the first principal component scores \\[\nz_{11},\\dots,z_{n1},\n\\] we need to find that loading vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) that maximizes the sample variance \\[\n\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(\\phi_{11}x_{i1} + \\dots + \\phi_{p1}x_{ip}\\right)^2\n\\] subject to the side constraint that \\(||\\phi_1||_2^2=1.\\) In other words, the first principal component loading vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\) is determined as the solution of the optimization problem \\[\n\\max_{\\phi_{11},\\dots,\\phi_{p1}}\\underbrace{\\left\\{\\frac{1}{n}\\sum_{i=1}^n\\left(\\sum_{j=1}^p\\phi_{j1}x_{ij}\\right)^2\\right\\}}_{=\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2}\\quad\\text{s.t.}\\quad\\sum_{j=1}^p\\phi_{j1}^2=1,\n\\] where \\(\\frac{1}{n}\\sum_{i=1}^nz_{i1}^2\\) equals the sample variance of \\(z_{11},\\dots,z_{n1}\\) (see Equation 5.13).\nThere is a nice geometric interpretation for the first principal component (likewise for the further principal components). The first loading vector \\[\n\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\n\\] defines a direction vector in the feature space along which the data points vary the most. If we project each of the \\(n\\) many \\(p\\)-dimensional data points \\[\n\\begin{align*}\nx_1 &=(x_{11},\\dots,x_{1p})'\\\\\n    &\\; \\vdots \\\\\nx_n &=(x_{n1},\\dots,x_{np})'\n\\end{align*}\n\\] onto the direction vector \\(\\phi_1=(\\phi_{11},\\dots,\\phi_{p1})'\\), the projected values are the principal component scores \\(z_{11},\\dots,z_{n1}\\) themselves, i.e \\[\n\\begin{align*}\nz_{11} & = \\phi_1'x_1 = \\sum_{j=1}^p \\phi_{j1}x_{1j}\\\\\n&\\;\\;\\;\\vdots \\\\\nz_{n1} & = \\phi_1'x_n = \\sum_{j=1}^p \\phi_{j1}x_{nj}.\n\\end{align*}\n\\]\nThe case \\(p=2\\) is simple to visualize. Figure 6.14 displays the direction of the first principal component loading vector \\(\\phi_1\\) (green solid line) on an advertising data set. \nThe left panel in Figure 6.15 displays the projection of the \\(i\\)th, \\(i=1,\\dots,n,\\) data vectors \\(x_i=(x_{i1},x_{i2})'\\) (purple circles 🟣) onto the first loading vector \\(\\phi_1=(\\phi_{11},\\phi_{21})'\\) leading to the \\(i\\)th principal component score \\(z_{i1},\\) \\(i=1,\\dots,n,\\) (black crosses \\(\\mathbf{\\times}\\).\n\n\n\nHigher order principal components\nAfter the first principal component of the features has been determined, we can find the second principal component. The second principal component is the linear combination of \\(X_1,X_2,\\dots,X_p\\) that has maximal variance out of all linear combinations that are uncorrelated with \\(Z_1.\\)\nThus, the second principal component scores \\(z_{12},\\dots,z_{np}\\) take the form \\[\nz_{i2} = \\phi_2'x_i = \\sum_{j=1}^p \\phi_{j2}x_{ij},\n\\] where \\(\\phi_2=(\\phi_{12},\\dots,\\phi_{p2})'\\) is the second principal component loading vector with \\(||\\phi_2||_2^2=1.\\)\nConstraining \\(Z_2\\) to be uncorrelated with \\(Z_1\\) is equivalent to constraining \\(\\phi_2\\) to be orthogonal to \\(\\phi_1\\), i.e. the inner product of \\(\\phi_1\\) and \\(\\phi_2\\) must be zero, \\[\n\\phi_2'\\phi_1=\\sum_{j=1}^p\\phi_{j2}\\phi_{j1}=0.\n\\] Therefore, the loading vector \\(\\phi_2=(\\phi_{12},\\dots,\\phi_{p2})'\\) is determined by the solution of \\[\n\\begin{align*}\n\\max_{\\phi_{12},\\dots,\\phi_{p2}}&\\underbrace{\\left\\{\\frac{1}{n}\\sum_{i=1}^n\\left(\\sum_{j=1}^p\\phi_{j2}x_{ij}\\right)^2\\right\\}}_{=\\frac{1}{n}\\sum_{i=1}^nz_{i2}^2}\\\\\n\\text{such that}& \\quad \\sum_{j=1}^p\\phi_{j2}^2=1\\quad\\\\\n\\text{and}& \\quad \\sum_{j=1}^p\\phi_{j1}\\phi_{j2}=0.\n\\end{align*}\n\\] Correspondingly, the \\(m\\)th \\((m=1,\\dots,M)\\) loading vector \\(\\phi_m=(\\phi_{1m},\\dots,\\phi_{pm})'\\) is determined by the solution of \\[\n\\begin{align*}\n\\max_{\\phi_{1m},\\dots,\\phi_{pm}}&\\underbrace{\\left\\{\\frac{1}{n}\\sum_{i=1}^n\\left(\\sum_{j=1}^p\\phi_{jm}x_{ij}\\right)^2\\right\\}}_{=\\frac{1}{n}\\sum_{i=1}^nz_{im}^2}\\\\\n\\text{such that}&\\quad\\sum_{j=1}^p\\phi_{jm}^2=1\\quad\\\\\n\\text{and}&\\quad \\sum_{j=1}^p\\phi_{j\\ell}\\phi_{jm}=0\\quad\\text{for all}\\quad 0\\leq \\ell<m,\n\\end{align*}\n\\] with \\(\\phi_0=(\\phi_{10},\\dots,\\phi_{p0})'=(0,\\dots,0)'.\\)\nAgain, the case \\(p=2\\) is simple to visualize. If \\(p=2\\) there are only \\(M=2\\) principal components. As shown in Figure 6.14, the second principal components loading vector \\(\\phi_2\\) (blue dashed line) is orthogonal to the first loading vector \\(\\phi_1\\) (green solid line). Thus when rotating the coordinate system such that the direction of the first loading vector becomes the x-axis, then the direction of the second loading vector becomes the the orthogonal y-axis (see right panel of Figure 6.15).\n\n\n\nThe Principal Components Regression Approach\nThe principal components regression (PCR) approach fits the model in Equation 5.11 which we repeat here for convenience: \\[\ny_i = \\theta_0 +  \\sum_{m=1}^M\\theta_m z_{im} + \\epsilon_i,\n\\] where \\(z_{im}\\) denotes the \\(i\\)th \\((i=1,\\dots,n)\\) observation of the \\(m\\)th \\((m=1,\\dots,M)\\) principal component score, and where the number of principal components \\(M\\) acts as a tuning parameter.\nThe key idea is that often a small number \\(M\\ll p\\) of principal components suffice to explain most of the variability in the data \\(\\mathbf{X}.\\) Therefore, often a relatively low number of principal components \\(M\\ll p\\) suffices to achieve very good predictions (low test MSE).\nGenerally, fitting a least squares model to \\(z_{i1},\\dots,z_{iM},\\) \\(i=1,\\dots,n,\\) leads to better results than fitting a least squares model to \\(x_{i1},\\dots,x_{ip},\\) particularly when \\(p\\approx n,\\) since typically most of the information in the data \\(\\mathbf{X}\\) that relates to the response is contained in \\(z_{i1},\\dots,z_{iM},\\) \\(i=1,\\dots,n,\\) and by estimating only \\(M\\ll p\\) coefficients we can mitigate overfitting (high variance).\nFigure 6.18 displays the PCR fits on the simulated data sets from Figures 6.8 and 6.9.\n\nRecall that both data sets were generated using \\(n = 50\\) observations and \\(p = 45\\) predictors. However, while the response in the first data set was a function of all the \\(p=45\\) predictors (Figure 6.8), the response in the second data set was generated using only \\(p=2\\) of the predictors (Figure 6.9).\n\n\nThe curves in Figure 6.18 are plotted as a function of \\(M,\\) the number of principal components used as predictors in the regression model.\n\nFor \\(M = p\\), PCR is equivalent to a classic least squares fit using all of the original predictors\nlarge \\(M\\) values lead to flexible models with low bias, but large variance\nsmall \\(M\\) values lead to inflexible models with large bias, but low variance\n\nThe latter two points lead to the typical U-shape of the test MSE.\nFigure 6.18 indicates that PCR based on an appropriate choice of \\(M\\) can substantially reduce the test MSE in comparison to classic least squares regression. In PCR, the number of principal components, \\(M,\\) is typically chosen by cross-validation.\nStandardize the predictor variables. When performing PCR, it is generally a good idea to standardize each predictor, using Equation 5.5, prior to generating the principal components. Standardization ensures that all variables are on the same scale. In the absence of standardization, the high-variance predictors will tend to play a larger role in the principal components obtained, and thus the scale on which the variables are measured will ultimately have an effect on the final PCR model."
  },
  {
    "objectID": "Ch5_LinModSelectRegul.html#r-lab-linear-model-selection-and-regularization",
    "href": "Ch5_LinModSelectRegul.html#r-lab-linear-model-selection-and-regularization",
    "title": "5  Linear Model Selection and Regularization",
    "section": "5.1 R-Lab: Linear Model Selection and Regularization",
    "text": "5.1 R-Lab: Linear Model Selection and Regularization\n\n5.1.1 Subset Selection Methods\n\n5.1.1.1 Best Subset Selection\nHere we apply the best subset selection approach to the Hitters data. We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.\nFirst of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements.\n\nlibrary(ISLR2) # load library 'ISLR2' (contains the data)\nnames(Hitters) # check variable names of the Hitters data set \n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)   # check sample size n and number of predictors p\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary)) # number of missing Salary observations\n\n[1] 59\n\n\nHence we see that Salary is missing for \\(59\\) players. The na.omit() function removes all of the rows that have missing values in any variable.\n\nHitters <- na.omit(Hitters) # remove all rows containing missing data points \ndim(Hitters)                # check sample size n and number of predictors p\n\n[1] 263  20\n\nsum(is.na(Hitters))         # no missing values anymore \n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.\n\nlibrary(leaps)\nregfit_full <- regsubsets(Salary ~ ., \n                          nvmax  = 8,           # largest number of predictors\n                          nbest  = 1,           # number of subsets of each size to record\n                          method = \"exhaustive\",# Best Subset Selection\n                          data   = Hitters)\nsummary(regfit_full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., nvmax = 8, nbest = 1, method = \"exhaustive\", \n    data = Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nAn asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Next, we fit up to a 19-variable model.\n\nregfit_full <- regsubsets(Salary ~ ., \n                          nvmax  = 19,          # largest number of predictors\n                          nbest  = 1,           # number of subsets of each size to record\n                          method = \"exhaustive\",# Best Subset Selection\n                          data   = Hitters)\nreg_summary <- summary(regfit_full)\n\nThe summary() function also returns \\(R^2\\), RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC. We can examine these to try to select the best overall model.\n\nnames(reg_summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nFor instance, we see that the \\(R^2\\) statistic increases from \\(32\\,\\%\\), when only one variable is included in the model, to almost \\(55\\,\\%\\), when all variables are included. That is, as expected, the \\(R^2\\) statistic increases monotonically as more variables are included. (Equivalently, RSS decreases monotonically as more variables are includes.)\n\nreg_summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nPlotting RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC for all of the models at once will help us decide which model to select. Note the type = \"l\" option tells R to connect the plotted points with lines.\n\npar(mfrow = c(1, 2))\nplot(reg_summary$rss, xlab = \"Number of Variables\",\n    ylab = \"RSS\", type = \"l\")\nplot(reg_summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\n\n\n\n\nThe points() command works like the plot() command, except that it puts points on a plot that has already been created, instead of creating a new plot. The which.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted \\(R^2\\) statistic.\n\nwhich.max(reg_summary$adjr2)\n\n[1] 11\n\nplot(reg_summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\npoints(11, reg_summary$adjr2[11], col = \"red\", cex = 2, \n    pch = 20)\n\n\n\n\nIn a similar fashion we can plot the \\(C_p\\) and BIC statistics, and indicate the models with the smallest statistic using which.min().\n\nplot(reg_summary$cp, xlab = \"Number of Variables\",\n    ylab = \"Cp\", type = \"l\")\nwhich.min(reg_summary$cp)\n\n[1] 10\n\npoints(10, reg_summary$cp[10], col = \"red\", cex = 2,\n    pch = 20)\n\n\n\nwhich.min(reg_summary$bic)\n\n[1] 6\n\nplot(reg_summary$bic, xlab = \"Number of Variables\",\n    ylab = \"BIC\", type = \"l\")\npoints(6, reg_summary$bic[6], col = \"red\", cex = 2,\n    pch = 20)\n\n\n\n\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, \\(C_p\\), adjusted \\(R^2\\), or AIC. To find out more about this function, type ?plot.regsubsets. The grey-shading represents a color-code for the selected information criterion.\n\nplot(regfit_full, scale = \"r2\")\n\n\n\nplot(regfit_full, scale = \"adjr2\")\n\n\n\nplot(regfit_full, scale = \"Cp\")\n\n\n\nplot(regfit_full, scale = \"bic\")\n\n\n\n\nThe top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to \\(-150\\). However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts.\nWe can use the coef() function to see the coefficient estimates associated with this model. The argument id selects the model with the id-many best predictors. \n\ncoef(regfit_full, id = 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n5.1.1.2 Forward and Backward Stepwise Selection\nWe can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method = \"forward\" or method = \"backward\".\n\nregfit.fwd <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\nregfit.bwd <- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\nFor instance, we see that using forward stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.\n\ncoef(regfit_full, id = 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd,  id = 7)\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\ncoef(regfit.bwd,  id = 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n5.1.1.3 Choosing Among Models Using the Validation-Set Approach and Cross-Validation\n\nValidation-Set Approach\nWe just saw that it is possible to choose among a set of models of different sizes using \\(C_p\\), BIC, and adjusted \\(R^2\\). We will now consider how to do this using the validation set and cross-validation approaches.\nIn order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting—including variable selection. Therefore, the determination of which model of a given size is best must be made using only the training observations. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\nIn order to use the validation set approach, we begin by splitting the observations into a training set and a test set. We do this by creating a random vector, train, of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. The vector test has a TRUE if the observation is in the test set, and a FALSE otherwise. Note the ! in the command to create test causes TRUEs to be switched to FALSEs and vice versa. We also set a random seed so that the user will obtain the same training/test set split.\n\nset.seed(1)\ntrain <- sample(x       = c(TRUE, FALSE), \n                size    = nrow(Hitters),\n                replace = TRUE)\ntest  <- (!train)\n\nNow, we apply regsubsets() to the training set in order to perform best subset selection.\n\nregfit.best <- regsubsets(Salary ~ .,\n                          data  = Hitters[train, ], \n                          nvmax = 19)\n\nNotice that we subset the Hitters data frame directly in the call in order to access only the training subset of the data, using the expression Hitters[train, ]. We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.\n\ntest.mat <- model.matrix(Salary ~ ., data = Hitters[test, ])\n\nThe model.matrix() function is used in many regression packages for building an \\(X\\) matrix from data. Now we run a loop, and for each size i, we extract the coefficients from regfit.best for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.\n\nval.errors <- rep(NA, 19)\nfor (i in 1:19){\n coefi         <- coef(regfit.best, id = i)\n pred          <- test.mat[, names(coefi)] %*% coefi\n val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)\n}\n\nWe find that the best model is the one that contains seven variables.\n\nval.errors\n\n [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4\n [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4\n[17] 141767.4 142339.6 142238.2\n\nwhich.min(val.errors)\n\n[1] 7\n\ncoef(regfit.best, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\nThis was a little tedious, partly because there is no predict() method for regsubsets(). Since we will be using this function again, we can capture our steps above and write our own predict method.\n\n predict.regsubsets <- function(object, newdata, id, ...) {\n    ## extract regression formula used in regsubsets()\n    form  <- as.formula(object$call[[2]])\n    ## build up the model matrix X of all predictors\n    mat   <- model.matrix(form, newdata)\n    ## extract the coefficients of the best model with 'id' many predictors\n    coefi <- coef(object, id = id)\n    ## ... and the names of those coefficients\n    xvars <- names(coefi)\n    ## compute the predicted values \n    mat[, xvars] %*% coefi\n }\n\nOur function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to regsubsets(). We demonstrate how we use this function below, when we do cross-validation.\nFinally, we perform best subset selection on the full data set, and select the best seven-variable model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates. Note that we perform best subset selection on the full data set and select the best seven-variable model, rather than simply using the variables that were obtained from the training set, because the best seven-variable model on the full data set may differ from the corresponding model on the training set.\n\nregfit.best <- regsubsets(Salary ~ ., \n                          data  = Hitters,\n                          nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\nIn fact, we see that the best seven-variable model on the full data set has a different set of variables than the best seven-variable model on the training set.\n\n\nCross-Validation\nWe now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy. First, we create a vector that allocates each observation to one of \\(k=10\\) folds, and we create a matrix in which we will store the results.\n\nset.seed(1)\n\nk         <- 10\nn         <- nrow(Hitters)\nfolds     <- sample(rep(1:k, length = n))\ncv.errors <- matrix(data     = NA, \n                    nrow     = k, \n                    ncol     = 19,\n                    dimnames = list(NULL, paste(1:19)))\n\nNow we write a for loop that performs cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors. Note that in the following code R will automatically use our predict.regsubsets() function when we call predict() because the best.fit object has class regsubsets.\n\nfor (j in 1:k) {\n  best.fit <- regsubsets(Salary ~ .,\n                         data = Hitters[folds != j, ],\n                         nvmax = 19)\n  for (i in 1:19) {\n    pred            <- predict.regsubsets(best.fit, \n                                 Hitters[folds == j, ], \n                                 id = i)\n    cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2)\n   }\n }\n\nThis has given us a \\(10 \\times 19\\) matrix of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\nmean.cv.errors <- apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\npar(mfrow = c(1, 1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\nWe see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.\n\nreg.best <- regsubsets(Salary ~ ., \n                       data  = Hitters,\n                       nvmax = 19)\ncoef(reg.best, 10)\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 \n        CRBI       CWalks    DivisionW      PutOuts      Assists \n   0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680 \n\n\n\n\n\n\n5.1.2 Ridge Regression and the Lasso\nWe will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far in this book. In particular, we must pass in an x matrix as well as a y vector, and we do not use the {} syntax. We will now perform ridge regression and the lasso in order to predict Salary on the Hitters data. Before proceeding ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nx <- model.matrix(Salary ~ ., Hitters)[, -1]\ny <- Hitters$Salary\n\nThe model.matrix() function is particularly useful for creating x; not only does it produce a matrix corresponding to the \\(19\\) predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs.\n\n5.1.2.1 Ridge Regression\nThe glmnet() function has an alpha argument that determines what type of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit. We first fit a ridge regression model.\n\nlibrary(\"glmnet\")\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-4\n\ngrid      <- 10^seq(10, -2, length = 100)\nridge.mod <- glmnet(x, y, \n                    alpha  = 0, \n                    lambda = grid)\n\nBy default the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. However, here we have chosen to implement the function over a grid of values ranging from \\(\\lambda=10^{10}\\) to \\(\\lambda=10^{-2}\\), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of \\(\\lambda\\) that is not one of the original grid values. Note that by default, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize = FALSE.\nAssociated with each value of \\(\\lambda\\) is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a \\(20 \\times 100\\) matrix, with \\(20\\) rows (one for each predictor, plus an intercept) and \\(100\\) columns (one for each value of \\(\\lambda\\)).\n\ndim(coef(ridge.mod))\n\n[1]  20 100\n\n\nWe expect the coefficient estimates to be much smaller, in terms of \\(\\ell_2\\) norm, when a large value of \\(\\lambda\\) is used, as compared to when a small value of \\(\\lambda\\) is used. These are the coefficients when \\(\\lambda=11498\\), along with their \\(\\ell_2\\) norm:\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\nsqrt(sum(coef(ridge.mod)[-1, 50]^2))\n\n[1] 6.360612\n\n\nIn contrast, here are the coefficients when \\(\\lambda=705\\), along with their \\(\\ell_2\\) norm. Note the much larger \\(\\ell_2\\) norm of the coefficients associated with this smaller value of \\(\\lambda\\).\n\nridge.mod$lambda[60]\n\n[1] 705.4802\n\ncoef(ridge.mod)[, 60]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 \n      Errors   NewLeagueN \n -0.70358655   8.61181213 \n\nsqrt(sum(coef(ridge.mod)[-1, 60]^2))\n\n[1] 57.11001\n\n\nWe can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of \\(\\lambda\\), say \\(50\\):\n\npredict(ridge.mod, s = 50, type = \"coefficients\")[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n 4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 \n          RBI         Walks         Years        CAtBat         CHits \n 8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n 6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 \n\n\nWe now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a data set. The first is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to TRUE for the training data. The second is to randomly choose a subset of numbers between \\(1\\) and \\(n\\); these can then be used as the indices for the training observations. The two approaches work equally well. We used the former method in Section 6.5.1. Here we demonstrate the latter approach.\nWe first set a random seed so that the results obtained will be reproducible.\n\nset.seed(1)\ntrain  <- sample(1:nrow(x), nrow(x) / 2)\ntest   <- (-train)\ny.test <- y[test]\n\nNext we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using \\(\\lambda=4\\). Note the use of the predict() function again. This time we get predictions for a test set, by replacing type=\"coefficients\" with the newx argument.\n\nridge.mod <- glmnet(x[train, ], y[train], \n                    alpha  = 0,\n                    lambda = grid, \n                    thresh = 1e-12)\nridge.pred <- predict(ridge.mod, \n                      s    = 4, \n                      newx = x[test, ])\n## test MSE:                       \nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\nThe test MSE is \\(142{,}199\\). Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:\n\nmean((mean(y[train]) - y.test)^2)\n\n[1] 224669.9\n\n\nWe could also get the same (“just an intercept”) result by fitting a ridge regression model with a very large value of \\(\\lambda\\). Note that 1e10 means \\(10^{10}\\).\n\nridge.pred <- predict(ridge.mod, \n                      s    = 1e10, \n                      newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 224669.8\n\n\nSo fitting a ridge regression model with \\(\\lambda=4\\) leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with \\(\\lambda=4\\) instead of just performing least squares regression. Recall that least squares is simply ridge regression with \\(\\lambda=0\\).\nNote: In order for glmnet() to yield the exact least squares coefficients when \\(\\lambda=0\\), we use the argument exact = TRUE when calling the predict() function. Otherwise, the predict() function will interpolate over the grid of \\(\\lambda\\) values used in fitting the glmnet() model, yielding approximate results. When we use exact = TRUE, there remains a slight discrepancy in the third decimal place between the output of glmnet() when \\(\\lambda = 0\\) and the output of lm(); this is due to numerical approximation on the part of glmnet().\n\nridge.pred <- predict(ridge.mod, \n                      s     = 0, \n                      newx  = x[test, ],\n                      exact = TRUE, \n                      x     = x[train, ], # need to be supplied when exact = TRUE\n                      y     = y[train]    # need to be supplied when exact = TRUE\n                      )\nmean((ridge.pred - y.test)^2)\n\n[1] 168588.6\n\n## coefficient fits of lm\nlm(y ~ x, subset = train)\n\n\nCall:\nlm(formula = y ~ x, subset = train)\n\nCoefficients:\n(Intercept)       xAtBat        xHits       xHmRun        xRuns         xRBI  \n   274.0145      -0.3521      -1.6377       5.8145       1.5424       1.1243  \n     xWalks       xYears      xCAtBat       xCHits      xCHmRun       xCRuns  \n     3.7287     -16.3773      -0.6412       3.1632       3.4008      -0.9739  \n      xCRBI      xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists  \n    -0.6005       0.3379     119.1486    -144.0831       0.1976       0.6804  \n    xErrors  xNewLeagueN  \n    -4.7128     -71.0951  \n\n## Coefficient fits of glmnet with exact == TRUE\npredict(ridge.mod, \n        s     = 0, \n        exact = TRUE, \n        type  = \"coefficients\",\n        x     = x[train, ], # need to be supplied when exact = TRUE\n        y     = y[train]    # need to be supplied when exact = TRUE\n        )[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 274.0200994   -0.3521900   -1.6371383    5.8146692    1.5423361    1.1241837 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n   3.7288406  -16.3795195   -0.6411235    3.1629444    3.4005281   -0.9739405 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  -0.6003976    0.3378422  119.1434637 -144.0853061    0.1976300    0.6804200 \n      Errors   NewLeagueN \n  -4.7127879  -71.0898914 \n\n\nIn general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients.\nIn general, instead of arbitrarily choosing \\(\\lambda=4\\), it would be better to use cross-validation to choose the tuning parameter \\(\\lambda\\). We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs 10-fold cross-validation, though this can be changed using the argument nfolds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random.\n\nset.seed(1)\n\n## Only on the train data to have comparison \n## with the above chosen lambda values (i.e., 4 and 1e10)\ncv.out  <- cv.glmnet(x[train, ], \n                     y[train], \n                     alpha = 0)\nplot(cv.out)\n\n\n\nbestlam <- cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\nTherefore, we see that the value of \\(\\lambda\\) that results in the smallest cross-validation error is \\(326\\). What is the test MSE associated with this value of \\(\\lambda\\)?\n\nridge.pred <- predict(ridge.mod, \n                      s    = bestlam,\n                      newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\nThis represents a further improvement over the test MSE that we got using \\(\\lambda=4\\). Finally, we refit our ridge regression model on the full data set, using the value of \\(\\lambda\\) chosen by cross-validation, and examine the coefficient estimates.\n\nout <- glmnet(x, \n              y, \n              alpha = 0)\npredict(out, \n        type = \"coefficients\", \n        s    = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\nAs expected, none of the coefficients are zero—ridge regression does not perform variable selection!\n\n\n5.1.2.2 The Lasso\nWe saw that ridge regression with a wise choice of \\(\\lambda\\) can outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. Other than that change, we proceed just as we did in fitting a ridge model.\n\nlasso.mod <- glmnet(x[train, ], \n                    y[train], \n                    alpha  = 1,\n                    lambda = grid)\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\nWe can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.\n\nset.seed(1)\ncv.out     <- cv.glmnet(x[train, ], \n                        y[train], \n                        alpha = 1)\nplot(cv.out)\n\n\n\n## optimal lambda (according to CV)\nbestlam    <- cv.out$lambda.min\n\nlasso.pred <- predict(lasso.mod, \n                      s    = bestlam, \n                      newx = x[test, ])\n## test MSE                      \nmean((lasso.pred - y.test)^2)\n\n[1] 143673.6\n\n\nThis is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with \\(\\lambda\\) chosen by cross-validation.\nHowever, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with \\(\\lambda\\) chosen by cross-validation contains only eleven variables.\n\nout        <- glmnet(x, y, \n                     alpha  = 1, \n                     lambda = grid)\n\nlasso.coef <- predict(out, \n                      type = \"coefficients\", \n                      s    = bestlam)[1:20, ]\n\nlasso.coef\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\nlasso.coef[lasso.coef != 0]\n\n  (Intercept)         AtBat          Hits         Walks         Years \n   1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 \n       CHmRun         CRuns          CRBI       LeagueN     DivisionW \n   0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 \n      PutOuts        Errors \n   0.23752385   -0.85629148 \n\n\n\n\n\n5.1.3 Principal Components Regression\nPrincipal components regression (PCR) can be performed using the pcr() function, which is part of the pls library. We now apply PCR to the Hitters data, in order to predict Salary. Again, we ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nset.seed(2)\n\npcr.fit <- pcr(Salary ~ ., \n               data       = Hitters, \n               scale      = TRUE,\n               validation = \"CV\")\n\nThe syntax for the pcr() function is similar to that for lm(), with a few additional options. Setting scale = TRUE has the effect of standardizing each predictor, using ( 6.6), prior to generating the principal components, so that the scale on which each variable is measured will not have an effect. Setting validation = \"CV\" causes pcr() to compute the ten-fold cross-validation error for each possible value of \\(M\\), the number of principal components used. The resulting fit can be examined using summary().\n\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452    351.9    353.2    355.0    352.8    348.4    343.6\nadjCV          452    351.6    352.7    354.4    352.1    347.6    342.7\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       345.5    347.7    349.6     351.4     352.1     353.5     358.2\nadjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        349.7     349.4     339.9     341.6     339.2     339.6\nadjCV     348.0     347.7     338.2     339.7     337.2     337.6\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n\nThe CV score is provided for each possible number of components, ranging from \\(M=0\\) onwards.\nNote: pcr() reports the root mean squared error; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of \\(352.8\\) corresponds to an MSE of \\(352.8^2=124{,}468\\).\nOne can also plot the cross-validation scores using the validationplot() function. Using val.type = \"MSEP\" will cause the cross-validation MSE to be plotted.\n\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\nWe see that the smallest cross-validation error occurs when \\(M=18\\) components are used. This is barely fewer than \\(M=19\\), which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs.\nHowever, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.\nThe summary() function also provides the percentage of variance explained in the predictors and in the response using different numbers of components. This concept is discussed in greater detail in Chapter 12. Briefly, we can think of this as the amount of information about the predictors or the response that is captured using \\(M\\) principal components. For example, setting \\(M=1\\) only captures \\(38.31\\,\\%\\) of all the variance, or information, in the predictors. In contrast, using \\(M=5\\) increases the value to \\(84.29\\,\\%\\). If we were to use all \\(M=p=19\\) components, this would increase to \\(100\\,\\%\\).\nWe now perform PCR on the training data and evaluate its test set performance.\n\nset.seed(1)\npcr.fit <- pcr(Salary ~ ., \n               data       = Hitters, \n               subset     = train, \n               scale      = TRUE, \n               validation = \"CV\")\n##                \nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\nNow we find that the lowest cross-validation error occurs when \\(M=5\\) components are used. We compute the test MSE as follows.\n\npcr.pred <- predict(pcr.fit, \n                    x[test, ], \n                    ncomp = 5)\nmean((pcr.pred - y.test)^2)\n\n[1] 142811.8\n\n\nThis test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates.\nFinally, we fit PCR on the full data set, using \\(M=5\\), the number of components identified by cross-validation.\n\npcr.fit <- pcr(y ~ x, \n               scale = TRUE, \n               ncomp = 5)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 5\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps  5 comps\nX    38.31    60.16    70.84    79.03    84.29\ny    40.63    41.58    42.17    43.22    44.90"
  },
  {
    "objectID": "Ch5_LinModSelectRegul.html#exercises",
    "href": "Ch5_LinModSelectRegul.html#exercises",
    "title": "5  Linear Model Selection and Regularization",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\nPrepare the following exercises of Chapter 6 in our course textbook ISLR:\n\nExercise 2\nExercise 4\nExercise 8\nExercise 10"
  },
  {
    "objectID": "Ch1_StatLearning.html#r-codes-of-this-chapter",
    "href": "Ch1_StatLearning.html#r-codes-of-this-chapter",
    "title": "1  Statistical Learning",
    "section": "R-Codes of this Chapter",
    "text": "R-Codes of this Chapter\n\nIntroduction to R: Ch1_0_Rcodes.R"
  },
  {
    "objectID": "Ch2_LinearRegression.html#simple-linear-regression",
    "href": "Ch2_LinearRegression.html#simple-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.1 Simple Linear Regression",
    "text": "2.1 Simple Linear Regression\nThe linear regression model assumes a linear relationship between \\(Y\\) and the predictor(s) \\(X\\).\nThe simple (only one predictor) linear regression model: \\[\nY\\approx \\beta_0 + \\beta_1 X\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV\n\n\n2.1.1 Estimating the Coefficients\nFor a given observed realization of the training data random sample \\[\n(x_1,y_1),\\dots,(x_n,y_n)\n\\] we choose \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) such that the Residual Sum of Squares criterion is minimized: \\[\n\\begin{align*}\n\\operatorname{RSS}\\equiv \\operatorname{RSS}(\\hat{\\beta}_0,\\hat{\\beta_1})\n& = e_1^2 + \\dots + e_n^2\\\\\n&=\\sum_{i=1}^n\\left(y_i - \\left(\\hat\\beta_0 + \\hat\\beta_1x_i\\right)\\right)^2\n\\end{align*}\n\\] The minimizers are \\[\n\\hat\\beta_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\] and \\[\n\\hat\\beta_0=\\bar{y} - \\hat\\beta_1\\bar{x},\n\\] where \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i\\) and \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i\\).\n\n\n\n\n2.1.2 Assessing the Accuracy of the Coefficient Estimates\nTrue unknown model \\[\nY=f(X)+\\epsilon\n\\]\nIn in linear regression analysis, we assume[^1] that \\[\nf(X) = \\beta_0 + \\beta_1 X\n\\tag{2.1}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe assumption \\(f(X) = \\beta_0 + \\beta_1 X\\) is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.\n\n\nIf the assumption of Equation 2.1 is correct, i.e., if the data is actually generated according to the model \\(f(X) = \\beta_0 + \\beta_1 X,\\) then ordinary least squares estimators \\[\n\\hat\\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\n\\] are unbiased, that is \\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat\\beta_0)&=E(\\hat\\beta_0)-\\beta_0=0\\\\\n\\operatorname{Bias}(\\hat\\beta_1)&=E(\\hat\\beta_1)-\\beta_1=0\n\\end{align*}\n\\] I.e., on average, the estimation results equal the true (unknown) parameters. However, in an actual data analysis, we only have one realization of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) computed from one give dataset and thus we cannot compute averages of estimation results. Each single estimation result will have estimation errors, i.e., \\[\n\\hat\\beta_0\\neq \\beta_0\\quad\\text{and}\\quad\\hat\\beta_1\\neq \\beta_1.\n\\]\nThe following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook ISLR.\n\n## ###############################\n## A function to generate data \n## similar to that shown in Fig 3.3\n## ##############################\n\n## A Function to simulate data\nmyDataGenerator <- function(){\n  n      <- 50                           # sample size\n  beta_0 <- 0.1                          # intercept parameter\n  beta_1 <- 5                            # slope parameter\n  X      <- runif(n, min = -2, max = 2)  # predictor\n  error  <- rnorm(n, mean = 0, sd = 8.5) # error term\n  Y      <- beta_0 + beta_1 * X + error  # outcome \n  ##\n  return(data.frame(\"Y\" = Y, \"X\" = X))\n}\n\n## Generate a first realization of the data\nset.seed(123)\ndata_sim <- myDataGenerator()\nhead(data_sim)\n\n            Y          X\n1 -18.4853427 -0.8496899\n2  12.9872926  1.1532205\n3  -0.4167901 -0.3640923\n4  -1.9138159  1.5320696\n5  19.5667725  1.7618691\n6  -5.3639241 -1.8177740\n\n\nUsing repeated samples form the data generating process defined in myDataGenerator(), we can generate multiple estimation results of the unknown simple linear regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) and plot the corresponding empirical regression lines:\n\n## Estimation\nlm_obj <- lm(Y ~ X, data = data_sim)\n\n## Plotting the results\n\n## True (usually unknown) parameter values\nbeta_0 <- 0.1  # intercept parameter\nbeta_1 <- 5    # slope parameter\n\npar(mfrow=c(1,2)) # Two plots side by side\n\n## First Plot (fit for the first realization of the data)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\")\nabline(a = beta_0, b = beta_1, col = \"red\")\nabline(lm_obj, col = \"blue\")\n\n## Second Plot (fits for multiple data realizations)\nplot(x = data_sim$X, y = data_sim$Y, xlab = \"X\", ylab = \"Y\", type = \"n\") # type = \"n\": empty plot\n##\nfor(r in 1:10){\n  data_sim_new <- myDataGenerator()\n  lm_obj_new   <- lm(Y ~ X, data=data_sim_new)\n  abline(lm_obj_new, col = \"lightskyblue\")\n}\n## Adding the first fit\nabline(a = beta_0, b = beta_1, col = \"red\", lwd = 2)\nabline(lm_obj, col = \"blue\", lwd = 2)\n\n\n\n\n\nThe magnitude of the estimation errors is expressed in unites of standard errors: \\[\n\\operatorname{SE}(\\hat\\beta_0)=\\sqrt{\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right]}\n\\] and \\[\n\\operatorname{SE}(\\hat\\beta_1)=\\sqrt{\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}},\n\\] where \\[\nVar(\\epsilon)=\\sigma^2.\n\\]\nTypically, \\(\\sigma^2\\) \\[\n\\sigma = \\operatorname{SD}(\\epsilon)=\\sqrt{Var(\\epsilon)}=\\sigma\n\\] is unknown, but can be estimated by \\[\\begin{align*}\n\\sigma\\approx\\hat{\\sigma}=\\operatorname{RSE}\n&=\\sqrt{\\frac{1}{n-2}\\operatorname{RSS}}\\\\[2ex]\n&=\\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2},\n\\end{align*}\\] where \\[\n\\operatorname{RSS}=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\] are the residual sum of squares.\n\n\n\n\n\n\nNote\n\n\n\nWe subtract \\(2\\) from the sample size \\(n\\) since \\(n-2\\) are the remaining degrees of freedom in the data after estimating two parameters \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\n\n\nKnowing \\(\\operatorname{SE}(\\hat\\beta_0)\\) and \\(\\operatorname{SE}(\\hat\\beta_1)\\) allows us to construct 95% Confidence Intervals: \\[\n\\begin{align*}\n\\operatorname{CI}_{\\beta_1}\n&=\\left[\\hat{\\beta}_1-2\\operatorname{SE}(\\hat\\beta_1),\\;\n        \\hat{\\beta}_1+2\\operatorname{SE}(\\hat\\beta_1)\\right]\n\\end{align*}\n\\] likewise for \\(\\operatorname{CI}_{\\beta_1}\\).\nShort notation: \\[\n\\begin{align*}\n\\operatorname{CI}_{\\beta_1}\n&=\\hat\\beta_1\\pm 2\\operatorname{SE}(\\hat\\beta_1)\n\\end{align*}\n\\]\nInterpretation: There is approximately a 95% change (in resamplings) that the (random) confidence interval \\(\\operatorname{CI}_{\\beta_1}\\) contains the true (fix) parameter value \\(\\beta_1\\).\n\n\n\n\n\n\nNote\n\n\n\nThus, a given confidence interval either contains the true parameter value or not and we typically we do not know it. To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:\n\nInteractive visualization for interpreting confidence intervals\n\n\n\n\nConfidence Intervals for Statistical Hypothesis Testing\nWe can use a 95% confidence interval to do statistical hypothesis testing. Let us consider the following standard null \\((H_0)\\) and alternative hypothesis \\((H_1):\\) \\[\n\\begin{align*}\nH_0:&\\;\\text{There is no relationship between $Y$ and $X$; i.e. $\\beta_1=0$}\\\\\nH_1:&\\;\\text{There is a relationship between $Y$ and $X$; i.e. $\\beta_1\\neq 0$}\n\\end{align*}\n\\]\n\nIf the observed realization of the confidence interval \\[\n\\begin{align*}\n\\operatorname{CI}_{\\beta_1}\n&=\\left[\\hat{\\beta}_1-2\\operatorname{SE}(\\hat\\beta_1),\\;\n      \\hat{\\beta}_1+2\\operatorname{SE}(\\hat\\beta_1)\\right]\n\\end{align*}\n\\] contains the value \\(0,\\) we cannot reject the null hypothesis that \\(\\beta_1=0.\\)\nIf, however, the observed realization of the confidence interval does not contain the value \\(0,\\) we can reject the null hypothesis and adopt the alternative that \\(\\beta_1\\neq 0.\\)\n\n\n\n\n\n\n\nNote\n\n\n\nThe probability of doing a false decision (i.e. reject the null hypothesis even though the null hypothesis is true) is here \\(1-95\\%=5\\%.\\)\n\n\n\n\nTest Statistics\nStandard errors can also be used to do hypothesis testing based on test statistics. Let us again consider the standard null and alternative hypothesis: \\[\n\\begin{align*}\nH_0:&\\;\\text{There is no relationship between $Y$ and $X$; i.e. $\\beta_1=0$}\\\\\nH_1:&\\;\\text{There is a relationship between $Y$ and $X$; i.e. $\\beta_1\\neq 0$}\n\\end{align*}\n\\]\nThis statistical hypothesis test can be conducted using the \\(t\\)-test statistic: \\[\nt=\\frac{\\hat\\beta_1 - 0}{\\operatorname{SE}(\\hat\\beta_1)}\\overset{H_0}{\\sim}t_{(n-1)}\n\\]\n\\(p\\)-value\n\\[\\begin{align*}\np\n&=P_{H_0}\\left(|t|\\geq|t_{obs}|\\right)\\\\[2ex]\n&=2\\cdot\\min\\{P_{H_0}\\left(t\\geq t_{obs} \\right),\\; P_{H_0}\\left(t\\leq t_{obs} \\right)\\},\n\\end{align*}\\] where \\(t_{obs}\\) denotes the observed value of the \\(t\\)-test statistic and where \\(t\\) is \\(t\\)-distributed with \\((n-2)\\) degrees of freedom.\nSelect a significance level \\(\\alpha\\) (e.g. \\(\\alpha=0.01\\) or \\(\\alpha=0.05\\)).\n\nIf the observed realization of the \\(p\\)-value is larger than or equal to the significance level \\[\np\\geq \\alpha,\n\\] then we cannot reject the null hypothesis.\nIf, however, the observed realization of the \\(p\\)-value is strictly smaller than the significance level \\[\np<\\alpha,\n\\] then we can reject the null hypothesis and adopt the alternative hypothesis.\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, the probability of doing a false test decision (i.e. reject the null hypothesis even though the null hypothesis is true) is here \\(1-95\\%=5\\%.\\)\n\n\n\n\n\n\n2.1.3 Assessing the Accuracy of the Model\nIn tendency an accurate model has …\n\na low \\(\\operatorname{RSE}\\) \\[\n\\operatorname{RSE}=\\hat\\sigma=\\sqrt{\\frac{\\operatorname{RSS}}{n-2}}\n\\]\na high \\(R^2\\)\n\n\\[\nR^2=\\frac{\\operatorname{TSS}-\\operatorname{RSS}}{\\operatorname{TSS}}=1-\\frac{\\operatorname{RSS}}{\\operatorname{TSS}},\n\\]\nwhere \\(0\\leq R^2\\leq 1\\) and\n\\[\n\\begin{align*}\n\\operatorname{TSS}&=\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2\\\\\n\\operatorname{RSS}&=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2\\\\\n\\hat{y}_i&=\\hat\\beta_0+\\hat\\beta_1x_i\n\\end{align*}\n\\]\nCautionary Note Nr 1: Do not forget that there is a irreducible error \\(Var(\\epsilon)=\\sigma^2>0\\). Thus\n\nvery low \\(\\operatorname{RSE}\\) values, \\(\\operatorname{RSE}\\approx 0\\), and\nvery high \\(R^2\\) values, \\(R^2\\approx 1\\),\n\ncan be warning signals indicating overfitting.\nCautionary Note Nr 2: The above assessment is only based on training data. We have seen in Chapter 1 that we usually want to assess the model accuracy using testing data.\n\n\n\\(R^2\\) and correlation coefficient\nIn the case of the simple linear regression model, \\(R^2\\) equals the squared sample correlation coefficient between \\(Y\\) and \\(X\\), \\[\nR^2 = r^2,\n\\] where \\[\nr=\\widehat{cor}(Y,X)=\\frac{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}.\n\\]"
  },
  {
    "objectID": "Ch2_LinearRegression.html#multiple-linear-regression",
    "href": "Ch2_LinearRegression.html#multiple-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.2 Multiple Linear Regression",
    "text": "2.2 Multiple Linear Regression\nThe multiple linear regression model allows for more than only one predictor:\n\\[\nY\\approx \\beta_0 + \\beta_1 X_1 +  \\dots + \\beta_p X_p + \\epsilon\n\\]\nFor instance,\n\nsales \\(\\approx \\beta_0 + \\beta_1\\) TV \\(+\\beta_2\\) radio \\(+\\beta_3\\) newspaper \\(+\\epsilon\\)\n\n\n2.2.1 Estimating the Regression Coefficients\nFor a given observed realization of the training data random sample \\[\n(x_{11},\\dots,x_{1p},y_1),\\dots,(x_{n1},\\dots,x_{np},y_n)\n\\] select \\[\n\\hat\\beta_0,\\dots,\\hat\\beta_p\n\\] by minimizing \\[\n\\operatorname{RSS}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2,\n\\] where \\[\n\\hat{y}_i=\\hat\\beta_0 + \\hat\\beta_1 x_{i1} \\dots + \\hat\\beta_p x_{ip}\n\\]\nLet \\[\\begin{equation*}\nY=\\left(\\begin{matrix}y_1\\\\ \\vdots\\\\y_n\\end{matrix}\\right),\n\\end{equation*}\\] denote the \\((n\\times 1)\\) vector containing all response values of the training data and \\[\\begin{equation*}\nX=\\left(\\begin{matrix}x_{11}&\\dots&x_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ x_{n1}&\\dots&x_{nK}\\\\\\end{matrix}\\right)\n\\end{equation*}\\] denote the \\((n\\times K)\\) matrix containing all predictor values of the training data.\nThen \\[\n\\left(\\begin{matrix}\\hat\\beta_1\\\\ \\vdots \\\\ \\hat\\beta_K\\end{matrix}\\right)=\\hat{\\beta}=(X'X)^{-1}X'Y\n\\] denotes the \\((K\\times 1)\\) vector of parameter estimates.\n\n\nInterpretation of Multiple Linear Regressions and the Omitted Variable Bias\nMultiple linear regression is more than mere composition of single simple linear regression models.\nTake a look at the following two simple linear regression results:\n\nThus in separate simple linear regressions, the effects of radio and the effect of newspaper on sales are both (but separately) statistically.\nBy contrast, when looking at the multiple linear regression when regressing sales onto both radio and newspaper, only the effect of radio remains statistically significant:\n\nReason: Omitted Variable Bias\n\nradio has an effect on sales\nnewspaper has actually no effect on sales\nBut, newspaper is “strongly” correlated with radio (cor(newspaper,radio)=0.3541); see Table 3.5\n\n\n\nThus, when omitting radio from the multiple regression model, newspaper becomes a surrogate for radio. This is called a Omitted Variable Bias.\n\nConclusion: Simple linear regression can be dangerous. We need to control for all possibly relevant variables if we want to interpret the estimation results (“Inference”).\nInterpretation of the Coefficients in Table 3.5\nFor fixed values of TV and newspaper, spending additionally 1000 USD for radio, increases on average sales by approximately 189 units.\n\n\n\n(Ch. 3.2.2) Some Important Questions\n1. Is There a Relationship Between the Response and Predictors?\n\\[\n\\begin{align*}\nH_0:&\\;\\beta_1=\\beta_2=\\dots=\\beta_p=0\\\\\nH_1:&\\;\\text{at least one $\\beta_j\\neq 0$; $j=1,\\dots,p$}\n\\end{align*}\n\\]\n\\(F\\)-test statistic \\[\nF=\\frac{(\\operatorname{TSS}-\\operatorname{RSS})/p}{\\operatorname{\n  RSS}/(n-p-1)}\n\\]\nIf \\(H_0\\) is correct \\[\n\\begin{align*}\nE(\\operatorname{RSS}/(n-p-1))&=\\sigma^2\\\\\nE((\\operatorname{TSS}-\\operatorname{RSS})/p)&=\\sigma^2\\\\\n\\end{align*}\n\\]\n\nThus, if \\(H_0\\) is correct, we expect values of \\(F\\approx 1\\).\nBut if \\(H_1\\) is correct, we expect values of \\(F\\gg 1\\).\n\nCaution: Cannot be computed if \\(p>n\\). (Chapter 6 on “high dimensional problems”)"
  }
]