<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computer-Aided Statistical Analysis (B.Sc.) - 4&nbsp; Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch5_ResamplingMethods.html" rel="next">
<link href="./Ch3_LinearRegression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_StatLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_LinearRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_Classification.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_ResamplingMethods.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_LinModSelectRegul.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Model Selection and Regularization</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#lecture-notes" id="toc-lecture-notes" class="nav-link active" data-scroll-target="#lecture-notes"><span class="toc-section-number">4.1</span>  Lecture Notes</a></li>
  <li><a href="#ch.-4.1-an-overview-of-classification" id="toc-ch.-4.1-an-overview-of-classification" class="nav-link" data-scroll-target="#ch.-4.1-an-overview-of-classification">(Ch. 4.1) An Overview of Classification</a></li>
  <li><a href="#ch.-4.2-why-not-linear-regression" id="toc-ch.-4.2-why-not-linear-regression" class="nav-link" data-scroll-target="#ch.-4.2-why-not-linear-regression">(Ch. 4.2) Why Not Linear Regression?</a></li>
  <li><a href="#ch.-4.3-logistic-regression" id="toc-ch.-4.3-logistic-regression" class="nav-link" data-scroll-target="#ch.-4.3-logistic-regression">(Ch. 4.3) Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#ch.-4.3.1-the-logistic-model" id="toc-ch.-4.3.1-the-logistic-model" class="nav-link" data-scroll-target="#ch.-4.3.1-the-logistic-model">(Ch. 4.3.1) The Logistic Model</a></li>
  <li><a href="#ch.-4.3.2-estimating-the-regression-coefficients" id="toc-ch.-4.3.2-estimating-the-regression-coefficients" class="nav-link" data-scroll-target="#ch.-4.3.2-estimating-the-regression-coefficients">(Ch. 4.3.2) Estimating the Regression Coefficients</a></li>
  <li><a href="#ch.-4.3.3-making-predictions" id="toc-ch.-4.3.3-making-predictions" class="nav-link" data-scroll-target="#ch.-4.3.3-making-predictions">(Ch. 4.3.3) Making Predictions</a></li>
  <li><a href="#ch.-4.3.4-multiple-logistic-regression" id="toc-ch.-4.3.4-multiple-logistic-regression" class="nav-link" data-scroll-target="#ch.-4.3.4-multiple-logistic-regression">(Ch. 4.3.4) Multiple Logistic Regression</a></li>
  <li><a href="#ch.-4.3.5-multinomial-logistic-regression" id="toc-ch.-4.3.5-multinomial-logistic-regression" class="nav-link" data-scroll-target="#ch.-4.3.5-multinomial-logistic-regression">(Ch. 4.3.5) Multinomial Logistic Regression</a></li>
  </ul></li>
  <li><a href="#ch.-4.4-discriminant-analysis-generative-models-for-classification" id="toc-ch.-4.4-discriminant-analysis-generative-models-for-classification" class="nav-link" data-scroll-target="#ch.-4.4-discriminant-analysis-generative-models-for-classification">(Ch. 4.4) Discriminant Analysis: Generative Models for Classification</a>
  <ul class="collapse">
  <li><a href="#ch.-4.4.1-linear-discriminant-analysis-for-p-1" id="toc-ch.-4.4.1-linear-discriminant-analysis-for-p-1" class="nav-link" data-scroll-target="#ch.-4.4.1-linear-discriminant-analysis-for-p-1">(Ch. 4.4.1) Linear Discriminant Analysis for <span class="math inline">\(p = 1\)</span></a></li>
  <li><a href="#ch.-4.4.2-linear-discriminant-analysis-for-p-1" id="toc-ch.-4.4.2-linear-discriminant-analysis-for-p-1" class="nav-link" data-scroll-target="#ch.-4.4.2-linear-discriminant-analysis-for-p-1"><span class="toc-section-number">4.1.1</span>  (Ch. 4.4.2) Linear Discriminant Analysis for <span class="math inline">\(p &gt; 1\)</span></a></li>
  </ul></li>
  <li><a href="#r-lab-classification" id="toc-r-lab-classification" class="nav-link" data-scroll-target="#r-lab-classification"><span class="toc-section-number">4.2</span>  <code>R</code>-Lab: Classification</a>
  <ul class="collapse">
  <li><a href="#the-stock-market-data" id="toc-the-stock-market-data" class="nav-link" data-scroll-target="#the-stock-market-data"><span class="toc-section-number">4.2.1</span>  The Stock Market Data</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="toc-section-number">4.2.2</span>  Logistic Regression</a></li>
  <li><a href="#linear-discriminant-analysis" id="toc-linear-discriminant-analysis" class="nav-link" data-scroll-target="#linear-discriminant-analysis"><span class="toc-section-number">4.2.3</span>  Linear Discriminant Analysis</a></li>
  <li><a href="#quadratic-discriminant-analysis" id="toc-quadratic-discriminant-analysis" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis"><span class="toc-section-number">4.2.4</span>  Quadratic Discriminant Analysis</a></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes"><span class="toc-section-number">4.2.5</span>  Naive Bayes</a></li>
  <li><a href="#k-nearest-neighbors" id="toc-k-nearest-neighbors" class="nav-link" data-scroll-target="#k-nearest-neighbors"><span class="toc-section-number">4.2.6</span>  <span class="math inline">\(K\)</span>-Nearest Neighbors</a></li>
  <li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression"><span class="toc-section-number">4.2.7</span>  Poisson Regression</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">4.3</span>  Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="lecture-notes" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="lecture-notes"><span class="header-section-number">4.1</span> Lecture Notes</h2>
</section>
<section id="ch.-4.1-an-overview-of-classification" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="ch.-4.1-an-overview-of-classification">(Ch. 4.1) An Overview of Classification</h2>
<p>Classification problems occur often, perhaps even more so than regression problems.</p>
<p>Some examples include:</p>
<ol type="1">
<li>A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?</li>
<li>An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.</li>
<li>On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.</li>
</ol>
<p>One of the running example for this chapter: The (simulated) <code>Default</code> data set which is part of the <a href="https://www.statlearning.com/resources-second-edition">online resources</a> of our textbook <code>ISLR</code>, but also contained in the <code>R</code> package <code>ISLR2</code>. <img src="images/Fig_4_1.png" class="img-fluid"></p>
<p>Let’s take a first look at the a priori default rate in this dataset:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(ISLR2))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Default)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(Default)       <span class="co"># sample size</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(Default<span class="sc">$</span>default)<span class="sc">/</span>n <span class="co"># Overall no-default and default-rate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    No    Yes 
0.9667 0.0333 </code></pre>
</div>
</div>
<ul>
<li>Overall default rate: approx. <span class="math inline">\(3\%.\)</span> (Figure 4.1 shows only a small fraction of the individuals who did not default.)</li>
</ul>
</section>
<section id="ch.-4.2-why-not-linear-regression" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="ch.-4.2-why-not-linear-regression">(Ch. 4.2) Why Not Linear Regression?</h2>
<p>Linear regression is often not appropriate in the case of a qualitative response <span class="math inline">\(Y.\)</span></p>
<p>Suppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses:</p>
<ul>
<li><code>stroke</code></li>
<li><code>drug overdose</code>, and</li>
<li><code>epileptic seizure</code></li>
</ul>
<p>We can encoding these values as a quantitative response variable, <span class="math display">\[
Y=\left\{
    \begin{array}{ll}
    1&amp;\quad\text{if }\texttt{stroke}\\
    2&amp;\quad\text{if }\texttt{drug overdose}\\
    3&amp;\quad\text{if }\texttt{epileptic seizure}\\
    \end{array}
\right.
\]</span> Using this coding, least squares could be used to fit a linear regression model to predict <span class="math inline">\(Y,\)</span> but:</p>
<ol type="1">
<li>The results would then depend on the numeric ordering <span class="math inline">\(1&lt;2&lt;3\)</span>, even though the ordering was completely arbitrary and could have been made differently.</li>
<li>The results would then depend on the assumption the gap <span class="math inline">\((2-1=1)\)</span> between <code>stroke</code> and <code>drug overdose</code> is comparable to the gap <span class="math inline">\((3-2=1)\)</span> between <code>drug overdose</code> and <code>epileptic seizure</code>.</li>
</ol>
<p>Generally, both points are quite a lot of nonsense for most applications.</p>
<p>Only if the response variable’s values did take on a natural ordering, such as <em>“mild”</em>, <em>“moderate”</em>, and <em>“severe”</em>, and we felt the gap between <em>mild</em> and <em>moderate</em> was similar to the gap between <em>moderate</em> and <em>severe</em>, then a 1, 2, 3 coding would be reasonable.</p>
<p>For a <strong>binary (two level) qualitative response</strong>, the situation is better. For instance, if there are only two conditions that we need to predict (e.g.&nbsp;either <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> or <code>default</code><span class="math inline">\(=\)</span><code>No</code>), we can use a dummy variable coding <span class="math display">\[
Y=\left\{
\begin{array}{ll}
    0&amp;\quad\text{if }\texttt{default}=\texttt{Yes}\\
    1&amp;\quad\text{if }\texttt{default}=\texttt{No}\\
\end{array}
\right.
\]</span> We could then fit a linear regression to this binary response, and predict drug overdose if <span class="math inline">\(\hat{Y}&gt; 0.5\)</span> and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.</p>
<p>For binary responses with a <span class="math inline">\(0/1\)</span> coding, linear regression is not completely unreasonable. It can be shown that <span class="math display">\[
Pr(\texttt{default}=\texttt{Yes}|X)\approx \hat{Y}=\beta_0+ \beta_1 X_1+\dots +\beta_p X_p.
\]</span></p>
<p>However, if we use linear regression, some of our estimates might be outside the <span class="math inline">\([0, 1]\)</span> interval (see Figure 4.2), which doesn’t make sense when predicting probabilities. <img src="images/Fig_4_2.png" class="img-fluid"></p>
<p><strong>Summary:</strong></p>
<ol type="a">
<li>A classic regression method cannot accommodate a qualitative response with more than two classes</li>
<li>A classic regression method may not provide meaningful estimates of <span class="math inline">\(Pr(Y |X),\)</span> even with just two classes.</li>
</ol>
<p>Thus, it is often preferable to use a classification method that is truly suited for qualitative response values.</p>
</section>
<section id="ch.-4.3-logistic-regression" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="ch.-4.3-logistic-regression">(Ch. 4.3) Logistic Regression</h2>
<p>Logistic regression models the probability that <span class="math inline">\(Y\)</span> belongs to a particular category.</p>
<p>For the <code>Default</code> data, logistic regression models the conditional probability of default given values for the predictor(s). For example, the probability of default given <code>balance</code>: <span class="math display">\[
Pr(\texttt{default}=\texttt{Yes}|\texttt{balance}) = p(\texttt{balance}),
\]</span> where <span class="math inline">\(p(\texttt{balance})\in[0,1]\)</span> is used as a short hand notation.</p>
<p>One might predict <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> for any individual for whom <span class="math inline">\(p(\texttt{balance}) &gt; 0.5.\)</span></p>
<p>However, <span class="math inline">\(0.5\)</span> this is not the only reasonable classification threshold!</p>
<p>For instance, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as <span class="math inline">\(p(\texttt{balance}) &gt; 0.1.\)</span></p>
<section id="ch.-4.3.1-the-logistic-model" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-4.3.1-the-logistic-model">(Ch. 4.3.1) The Logistic Model</h3>
<p>For a binary coded dependen variable <span class="math inline">\(Y\in\{0,1\}\)</span> we aim to model the relationship between <span class="math display">\[
p(X)=Pr(Y=1|X)\quad\text{and}\quad X.
\]</span></p>
<p>As discussed above, a linear regression model, e.g., <span class="math display">\[
p(X)=\beta_0+\beta_1 X
\]</span> can produce nonsense predictions <span class="math inline">\(p(X)&lt;0\)</span> or <span class="math inline">\(p(X)&gt;1\)</span>; see the left-hand panel of Figure 4.2.</p>
<p>Logistic regression avoids this problem by using the <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a>, <span class="math display">\[
p(X)=\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}.
\]</span> To fit the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> we use an estimation method called <strong>maximum likelihood</strong>.</p>
<p>The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the <code>Default</code> data.</p>
<p>Note that <span class="math display">\[
\begin{align*}
p(X) &amp; = \frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}
%\frac{p(X)}{1-p(X)} &amp; = \frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{1-p(X)} \\
%\frac{p(X)}{1-p(X)} &amp; = \frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{1-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
%\frac{p(X)}{1-p(X)} &amp; = \frac{\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}}{\frac{1+e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}-\frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}} \\
\quad \Leftrightarrow\quad  \frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1 X}
\end{align*}
\]</span></p>
<p>The quantity <span class="math display">\[
\frac{p(X)}{1 − p(X)}
\]</span> is called the <strong>odds</strong>, and can take any value between 0 and plus infinity.</p>
<ul>
<li>A small odds value (close to zero) indicates a low probability of default.</li>
<li>A large odds value indicates a high probability of default.</li>
</ul>
<p>For instance</p>
<ul>
<li><p>An odds value of <span class="math inline">\(\frac{1}{4}\)</span> means that <span class="math inline">\(0.2=20\%\)</span> of the people will default <span class="math display">\[
\frac{0.2}{1-0.2}=\frac{1}{4}
\]</span></p></li>
<li><p>An odds value of <span class="math inline">\(9\)</span> means that <span class="math inline">\(0.9=90\%\)</span> of the people will default <span class="math display">\[
\frac{0.9}{1-0.9}=9
\]</span></p></li>
</ul>
<p>By taking the logarithm, we arrive at <strong>log odds</strong> or <strong>logit</strong> <span class="math display">\[
\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0+\beta_1 X
\]</span></p>
<p>Thus increasing <span class="math inline">\(X\)</span> by one unit …</p>
<ul>
<li>… changes the <em>log odds</em> by <span class="math inline">\(\beta_1\)</span></li>
<li>… multiplies the odds by <span class="math inline">\(e^{\beta_1}\)</span></li>
</ul>
<p><strong>Caution:</strong> The amount that <span class="math inline">\(p(X)\)</span> changes due to a one-unit change in <span class="math inline">\(X\)</span> depends on the current value of <span class="math inline">\(X.\)</span> (The logistic regression model is a non-linear model.)</p>
<p>But regardless of the value of <span class="math inline">\(X\)</span>, if <span class="math inline">\(\beta_1\)</span> is positive then increasing <span class="math inline">\(X\)</span> will be associated with increasing <span class="math inline">\(p(X)\)</span>, and if <span class="math inline">\(\beta_1\)</span> is negative then increasing <span class="math inline">\(X\)</span> will be associated with decreasing <span class="math inline">\(p(X).\)</span></p>
</section>
<section id="ch.-4.3.2-estimating-the-regression-coefficients" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-4.3.2-estimating-the-regression-coefficients">(Ch. 4.3.2) Estimating the Regression Coefficients</h3>
<p>In logistic regression analysis, the unknown model parameters are estimated using <em>maximum likelihood</em>.</p>
<p>Basic intuition:</p>
<p>Find estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> for each person <span class="math inline">\(i\)</span> corresponds as close as possible to its default status. (I.e. <span class="math inline">\(\hat{p}(x_i)\approx 1\)</span> if person <span class="math inline">\(i\)</span> defaulted and <span class="math inline">\(\hat{p}(x_i)\approx 0\)</span> if not.)</p>
<p>This intuition can be formalized using a mathematical equation called a <em>likelihood function</em>: <span class="math display">\[
\ell(\beta_0,\beta_1)=\prod_{i:y_{i}=1} p(x_i)\prod_{i:y_{i}=0} (1-p(x_i))
\]</span> The estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are chosen to <em>maximize</em> this likelihood function.</p>
<p>Maximum likelihood is a very general estimation method that allows to estimate also non-linear models (like the logistic regression model).</p>
<p>Table 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the <code>Default</code> data in order to predict the probability of <code>default</code><span class="math inline">\(=\)</span><code>Yes</code> using <code>balance</code> as the only predictor. <img src="images/Tab_4_1.png" class="img-fluid"></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>We see that <span class="math inline">\(\hat\beta_1=0.0055\)</span>; this indicates that an increase in <code>balance</code> is associated with an increase in the probability of default.
<ul>
<li>To be precise, a one-unit increase in <code>balance</code> is associated with an increase in the log odds of default by 0.0055 units.</li>
</ul></li>
<li>The <span class="math inline">\(z\)</span>-statistic in Table 4.1 plays the same role as the <span class="math inline">\(t\)</span>-statistic in the linear regression output: a large (absolute) value of the <span class="math inline">\(z\)</span>-statistic indicates evidence against the null hypothesis <span class="math inline">\(H_0: \beta_1= 0.\)</span></li>
</ul>
</section>
<section id="ch.-4.3.3-making-predictions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-4.3.3-making-predictions">(Ch. 4.3.3) Making Predictions</h3>
<p>Once the coefficients have been estimated, we can compute the probability of <code>default</code><span class="math inline">\(=1\)</span> for any given credit card balance.</p>
<p>For example, using the coefficient estimates given in Table 4.1, we predict that the default probability for an individual with a <code>balance</code>-value of 1,000 [USD] is <span class="math display">\[
\begin{align*}
\hat{p}(\texttt{balance})
&amp;=Pr(\texttt{default}=\texttt{Yes}|\texttt{balance})\\
&amp;=\frac{e^{\hat\beta_0+\hat\beta_1 \texttt{balance}}}{1+e^{\hat\beta_0+\hat\beta_1 \texttt{balance}}}\\
&amp;=\frac{e^{-10.6513+ 0.0055\times 1000}}{1+e^{-10.6513+ 0.0055\times 1000}} = 0.00576 &lt; 1\%
\end{align*}
\]</span></p>
<p>By contrast, the default probability for an individual with a <code>balance</code>-value of 2,000 [USD] equals <span class="math inline">\(0.586\)</span> (or <span class="math inline">\(58,6\%\)</span>) and is thus much higher.</p>
<section id="qualitative-predictors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qualitative-predictors">Qualitative Predictors:</h4>
<p><img src="images/Tab_4_2.png" class="img-fluid"></p>
<p><span class="math display">\[
\begin{align*}
Pr(\texttt{default}=\texttt{Yes}|\texttt{student}=\texttt{Yes})
&amp;=\frac{e^{-3.5041+0.4049\times 1}}{1+e^{-3.5041+0.4049\times 1}}\\
&amp;= 0.0431\\
Pr(\texttt{default}=\texttt{Yes}|\texttt{student}=\texttt{No})
&amp;=\frac{e^{-3.5041+0.4049\times 0}}{1+e^{-3.5041+0.4049\times 0}}\\
&amp;= 0.0292\\
\end{align*}
\]</span></p>
<p>This may indicate that students tend to have higher default probabilities than non-students. But we may <strong>missed further factors</strong> here since we only use one predictor variable.</p>
</section>
</section>
<section id="ch.-4.3.4-multiple-logistic-regression" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-4.3.4-multiple-logistic-regression">(Ch. 4.3.4) Multiple Logistic Regression</h3>
<p>By analogy with the extension from simple to multiple linear, we can generalize the (simple) logistic regression model as following <span class="math display">\[
\begin{align*}
p(X)
&amp;=\frac{e^{\beta_0+\beta_1 X_1+\dots+\beta_p X_p}}{1+e^{\beta_0+\beta_1 X_1+\dots+\beta_p X_p}}\\
\log\left(\frac{p(X)}{1-p(X)}\right)
&amp;= \beta_0+\beta_1 X_1+\dots+\beta_p X_p,\\
\end{align*}
\]</span> where <span class="math inline">\(X=(X_1,\dots,X_p)\)</span>, and where the unknown parameters <span class="math inline">\(\beta_0,\dots,\beta_p\)</span> are estimated by maximum likelihood. <img src="images/Tab_4_3.png" class="img-fluid"></p>
<p>The negative coefficient for student in the multiple logistic regression indicates that for a fixed value of balance and income, a student is less likely to default than a non-student.</p>
<!-- *  The variables `student` and `balance` are correlated. -->
<p>Interestingly, the effect of the dummy variable <code>student[Yes]</code> is now <em>negative</em>, in contrast to the estimation results of the (simple) logistic regression in Table 4.2 where it was <em>positive</em>.</p>
<p>The left-hand panel of Figure 4.3 provides a graphical illustration of this apparent paradox:</p>
<ul>
<li>Without considering <code>balance</code>, the (overall) default rates of students are <em>higher</em> than those of non-students (horizontal broken lines). This overall effect was shown in Table 4.2.</li>
<li>However, for given <code>balance</code>-values, the default rate for students is <em>lower</em> than for non-students (solid lines).</li>
</ul>
<p><img src="images/Fig_4_3.png" class="img-fluid"></p>
<p>The right-hand panel of Figure 4.3 provides an explanation for this discrepancy: Students tend to hold higher levels of debt, which is in turn associated with higher probability of default.</p>
<p><strong>Summary:</strong></p>
<ul>
<li>An individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance</li>
<li>However, overall, students tend to default at a higher rate than non-students since, overall, they tend to have higher credit card balances.</li>
</ul>
<p>In other words: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!</p>
</section>
<section id="ch.-4.3.5-multinomial-logistic-regression" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-4.3.5-multinomial-logistic-regression">(Ch. 4.3.5) Multinomial Logistic Regression</h3>
<p>It is possible to extend the two-class logistic regression approach to the setting of <span class="math inline">\(K &gt; 2\)</span> classes. This extension is sometimes known as <em>multinomial logistic regression</em>.</p>
<p>To do this, we first select a single class to serve as the baseline; without loss of generality, we select the <span class="math inline">\(K\)</span>th class for this role.</p>
<p>We model the probabilities that <span class="math inline">\(Y=k\)</span>, for <span class="math inline">\(k=1,\dots,K\)</span>, using <span class="math inline">\(k\)</span>-specific parameters <span class="math inline">\(\beta_{k0},\dots,\beta_{kp}\)</span> with <span class="math display">\[
p_k(x)=Pr(Y=k|X=x)=\frac{e^{\beta_{k0} + \beta_{k1} x_1 + \dots +  \beta_{kp} x_p}}{1+\sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} x_1 + \dots +  \beta_{lp} x_l}}
\]</span> for <span class="math inline">\(k=1,\dots,K-1\)</span>, and <span class="math display">\[
p_K(x)=Pr(Y=K|X=x)=\frac{1}{1+\sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} x_1 + \dots +  \beta_{lp} x_l}}
\]</span></p>
<p>For <span class="math inline">\(k=1,\dots,K-1\)</span> it holds that <span class="math display">\[
\log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)=\beta_{k0} + \beta_{k1} x_1 + \dots +  \beta_{kp} x_p
\]</span> which is the counterpart to the log odds equation for <span class="math inline">\(K=2.\)</span></p>
<p>Note that:</p>
<ul>
<li><p>The predictions <span class="math inline">\(\hat{p}_k(x)\)</span>, for <span class="math inline">\(k=1,\dots,K\)</span> do not depend on the choice of the baseline class.</p></li>
<li><p>However, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.</p></li>
</ul>
</section>
</section>
<section id="ch.-4.4-discriminant-analysis-generative-models-for-classification" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="ch.-4.4-discriminant-analysis-generative-models-for-classification">(Ch. 4.4) Discriminant Analysis: Generative Models for Classification</h2>
<p>We now consider an alternative and less direct approach to estimating the probabilities <span class="math inline">\(Pr(Y=K|X=x).\)</span></p>
<p>Suppose that we wish to classify an observation into one of <span class="math inline">\(K\geq 2\)</span> classes.</p>
<p><strong>Prior probability:</strong> Let <span class="math display">\[
\pi_k=Pr(Y=k)
\]</span> represent the overall <em>prior probability</em> that a randomly chosen observation comes from class <span class="math inline">\(k.\)</span> We have that <span class="math display">\[
\pi_1+\dots+\pi_K=1.
\]</span></p>
<p><strong>Density function of <span class="math inline">\(X\)</span>:</strong> Let <span class="math display">\[
f_k(x)=Pr(X=x|Y=k)
\]</span> denote the (conditional) density of <span class="math inline">\(X\)</span> for an observation that comes from class <span class="math inline">\(k.\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><strong>Posterior probability:</strong> Then <em>Bayes’ theorem</em> states that the probability that a observation with predictor values <span class="math inline">\(X=x\)</span> comes from class <span class="math inline">\(k\)</span> (i.e.&nbsp;the posterior probability), is given by <span id="eq-postprob1"><span class="math display">\[
p_k(x) = Pr(Y=k|X=x)=\frac{\pi_k f_k(x)}{\sum_{l=1}^K\pi_l f_l(x)}.
\tag{4.1}\]</span></span></p>
<p>While logistic regression aims at estimating the posterior probability <span class="math inline">\(p_k(x)\)</span> directly, Bayes’s theorem gives us a way to estimate <span class="math inline">\(p_k(x)\)</span> <em>indirectly</em> simply by plugging in estimates of <span class="math inline">\(\pi_k\)</span> and <span class="math inline">\(f_k(x)\)</span>.</p>
<p>However, estimating the densities <span class="math inline">\(f_k(x)\)</span> can be very challenging and we therefore have to make some simplifying assumptions. (E.g. assuming that the data is normal distributed.)</p>
<p>We know from Chapter 2 that the <strong>Bayes classifier</strong>, which classifies an observation <span class="math inline">\(x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math inline">\(p_k(x)\)</span> is largest, has the lowest possible error rate out of all classifiers.</p>
<p>In the following sections, we discuss three classifiers that use different estimates of <span class="math inline">\(f_k(x)\)</span> to approximate the Bayes classifier:</p>
<ul>
<li>linear discriminant analysis</li>
<li>quadratic discriminant analysis</li>
<li>Naive Bayes</li>
</ul>
<section id="ch.-4.4.1-linear-discriminant-analysis-for-p-1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-4.4.1-linear-discriminant-analysis-for-p-1">(Ch. 4.4.1) Linear Discriminant Analysis for <span class="math inline">\(p = 1\)</span></h3>
<p>For the beginning, let us assume that we have only one predictor, i.e., <span class="math inline">\(p=1.\)</span></p>
<p>To estimate <span class="math inline">\(f_k(x)\)</span>, we will assume that <span class="math inline">\(f_k\)</span> is <em>normal</em> (or <em>Gaussian</em>). In the simple <span class="math inline">\(p=1\)</span> dimensional setting, the normal distribution is <span class="math display">\[
f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2\right),
\]</span> where</p>
<ul>
<li><span class="math inline">\(\mu_k\)</span> is the mean of the <span class="math inline">\(k\)</span>th class and</li>
<li><span class="math inline">\(\sigma_k^2\)</span> is the variance of the <span class="math inline">\(k\)</span>th class</li>
<li><span class="math inline">\(\pi\approx 3.14159\)</span> is the mathematical constant <span class="math inline">\(\pi\)</span>. (Do not confuse it with the prior probabilities <span class="math inline">\(\pi_k.\)</span>)</li>
</ul>
<p>For now, let us further assume the simplifying case of equal variances across all classes, i.e.&nbsp; <span class="math display">\[
\sigma_1^2=\dots = \sigma_K^2\equiv \sigma^2.
\]</span> Plugging this assumed version of <span class="math inline">\(f_k\)</span> into <a href="#eq-postprob1">Equation&nbsp;<span>4.1</span></a>, leads to <span id="eq-LDp1"><span class="math display">\[
p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(x-\mu_k)^2\right)}{\sum_{l=1}^K\pi_l \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(x-\mu_l)^2\right)}.
\tag{4.2}\]</span></span></p>
<p>The Bayes classifier involves assigning an observation <span class="math inline">\(X = x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math inline">\(p_k(x)\)</span> is largest. Taking the <span class="math inline">\(\log\)</span> of <a href="#eq-LDp1">Equation&nbsp;<span>4.2</span></a> (i.e.&nbsp;a monotonic transformation) and rearranging terms shows that this is equivalent to assigning an observation <span class="math inline">\(X=x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math display">\[
\delta_k(x)=x\cdot\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
\]</span> is largest. The function <span class="math inline">\(\delta_k(x)\)</span> is called the <strong>discriminant function</strong>.</p>
<p><strong>Example:</strong></p>
<p>In the case of only two classes, <span class="math inline">\(K=2\)</span>, with equal a priori probabilities <span class="math inline">\(\pi_1=\pi_2\equiv \pi^*\)</span>, the Bayes classifier assigns an observation <span class="math inline">\(X=x\)</span> to class <span class="math inline">\(1\)</span> if <span class="math display">\[
\begin{align*}
\delta_1(x) &amp; &gt; \delta_2(x)\\
%%%
x\cdot\frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2\sigma^2} + \log(\pi^*)
&amp; &gt;
x\cdot\frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2} + \log(\pi^*)\\
%%%
x\cdot\frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2\sigma^2}  
&amp; &gt;
x\cdot\frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2}\\
%%%
x\cdot\mu_1 - \frac{\mu_1^2}{2}  
&amp; &gt;
x\cdot\mu_2 - \frac{\mu_2^2}{2}\\
%%%
2x\cdot(\mu_1-\mu_2)   
&amp; &gt;
\mu_1^2 - \mu_2^2\\
%%%
x   
&amp; &gt; \frac{\mu_1^2 - \mu_2^2}{2(\mu_1-\mu_2)}
\end{align*}
\]</span> The Bayes decision boundary is the point for which <span class="math inline">\(\delta_1(x)=\delta_2(x)\)</span> <span class="math display">\[
\begin{align*}
x   
=\frac{\mu_1^2 - \mu_2^2}{2(\mu_1-\mu_2)}
&amp;=\frac{(\mu_1 - \mu_2)(\mu_1 + \mu_2)}{2(\mu_1-\mu_2)}\\
&amp;=\frac{(\mu_1 + \mu_2)}{2}
\end{align*}
\]</span> Figure 4.4 shows a specific example with</p>
<ul>
<li><span class="math inline">\(\pi_1=\pi_2\)</span></li>
<li><span class="math inline">\(\mu_1=-1.25\)</span> and <span class="math inline">\(\mu_2=1.25\)</span> and</li>
<li><span class="math inline">\(\sigma_1=\sigma_2\equiv\sigma =1.\)</span></li>
</ul>
<p>The two densities <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> overlap such that for given <span class="math inline">\(X_x\)</span> there is some uncertainty about the class to which the observation belongs to. In this example, the Bayes classifier assigns an observation <span class="math inline">\(X=x\)</span> …</p>
<ul>
<li>… to class 1 if <span class="math inline">\(x&lt;0\)</span></li>
<li>… to class 2 if <span class="math inline">\(x&gt;0\)</span></li>
</ul>
<p><img src="images/Fig_4_4.png" class="img-fluid"></p>
<p>In the above example (Figure 4.4) we know all parameters <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(k=1,\dots,K\)</span>, and <span class="math inline">\(\sigma\)</span>. In practice, however, these parameters are usually unknown, and thus need to estimated from the data.</p>
<p>The <strong>Linear Discriminant Analysis (LDA)</strong> method approximates the Bayes classifier by using the normality assumption and by plugging in estimates for the unknown parameters <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(k=1,\dots,K\)</span>, and <span class="math inline">\(\sigma\)</span>. The following estimates are used: <span class="math display">\[
\begin{align*}
\hat\pi_k    &amp; = \frac{n_k}{n}\\
\hat\mu      &amp; = \frac{1}{n_k}\sum_{i:y_i=k}x_i\\
\hat\sigma   &amp; = \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat\mu_k)^2\\
\end{align*}
\]</span> The LDA classifier assigns an observation <span class="math inline">\(X=x\)</span> to the class <span class="math inline">\(k\)</span> for which <span class="math display">\[
\hat\delta_k(x)=x\cdot\frac{\hat\mu_k}{\hat\sigma^2} - \frac{\hat\mu_k^2}{2\hat\sigma^2} + \log(\hat\pi_k)
\]</span> is largest.</p>
<p>LDA is named <strong>linear</strong> since the discriminant functions <span class="math inline">\(\hat\delta_k(x)\)</span>, <span class="math inline">\(k=1,\dots,K\)</span> are linear functions of <span class="math inline">\(x.\)</span></p>
<p>Assumptions made by the LDA method:</p>
<ul>
<li>Observation for each class come from a normal distribution</li>
<li>The class-specific normal distributions have class-specific means, <span class="math inline">\(\mu_k\)</span>, but <strong>equal variances</strong> <span class="math inline">\(\sigma\)</span></li>
</ul>
</section>
<section id="ch.-4.4.2-linear-discriminant-analysis-for-p-1" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="ch.-4.4.2-linear-discriminant-analysis-for-p-1"><span class="header-section-number">4.1.1</span> (Ch. 4.4.2) Linear Discriminant Analysis for <span class="math inline">\(p &gt; 1\)</span></h3>
<p>In the case of multiple predictors <span class="math display">\[
X=(X_1,\dots,X_p)\quad \text{with}\quad p&gt;1
\]</span> we need to assume that the observations for each class <span class="math inline">\(k\)</span> come from a <strong>multivariate Gaussian distribution</strong> with</p>
<ul>
<li>Class-specific <span class="math inline">\((p\times 1)\)</span> mean vectors <span class="math inline">\(\mu_k\)</span></li>
<li>Equal <span class="math inline">\((p\times p)\)</span> covariance matrix <span class="math inline">\(\Sigma\)</span> for all classes <span class="math inline">\(k=1,\dots,K\)</span></li>
</ul>
<p>That is, a <span class="math inline">\((p\times 1)\)</span> random vector <span class="math inline">\(X\)</span> of class <span class="math inline">\(k\)</span> is distributed as <span class="math display">\[
X\sim N\left(\mu_k,\Sigma\right)
\]</span> with class-specific <span class="math inline">\((p\times 1)\)</span> mean vector <span class="math inline">\(\mu_k\)</span>, <span class="math display">\[
E(X)=\mu_k,
\]</span> and <span class="math inline">\((p\times p)\)</span> covariance matrix <span class="math display">\[
Cov(X)=\Sigma
\]</span> that is common for all classes <span class="math inline">\(k=1,\dots,K.\)</span></p>
<p>The multivariate Gaussian density for class <span class="math inline">\(k=1,\dots,K\)</span> is defined as <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_k)^T\sigma^{-1}(x-\mu_k)\right)
\]</span></p>
<p>Figure 4.5 shows the case of bivariate (<span class="math inline">\(p=2\)</span>) Gaussian distribution predictors.</p>
<p><img src="images/Fig_4_5.png" class="img-fluid"></p>
<p>Under the multivariate normality assumption (with common covariance matrix and group-specific mean vectors) the Bayes classifier assigns an multivariate observation <span class="math inline">\(X=x\)</span> to the class for which the discriminant function <span class="math display">\[
\delta_k(x)=x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log(\pi_k)
\]</span> is largest.</p>
<p>An example for <span class="math inline">\(p=2\)</span> and <span class="math inline">\(K=3\)</span> with three equally sized (i.e.&nbsp;<span class="math inline">\(\pi_1=\pi_2=\pi_3\)</span>) Gaussian classes is shown in the left-hand panel of Figure 4.6. The three ellipses represent regions that contain <span class="math inline">\(95\%\)</span> of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which <span class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, <span class="math inline">\(k\neq l\)</span>.</p>
<p><img src="images/Fig_4_6.png" class="img-fluid"></p>
<p>Again, the unknown parameters <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\pi_k\)</span>, and <span class="math inline">\(\Sigma\)</span> must be estimated from the data with formulas similar to those for the <span class="math inline">\(p=1\)</span> case.</p>
<section id="the-default-data-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-default-data-example">The <code>Default</code> data example</h4>
<p>We can perform LDA on the <code>Default</code> data in order to predict whether or not an individual will default on the basis of</p>
<ul>
<li>credit card balance (real variable) and</li>
<li>student status (qualitative variable).</li>
</ul>
<p><strong>Note:</strong> Student status is qualitative, and thus the normality assumption made by LDA is clearly violated in this example! However, LDA is often remarkably robust to model violations, as this example shows.</p>
<p>The LDA model fit to the <span class="math inline">\(10,000\)</span> training samples results in a training error rate of <span class="math inline">\(2.75\%:\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MASS package contains the lda() function</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(MASS)) </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Sample size </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>n              <span class="ot">&lt;-</span> <span class="fu">nrow</span>(Default)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>lda_obj        <span class="ot">&lt;-</span> <span class="fu">lda</span>(default <span class="sc">~</span> balance <span class="sc">+</span> student, </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> Default)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>ldaPredict_obj <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda_obj)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Training error rate:</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(Default<span class="sc">$</span>default <span class="sc">!=</span> ldaPredict_obj<span class="sc">$</span>class)<span class="sc">/</span>n </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0275</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Training error rate of the null classifier:</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(Default<span class="sc">$</span>default <span class="sc">!=</span> <span class="fu">rep</span>(<span class="st">"No"</span>, n))<span class="sc">/</span>n </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0333</code></pre>
</div>
</div>
<p>This sounds like a low error rate, but two caveats must be noted:</p>
<ul>
<li>Training error rates are usually smaller than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a <strong>new set of individuals</strong> will default.
<ul>
<li>The reason is that we specifically adjust the parameters of our model to do well on the training data.</li>
<li>Generally, the higher the ratio of parameters <span class="math inline">\(p\)</span> to number of samples <span class="math inline">\(n\)</span>, the more we expect this <em>overfitting</em> to play a role. However, for these data we don’t expect this to be a problem, since <span class="math inline">\(p = 2\)</span> and <span class="math inline">\(n = 10,000.\)</span></li>
</ul></li>
<li>Since only <span class="math inline">\(3.33\%\)</span> of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that an individual will “not default”, regardless of his or her credit card balance and student status, will result in an error rate of <span class="math inline">\(3.33\%.\)</span> In other words, the trivial <strong>null classifier</strong> will achieve an error rate that is only a bit higher than the LDA training set error rate.</li>
</ul>
<p>Any binary classifier can make two types of errors - here:</p>
<ul>
<li>incorrectly assignment of an individual who defaults to the no default category (false negatives)</li>
<li>incorrectly assign of an individual who does not default to the default category (false positives)</li>
</ul>
<p>A confusion matrix shows both types of errors (see also Table 4.4):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion matrix (absolute frequencies)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(ldaPredict_obj<span class="sc">$</span>class, Default<span class="sc">$</span>default, </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"Predicted default status"</span>, </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>              <span class="st">"True default status"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        True default status
Predicted default status   No  Yes
                     No  9644  252
                     Yes   23   81</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion matrix (relaive frequencies)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(ldaPredict_obj<span class="sc">$</span>class, Default<span class="sc">$</span>default, </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"Predicted default status"</span>, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>              <span class="st">"True default status"</span>))<span class="sc">/</span>n</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        True default status
Predicted default status     No    Yes
                     No  0.9644 0.0252
                     Yes 0.0023 0.0081</code></pre>
</div>
</div>
<p><img src="images/Tab_4_4.png" class="img-fluid"></p>
<p>A perfect confusion matrix has zeros in the off-diagonal entries.</p>
<ul>
<li>LDA predicts that <span class="math inline">\(104\)</span> people default
<ul>
<li><span class="math inline">\(81\)</span> of these <span class="math inline">\(104\)</span> actually default (true positives)</li>
<li><span class="math inline">\(23\)</span> of these <span class="math inline">\(104\)</span> actually do not default (false positives)</li>
</ul></li>
</ul>

<!-- 
OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? 
Why do we need another method, when we have logistic regression?
There are several reasons:

* When there is substantial separation between the two classes, the
parameter estimates for the logistic regression model are surprisingly
unstable. The methods that we consider in this section do not suffer
from this problem.

* If the distribution of the predictors $X$ is approximately normal in
each of the classes and the sample size is small, then the approaches
in this section may be more accurate than logistic regression.
OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN? OUT OR IN?  
-->
</section>
</section>
</section>
<section id="r-lab-classification" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="r-lab-classification"><span class="header-section-number">4.2</span> <code>R</code>-Lab: Classification</h2>
<section id="the-stock-market-data" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="the-stock-market-data"><span class="header-section-number">4.2.1</span> The Stock Market Data</h3>
<p>We will begin by examining some numerical and graphical summaries of the <code>Smarket</code> data, which is part of the <code>ISLR2</code> library. This data set consists of percentage returns for the S&amp;P 500 stock index over <span class="math inline">\(1,250\)</span>~days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, <code>lagone</code> through <code>lagfive</code>. We have also recorded <code>volume</code> (the number of shares traded on the previous day, in billions), <code>Today</code> (the percentage return on the date in question) and <code>direction</code> (whether the market was <code>Up</code> or <code>Down</code> on this date). Our goal is to predict <code>direction</code> (a qualitative response) using the other features.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(Smarket)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Year"      "Lag1"      "Lag2"      "Lag3"      "Lag4"      "Lag5"     
[7] "Volume"    "Today"     "Direction"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(Smarket)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1250    9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Smarket)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Year           Lag1                Lag2                Lag3          
 Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  
 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  
 Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  
 Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  
 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  
 Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  
      Lag4                Lag5              Volume           Today          
 Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  
 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  
 Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  
 Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  
 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  
 Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  
 Direction 
 Down:602  
 Up  :648  
           
           
           
           </code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(Smarket)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The <code>cor()</code> function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the <code>direction</code> variable is qualitative.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(Smarket)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in cor(Smarket): 'x' must be numeric</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(Smarket[, <span class="sc">-</span><span class="dv">9</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Year         Lag1         Lag2         Lag3         Lag4
Year   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718
Lag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911
Lag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533
Lag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036
Lag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000
Lag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641
Volume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246
Today  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527
               Lag5      Volume        Today
Year    0.029787995  0.53900647  0.030095229
Lag1   -0.005674606  0.04090991 -0.026155045
Lag2   -0.003557949 -0.04338321 -0.010250033
Lag3   -0.018808338 -0.04182369 -0.002447647
Lag4   -0.027083641 -0.04841425 -0.006899527
Lag5    1.000000000 -0.02200231 -0.034860083
Volume -0.022002315  1.00000000  0.014591823
Today  -0.034860083  0.01459182  1.000000000</code></pre>
</div>
</div>
<p>As one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between <code>Year</code> and <code>volume</code>. By plotting the data, which is ordered chronologically, we see that <code>volume</code> is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Smarket)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Volume)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="logistic-regression" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">4.2.2</span> Logistic Regression</h3>
<p>Next, we will fit a logistic regression model in order to predict <code>direction</code> using <code>lagone</code> through <code>lagfive</code> and <code>volume</code>. The <code>glm()</code> function can be used to fit many types of generalized linear models , including logistic regression. The syntax of the <code>glm()</code> function is similar to that of <code>lm()</code>, except that we must pass in the argument <code>family = binomial</code> in order to tell <code>R</code> to run a logistic regression rather than some other type of generalized linear model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>glm.fits <span class="ot">&lt;-</span> <span class="fu">glm</span>(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4 <span class="sc">+</span> Lag5 <span class="sc">+</span> Volume,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Smarket, <span class="at">family =</span> binomial</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm.fits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
    Volume, family = binomial, data = Smarket)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.446  -1.203   1.065   1.145   1.326  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -0.126000   0.240736  -0.523    0.601
Lag1        -0.073074   0.050167  -1.457    0.145
Lag2        -0.042301   0.050086  -0.845    0.398
Lag3         0.011085   0.049939   0.222    0.824
Lag4         0.009359   0.049974   0.187    0.851
Lag5         0.010313   0.049511   0.208    0.835
Volume       0.135441   0.158360   0.855    0.392

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1731.2  on 1249  degrees of freedom
Residual deviance: 1727.6  on 1243  degrees of freedom
AIC: 1741.6

Number of Fisher Scoring iterations: 3</code></pre>
</div>
</div>
<p>The smallest <span class="math inline">\(p\)</span>-value here is associated with <code>lagone</code>. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of <span class="math inline">\(0.15\)</span>, the <span class="math inline">\(p\)</span>-value is still relatively large, and so there is no clear evidence of a real association between <code>lagone</code> and <code>direction</code>.</p>
<p>We use the <code>coef()</code> function in order to access just the coefficients for this fitted model. We can also use the <code>summary()</code> function to access particular aspects of the fitted model, such as the <span class="math inline">\(p\)</span>-values for the coefficients.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(glm.fits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 
-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 
      Volume 
 0.135440659 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm.fits)<span class="sc">$</span>coef</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983
Lag1        -0.073073746 0.05016739 -1.4565986 0.1452272
Lag2        -0.042301344 0.05008605 -0.8445733 0.3983491
Lag3         0.011085108 0.04993854  0.2219750 0.8243333
Lag4         0.009358938 0.04997413  0.1872757 0.8514445
Lag5         0.010313068 0.04951146  0.2082966 0.8349974
Volume       0.135440659 0.15835970  0.8552723 0.3924004</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm.fits)<span class="sc">$</span>coef[, <span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 
  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 
     Volume 
  0.3924004 </code></pre>
</div>
</div>
<p>The <code>predict()</code> function can be used to predict the probability that the market will go up, given values of the predictors. The <code>type = "response"</code> option tells <code>R</code> to output probabilities of the form <span class="math inline">\(P(Y=1|X)\)</span>, as opposed to other information such as the logit. If no data set is supplied to the <code>predict()</code> function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down, because the <code>contrasts()</code> function indicates that <code>R</code> has created a dummy variable with a 1 for <code>Up</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>glm.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fits, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>glm.probs[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2         3         4         5         6         7         8 
0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 
        9        10 
0.5176135 0.4888378 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(Direction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Up
Down  0
Up    1</code></pre>
</div>
</div>
<p>In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, <code>Up</code> or <code>Down</code>. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than <span class="math inline">\(0.5\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">"Down"</span>, <span class="dv">1250</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>glm.pred[glm.probs <span class="sc">&gt;</span> .<span class="dv">5</span>] <span class="ot">=</span> <span class="st">"Up"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first command creates a vector of 1,250 <code>Down</code> elements. The second line transforms to <code>Up</code> all of the elements for which the predicted probability of a market increase exceeds <span class="math inline">\(0.5\)</span>. Given these predictions, the <code>table()</code> function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified. %By inputting two qualitative vectors R will create a two by two table with counts of the number of times each combination occurred e.g.&nbsp;predicted {} and market increased, predicted {} and the market decreased etc.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, Direction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction
glm.pred Down  Up
    Down  145 141
    Up    457 507</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">507</span> <span class="sc">+</span> <span class="dv">145</span>) <span class="sc">/</span> <span class="dv">1250</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5216</code></pre>
</div>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(glm.pred <span class="sc">==</span> Direction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5216</code></pre>
</div>
</div>
<p>The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on <span class="math inline">\(507\)</span>~days and that it would go down on <span class="math inline">\(145\)</span>~days, for a total of <span class="math inline">\(507+145 = 652\)</span> correct predictions. The <code>mean()</code> function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market <span class="math inline">\(52.2\)</span>,% of the time.</p>
<p>At first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of <span class="math inline">\(1,250\)</span> observations. In other words, <span class="math inline">\(100\%-52.2\%=47.8\%\)</span>, is the <em>training</em> error rate. As we have seen previously, the training error rate is often overly optimistic—it tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the <em>held out</em> data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.</p>
<p>To implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> (Year <span class="sc">&lt;</span> <span class="dv">2005</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>Smarket<span class="fl">.2005</span> <span class="ot">&lt;-</span> Smarket[<span class="sc">!</span>train, ]</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(Smarket<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 252   9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>Direction<span class="fl">.2005</span> <span class="ot">&lt;-</span> Direction[<span class="sc">!</span>train]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The object <code>train</code> is a vector of <span class="math inline">\(1{,}250\)</span> elements, corresponding to the observations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to <code>TRUE</code>, whereas those that correspond to observations in 2005 are set to <code>FALSE</code>. The object <code>train</code> is a <em>Boolean</em> vector, since its elements are <code>TRUE</code> and <code>FALSE</code>. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command <code>Smarket[train, ]</code> would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of <code>train</code> are <code>TRUE</code>. The <code>!</code> symbol can be used to reverse all of the elements of a Boolean vector. That is, <code>!train</code> is a vector similar to <code>train</code>, except that the elements that are <code>TRUE</code> in <code>train</code> get swapped to <code>FALSE</code> in <code>!train</code>, and the elements that are <code>FALSE</code> in <code>train</code> get swapped to <code>TRUE</code> in <code>!train</code>. Therefore, <code>Smarket[!train, ]</code> yields a submatrix of the stock market data containing only the observations for which <code>train</code> is <code>FALSE</code>—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.</p>
<p>We now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the <code>subset</code> argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>glm.fits <span class="ot">&lt;-</span> <span class="fu">glm</span>(</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4 <span class="sc">+</span> Lag5 <span class="sc">+</span> Volume,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Smarket, <span class="at">family =</span> binomial, <span class="at">subset =</span> train</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>glm.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fits, Smarket<span class="fl">.2005</span>,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">"response"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">"Down"</span>, <span class="dv">252</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>glm.pred[glm.probs <span class="sc">&gt;</span> .<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="st">"Up"</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction.2005
glm.pred Down Up
    Down   77 97
    Up     34 44</code></pre>
</div>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(glm.pred <span class="sc">==</span> Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4801587</code></pre>
</div>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(glm.pred <span class="sc">!=</span> Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5198413</code></pre>
</div>
</div>
<p>The <code>!=</code> notation means <em>not equal to</em>, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is <span class="math inline">\(52\)</span>,%, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)</p>
<p>We recall that the logistic regression model had very underwhelming <span class="math inline">\(p\)</span>-values associated with all of the predictors, and that the smallest <span class="math inline">\(p\)</span>-value, though not very small, corresponded to <code>lagone</code>. Perhaps by removing the variables that appear not to be helpful in predicting <code>direction</code>, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. Below we have refit the logistic regression using just <code>lagone</code> and <code>lagtwo</code>, which seemed to have the highest predictive power in the original logistic regression model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>glm.fits <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> Smarket,</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> binomial, <span class="at">subset =</span> train)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>glm.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fits, Smarket<span class="fl">.2005</span>,</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">"Down"</span>, <span class="dv">252</span>)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>glm.pred[glm.probs <span class="sc">&gt;</span> .<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="st">"Up"</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction.2005
glm.pred Down  Up
    Down   35  35
    Up     76 106</code></pre>
</div>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(glm.pred <span class="sc">==</span> Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5595238</code></pre>
</div>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="dv">106</span> <span class="sc">/</span> (<span class="dv">106</span> <span class="sc">+</span> <span class="dv">76</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5824176</code></pre>
</div>
</div>
<p>Now the results appear to be a little better: <span class="math inline">\(56\%\)</span> of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct <span class="math inline">\(56\%\)</span> of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a <span class="math inline">\(58\%\)</span> accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.</p>
<p>Suppose that we want to predict the returns associated with particular values of <code>lagone</code> and <code>lagtwo</code>. In particular, we want to predict <code>direction</code> on a day when <code>lagone</code> and <code>lagtwo</code> equal 1.2 and~1.1, respectively, and on a day when they equal 1.5 and $-$0.8. We do this using the <code>predict()</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(glm.fits,</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">newdata =</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">Lag1 =</span> <span class="fu">c</span>(<span class="fl">1.2</span>, <span class="fl">1.5</span>),  <span class="at">Lag2 =</span> <span class="fu">c</span>(<span class="fl">1.1</span>, <span class="sc">-</span><span class="fl">0.8</span>)),</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">"response"</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2 
0.4791462 0.4960939 </code></pre>
</div>
</div>
</section>
<section id="linear-discriminant-analysis" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="linear-discriminant-analysis"><span class="header-section-number">4.2.3</span> Linear Discriminant Analysis</h3>
<p>Now we will perform LDA on the <code>Smarket</code> data. In <code>R</code>, we fit an LDA model using the <code>lda()</code> function, which is part of the <code>MASS</code> library. Notice that the syntax for the <code>lda()</code> function is identical to that of <code>lm()</code>, and to that of <code>glm()</code> except for the absence of the <code>family</code> option. We fit the model using only the observations before 2005.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>lda.fit <span class="ot">&lt;-</span> <span class="fu">lda</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> Smarket,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">subset =</span> train)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>lda.fit</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)

Prior probabilities of groups:
    Down       Up 
0.491984 0.508016 

Group means:
            Lag1        Lag2
Down  0.04279022  0.03389409
Up   -0.03954635 -0.03132544

Coefficients of linear discriminants:
            LD1
Lag1 -0.6420190
Lag2 -0.5135293</code></pre>
</div>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The LDA output indicates that <span class="math inline">\(\hat\pi_1=0.492\)</span> and <span class="math inline">\(\hat\pi_2=0.508\)</span>; in other words, <span class="math inline">\(49.2\)</span>,% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of <span class="math inline">\(\mu_k\)</span>. These suggest that there is a tendency for the previous 2~days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The <em>coefficients of linear discriminants</em> output provides the linear combination of <code>lagone</code> and <code>lagtwo</code> that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of <span class="math inline">\(X=x\)</span> in (4.24). If <span class="math inline">\(-0.642\times\)</span><code>lagone</code><span class="math inline">\(- 0.514 \times\)</span> <code>lagtwo</code> is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.</p>
<p>The <code>plot()</code> function produces plots of the <em>linear discriminants</em>, obtained by computing <span class="math inline">\(-0.642\times\)</span><code>lagone</code><span class="math inline">\(- 0.514 \times\)</span> <code>lagtwo</code> for each of the training observations. The <code>Up</code> and <code>Down</code> observations are displayed separately.</p>
<p>The <code>predict()</code> function returns a list with three elements. The first element, <code>class</code>, contains LDA’s predictions about the movement of the market. The second element, <code>posterior</code>, is a matrix whose <span class="math inline">\(k\)</span>th column contains the posterior probability that the corresponding observation belongs to the <span class="math inline">\(k\)</span>th class, computed from ( 4.15). Finally, <code>x</code> contains the linear discriminants, described earlier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>lda.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda.fit, Smarket<span class="fl">.2005</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(lda.pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "class"     "posterior" "x"        </code></pre>
</div>
</div>
<p>As we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>lda.class <span class="ot">&lt;-</span> lda.pred<span class="sc">$</span>class</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(lda.class, Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction.2005
lda.class Down  Up
     Down   35  35
     Up     76 106</code></pre>
</div>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(lda.class <span class="sc">==</span> Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5595238</code></pre>
</div>
</div>
<p>Applying a <span class="math inline">\(50\)</span>,% threshold to the posterior probabilities allows us to recreate the predictions contained in <code>lda.pred$class</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(lda.pred<span class="sc">$</span>posterior[, <span class="dv">1</span>] <span class="sc">&gt;=</span> .<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 70</code></pre>
</div>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(lda.pred<span class="sc">$</span>posterior[, <span class="dv">1</span>] <span class="sc">&lt;</span> .<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 182</code></pre>
</div>
</div>
<p>Notice that the posterior probability output by the model corresponds to the probability that the market will <em>decrease</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>lda.pred<span class="sc">$</span>posterior[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      999      1000      1001      1002      1003      1004      1005      1006 
0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 
     1007      1008      1009      1010      1011      1012      1013      1014 
0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 
     1015      1016      1017      1018 
0.4935775 0.5030894 0.4978806 0.4886331 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>lda.class[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  
[16] Up   Up   Down Up   Up  
Levels: Down Up</code></pre>
</div>
</div>
<p>If we wanted to use a posterior probability threshold other than <span class="math inline">\(50\)</span>,% in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least <span class="math inline">\(90\)</span>,%.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(lda.pred<span class="sc">$</span>posterior[, <span class="dv">1</span>] <span class="sc">&gt;</span> .<span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>
<p>No days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was <span class="math inline">\(52.02\)</span>,%.</p>
</section>
<section id="quadratic-discriminant-analysis" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="quadratic-discriminant-analysis"><span class="header-section-number">4.2.4</span> Quadratic Discriminant Analysis</h3>
<p>We will now fit a QDA model to the <code>Smarket</code> data. QDA is implemented in <code>R</code> using the <code>qda()</code> function, which is also part of the <code>MASS</code> library. The syntax is identical to that of <code>lda()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>qda.fit <span class="ot">&lt;-</span> <span class="fu">qda</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> Smarket,</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">subset =</span> train)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>qda.fit</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)

Prior probabilities of groups:
    Down       Up 
0.491984 0.508016 

Group means:
            Lag1        Lag2
Down  0.04279022  0.03389409
Up   -0.03954635 -0.03132544</code></pre>
</div>
</div>
<p>The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The <code>predict()</code> function works in exactly the same fashion as for LDA.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>qda.class <span class="ot">&lt;-</span> <span class="fu">predict</span>(qda.fit, Smarket<span class="fl">.2005</span>)<span class="sc">$</span>class</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(qda.class, Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction.2005
qda.class Down  Up
     Down   30  20
     Up     81 121</code></pre>
</div>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(qda.class <span class="sc">==</span> Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5992063</code></pre>
</div>
</div>
<p>Interestingly, the QDA predictions are accurate almost <span class="math inline">\(60\)</span>,% of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!</p>
</section>
<section id="naive-bayes" class="level3" data-number="4.2.5">
<h3 data-number="4.2.5" class="anchored" data-anchor-id="naive-bayes"><span class="header-section-number">4.2.5</span> Naive Bayes</h3>
<p>Next, we fit a naive Bayes model to the <code>Smarket</code> data. Naive Bayes is implemented in <code>R</code> using the <code>naiveBayes()</code> function, which is part of the <code>e1071</code> library. The syntax is identical to that of <code>lda()</code> and <code>qda()</code>. By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>nb.fit <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> Smarket,</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">subset =</span> train)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>nb.fit</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = X, y = Y, laplace = laplace)

A-priori probabilities:
Y
    Down       Up 
0.491984 0.508016 

Conditional probabilities:
      Lag1
Y             [,1]     [,2]
  Down  0.04279022 1.227446
  Up   -0.03954635 1.231668

      Lag2
Y             [,1]     [,2]
  Down  0.03389409 1.239191
  Up   -0.03132544 1.220765</code></pre>
</div>
</div>
<p>The output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for <code>lagone</code> is <span class="math inline">\(0.0428\)</span> for</p>
<p><code>Direction=Down</code>, and the standard deviation is <span class="math inline">\(1.23\)</span>. We can easily verify this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Lag1[train][Direction[train] <span class="sc">==</span> <span class="st">"Down"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.04279022</code></pre>
</div>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(Lag1[train][Direction[train] <span class="sc">==</span> <span class="st">"Down"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.227446</code></pre>
</div>
</div>
<p>The <code>predict()</code> function is straightforward.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>nb.class <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.fit, Smarket<span class="fl">.2005</span>)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(nb.class, Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction.2005
nb.class Down  Up
    Down   28  20
    Up     83 121</code></pre>
</div>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(nb.class <span class="sc">==</span> Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5912698</code></pre>
</div>
</div>
<p>Naive Bayes performs very well on this data, with accurate predictions over <span class="math inline">\(59\%\)</span> of the time. This is slightly worse than QDA, but much better than LDA.</p>
<p>The <code>predict()</code> function can also generate estimates of the probability that each observation belongs to a particular class. %</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>nb.preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.fit, Smarket<span class="fl">.2005</span>, <span class="at">type =</span> <span class="st">"raw"</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>nb.preds[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Down        Up
[1,] 0.4873164 0.5126836
[2,] 0.4762492 0.5237508
[3,] 0.4653377 0.5346623
[4,] 0.4748652 0.5251348
[5,] 0.4901890 0.5098110</code></pre>
</div>
</div>
</section>
<section id="k-nearest-neighbors" class="level3" data-number="4.2.6">
<h3 data-number="4.2.6" class="anchored" data-anchor-id="k-nearest-neighbors"><span class="header-section-number">4.2.6</span> <span class="math inline">\(K\)</span>-Nearest Neighbors</h3>
<p>We will now perform KNN using the <code>knn()</code> function, which is part of the <code>class</code> library. This function works rather differently from the other model-fitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, <code>knn()</code> forms predictions using a single command. The function requires four inputs.</p>
<ul>
<li>A matrix containing the predictors associated with the training data, labeled <code>train.X</code> below.</li>
<li>A matrix containing the predictors associated with the data for which we wish to make predictions, labeled <code>test.X</code> below.</li>
<li>A vector containing the class labels for the training observations, labeled <code>train.Direction</code> below.</li>
<li>A value for <span class="math inline">\(K\)</span>, the number of nearest neighbors to be used by the classifier.</li>
</ul>
<p>We use the <code>cbind()</code> function, short for <em>column bind</em>, to bind the <code>lagone</code> and <code>lagtwo</code> variables together into two matrices, one for the training set and the other for the test set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>train.X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(Lag1, Lag2)[train, ]</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>test.X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(Lag1, Lag2)[<span class="sc">!</span>train, ]</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>train.Direction <span class="ot">&lt;-</span> Direction[train]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now the <code>knn()</code> function can be used to predict the market’s movement for the dates in 2005. We set a random seed before we apply <code>knn()</code> because if several observations are tied as nearest neighbors, then <code>R</code> will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train.X, test.X, train.Direction, <span class="at">k =</span> <span class="dv">1</span>)</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(knn.pred, Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction.2005
knn.pred Down Up
    Down   43 58
    Up     68 83</code></pre>
</div>
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">83</span> <span class="sc">+</span> <span class="dv">43</span>) <span class="sc">/</span> <span class="dv">252</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5</code></pre>
</div>
</div>
<p>The results using <span class="math inline">\(K=1\)</span> are not very good, since only <span class="math inline">\(50\)</span>,% of the observations are correctly predicted. Of course, it may be that <span class="math inline">\(K=1\)</span> results in an overly flexible fit to the data. Below, we repeat the analysis using <span class="math inline">\(K=3\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train.X, test.X, train.Direction, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(knn.pred, Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Direction.2005
knn.pred Down Up
    Down   48 54
    Up     63 87</code></pre>
</div>
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(knn.pred <span class="sc">==</span> Direction<span class="fl">.2005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5357143</code></pre>
</div>
</div>
<p>The results have improved slightly. But increasing <span class="math inline">\(K\)</span> further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.</p>
<p>KNN does not perform well on the <code>Smarket</code> data but it does often provide impressive results. As an example we will apply the KNN approach to the <code>Insurance</code> data set, which is part of the <code>ISLR2</code> library. This data set includes <span class="math inline">\(85\)</span> predictors that measure demographic characteristics for 5,822 individuals. The response variable is <code>Purchase</code>, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only <span class="math inline">\(6\)</span>,% of people purchased caravan insurance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(Caravan)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5822   86</code></pre>
</div>
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Caravan)</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Purchase)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  No  Yes 
5474  348 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="dv">348</span> <span class="sc">/</span> <span class="dv">5822</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05977327</code></pre>
</div>
</div>
<p>Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the <em>distance</em> between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, <code>salary</code> and <code>age</code> (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of <span class="math inline">\(50\)</span>~years in age. Consequently, <code>salary</code> will drive the KNN classification results, and <code>age</code> will have almost no effect. This is contrary to our intuition that a salary difference of $<span class="math inline">\(1{,}000\)</span> is quite small compared to an age difference of <span class="math inline">\(50\)</span>~years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured <code>salary</code> in Japanese yen, or if we measured <code>age</code> in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.</p>
<p>A good way to handle this problem is to the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The <code>scale()</code> function does just this. In standardizing the data, we exclude column <span class="math inline">\(86\)</span>, because that is the qualitative <code>Purchase</code> variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>standardized.X <span class="ot">&lt;-</span> <span class="fu">scale</span>(Caravan[, <span class="sc">-</span><span class="dv">86</span>])</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(Caravan[, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 165.0378</code></pre>
</div>
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(Caravan[, <span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1647078</code></pre>
</div>
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(standardized.X[, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(standardized.X[, <span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
</div>
<p>Now every column of <code>standardized.X</code> has a standard deviation of one and a mean of zero.</p>
<p>We now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using <span class="math inline">\(K=1\)</span>, and evaluate its performance on the test data.%</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>train.X <span class="ot">&lt;-</span> standardized.X[<span class="sc">-</span>test, ]</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>test.X <span class="ot">&lt;-</span> standardized.X[test, ]</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>train.Y <span class="ot">&lt;-</span> Purchase[<span class="sc">-</span>test]</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>test.Y <span class="ot">&lt;-</span> Purchase[test]</span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a>knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train.X, test.X, train.Y, <span class="at">k =</span> <span class="dv">1</span>)</span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(test.Y <span class="sc">!=</span> knn.pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.118</code></pre>
</div>
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(test.Y <span class="sc">!=</span> <span class="st">"No"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.059</code></pre>
</div>
</div>
<p>The vector <code>test</code> is numeric, with values from <span class="math inline">\(1\)</span> through <span class="math inline">\(1,000\)</span>. Typing <code>standardized.X[test, ]</code> yields the submatrix of the data containing the observations whose indices range from <span class="math inline">\(1\)</span> to <span class="math inline">\(1,000\)</span>, whereas typing</p>
<p><code>standardized.X[-test, ]</code> yields the submatrix containing the observations whose indices do <em>not</em> range from <span class="math inline">\(1\)</span> to <span class="math inline">\(1,000\)</span>. The KNN error rate on the 1,000 test observations is just under <span class="math inline">\(12\)</span>,%. At first glance, this may appear to be fairly good. However, since only <span class="math inline">\(6\)</span>,% of customers purchased insurance, we could get the error rate down to <span class="math inline">\(6\)</span>,% by always predicting <code>No</code> regardless of the values of the predictors!</p>
<p>Suppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, perhaps a salesperson must visit each potential customer. If the company tries to sell insurance to a random selection of customers, then the success rate will be only <span class="math inline">\(6\)</span>,%, which may be far too low given the costs involved. Instead, the company would like to try to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest. Instead, the fraction of individuals that are correctly predicted to buy insurance is of interest.</p>
<p>It turns out that KNN with <span class="math inline">\(K=1\)</span> does far better than random guessing among the customers that are predicted to buy insurance. Among <span class="math inline">\(77\)</span> such customers, <span class="math inline">\(9\)</span>, or <span class="math inline">\(11.7\)</span>,%, actually do purchase insurance. This is double the rate that one would obtain from random guessing.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(knn.pred, test.Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
knn.pred  No Yes
     No  873  50
     Yes  68   9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span> <span class="sc">/</span> (<span class="dv">68</span> <span class="sc">+</span> <span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1168831</code></pre>
</div>
</div>
<p>Using <span class="math inline">\(K=3\)</span>, the success rate increases to <span class="math inline">\(19\)</span>,%, and with <span class="math inline">\(K=5\)</span> the rate is <span class="math inline">\(26.7\)</span>,%. This is over four times the rate that results from random guessing. It appears that KNN is finding some real patterns in a difficult data set!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train.X, test.X, train.Y, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(knn.pred, test.Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
knn.pred  No Yes
     No  920  54
     Yes  21   5</code></pre>
</div>
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span> <span class="sc">/</span> <span class="dv">26</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1923077</code></pre>
</div>
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train.X, test.X, train.Y, <span class="at">k =</span> <span class="dv">5</span>)</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(knn.pred, test.Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
knn.pred  No Yes
     No  930  55
     Yes  11   4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span> <span class="sc">/</span> <span class="dv">15</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2666667</code></pre>
</div>
</div>
<p>However, while this strategy is cost-effective, it is worth noting that only 15 customers are predicted to purchase insurance using KNN with <span class="math inline">\(K=5\)</span>. In practice, the insurance company may wish to expend resources on convincing more than just 15 potential customers to buy insurance.</p>
<p>As a comparison, we can also fit a logistic regression model to the data. If we use <span class="math inline">\(0.5\)</span> as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of <span class="math inline">\(0.5\)</span>. If we instead predict a purchase any time the predicted probability of purchase exceeds <span class="math inline">\(0.25\)</span>, we get much better results: we predict that 33 people will purchase insurance, and we are correct for about <span class="math inline">\(33\)</span>,% of these people. This is over five times better than random guessing!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>glm.fits <span class="ot">&lt;-</span> <span class="fu">glm</span>(Purchase <span class="sc">~</span> ., <span class="at">data =</span> Caravan,</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> binomial, <span class="at">subset =</span> <span class="sc">-</span>test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
</div>
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>glm.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fits, Caravan[test, ],</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">"No"</span>, <span class="dv">1000</span>)</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>glm.pred[glm.probs <span class="sc">&gt;</span> .<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="st">"Yes"</span></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, test.Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
glm.pred  No Yes
     No  934  59
     Yes   7   0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>glm.pred <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">"No"</span>, <span class="dv">1000</span>)</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>glm.pred[glm.probs <span class="sc">&gt;</span> .<span class="dv">25</span>] <span class="ot">&lt;-</span> <span class="st">"Yes"</span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(glm.pred, test.Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        test.Y
glm.pred  No Yes
     No  919  48
     Yes  22  11</code></pre>
</div>
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span> <span class="sc">/</span> (<span class="dv">22</span> <span class="sc">+</span> <span class="dv">11</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3333333</code></pre>
</div>
</div>
</section>
<section id="poisson-regression" class="level3" data-number="4.2.7">
<h3 data-number="4.2.7" class="anchored" data-anchor-id="poisson-regression"><span class="header-section-number">4.2.7</span> Poisson Regression</h3>
<p>Finally, we fit a Poisson regression model to the <code>Bikeshare</code> data set, which measures the number of bike rentals (<code>bikers</code>) per hour in Washington, DC. The data can be found in the <code>ISLR2</code> library.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Bikeshare)</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(Bikeshare)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 8645   15</code></pre>
</div>
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(Bikeshare)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "season"     "mnth"       "day"        "hr"         "holiday"   
 [6] "weekday"    "workingday" "weathersit" "temp"       "atemp"     
[11] "hum"        "windspeed"  "casual"     "registered" "bikers"    </code></pre>
</div>
</div>
<p>We begin by fitting a least squares linear regression model to the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>mod.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>    bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Bikeshare</span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod.lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    data = Bikeshare)

Residuals:
    Min      1Q  Median      3Q     Max 
-299.00  -45.70   -6.23   41.08  425.29 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                -68.632      5.307 -12.932  &lt; 2e-16 ***
mnthFeb                      6.845      4.287   1.597 0.110398    
mnthMarch                   16.551      4.301   3.848 0.000120 ***
mnthApril                   41.425      4.972   8.331  &lt; 2e-16 ***
mnthMay                     72.557      5.641  12.862  &lt; 2e-16 ***
mnthJune                    67.819      6.544  10.364  &lt; 2e-16 ***
mnthJuly                    45.324      7.081   6.401 1.63e-10 ***
mnthAug                     53.243      6.640   8.019 1.21e-15 ***
mnthSept                    66.678      5.925  11.254  &lt; 2e-16 ***
mnthOct                     75.834      4.950  15.319  &lt; 2e-16 ***
mnthNov                     60.310      4.610  13.083  &lt; 2e-16 ***
mnthDec                     46.458      4.271  10.878  &lt; 2e-16 ***
hr1                        -14.579      5.699  -2.558 0.010536 *  
hr2                        -21.579      5.733  -3.764 0.000168 ***
hr3                        -31.141      5.778  -5.389 7.26e-08 ***
hr4                        -36.908      5.802  -6.361 2.11e-10 ***
hr5                        -24.135      5.737  -4.207 2.61e-05 ***
hr6                         20.600      5.704   3.612 0.000306 ***
hr7                        120.093      5.693  21.095  &lt; 2e-16 ***
hr8                        223.662      5.690  39.310  &lt; 2e-16 ***
hr9                        120.582      5.693  21.182  &lt; 2e-16 ***
hr10                        83.801      5.705  14.689  &lt; 2e-16 ***
hr11                       105.423      5.722  18.424  &lt; 2e-16 ***
hr12                       137.284      5.740  23.916  &lt; 2e-16 ***
hr13                       136.036      5.760  23.617  &lt; 2e-16 ***
hr14                       126.636      5.776  21.923  &lt; 2e-16 ***
hr15                       132.087      5.780  22.852  &lt; 2e-16 ***
hr16                       178.521      5.772  30.927  &lt; 2e-16 ***
hr17                       296.267      5.749  51.537  &lt; 2e-16 ***
hr18                       269.441      5.736  46.976  &lt; 2e-16 ***
hr19                       186.256      5.714  32.596  &lt; 2e-16 ***
hr20                       125.549      5.704  22.012  &lt; 2e-16 ***
hr21                        87.554      5.693  15.378  &lt; 2e-16 ***
hr22                        59.123      5.689  10.392  &lt; 2e-16 ***
hr23                        26.838      5.688   4.719 2.41e-06 ***
workingday                   1.270      1.784   0.711 0.476810    
temp                       157.209     10.261  15.321  &lt; 2e-16 ***
weathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***
weathersitlight rain/snow  -66.494      2.965 -22.425  &lt; 2e-16 ***
weathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 76.5 on 8605 degrees of freedom
Multiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 
F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Due to space constraints, we truncate the output of <code>summary(mod.lm)</code>. In <code>mod.lm</code>, the first level of <code>hr</code> (0) and <code>mnth</code> (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines. For example, the Feb coefficient of <span class="math inline">\(6.845\)</span> signifies that, holding all other variables constant, there are on average about 7 more riders in February than in January. Similarly there are about 16.5 more riders in March than in January.</p>
<p>The results seen in Section 4.6.1 used a slightly different coding of the variables <code>hr</code> and <code>mnth</code>, as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb150"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(Bikeshare<span class="sc">$</span>hr) <span class="ot">=</span> <span class="fu">contr.sum</span>(<span class="dv">24</span>)</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(Bikeshare<span class="sc">$</span>mnth) <span class="ot">=</span> <span class="fu">contr.sum</span>(<span class="dv">12</span>)</span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>mod.lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>    bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Bikeshare</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod.lm2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    data = Bikeshare)

Residuals:
    Min      1Q  Median      3Q     Max 
-299.00  -45.70   -6.23   41.08  425.29 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                 73.5974     5.1322  14.340  &lt; 2e-16 ***
mnth1                      -46.0871     4.0855 -11.281  &lt; 2e-16 ***
mnth2                      -39.2419     3.5391 -11.088  &lt; 2e-16 ***
mnth3                      -29.5357     3.1552  -9.361  &lt; 2e-16 ***
mnth4                       -4.6622     2.7406  -1.701  0.08895 .  
mnth5                       26.4700     2.8508   9.285  &lt; 2e-16 ***
mnth6                       21.7317     3.4651   6.272 3.75e-10 ***
mnth7                       -0.7626     3.9084  -0.195  0.84530    
mnth8                        7.1560     3.5347   2.024  0.04295 *  
mnth9                       20.5912     3.0456   6.761 1.46e-11 ***
mnth10                      29.7472     2.6995  11.019  &lt; 2e-16 ***
mnth11                      14.2229     2.8604   4.972 6.74e-07 ***
hr1                        -96.1420     3.9554 -24.307  &lt; 2e-16 ***
hr2                       -110.7213     3.9662 -27.916  &lt; 2e-16 ***
hr3                       -117.7212     4.0165 -29.310  &lt; 2e-16 ***
hr4                       -127.2828     4.0808 -31.191  &lt; 2e-16 ***
hr5                       -133.0495     4.1168 -32.319  &lt; 2e-16 ***
hr6                       -120.2775     4.0370 -29.794  &lt; 2e-16 ***
hr7                        -75.5424     3.9916 -18.925  &lt; 2e-16 ***
hr8                         23.9511     3.9686   6.035 1.65e-09 ***
hr9                        127.5199     3.9500  32.284  &lt; 2e-16 ***
hr10                        24.4399     3.9360   6.209 5.57e-10 ***
hr11                       -12.3407     3.9361  -3.135  0.00172 ** 
hr12                         9.2814     3.9447   2.353  0.01865 *  
hr13                        41.1417     3.9571  10.397  &lt; 2e-16 ***
hr14                        39.8939     3.9750  10.036  &lt; 2e-16 ***
hr15                        30.4940     3.9910   7.641 2.39e-14 ***
hr16                        35.9445     3.9949   8.998  &lt; 2e-16 ***
hr17                        82.3786     3.9883  20.655  &lt; 2e-16 ***
hr18                       200.1249     3.9638  50.488  &lt; 2e-16 ***
hr19                       173.2989     3.9561  43.806  &lt; 2e-16 ***
hr20                        90.1138     3.9400  22.872  &lt; 2e-16 ***
hr21                        29.4071     3.9362   7.471 8.74e-14 ***
hr22                        -8.5883     3.9332  -2.184  0.02902 *  
hr23                       -37.0194     3.9344  -9.409  &lt; 2e-16 ***
workingday                   1.2696     1.7845   0.711  0.47681    
temp                       157.2094    10.2612  15.321  &lt; 2e-16 ***
weathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***
weathersitlight rain/snow  -66.4944     2.9652 -22.425  &lt; 2e-16 ***
weathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 76.5 on 8605 degrees of freedom
Multiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 
F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>What is the difference between the two codings? In <code>mod.lm2</code>, a coefficient estimate is reported for all but the last level of <code>hr</code> and <code>mnth</code>. Importantly, in <code>mod.lm2</code>, the coefficient estimate for the last level of <code>mnth</code> is not zero: instead, it equals the <em>negative of the sum of the coefficient estimates for all of the other levels</em>. Similarly, in <code>mod.lm2</code>, the coefficient estimate for the last level of <code>hr</code> is the negative of the sum of the coefficient estimates for all of the other levels. This means that the coefficients of <code>hr</code> and <code>mnth</code> in <code>mod.lm2</code> will always sum to zero, and can be interpreted as the difference from the mean level. For example, the coefficient for January of <span class="math inline">\(-46.087\)</span> indicates that, holding all other variables constant, there are typically 46 fewer riders in January relative to the yearly average.</p>
<p>It is important to realize that the choice of coding really does not matter, provided that we interpret the model output correctly in light of the coding used. For example, we see that the predictions from the linear model are the same regardless of coding:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((<span class="fu">predict</span>(mod.lm) <span class="sc">-</span> <span class="fu">predict</span>(mod.lm2))<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.586608e-18</code></pre>
</div>
</div>
<p>The sum of squared differences is zero. We can also see this using the <code>all.equal()</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="fu">all.equal</span>(<span class="fu">predict</span>(mod.lm), <span class="fu">predict</span>(mod.lm2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>To reproduce the left-hand side of Figure 4.13, we must first obtain the coefficient estimates associated with <code>mnth</code>. The coefficients for January through November can be obtained directly from the <code>mod.lm2</code> object. The coefficient for December must be explicitly computed as the negative sum of all the other months.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb156"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>coef.months <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>],</span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To make the plot, we manually label the <span class="math inline">\(x\)</span>-axis with the names of the months.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(coef.months, <span class="at">xlab =</span> <span class="st">"Month"</span>, <span class="at">ylab =</span> <span class="st">"Coefficient"</span>,</span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">xaxt =</span> <span class="st">"n"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">"o"</span>)</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"J"</span>, <span class="st">"F"</span>, <span class="st">"M"</span>, <span class="st">"A"</span>,</span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"M"</span>, <span class="st">"J"</span>, <span class="st">"J"</span>, <span class="st">"A"</span>, <span class="st">"S"</span>, <span class="st">"O"</span>, <span class="st">"N"</span>, <span class="st">"D"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk41-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Reproducing the right-hand side of Figure 4.13 follows a similar process.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb158"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>coef.hours <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>],</span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>]))</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(coef.hours, <span class="at">xlab =</span> <span class="st">"Hour"</span>, <span class="at">ylab =</span> <span class="st">"Coefficient"</span>,</span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">"o"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk42-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Now, we consider instead fitting a Poisson regression model to the <code>Bikeshare</code> data. Very little changes, except that we now use the function <code>glm()</code> with the argument <code>family = poisson</code> to specify that we wish to fit a Poisson regression model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>mod.pois <span class="ot">&lt;-</span> <span class="fu">glm</span>(</span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a>    bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Bikeshare, <span class="at">family =</span> poisson</span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod.pois)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    family = poisson, data = Bikeshare)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-20.7574   -3.3441   -0.6549    2.6999   21.9628  

Coefficients:
                           Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)                4.118245   0.006021  683.964  &lt; 2e-16 ***
mnth1                     -0.670170   0.005907 -113.445  &lt; 2e-16 ***
mnth2                     -0.444124   0.004860  -91.379  &lt; 2e-16 ***
mnth3                     -0.293733   0.004144  -70.886  &lt; 2e-16 ***
mnth4                      0.021523   0.003125    6.888 5.66e-12 ***
mnth5                      0.240471   0.002916   82.462  &lt; 2e-16 ***
mnth6                      0.223235   0.003554   62.818  &lt; 2e-16 ***
mnth7                      0.103617   0.004125   25.121  &lt; 2e-16 ***
mnth8                      0.151171   0.003662   41.281  &lt; 2e-16 ***
mnth9                      0.233493   0.003102   75.281  &lt; 2e-16 ***
mnth10                     0.267573   0.002785   96.091  &lt; 2e-16 ***
mnth11                     0.150264   0.003180   47.248  &lt; 2e-16 ***
hr1                       -0.754386   0.007879  -95.744  &lt; 2e-16 ***
hr2                       -1.225979   0.009953 -123.173  &lt; 2e-16 ***
hr3                       -1.563147   0.011869 -131.702  &lt; 2e-16 ***
hr4                       -2.198304   0.016424 -133.846  &lt; 2e-16 ***
hr5                       -2.830484   0.022538 -125.586  &lt; 2e-16 ***
hr6                       -1.814657   0.013464 -134.775  &lt; 2e-16 ***
hr7                       -0.429888   0.006896  -62.341  &lt; 2e-16 ***
hr8                        0.575181   0.004406  130.544  &lt; 2e-16 ***
hr9                        1.076927   0.003563  302.220  &lt; 2e-16 ***
hr10                       0.581769   0.004286  135.727  &lt; 2e-16 ***
hr11                       0.336852   0.004720   71.372  &lt; 2e-16 ***
hr12                       0.494121   0.004392  112.494  &lt; 2e-16 ***
hr13                       0.679642   0.004069  167.040  &lt; 2e-16 ***
hr14                       0.673565   0.004089  164.722  &lt; 2e-16 ***
hr15                       0.624910   0.004178  149.570  &lt; 2e-16 ***
hr16                       0.653763   0.004132  158.205  &lt; 2e-16 ***
hr17                       0.874301   0.003784  231.040  &lt; 2e-16 ***
hr18                       1.294635   0.003254  397.848  &lt; 2e-16 ***
hr19                       1.212281   0.003321  365.084  &lt; 2e-16 ***
hr20                       0.914022   0.003700  247.065  &lt; 2e-16 ***
hr21                       0.616201   0.004191  147.045  &lt; 2e-16 ***
hr22                       0.364181   0.004659   78.173  &lt; 2e-16 ***
hr23                       0.117493   0.005225   22.488  &lt; 2e-16 ***
workingday                 0.014665   0.001955    7.502 6.27e-14 ***
temp                       0.785292   0.011475   68.434  &lt; 2e-16 ***
weathersitcloudy/misty    -0.075231   0.002179  -34.528  &lt; 2e-16 ***
weathersitlight rain/snow -0.575800   0.004058 -141.905  &lt; 2e-16 ***
weathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 1052921  on 8644  degrees of freedom
Residual deviance:  228041  on 8605  degrees of freedom
AIC: 281159

Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<p>We can plot the coefficients associated with <code>mnth</code> and <code>hr</code>, in order to reproduce Figure 4.15:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>coef.mnth <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>],</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]))</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(coef.mnth, <span class="at">xlab =</span> <span class="st">"Month"</span>, <span class="at">ylab =</span> <span class="st">"Coefficient"</span>,</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">"n"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">"o"</span>)</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"J"</span>, <span class="st">"F"</span>, <span class="st">"M"</span>, <span class="st">"A"</span>, <span class="st">"M"</span>, <span class="st">"J"</span>, <span class="st">"J"</span>, <span class="st">"A"</span>, <span class="st">"S"</span>, <span class="st">"O"</span>, <span class="st">"N"</span>, <span class="st">"D"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk44-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>coef.hours <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>],</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>     <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>]))</span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(coef.hours, <span class="at">xlab =</span> <span class="st">"Hour"</span>, <span class="at">ylab =</span> <span class="st">"Coefficient"</span>,</span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">"o"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk44-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can once again use the <code>predict()</code> function to obtain the fitted values (predictions) from this Poisson regression model. However, we must use the argument <code>type = "response"</code> to specify that we want <code>R</code> to output <span class="math inline">\(\exp(\hat\beta_0 + \hat\beta_1 X_1 + \ldots +\hat\beta_p X_p)\)</span> rather than <span class="math inline">\(\hat\beta_0 + \hat\beta_1 X_1 + \ldots + \hat\beta_p X_p\)</span>, which it will output by default.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(mod.lm2), <span class="fu">predict</span>(mod.pois, <span class="at">type =</span> <span class="st">"response"</span>))</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch4_Classification_files/figure-html/chunk45-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative. As a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership.</p>
<p>In this section, we used the <code>glm()</code> function with the argument <code>family = poisson</code> in order to perform Poisson regression. Earlier in this lab we used the <code>glm()</code> function with <code>family = binomial</code> to perform logistic regression. Other choices for the <code>family</code> argument can be used to fit other types of GLMs. For instance, <code>family = Gamma</code> fits a gamma regression model.</p>
</section>
</section>
<section id="exercises" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="exercises"><span class="header-section-number">4.3</span> Exercises</h2>
<p>Prepare the following exercises of Chapter 4 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 6</li>
<li>Exercise 13</li>
<li>Exercise 15</li>
<li>Exercise 16</li>
</ul>
<!-- {{< include Ch4_Solutions.qmd >}} -->


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Technically, this definition is only correct if <span class="math inline">\(X\)</span> is a qualitative random variable. If <span class="math inline">\(X\)</span> is quantitative, then <span class="math inline">\(f_k(x)dx\)</span> corresponds to the probability of <span class="math inline">\(X\)</span> falling in a small region <span class="math inline">\(dx\)</span> around <span class="math inline">\(x.\)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch3_LinearRegression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch5_ResamplingMethods.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>