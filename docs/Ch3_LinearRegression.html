<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computer-Aided Statistical Analysis (B.Sc.) - 3&nbsp; Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch4_Classification.html" rel="next">
<link href="./Ch2_StatLearning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_StatLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_LinearRegression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_Classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_ResamplingMethods.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_LinModSelectRegul.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Model Selection and Regularization</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#lecture-notes" id="toc-lecture-notes" class="nav-link active" data-scroll-target="#lecture-notes"> <span class="header-section-number">3.1</span> Lecture Notes</a></li>
  <li><a href="#ch.-3.1-simple-linear-regression" id="toc-ch.-3.1-simple-linear-regression" class="nav-link" data-scroll-target="#ch.-3.1-simple-linear-regression">(Ch. 3.1) Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#ch.-3.1.1-estimating-the-coefficients" id="toc-ch.-3.1.1-estimating-the-coefficients" class="nav-link" data-scroll-target="#ch.-3.1.1-estimating-the-coefficients">(Ch. 3.1.1) Estimating the Coefficients</a></li>
  <li><a href="#ch.-3.1.2-assessing-the-accuracy-of-the-coefficient-estimates" id="toc-ch.-3.1.2-assessing-the-accuracy-of-the-coefficient-estimates" class="nav-link" data-scroll-target="#ch.-3.1.2-assessing-the-accuracy-of-the-coefficient-estimates">(Ch. 3.1.2) Assessing the Accuracy of the Coefficient Estimates</a></li>
  <li><a href="#ch.-3.1.3-assessing-the-accuracy-of-the-model" id="toc-ch.-3.1.3-assessing-the-accuracy-of-the-model" class="nav-link" data-scroll-target="#ch.-3.1.3-assessing-the-accuracy-of-the-model">(Ch. 3.1.3) Assessing the Accuracy of the Model</a></li>
  </ul></li>
  <li><a href="#multiple-linear-regression-ch.-3.2" id="toc-multiple-linear-regression-ch.-3.2" class="nav-link" data-scroll-target="#multiple-linear-regression-ch.-3.2">Multiple Linear Regression (Ch. 3.2)</a>
  <ul class="collapse">
  <li><a href="#ch.-3.2.1-estimating-the-regression-coefficients" id="toc-ch.-3.2.1-estimating-the-regression-coefficients" class="nav-link" data-scroll-target="#ch.-3.2.1-estimating-the-regression-coefficients">(Ch. 3.2.1) Estimating the Regression Coefficients</a></li>
  <li><a href="#ch.-3.2.2-some-important-questions" id="toc-ch.-3.2.2-some-important-questions" class="nav-link" data-scroll-target="#ch.-3.2.2-some-important-questions">(Ch. 3.2.2) Some Important Questions</a></li>
  </ul></li>
  <li><a href="#ch.-3.3-other-considerations-in-the-regression-model" id="toc-ch.-3.3-other-considerations-in-the-regression-model" class="nav-link" data-scroll-target="#ch.-3.3-other-considerations-in-the-regression-model">(Ch. 3.3) Other Considerations in the Regression Model</a>
  <ul class="collapse">
  <li><a href="#ch.-3.3.1-qualitative-predictors" id="toc-ch.-3.3.1-qualitative-predictors" class="nav-link" data-scroll-target="#ch.-3.3.1-qualitative-predictors">(Ch. 3.3.1) Qualitative Predictors</a></li>
  <li><a href="#ch.-3.3.2-extensions-of-the-linear-model" id="toc-ch.-3.3.2-extensions-of-the-linear-model" class="nav-link" data-scroll-target="#ch.-3.3.2-extensions-of-the-linear-model">(Ch. 3.3.2) Extensions of the Linear Model</a></li>
  <li><a href="#ch.-3.3.3-potential-problems" id="toc-ch.-3.3.3-potential-problems" class="nav-link" data-scroll-target="#ch.-3.3.3-potential-problems">(Ch. 3.3.3) Potential Problems</a></li>
  </ul></li>
  <li><a href="#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors" id="toc-ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors" class="nav-link" data-scroll-target="#ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors">(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors</a></li>
  <li><a href="#r-lab-linear-regression" id="toc-r-lab-linear-regression" class="nav-link" data-scroll-target="#r-lab-linear-regression"> <span class="header-section-number">3.2</span> <code>R</code>-Lab: Linear Regression</a>
  <ul class="collapse">
  <li><a href="#libraries" id="toc-libraries" class="nav-link" data-scroll-target="#libraries"> <span class="header-section-number">3.2.1</span> Libraries</a></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression"> <span class="header-section-number">3.2.2</span> Simple Linear Regression</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"> <span class="header-section-number">3.2.3</span> Multiple Linear Regression</a></li>
  <li><a href="#interaction-terms" id="toc-interaction-terms" class="nav-link" data-scroll-target="#interaction-terms"> <span class="header-section-number">3.2.4</span> Interaction Terms</a></li>
  <li><a href="#non-linear-transformations-of-the-predictors" id="toc-non-linear-transformations-of-the-predictors" class="nav-link" data-scroll-target="#non-linear-transformations-of-the-predictors"> <span class="header-section-number">3.2.5</span> Non-linear Transformations of the Predictors</a></li>
  <li><a href="#qualitative-predictors" id="toc-qualitative-predictors" class="nav-link" data-scroll-target="#qualitative-predictors"> <span class="header-section-number">3.2.6</span> Qualitative Predictors</a></li>
  <li><a href="#writing-functions" id="toc-writing-functions" class="nav-link" data-scroll-target="#writing-functions"> <span class="header-section-number">3.2.7</span> Writing Functions</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"> <span class="header-section-number">3.3</span> Exercises</a></li>
  <li><a href="#solutions" id="toc-solutions" class="nav-link" data-scroll-target="#solutions"> <span class="header-section-number">3.4</span> Solutions</a>
  <ul class="collapse">
  <li><a href="#exercise-1" id="toc-exercise-1" class="nav-link" data-scroll-target="#exercise-1">Exercise 1</a></li>
  <li><a href="#exercise-2" id="toc-exercise-2" class="nav-link" data-scroll-target="#exercise-2">Exercise 2</a></li>
  <li><a href="#exercise-3" id="toc-exercise-3" class="nav-link" data-scroll-target="#exercise-3">Exercise 3</a></li>
  <li><a href="#exercise-8" id="toc-exercise-8" class="nav-link" data-scroll-target="#exercise-8">Exercise 8</a></li>
  <li><a href="#exercise-9" id="toc-exercise-9" class="nav-link" data-scroll-target="#exercise-9">Exercise 9</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="lecture-notes" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="lecture-notes"><span class="header-section-number">3.1</span> Lecture Notes</h2>
</section>
<section id="ch.-3.1-simple-linear-regression" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="ch.-3.1-simple-linear-regression">(Ch. 3.1) Simple Linear Regression</h2>
<p>The linear regression model assumes a <em>linear relationship</em> between <span class="math inline">\(Y\)</span> and the predictor(s) <span class="math inline">\(X\)</span>.</p>
<p>The simple (only one predictor) linear regression model:</p>
<p><span class="math display">\[
Y\approx \beta_0 + \beta_1 X
\]</span></p>
For instance,
<center>
<code>sales</code> <span class="math inline">\(\approx \beta_0 + \beta_1\)</span> <code>TV</code>
</center>
<section id="ch.-3.1.1-estimating-the-coefficients" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.1.1-estimating-the-coefficients">(Ch. 3.1.1) Estimating the Coefficients</h3>
<p>We choose <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> such that the Residual Sum of Squares criterion is minimized:</p>
<p><span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
&amp; = e_1^2 + \dots + e_n^2\\
&amp;=)\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2
\end{align*}
\]</span></p>
<p>The minimizers are</p>
<p><span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
\]</span></p>
<p>where <span class="math inline">\(\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i\)</span> and <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\)</span>.</p>
<p><img src="images/Fig_3_1.png" class="img-fluid"></p>
<p><img src="images/Fig_3_2.png" class="img-fluid"></p>
</section>
<section id="ch.-3.1.2-assessing-the-accuracy-of-the-coefficient-estimates" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.1.2-assessing-the-accuracy-of-the-coefficient-estimates">(Ch. 3.1.2) Assessing the Accuracy of the Coefficient Estimates</h3>
<p>True unknown model</p>
<p><span class="math display">\[
Y=f(X)+\epsilon
\]</span></p>
<p>In in linear regression analysis, we assume<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> that</p>
<p><span class="math display">\[
f(X) = \beta_0 + \beta_1 X
\]</span></p>
<p>Ordinary least squares estimators</p>
<p><span class="math display">\[
\hat\beta_0\quad\text{and}\quad\hat\beta_1
\]</span></p>
<p>are <strong>unbiased</strong>, that is</p>
<p><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta_0)&amp;=E(\hat\beta_0)-\beta_0=0\\
\operatorname{Bias}(\hat\beta_1)&amp;=E(\hat\beta_1)-\beta_1=0
\end{align*}
\]</span></p>
<p>I.e., on average, the estimation results equal the true (unknown) parameters. However, in an actual data analysis, we only have one realization of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> computed from one give dataset and thus we cannot compute averages of estimation results. Each single estimation result will have estimation errors, i.e.,</p>
<p><span class="math display">\[
\hat\beta_0\neq \beta_0\quad\text{and}\quad\hat\beta_1\neq \beta_1.
\]</span></p>
<p>The following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook <code>ISLR</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ###############################</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="do">## A function to generate data </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="do">## similar to that shown in Fig 3.3</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="do">## ##############################</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="fl">0.1</span>                          <span class="co"># intercept parameter</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="dv">5</span>  </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="do">## A Function to simulate data</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>myDataGenerator <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  n      <span class="ot">&lt;-</span> <span class="dv">50</span>                           <span class="co"># sample size</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  beta_0 <span class="ot">&lt;-</span> <span class="fl">0.1</span>                          <span class="co"># intercept parameter</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  beta_1 <span class="ot">&lt;-</span> <span class="dv">5</span>                            <span class="co"># slope parameter</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  X      <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">min =</span> <span class="sc">-</span><span class="dv">2</span>, <span class="at">max =</span> <span class="dv">2</span>)  <span class="co"># predictor</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  error  <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">8.5</span>) <span class="co"># error term</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  Y      <span class="ot">&lt;-</span> beta_0 <span class="sc">+</span> beta_1 <span class="sc">*</span> X <span class="sc">+</span> error  <span class="co"># outcome </span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="do">##</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">data.frame</span>(<span class="st">"Y"</span> <span class="ot">=</span> Y, <span class="st">"X"</span> <span class="ot">=</span> X))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate a first realization of the data</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>data_sim <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_sim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Y          X
1 -18.4853427 -0.8496899
2  12.9872926  1.1532205
3  -0.4167901 -0.3640923
4  -1.9138159  1.5320696
5  19.5667725  1.7618691
6  -5.3639241 -1.8177740</code></pre>
</div>
</div>
<p>Using repeated samples form the data generating process defined in <code>myDataGenerator()</code>, we can generate multiple estimation results of the unknown simple linear regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and plot the corresponding empirical regression lines:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Estimation</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>lm_obj <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> data_sim)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Plotting the results</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)) <span class="co"># Two plots side by side</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="do">## First Plot (fit for the first realization of the data)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> data_sim<span class="sc">$</span>X, <span class="at">y =</span> data_sim<span class="sc">$</span>Y, <span class="at">xlab =</span> <span class="st">"X"</span>, <span class="at">ylab =</span> <span class="st">"Y"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> beta_0, <span class="at">b =</span> beta_1, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm_obj, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Second Plot (fits for multiple data realizations)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> data_sim<span class="sc">$</span>X, <span class="at">y =</span> data_sim<span class="sc">$</span>Y, <span class="at">xlab =</span> <span class="st">"X"</span>, <span class="at">ylab =</span> <span class="st">"Y"</span>, <span class="at">type =</span> <span class="st">"n"</span>) <span class="co"># type = "n": empty plot</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>){</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  data_sim_new <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  lm_obj_new   <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data=</span>data_sim_new)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(lm_obj_new, <span class="at">col =</span> <span class="st">"lightskyblue"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Adding the first fit</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> beta_0, <span class="at">b =</span> beta_1, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm_obj, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<ul>
<li>Coding-Questions: Can you do this animated? https://gganimate.com/articles/gganimate.html</li>
</ul>
<p>The magnitude of the estimation errors is expressed in unites of <strong>standard errors</strong>:</p>
<p><span class="math display">\[
\operatorname{SE}(\hat\beta_0)=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\operatorname{SE}(\hat\beta_1)=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2},
\]</span></p>
<p>where <span class="math inline">\(Var(\epsilon)=\sigma^2\Leftrightarrow \operatorname{SD}(\epsilon)=\sqrt{Var(\epsilon)}=\sigma\)</span>.</p>
<p>Typically, <span class="math inline">\(\sigma\)</span> is unknown, but can be estimated by</p>
<p><span class="math display">\[
\sigma\approx\hat{\sigma}=\operatorname{RSE}=\sqrt{\frac{\operatorname(RSS)}{n-2}},
\]</span></p>
<p>where we subtract <span class="math inline">\(2\)</span> from the sampel size <span class="math inline">\(n\)</span> since <span class="math inline">\(n-2\)</span> are the remaining degrees of freedom in the data after estimating two parameters <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p>Knowing <span class="math inline">\(\operatorname{SE}(\hat\beta_0)\)</span> and <span class="math inline">\(\operatorname{SE}(\hat\beta_1)\)</span> allows us to construct <strong>Confidence Intervals</strong>:</p>
<p><span class="math display">\[
\begin{align*}
\operatorname{CI}_{\beta_1}
&amp;=\left[\hat{\beta_1}-2\operatorname{SE}(\hat\beta_1),\;
        \hat{\beta_1}+2\operatorname{SE}(\hat\beta_1)\right]\\
&amp;=\hat\beta_1\pm 2\operatorname{SE}(\hat\beta_1)
\end{align*}
\]</span></p>
<p>likewise for <span class="math inline">\(\operatorname{CI}_{\beta_1}\)</span>.</p>
<p><strong>Interpretation:</strong> There is approximately a 95% change (in infinite resamplings) that the (random) confidence interval <span class="math inline">\(\operatorname{CI}_{\beta_1}\)</span> contains the true (fix) parameter value <span class="math inline">\(\beta_1\)</span>.</p>
<p>Thus, a given confidence interval either contains the true parameter value or not and we usually do not know it. To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:</p>
<ul>
<li><a href="https://rpsychologist.com/d3/ci/">Interactive visualization for interpreting confidence intervals</a></li>
</ul>
<p>Standard errors can also be used to do hypothesis testing:</p>
<p><span class="math display">\[
\begin{align*}
H_0:&amp;\;\text{There is no relationship between $Y$ and $X$; i.e. $\beta_1=0$}\\
H_1:&amp;\;\text{There is a relationship between $Y$ and $X$; i.e. $\beta_1\neq 0$}
\end{align*}
\]</span></p>
<p><span class="math inline">\(t\)</span>-test statistic</p>
<p><span class="math display">\[
t=\frac{\hat\beta_1 - 0}{\operatorname{SE}(\hat\beta_1)}\overset{H_0}{\sim}t_{(n-1)}
\]</span></p>
<p><span class="math inline">\(p\)</span>-value</p>
<p><span class="math display">\[
\begin{align*}
p
&amp;=P_{H_0}\left(|t|\geq|t_{obs}|\right)
&amp;=2\cdot\min\{P_{H_0}\left(t\geq t_{obs} \right),\; P_{H_0}\left(t\leq t_{obs} \right)\},
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(t_{obs}\)</span> denotes the observed value of the <span class="math inline">\(t\)</span>-test statistic and where <span class="math inline">\(t\)</span> is <span class="math inline">\(t\)</span>-distributed with <span class="math inline">\((n-2)\)</span> degrees of freedom.</p>
<p>Select a significance level <span class="math inline">\(\alpha\)</span> (e.g.&nbsp;<span class="math inline">\(\alpha=0.01\)</span> or <span class="math inline">\(\alpha=0.05\)</span>) and reject <span class="math inline">\(H_0\)</span> if</p>
<p><span class="math display">\[
p&lt;\alpha
\]</span></p>
<p><img src="images/Tab_3_1.png" class="img-fluid"></p>
</section>
<section id="ch.-3.1.3-assessing-the-accuracy-of-the-model" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.1.3-assessing-the-accuracy-of-the-model">(Ch. 3.1.3) Assessing the Accuracy of the Model</h3>
<p>In tendency an accurate model has …</p>
<ul>
<li>a low <span class="math inline">\(\operatorname{RSE}\)</span></li>
</ul>
<p><span class="math display">\[
\operatorname{RSE}=\hat\sigma=\sqrt{\frac{\operatorname{RSS}}{n-2}}
\]</span></p>
<ul>
<li>a high <span class="math inline">\(R^2\)</span></li>
</ul>
<p><span class="math display">\[
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
\]</span></p>
<p>where <span class="math inline">\(0\leq R^2\leq 1\)</span> and</p>
<p><span class="math display">\[
\begin{align*}
\operatorname{TSS}&amp;=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2\\
\operatorname{RSS}&amp;=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2\\
\hat{y}_i&amp;=\hat\beta_0+\hat\beta_1x_i
\end{align*}
\]</span></p>
<p><strong>Caution:</strong> Do not forget that there is a <strong>irreducible error</strong> <span class="math inline">\(Var(\epsilon)=\sigma^2&gt;0\)</span>. Thus</p>
<ul>
<li>very low <span class="math inline">\(\operatorname{RSE}\)</span> values, <span class="math inline">\(\operatorname{RSE}\approx 0\)</span>, and</li>
<li>very high <span class="math inline">\(R^2\)</span> values, <span class="math inline">\(R^2\approx 1\)</span>,</li>
</ul>
<p>can be warning signals indicating overfitting.</p>
<p>In the case of the simple linear regression model, <span class="math inline">\(R^2\)</span> equals the squared sample correlation coefficient between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[
R^2 = r^2,
\]</span></p>
<p>where</p>
<p><span class="math display">\[
r=\widehat{cor}(Y,X)=\frac{\sqrt{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}.
\]</span></p>
</section>
</section>
<section id="multiple-linear-regression-ch.-3.2" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="multiple-linear-regression-ch.-3.2">Multiple Linear Regression (Ch. 3.2)</h2>
<p>The multiple linear regression model allows for more than only one predictor:</p>
<p><span class="math display">\[
Y\approx \beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p + \epsilon
\]</span></p>
For instance,
<center>
<code>sales</code> <span class="math inline">\(\approx \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+\beta_2\)</span> <code>radio</code> <span class="math inline">\(+\beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<section id="ch.-3.2.1-estimating-the-regression-coefficients" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.2.1-estimating-the-regression-coefficients">(Ch. 3.2.1) Estimating the Regression Coefficients</h3>
<p>Select</p>
<p><span class="math display">\[
\hat\beta_0,\dots,\hat\beta_p
\]</span></p>
<p>by minimizing</p>
<p><span class="math display">\[
\operatorname{RSS}=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2,
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{y}_i=\hat\beta_0 + \hat\beta_1 x_{i1} \dots + \hat\beta_p x_{ip}
\]</span></p>
<p><img src="images/Fig_3_4.png" class="img-fluid"></p>
<p>Multiple linear regression is more than mere composition of single simple linear regression models.</p>
<p>Take a look at the following two simple linear regression results:</p>
<p><img src="images/Tab_3_3.png" class="img-fluid"></p>
<p>Thus in separate simple linear regressions, the effects of <code>radio</code> and the effect of <code>newspaper</code> on <code>sales</code> are both (but separately) statistically.</p>
<p>By contrast, when looking at the multiple linear regression when regressing <code>sales</code> onto both <code>radio</code> and <code>newspaper</code>, only the effect of <code>radio</code> remains statistically significant:</p>
<p><img src="images/Tab_3_4.png" class="img-fluid"></p>
<p><strong>Reason: Omitted Variable Bias</strong></p>
<ul>
<li><code>radio</code> has an effect on <code>sales</code></li>
<li><code>newspaper</code> has actually no effect on <code>sales</code></li>
<li>But, <code>newspaper</code> is “strongly” correlated with <code>radio</code> (cor(<code>newspaper</code>,<code>radio</code>)=0.3541); see Table 3.5</li>
</ul>
<p><img src="images/Tab_3_5.png" class="img-fluid"></p>
<ul>
<li>Thus, when omitting <code>radio</code> from the multiple regression model, <code>newspaper</code> becomes a surrogate for <code>radio</code>. This is called a <em>Omitted Variable Bias</em>.</li>
</ul>
<p>Conclusion: Simple linear regression can be dangerous. We need to control for all possibly relevant variables if we want to interpret the estimation results (“Inference”).</p>
<p><strong>Interpretation of the Coefficients in Table 3.5</strong></p>
<p>For fixed values of <code>TV</code> and <code>newspaper</code>, spending additionally 1000 USD for <code>radio</code>, increases on average <code>sales</code> by approximately 189 units.</p>
</section>
<section id="ch.-3.2.2-some-important-questions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.2.2-some-important-questions">(Ch. 3.2.2) Some Important Questions</h3>
<p><strong>1. Is There a Relationship Between the Response and Predictors?</strong></p>
<p><span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta_2=\dots=\beta_p=0\\
H_1:&amp;\;\text{at least one $\beta_j\neq 0$; $j=1,\dots,p$}
\end{align*}
\]</span></p>
<p><span class="math inline">\(F\)</span>-test statistic</p>
<p><span class="math display">\[
F=\frac{(\operatorname{TSS}-\operatorname{RSS})/p}{\operatorname{
  RSS}/(n-p-1)}
\]</span></p>
<p>If <span class="math inline">\(H_0\)</span> is correct</p>
<p><span class="math display">\[
\begin{align*}
E(\operatorname{RSS}/(n-p-1))&amp;=\sigma^2\\
E((\operatorname{TSS}-\operatorname{RSS})/p)&amp;=\sigma^2\\
\end{align*}
\]</span></p>
<ul>
<li>Thus, if <span class="math inline">\(H_0\)</span> is correct, we expect values of <span class="math inline">\(F\approx 1\)</span>.</li>
<li>But if <span class="math inline">\(H_1\)</span> is correct, we expect values of <span class="math inline">\(F\gg 1\)</span>.</li>
</ul>
<p>Caution: Cannot be computed if <span class="math inline">\(p&gt;n\)</span>. (Chapter 6 on “high dimensional problems”)</p>
</section>
</section>
<section id="ch.-3.3-other-considerations-in-the-regression-model" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="ch.-3.3-other-considerations-in-the-regression-model">(Ch. 3.3) Other Considerations in the Regression Model</h2>
<section id="ch.-3.3.1-qualitative-predictors" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.3.1-qualitative-predictors">(Ch. 3.3.1) Qualitative Predictors</h3>
<p>Often some predictors are <em>qualitative</em> variables (also known as a <em>factor</em> variables). For instance, the <code>Credit</code> dataset contains the following qualitative predictors:</p>
<ul>
<li><code>own</code> (house ownership)</li>
<li><code>student</code> (student status)</li>
<li><code>status</code> (marital status)</li>
<li><code>region</code> (East, West or South)</li>
</ul>
<section id="predictors-with-only-two-levels" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="predictors-with-only-two-levels">Predictors with Only Two Levels</h4>
<p>If a qualitative predictor (factor) only has two levels (i.e.&nbsp;possible values), then incorporating it into a regression model is very simple. We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values; for instance,</p>
<p><span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person owns a house}\\
  0&amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span></p>
<p>Using this dummy variable as a predictor in the regression equation results in the following regression model:</p>
<p><span class="math display">\[
y_{i}=\beta_0 + \beta_1 x_i + \epsilon_i = \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i &amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 + \epsilon_i           &amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: The average credit card balance among those who do not own a house</li>
<li><span class="math inline">\(\beta_0+\beta_1\)</span>: The average credit card balance among those who do own a house</li>
<li><span class="math inline">\(\beta_1\)</span>: The average difference in credit card balance between owners and non-owners</li>
</ul>
<p><img src="images/Tab_3_7.png" class="img-fluid"></p>
<p>Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable</p>
<p><span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1 &amp;\quad \text{if the $i$th person owns a house}\\
-1 &amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span></p>
<p><span class="math display">\[
y_{i}=\beta_0 + \beta_1 x_i + \epsilon_i = \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 - \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: The overall average credit card balance (ignoring the house ownership effect)</li>
<li><span class="math inline">\(\beta_1\)</span>: The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.</li>
</ul>
</section>
<section id="qualitative-predictors-with-more-than-two-levels" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h4>
<p>When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the <code>region</code> <span class="math inline">\(\in\{\)</span><code>South</code>, <code>West</code>, <code>East</code><span class="math inline">\(\}\)</span> variable we create <strong>two</strong> dummy variables. The first could be</p>
<p><span class="math display">\[
x_{i1} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the South}\\
  0&amp;\quad \text{if the $i$th person is not from the South,}
  \end{array}\right.
\]</span></p>
<p>and the second could be</p>
<p><span class="math display">\[
x_{i2} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the West}\\
  0&amp;\quad \text{if the $i$th person is not from the West.}
  \end{array}\right.
\]</span></p>
<p>Using both of these dummy variables results in the following regression model: order to obtain the model</p>
<p><span class="math display">\[
y_{i}=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i = \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1  + \epsilon_i&amp; \quad \text{if the $i$th person is from the South}\\
  \beta_0 + \beta_2  + \epsilon_i&amp; \quad \text{if the $i$th person is from the West}\\
  \beta_0            + \epsilon_i&amp; \quad \text{if the $i$th person is from the East.}\\
  \end{array}\right.
\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: The average credit card balance for individuals from the East</li>
<li><span class="math inline">\(\beta_1\)</span>: The difference in the average balance between people from the South versus the East</li>
<li><span class="math inline">\(\beta_2\)</span>: The difference in the average balance between people from the West versus the East</li>
</ul>
<p><img src="images/Tab_3_8.png" class="img-fluid"></p>
<p>There are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular <strong>contrasts</strong>. (A detailed discussion of <em>contrasts</em> is beyond the scope of this lecture.)</p>
</section>
</section>
<section id="ch.-3.3.2-extensions-of-the-linear-model" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.3.2-extensions-of-the-linear-model">(Ch. 3.3.2) Extensions of the Linear Model</h3>
<section id="interaction-effects-removing-the-additive-assumption-using-interaction-effects" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="interaction-effects-removing-the-additive-assumption-using-interaction-effects">Interaction Effects: Removing the Additive Assumption using Interaction Effects</h4>
<p>Previously, we used the following model</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p>which states that the average increase in sales associated with a one-unit increase in <code>TV</code> is always <span class="math inline">\(\beta_1,\)</span> regardless of the amount spent on radio.</p>
<p>However, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on radio advertising actually increases the effectiveness of TV advertising.</p>
<p>Figure 3.5 suggests that such an effect may be present in the advertising data:</p>
<ul>
<li>When levels of either <code>TV</code> or <code>radio</code> are low, then the true <code>sales</code> are lower than predicted by the linear model.</li>
<li>But when advertising is split between the two media, then the model tends to <strong>underestimate</strong> sales. <img src="images/Fig_3_5.png" class="img-fluid"></li>
</ul>
<p><strong>Solution: Interaction Effects:</strong></p>
<p>Consider the standard linear regression model with two variables,</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon.
\]</span></p>
<p>Here each predictor <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> has a given effect, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, on <span class="math inline">\(Y\)</span> and this effect does not depend on the value of the other predictor. (Additive Assumption)</p>
<p>One way of extending this model is to include a third predictor, called an <strong>interaction term</strong>, which is constructed by computing the product of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2.\)</span> This results in the model</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2 + \epsilon.
\]</span></p>
<p>This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as</p>
<p><span class="math display">\[
\begin{align*}
Y &amp;= \beta_0 + \underbrace{(\beta_1 + \beta_3 X_2)}_{=\tilde{\beta}_1} X_1 + \beta_2 X_2 + \epsilon,
\end{align*}
\]</span></p>
<p>where the new slope parameter <span class="math inline">\(\tilde{\beta}_2\)</span> is a linear function of <span class="math inline">\(X_2\)</span></p>
<p><span class="math display">\[
\tilde{\beta}_1\equiv\tilde{\beta}_1(X_2)=\beta_1 + \beta_3 X_2.
\]</span></p>
<p>Thus, a change in the value of <span class="math inline">\(X_2\)</span> will change the association between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>A similar argument shows that a change in the value of <span class="math inline">\(X_1\)</span> changes the association between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>Let us return to the <code>Advertising</code> example. A linear model that uses <code>radio</code>, <code>TV</code>, and an interaction, <code>radio</code><span class="math inline">\(\times\)</span><code>radio</code>, between the two to predict <code>sales</code> takes the form</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\times(\)</span> <code>radio</code><span class="math inline">\(\times\)</span> <code>TV</code><span class="math inline">\()+\epsilon\)</span>
</center>
<p>which can be rewritten as</p>
<center>
<code>sales</code> <span class="math inline">\(=\beta_0 + (\beta_1+ \beta_3\times\)</span> <code>radio</code> <span class="math inline">\()\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p><br></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_3\)</span> denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising (or vice-versa).</li>
</ul>
<p><img src="images/Tab_3_9.png" class="img-fluid"></p>
<p><strong>Interpretation of Table 3.9:</strong></p>
<ul>
<li>Both (separate) main effects, <code>TV</code> and <code>radio</code>, are statistically significant (<span class="math inline">\(p\)</span>-values smaller than 0.01).</li>
<li>Additionally, the <span class="math inline">\(p\)</span>-value for the interaction term, <code>TV</code><span class="math inline">\(\times\)</span><code>radio</code>, is extremely low, indicating that there is strong evidence for <span class="math inline">\(H_1: \beta_3\neq 0.\)</span> In other words, it is clear that the true relationship is not additive.</li>
</ul>
<p><strong>Hierarchical Principle:</strong></p>
<p>If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
<p><strong>Interactions with Qualitative Variables:</strong></p>
<p>An interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.</p>
<p>Consider the <code>Credit</code> data set and suppose that we wish to predict <code>balance</code> using the predictors:</p>
<ul>
<li><code>income</code> (quantitative) and</li>
<li><code>student</code> (qualitative) using a dummy variable with <span class="math inline">\(x_{i2}=1\)</span> if <span class="math inline">\(i\)</span>th person is a student and <span class="math inline">\(x_{i2}=0\)</span> if not.</li>
</ul>
<p>In the absence of an interaction term, the model takes the form <img src="images/Eq_3_34.png" class="img-fluid"></p>
<p>Thus, the regression lines for students and non-students have different intercepts, <span class="math inline">\(\beta_0+\beta_2\)</span> versus <span class="math inline">\(\beta_0\)</span>, but the same slope <span class="math inline">\(\beta_1\)</span>.</p>
<p>This represents a potentially serious limitation of the model, since in fact a change in <code>income</code> may have a very different effect on the credit card balance of a student versus a non-student.</p>
<p>This limitation can be addressed by adding an interaction variable, created by multiplying <code>income</code> with the dummy variable for student. Our model now becomes <img src="images/Eq_3_35.png" class="img-fluid"></p>
<p>Now we have different intercepts for students and non-students but also different slopes for these groups. <img src="images/Fig_3_7.png" class="img-fluid"></p>
</section>
<section id="polynomial-regression-non-linear-relationships" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="polynomial-regression-non-linear-relationships">Polynomial Regression: Non-linear Relationships</h4>
<p>Polynomial regression allows to accommodate non-linear relationships between the predictors <span class="math inline">\(X\)</span> and the outcome <span class="math inline">\(Y.\)</span> <img src="images/Fig_3_8.png" class="img-fluid"></p>
<p>For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form</p>
<center>
<code>mpg</code> <span class="math inline">\(=\beta_0 + \beta_1\times\)</span> <code>horsepower</code> <span class="math inline">\(+ \beta_2\times(\)</span><code>horsepower</code><span class="math inline">\()^2+\epsilon\)</span>
</center>
<p>This regression model involves predicting <code>mpg</code> using a non-linear function of <code>horsepower</code>. <strong>But it is still a linear model!</strong> It’s simply a multiple linear regression model with <span class="math inline">\(X_1=\)</span><code>horsepower</code> and <span class="math inline">\(X_2 =(\)</span><code>horsepower</code><span class="math inline">\()^2.\)</span></p>
<p>So we can use standard linear regression software to estimate <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> in order to produce a non-linear fit.</p>
<p><img src="images/Tab_3_10.png" class="img-fluid"></p>
</section>
</section>
<section id="ch.-3.3.3-potential-problems" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ch.-3.3.3-potential-problems">(Ch. 3.3.3) Potential Problems</h3>
<p><strong>1. Non-linearity of the response-predictor relationships.</strong></p>
<p>Diagnostic residual plots are most useful to detect possible non-linear response-predictor relationships.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ISLR2"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Auto) </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Gives the variable names in the Auto dataset</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># names(Auto)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Simple linear regression</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>lmobj_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Quadratic regression </span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>lmobj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_1, <span class="at">which =</span> <span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_2, <span class="at">which =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><em>Residual plots</em> are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals,</p>
<p><span class="math display">\[
e_i = y_i - \hat{y}_i,
\]</span></p>
<p>versus the predictor <span class="math inline">\(x_i.\)</span></p>
<p>In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values <span class="math inline">\(\hat{y}_i.\)</span> Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.</p>
<p>If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as</p>
<p><span class="math display">\[
\log(X),\; \sqrt{X},\; \text{or}\; X^2
\]</span></p>
<p>in the regression model. In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue.</p>
<p><strong>2. Correlation of Error Terms</strong></p>
<p>An important assumption of the linear regression model is that the error terms, <span class="math inline">\(\epsilon_1, \epsilon_2, \dots , \epsilon_n\)</span>, are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that <span class="math inline">\(\epsilon_i\)</span> is positive provides little or no information about the sign of <span class="math inline">\(\epsilon_{i+1}.\)</span> The <strong>standard errors</strong> that are computed for the estimated regression coefficients or the fitted values are based on the <strong>assumption of uncorrelated error terms</strong>. If in fact there is correlation among the error terms, then the estimated standard errors will tend to <strong>underestimate</strong> the true standard errors.</p>
<p>Correlations among the error terms typically occur in time series data (see Fig. 3.10).</p>
<p><img src="images/Fig_3_10.png" class="img-fluid"></p>
<p><strong>3. Non-Constant Variance of Error Terms</strong></p>
<p>Another important assumption of the linear regression model is that the error terms have a constant variance,</p>
<p><span class="math display">\[
Var(\epsilon_i) = \sigma^2.
\]</span></p>
<p>The standard formulas for standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.</p>
<p>One can identify <strong>non-constant variances “heteroscedasticity”</strong> in the errors, using diagnostic residual plots.</p>
<p>Often one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values which indicates. When faced with this problem, one possible solution is to transform the response <span class="math inline">\(Y\)</span> using a concave function such as</p>
<p><span class="math display">\[
\log(Y)\;\text{ or }\; \sqrt{Y}.
\]</span></p>
<p>Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Quadratic regression </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>lmobj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Quadratic regression with transformed response log(Y)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>lmobj_3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">I</span>(<span class="fu">log</span>(mpg)) <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_2, <span class="at">which =</span> <span class="dv">1</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_3, <span class="at">which =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>4. Outliers</strong></p>
<p>An outlier is a point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the outlier model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p>
<p>Outliers typically have a strong effect on the <span class="math inline">\(R^2\)</span> value since they add a <strong>very large residual</strong> to its computation.</p>
<p>Figure 3.12 in the textbook <code>ISLR</code> shows a clear outlier (observation 20) which, however, has a typical predictor value <span class="math inline">\(x_i\)</span>. Such outliers have little effect on the regression fit. <img src="images/Fig_3_12.png" class="img-fluid"></p>
<p>Figure 3.13 in the textbook <code>ISLR</code> shows again a clear outlier (observation 41) which has a predictor value <span class="math inline">\(x_i\)</span> that is <em>very atypical</em>. Such outliers are said to have large <strong>leverage</strong> giving them power to affect the regression fit considerably. <img src="images/Fig_3_13.png" class="img-fluid"></p>
<p>Summary: Critical outliers have both, large residuals <em>and</em> large leverage.</p>
<p><strong>5. High Leverage Points</strong></p>
<p>In order to quantify an observation’s leverage, we compute the <strong>leverage statistic</strong> <span class="math inline">\(h_i\)</span> for each observation <span class="math inline">\(i=1,\dots,n.\)</span> A large value of this statistic indicates an observation with high leverage. For a simple linear regression,</p>
<p><span class="math display">\[
h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_{j=1}^n(x_j-\bar{x})^2}
\]</span></p>
<p>There is a simple extension of <span class="math inline">\(h_i\)</span> to the case of multiple predictors, though we do not provide the formula here.</p>
<ul>
<li>The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span></li>
<li>The average leverage for all the observations is equal to <span class="math inline">\(\bar{h}=\frac{1}{n}\sum_{i=1}^n h_i=(p + 1)/n.\)</span></li>
<li>If a given observation has a leverage statistic <span class="math inline">\(h_i\)</span> that greatly exceeds <span class="math inline">\((p+1)/n,\)</span> then we may suspect that the corresponding point has high leverage.</li>
</ul>
<p><strong>6. Collinearity</strong></p>
<p>Collinearity refers to the situation in which two or more predictor variables are closely related to one another.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ISLR2"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Credit) <span class="co"># names(Credit)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y =</span> Credit<span class="sc">$</span>Age,    <span class="at">x =</span> Credit<span class="sc">$</span>Limit, <span class="at">main =</span> <span class="st">"No Collinearity"</span>, <span class="at">ylab =</span> <span class="st">"Age"</span>, <span class="at">xlab =</span> <span class="st">"Limit"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y =</span> Credit<span class="sc">$</span>Rating, <span class="at">x =</span> Credit<span class="sc">$</span>Limit, <span class="at">main =</span> <span class="st">"Strong Collinearity"</span>, <span class="at">ylab =</span> <span class="st">"Rating"</span>, <span class="at">xlab =</span> <span class="st">"Limit"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><img src="images/Fig_3_15.png" class="img-fluid"></p>
<p><img src="images/Tab_3_11.png" class="img-fluid"></p>
<p>We call this situation <strong>multicollinearity</strong>.</p>
<p>To detect multicollinearity issues, one can use the variance inflation factor (VIF)</p>
<p><span class="math display">\[
\operatorname{VIF}(\hat{\beta}_j)=\frac{1}{1-R^2_{X_j|X_-j}},
\]</span></p>
<p>where <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors.</p>
<ul>
<li>If <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is close to one, then multicollinearity is present, and <span class="math inline">\(\operatorname{VIF}(\hat{\beta}_j)\)</span> will be large.</li>
</ul>
<p>In the <code>Credit</code> data, a regression of balance on <code>age</code>, <code>rating</code>, and <code>limit</code> indicates that the predictors have VIF values of 1.01 (<code>age</code>), 160.67 (<code>rating</code>), and 160.59 (<code>limit</code>). Thus, as we suspected, there is considerable collinearity in the data!</p>
<p>Possible solutions:</p>
<ol type="1">
<li><p>Drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. <br> <strong>Caution:</strong> In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.</p></li>
<li><p>Combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.</p></li>
<li><p>Use a different estimation procedure like ridge regression.</p></li>
<li><p>Live with it. Sometimes you’re not allowed to drop or combine variables (e.g.&nbsp;important control variables) and also no other estimation procedure can be used. Then you have to live with large standard errors due to multicollinearity. But at least you know where the large stand errors are coming from.</p></li>
</ol>
</section>
</section>
<section id="ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="ch.-3.5-comparison-of-linear-regression-with-k-nearest-neighbors">(Ch. 3.5) Comparison of Linear Regression with K-Nearest Neighbors</h2>
<p>Linear regression is an example of a parametric approach because it assumes a linear model form for <span class="math inline">\(f(X).\)</span></p>
<p><strong>Advantages of parametric approaches:</strong></p>
<ul>
<li>Typically easy to fit</li>
<li>Simple interpretation</li>
<li>Simple inference</li>
</ul>
<p><strong>Disadvantages of parametric approaches:</strong></p>
<ul>
<li>The parametric model assumption can be far from true; i.e.</li>
</ul>
<p><span class="math display">\[
f(X) \neq \beta_0+\beta_1X_1+\dots+\beta_pX_p
\]</span></p>
<p>Alternative: <strong>Non-parametric methods</strong> such as <em>K-nearest neighbors regression</em> since non-parametric approaches do not explicitly assume a parametric form for <span class="math inline">\(f(X).\)</span></p>
<section id="k-nearest-neighbors-regression-knn-regression" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="k-nearest-neighbors-regression-knn-regression">K-nearest neighbors regression (KNN regression)</h4>
<p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0,\)</span> KNN regression regression …</p>
<ol type="1">
<li>identifies the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by the index set <span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}.\)</span></li>
<li>estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses <span class="math inline">\(y_i\)</span> with <span class="math inline">\(i\in\mathcal{N}_0.\)</span></li>
</ol>
<p>In other words,</p>
<p><span class="math display">\[
\hat{f}(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}y_i.
\]</span></p>
<p><img src="images/Fig_3_16.png" class="img-fluid"></p>
<p>In general, the optimal value for <span class="math inline">\(K\)</span> will depend on the <em>bias-variance tradeoff</em>, which we introduced in Chapter 2.</p>
<ul>
<li>A small value for <span class="math inline">\(K\)</span> provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation of <span class="math inline">\(K=1\)</span>.</li>
<li>A large value of <span class="math inline">\(K\)</span> provide a smoother and less wiggly fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in <span class="math inline">\(f(X).\)</span></li>
</ul>
<p>In Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of <span class="math inline">\(K\)</span> in KNN regression.</p>
<p>Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span> and vice versa.</p>
<p>Figure 3.17 of our textbook <code>ISLR</code> provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent the true <span class="math inline">\(f(X)\)</span>, while the blue curves correspond to the KNN fits using <span class="math inline">\(K = 1\)</span> (left plot) and <span class="math inline">\(K = 9\)</span> (right plot). In this case, the <span class="math inline">\(K = 1\)</span> predictions are far too variable, while the smoother <span class="math inline">\(K = 9\)</span> fit is much closer to the true <span class="math inline">\(f(X).\)</span> However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. <img src="images/Fig_3_17.png" class="img-fluid"></p>
<p>The blue dashed line in the left-hand panel of Figure 3.18 represents the linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. <img src="images/Fig_3_18.png" class="img-fluid"></p>
<p>Figure 3.19 displays a non-linear situations in which KNN performs much better than linear regression. <img src="images/Fig_3_19.png" class="img-fluid"></p>
<p><strong>Curse of dimensionality:</strong></p>
<p>Unfortunately, in higher dimensions, KNN often performs worse than linear regression, since non-parametric approaches suffer from the <strong>curse of dimensionality</strong>. Figure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise (i.e.&nbsp;redundant) predictors that are not associated with the response.</p>
<ul>
<li>When <span class="math inline">\(p = 1\)</span> or <span class="math inline">\(p = 2\)</span>, KNN outperforms linear regression.</li>
<li>But for <span class="math inline">\(p = 3\)</span> the results are mixed, and for <span class="math inline">\(p\geq 4\)</span> linear regression is superior to KNN. <img src="images/Fig_3_20.png" class="img-fluid"></li>
</ul>
<p>When <span class="math inline">\(p=1\)</span>, <span class="math inline">\(50\)</span> data points can provide enough information to estimate <span class="math inline">\(f(X)\)</span> accurately using non-parametric methods since the <span class="math inline">\(K\)</span> nearest neighbors can actually be close to a given test observation <span class="math inline">\(x_0.\)</span> However, when spreading the <span class="math inline">\(50\)</span> data points over a large number of, for instance, <span class="math inline">\(p=20\)</span> dimensions, even the <span class="math inline">\(K\)</span> nearest neighbors tend to become far away from <span class="math inline">\(x_0.\)</span></p>
</section>
</section>
<section id="r-lab-linear-regression" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="r-lab-linear-regression"><span class="header-section-number">3.2</span> <code>R</code>-Lab: Linear Regression</h2>
<section id="libraries" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="libraries"><span class="header-section-number">3.2.1</span> Libraries</h3>
<p>The <code>library()</code> function is used to load <em>libraries</em>, or groups of functions and data sets that are not included in the base <code>R</code> distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the <code>MASS</code> package, which is a very large collection of data sets and functions. We also load the <code>ISLR2</code> package, which includes the data sets associated with this book.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(MASS))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(ISLR2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as <code>MASS</code>, come with <code>R</code> and do not need to be separately installed on your computer. However, other packages, such as <code>ISLR2</code>, must be downloaded the first time they are used. This can be done directly from within <code>R</code>. For example, on a Windows system, select the <code>Install package</code> option under the <code>Packages</code> tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and <code>R</code> will automatically download the package. Alternatively, this can be done at the <code>R</code> command line via <code>install.packages("ISLR2")</code>. This installation only needs to be done the first time you use a package. However, the <code>library()</code> function must be called within each <code>R</code> session.</p>
</section>
<section id="simple-linear-regression" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="simple-linear-regression"><span class="header-section-number">3.2.2</span> Simple Linear Regression</h3>
<p>The <code>ISLR2</code> library contains the <code>Boston</code> data set, which records <code>medv</code> (median house value) for <span class="math inline">\(506\)</span> census tracts in Boston. We will seek to predict <code>medv</code> using <span class="math inline">\(12\)</span> predictors such as <code>rmvar</code> (average number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code> (percent of households with low socioeconomic status).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Boston)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
  medv
1 24.0
2 21.6
3 34.7
4 33.4
5 36.2
6 28.7</code></pre>
</div>
</div>
<p>To find out more about the data set, we can type <code>?Boston</code>.</p>
<p>We will start by using the <code>lm()</code> function to fit a simple linear regression model, with <code>medv</code> as the response and <code>lstat</code> as the predictor. The basic syntax is <code>lm(y ~ x, data)</code>, where <code>y</code> is the response, <code>x</code> is the predictor, and <code>data</code> is the data set in which these two variables are kept.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(predvars, data, env): object 'medv' not found</code></pre>
</div>
</div>
<p>The command causes an error because <code>R</code> does not know where to find the variables <code>medv</code> and <code>lstat</code>.</p>
<p>The next line tells <code>R</code> that the variables are in <code>Boston</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat, <span class="at">data =</span> Boston)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Alternatively, we can attach the <code>Boston</code> object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Boston)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we type <code>lm.fit</code>, some basic information about the model is output. For more detailed information, we use <code>summary(lm.fit)</code>. This gives us <span class="math inline">\(p\)</span>-values and standard errors for the coefficients, as well as the <span class="math inline">\(R^2\)</span> statistic and <span class="math inline">\(F\)</span>-statistic for the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>lm.fit</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Coefficients:
(Intercept)        lstat  
      34.55        -0.95  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.168  -3.990  -1.318   2.034  24.500 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.216 on 504 degrees of freedom
Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can use the <code>names()</code> function in order to find out what other pieces of information are stored in <code>lm.fit</code>. Although we can extract these quantities by name—e.g.&nbsp;<code>lm.fit$coefficients</code>—it is safer to use the extractor functions like <code>coef()</code> to access them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        </code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)       lstat 
 34.5538409  -0.9500494 </code></pre>
</div>
</div>
<p>In order to obtain a confidence interval for the coefficient estimates, we can use the <code>confint()</code> command.</p>
<p>Type <code>confint(lm.fit)</code> at the command line to obtain the confidence intervals for the linear regression coefficients.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                2.5 %     97.5 %
(Intercept) 33.448457 35.6592247
lstat       -1.026148 -0.8739505</code></pre>
</div>
</div>
<p>The <code>predict()</code> function can be used to produce confidence intervals and prediction intervals for the prediction of <code>medv</code> for a given value of <code>lstat</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm.fit, <span class="fu">data.frame</span>(<span class="at">lstat =</span> (<span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>))), </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">interval =</span> <span class="st">"confidence"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 29.80359 29.00741 30.59978
2 25.05335 24.47413 25.63256
3 20.30310 19.73159 20.87461</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm.fit, <span class="fu">data.frame</span>(<span class="at">lstat =</span> (<span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>))), </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">interval =</span> <span class="st">"prediction"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit       lwr      upr
1 29.80359 17.565675 42.04151
2 25.05335 12.827626 37.27907
3 20.30310  8.077742 32.52846</code></pre>
</div>
</div>
<p>For instance, the 95% confidence interval associated with a <code>lstat</code> value of 10 is <span class="math inline">\((24.47, 25.63)\)</span>, and the 95% prediction interval is <span class="math inline">\((12.828, 37.28)\)</span>. As expected, the confidence and prediction intervals are centered around the same point (a predicted value of <span class="math inline">\(25.05\)</span> for <code>medv</code> when <code>lstat</code> equals 10), but the latter are substantially wider.</p>
<p>We will now plot <code>medv</code> and <code>lstat</code> along with the least squares regression line using the <code>plot()</code> and <code>abline()</code> functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>There is some evidence for non-linearity in the relationship between <code>lstat</code> and <code>medv</code>. We will explore this issue later in this lab.</p>
<p>The <code>abline()</code> function can be used to draw any line, not just the least squares regression line. To draw a line with intercept <code>a</code> and slope <code>b</code>, we type <code>abline(a, b)</code>. Below we experiment with some additional settings for plotting lines and points. The <code>lwd = 3</code> command causes the width of the regression line to be increased by a factor of 3; this works for the <code>plot()</code> and <code>lines()</code> functions also. We can also use the <code>pch</code> option to create different plotting symbols.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm.fit, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk10-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk10-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv, <span class="at">pch =</span> <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk10-3.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv, <span class="at">pch =</span> <span class="st">"+"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk10-4.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, <span class="at">pch =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk10-5.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Next we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the <code>plot()</code> function directly to the output from <code>lm()</code>. In general, this command will produce one plot at a time, and hitting <em>Enter</em> will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the <code>par()</code> and <code>mfrow()</code> functions, which tell <code>R</code> to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, <code>par(mfrow = c(2, 2))</code> divides the plotting region into a <span class="math inline">\(2 \times 2\)</span> grid of panels.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Alternatively, we can compute the residuals from a linear regression fit using the <code>residuals()</code> function. The function <code>rstudent()</code> will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(lm.fit), <span class="fu">residuals</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk12-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(lm.fit), <span class="fu">rstudent</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk12-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>On the basis of the residual plots, there is some evidence of non-linearity.</p>
<p>Leverage statistics can be computed for any number of predictors using the <code>hatvalues()</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">hatvalues</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk13-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.max</span>(<span class="fu">hatvalues</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>375 
375 </code></pre>
</div>
</div>
<p>The <code>which.max()</code> function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">hatvalues</span>(lm.fit), <span class="at">decreasing =</span> <span class="cn">TRUE</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       375        415        374 
0.02686517 0.02495670 0.02097101 </code></pre>
</div>
</div>
<p>The <code>sort()</code> function can be used to sort and print values of a vector like <code>hatvalues(lm.fit)</code>.</p>
</section>
<section id="multiple-linear-regression" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">3.2.3</span> Multiple Linear Regression</h3>
<p>In order to fit a multiple linear regression model using least squares, we again use the <code>lm()</code> function. The syntax <code>lm(y ~ x1 + x2 + x3)</code> is used to fit a model with three predictors, <code>x1</code>, <code>x2</code>, and <code>x3</code>. The <code>summary()</code> function now outputs the regression coefficients for all the predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat <span class="sc">+</span> age, <span class="at">data =</span> Boston)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.981  -3.978  -1.283   1.968  23.158 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***
lstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***
age          0.03454    0.01223   2.826  0.00491 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.173 on 503 degrees of freedom
Multiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 
F-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>Boston</code> data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ ., data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can access the individual components of a summary object by name (type <code>?summary.lm</code> to see what is available). Hence <code>summary(lm.fit)$r.sq</code> gives us the <span class="math inline">\(R^2\)</span>, and <code>summary(lm.fit)$sigma</code> gives us the RSE. The <code>vif()</code> function, part of the <code>car</code> package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The <code>car</code> package is not part of the base <code>R</code> installation so it must be downloaded the first time you use it via the <code>install.packages()</code> function in <code>R</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(car)) <span class="co"># contains the vif() function</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">vif</span>(lm.fit)) <span class="co"># computes the VIF statistics and sorts them</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    chas    black     crim  ptratio       rm       zn    lstat      age 
1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 
     dis    indus      nox      rad      tax 
3.955945 3.991596 4.393720 7.484496 9.008554 </code></pre>
</div>
</div>
<p>What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, <code>age</code> has a high <span class="math inline">\(p\)</span>-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except <code>age</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>lm.fit1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> . <span class="sc">-</span> age, <span class="at">data =</span> Boston)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ . - age, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.6054  -2.7313  -0.5188   1.7601  26.2243 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
crim         -0.108006   0.032832  -3.290 0.001075 ** 
zn            0.046334   0.013613   3.404 0.000719 ***
indus         0.020562   0.061433   0.335 0.737989    
chas          2.689026   0.859598   3.128 0.001863 ** 
nox         -17.713540   3.679308  -4.814 1.97e-06 ***
rm            3.814394   0.408480   9.338  &lt; 2e-16 ***
dis          -1.478612   0.190611  -7.757 5.03e-14 ***
rad           0.305786   0.066089   4.627 4.75e-06 ***
tax          -0.012329   0.003755  -3.283 0.001099 ** 
ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
black         0.009321   0.002678   3.481 0.000544 ***
lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.74 on 493 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 
F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Alternatively, the <code>update()</code> function can be used.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>lm.fit1 <span class="ot">&lt;-</span> <span class="fu">update</span>(lm.fit, <span class="sc">~</span> . <span class="sc">-</span> age)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="interaction-terms" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="interaction-terms"><span class="header-section-number">3.2.4</span> Interaction Terms</h3>
<p>It is easy to include interaction terms in a linear model using the <code>lm()</code> function. The syntax <code>lstat:black</code> tells <code>R</code> to include an interaction term between <code>lstat</code> and <code>black</code>. The syntax <code>lstat * age</code> simultaneously includes <code>lstat</code>, <code>age</code>, and the interaction term <code>lstat</code><span class="math inline">\(\times\)</span><code>age</code> as predictors; it is a shorthand for <code>lstat + age + lstat:age</code>. %We can also pass in transformed versions of the predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(medv <span class="sc">~</span> lstat <span class="sc">*</span> age, <span class="at">data =</span> Boston))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat * age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.806  -4.045  -1.333   2.085  27.552 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***
lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
age         -0.0007209  0.0198792  -0.036   0.9711    
lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.149 on 502 degrees of freedom
Multiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 
F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="non-linear-transformations-of-the-predictors" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="non-linear-transformations-of-the-predictors"><span class="header-section-number">3.2.5</span> Non-linear Transformations of the Predictors</h3>
<p>The <code>lm()</code> function can also accommodate non-linear transformations of the predictors. For instance, given a predictor <span class="math inline">\(X\)</span>, we can create a predictor <span class="math inline">\(X^2\)</span> using <code>I(X^2)</code>. The function <code>I()</code> is needed since the <code>^</code> has a special meaning in a formula object; wrapping as we do allows the standard usage in <code>R</code>, which is to raise <code>X</code> to the power <code>2</code>. We now perform a regression of <code>medv</code> onto <code>lstat</code> and <code>lstat^2</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>lm.fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat <span class="sc">+</span> <span class="fu">I</span>(lstat<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + I(lstat^2))

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2834  -3.8313  -0.5295   2.3095  25.4148 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***
lstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***
I(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.524 on 503 degrees of freedom
Multiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 
F-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The near-zero <span class="math inline">\(p\)</span>-value associated with the quadratic term suggests that it leads to an improved model. We use the <code>anova()</code> function to further quantify the extent to which the quadratic fit is superior to the linear fit.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.fit, lm.fit2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Here Model 1 represents the linear submodel containing only one predictor, <code>lstat</code>, while Model 2 corresponds to the larger quadratic model that has two predictors, <code>lstat</code> and <code>lstat^2</code>. The <code>anova()</code> function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the <span class="math inline">\(F\)</span>-statistic is <span class="math inline">\(135\)</span> and the associated <span class="math inline">\(p\)</span>-value is virtually zero. This provides very clear evidence that the model containing the predictors <code>lstat</code> and <code>lstat^2</code> is far superior to the model that only contains the predictor <code>lstat</code>. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between <code>medv</code> and <code>lstat</code>. If we type</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm.fit2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/chunk22-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>then we see that when the <code>lstat^2</code> term is included in the model, there is little discernible pattern in the residuals.</p>
<p>In order to create a cubic fit, we can include a predictor of the form <code>I(X^3)</code>. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the <code>poly()</code> function to create the polynomial within <code>lm()</code>. For example, the following command produces a fifth-order polynomial fit:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>lm.fit5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> <span class="fu">poly</span>(lstat, <span class="dv">5</span>))</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit5)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ poly(lstat, 5))

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5433  -3.1039  -0.7052   2.0844  27.1153 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***
poly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***
poly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***
poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***
poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***
poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.215 on 500 degrees of freedom
Multiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 
F-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant <span class="math inline">\(p\)</span>-values in a regression fit.</p>
<p>By default, the <code>poly()</code> function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the <code>poly()</code> function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the <code>poly()</code> function, the argument <code>raw = TRUE</code> must be used.</p>
<p>Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(medv <span class="sc">~</span> <span class="fu">log</span>(rm), <span class="at">data =</span> Boston))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ log(rm), data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.487  -2.875  -0.104   2.837  39.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***
log(rm)       54.055      2.739   19.73   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.915 on 504 degrees of freedom
Multiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 
F-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="qualitative-predictors" class="level3" data-number="3.2.6">
<h3 data-number="3.2.6" class="anchored" data-anchor-id="qualitative-predictors"><span class="header-section-number">3.2.6</span> Qualitative Predictors</h3>
<p>We will now examine the <code>Carseats</code> data, which is part of the <code>ISLR2</code> library. We will attempt to predict <code>Sales</code> (child car seat sales) in <span class="math inline">\(400\)</span> locations based on a number of predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Carseats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education
1  9.50       138     73          11        276   120       Bad  42        17
2 11.22       111     48          16        260    83      Good  65        10
3 10.06       113     35          10        269    80    Medium  59        12
4  7.40       117    100           4        466    97    Medium  55        14
5  4.15       141     64           3        340   128       Bad  38        13
6 10.81       124    113          13        501    72       Bad  78        16
  Urban  US
1   Yes Yes
2   Yes Yes
3   Yes Yes
4   Yes Yes
5   Yes  No
6    No Yes</code></pre>
</div>
</div>
<p>The <code>Carseats</code> data includes qualitative predictors such as <code>shelveloc</code>, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor <code>shelveloc</code> takes on three possible values: <em>Bad</em>, <em>Medium</em>, and <em>Good</em>. Given a qualitative variable such as <code>shelveloc</code>, <code>R</code> generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sales <span class="sc">~</span> . <span class="sc">+</span> Income<span class="sc">:</span>Advertising <span class="sc">+</span> Price<span class="sc">:</span>Age, </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Carseats)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9208 -0.7503  0.0177  0.6754  3.3413 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***
CompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***
Income              0.0108940  0.0026044   4.183 3.57e-05 ***
Advertising         0.0702462  0.0226091   3.107 0.002030 ** 
Population          0.0001592  0.0003679   0.433 0.665330    
Price              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***
ShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***
ShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***
Age                -0.0579466  0.0159506  -3.633 0.000318 ***
Education          -0.0208525  0.0196131  -1.063 0.288361    
UrbanYes            0.1401597  0.1124019   1.247 0.213171    
USYes              -0.1575571  0.1489234  -1.058 0.290729    
Income:Advertising  0.0007510  0.0002784   2.698 0.007290 ** 
Price:Age           0.0001068  0.0001333   0.801 0.423812    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.011 on 386 degrees of freedom
Multiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 
F-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>contrasts()</code> function returns the coding that <code>R</code> uses for the dummy variables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Carseats)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(ShelveLoc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Good Medium
Bad       0      0
Good      1      0
Medium    0      1</code></pre>
</div>
</div>
<p>Use <code>?contrasts</code> to learn about other contrasts, and how to set them.</p>
<p><code>R</code> has created a <code>ShelveLocGood</code> dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a <code>ShelveLocMedium</code> dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for <code>ShelveLocGood</code> in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And <code>ShelveLocMedium</code> has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.</p>
</section>
<section id="writing-functions" class="level3" data-number="3.2.7">
<h3 data-number="3.2.7" class="anchored" data-anchor-id="writing-functions"><span class="header-section-number">3.2.7</span> Writing Functions</h3>
<p>As we have seen, <code>R</code> comes with many useful functions, and still more functions are available by way of <code>R</code> libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the <code>ISLR2</code> and <code>MASS</code> libraries, called <code>LoadLibraries()</code>. Before we have created the function, <code>R</code> returns an error if we try to call it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>LoadLibraries</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(expr, envir, enclos): object 'LoadLibraries' not found</code></pre>
</div>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">LoadLibraries</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in LoadLibraries(): could not find function "LoadLibraries"</code></pre>
</div>
</div>
<p>We now create the function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>LoadLibraries <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">library</span>(ISLR2)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">library</span>(MASS)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">print</span>(<span class="st">"The libraries have been loaded."</span>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now if we type in <code>LoadLibraries</code>, <code>R</code> will tell us what is in the function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>LoadLibraries</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>function() {
 library(ISLR2)
 library(MASS)
 print("The libraries have been loaded.")
}</code></pre>
</div>
</div>
<p>If we call the function, the libraries are loaded in and the print statement is output.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">LoadLibraries</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "The libraries have been loaded."</code></pre>
</div>
</div>
</section>
</section>
<section id="exercises" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="exercises"><span class="header-section-number">3.3</span> Exercises</h2>
<p>Prepare the following exercises of Chapter 3 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 2</li>
<li>Exercise 3</li>
<li>Exercise 8</li>
<li>Exercise 9</li>
</ul>
</section>
<section id="solutions" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="solutions"><span class="header-section-number">3.4</span> Solutions</h2>
<section id="exercise-1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-1">Exercise 1</h3>
<p><strong>1 a)</strong> Describe the null hypotheses to which the <span class="math inline">\(p\)</span>-values given in Table 3.4 correspond.</p>
<p><img src="DATA/Table.PNG" class="img-fluid"></p>
<p><strong>1 b)</strong> Explain what conclusions you can draw based on these <span class="math inline">\(p\)</span>-values. Your explanation should be phrased in terms of <code>sales</code>, <code>TV</code>, <code>radio</code>, and <code>newspaper</code>, rather than in terms of the coefficients of the linear model.</p>
<p><strong>Answers:</strong></p>
<p><strong>1 a)</strong> In Table 3.4, the null hypothesis for <code>TV</code> is that in the presence of <code>radio</code> ads and <code>newspaper</code> ads, <code>TV</code> ads have no effect on sales. Similarly, the null hypothesis for <code>radio</code> is that in the presence of <code>TV</code> ads and <code>newspaper</code> ads, <code>radio</code> ads have no effect on sales.</p>
<p><strong>1 b)</strong> On the one hand, the low p-values of <code>TV</code> and <code>radio</code> allow us to reject the “no effect” null hypotheses for <code>TV</code> and <code>radio</code>. Hence, we believe that <code>TV</code> (<code>radio</code>) ads have an effect on <code>sales</code> in the presence of <code>radio</code> (<code>TV</code>) and <code>newspaper</code> ads.<br> On the other hand, the high p-value of <code>newspaper</code> does <em>not</em> allow us to reject the “no effect” null-hypothesis. This constitutes an <em>inconclusive result</em> and only says that the possible effects of <code>newspaper</code> ads are not large enough to stand out from the estimation errors.</p>
<p><strong>Remember:</strong> An insignificant hypothesis test result is never informative about whether the tested null hypothesis is true. We do not have an error-control for falsely accepting the null-hypothesis. We only have an error-control (by the significance level) for falsely rejecting the null-hypothesis.</p>
</section>
<section id="exercise-2" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-2">Exercise 2</h3>
<p>Carefully explain the main difference between the KNN classifier and KNN regression methods.</p>
<p><strong>Answer:</strong></p>
<p>KNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for <span class="math inline">\(Y\)</span> (qualitative), given a certain predictor <span class="math inline">\(x_0\)</span>, where as the output for a KNN regression predicts the quantitative value for <span class="math inline">\(f(x_0)\)</span>, given a certain predictor <span class="math inline">\(x_0\)</span>.</p>
</section>
<section id="exercise-3" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-3">Exercise 3</h3>
<p>Suppose we have a data set with five predictors:</p>
<p><span class="math inline">\(X_1 =GPA\)</span></p>
<p><span class="math inline">\(X_2 = IQ\)</span></p>
<p><span class="math inline">\(X_3 = Gender\)</span> (<span class="math inline">\(1\)</span> for Female and <span class="math inline">\(0\)</span> for Male)</p>
<p><span class="math inline">\(X_4 =\)</span> Interaction between <span class="math inline">\(GPA\)</span> and <span class="math inline">\(IQ\)</span></p>
<p><span class="math inline">\(X_5 =\)</span> Interaction between <span class="math inline">\(GPA\)</span> and <span class="math inline">\(Gender\)</span></p>
<p>The response variable (in thousands of dollars) is defined as:</p>
<p><span class="math inline">\(Y =\)</span> starting salary after graduation</p>
<p>Suppose we use least squares to fit the model, and get:</p>
<p><span class="math inline">\(\hat{\beta}_0 = 50\)</span>, <span class="math inline">\(\hat{\beta}_1 = 20\)</span>, <span class="math inline">\(\hat{\beta}_2 = 0.07\)</span>, <span class="math inline">\(\hat{\beta}_3 = 35\)</span>, <span class="math inline">\(\hat{\beta}_4 = 0.01\)</span>, and <span class="math inline">\(\hat{\beta}_5 = −10\)</span>.</p>
<p>Thus we have:</p>
<p><span class="math display">\[
\begin{align*}
&amp;E[Y|X] = \\
&amp; 50 + 20\,\overbrace{GPA}^{X_1} + 0.07\,\overbrace{IQ}^{X_2} + 35\,\overbrace{Gender}^{X_3} +
0.01\,\overbrace{GPA\cdot IQ}^{X_4=X_1\cdot X_2} - 10\,\overbrace{GPA\cdot Gender}^{X_5=X_1\cdot X_3}
\end{align*}
\]</span></p>
<p><strong>3 a)</strong> Which answer is correct, and why?</p>
<ol type="i">
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, males earn more on average than females.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, females earn more on average than males.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, males earn more on average than females provided that the <span class="math inline">\(GPA\)</span> is high enough.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, females earn more on average than males provided that the <span class="math inline">\(GPA\)</span> is high enough.</li>
</ol>
<p><strong>Answer:</strong> Observe that:</p>
<p><span class="math display">\[
\begin{align*}
\text{Male\; $(X_3 = 0)$:}\quad   &amp; 50 + 20 X_1 + 0.07 X_2 + \phantom{3}0 + 0.01\,(X_1 \cdot X_2) -0     \\[1.5ex]
\text{Female\; $(X_3 = 1)$:}\quad &amp; 50 + 20 X_1 + 0.07 X_2 + 35 + 0.01(X_1 \cdot X_2) - 10\,X_1
\end{align*}
\]</span></p>
<p>Thus 3 a) iii. is correct, since once the <span class="math inline">\(X_1=\)</span><code>GPA</code> is high enough (<span class="math inline">\(35-10\,X_1&lt;0 \Leftrightarrow X_1&gt;3.5\)</span>), males earn more on average.</p>
<p><strong>3 b)</strong> Predict the salary of a female with <code>IQ</code> of 110 and a <code>GPA</code> of 4.0.</p>
<p><strong>Answer:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>GPA    <span class="ot">&lt;-</span>   <span class="dv">4</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>IQ     <span class="ot">&lt;-</span> <span class="dv">110</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>Gender <span class="ot">&lt;-</span>   <span class="dv">1</span> <span class="co"># female = 1</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>Y_hat  <span class="ot">&lt;-</span> <span class="dv">50</span> <span class="sc">+</span> <span class="dv">20</span><span class="sc">*</span>GPA <span class="sc">+</span> <span class="fl">0.07</span><span class="sc">*</span>IQ <span class="sc">+</span> <span class="dv">35</span><span class="sc">*</span>Gender <span class="sc">+</span> <span class="fl">0.01</span><span class="sc">*</span>GPA<span class="sc">*</span>IQ <span class="sc">-</span> <span class="dv">10</span><span class="sc">*</span>GPA</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>Y_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 137.1</code></pre>
</div>
</div>
<p><strong>3 c)</strong> True or false: Since the coefficient for the <code>GPA</code><span class="math inline">\(\times\)</span><code>IQ</code> interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.</p>
<p><strong>Answer:</strong></p>
<p>False. We must examine the <span class="math inline">\(p\)</span>-value (or the <span class="math inline">\(t\)</span>-statistic) of the regression coefficient to determine if the interaction term is statistically significant or not.</p>
</section>
<section id="exercise-8" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-8">Exercise 8</h3>
<p>This question involves the use of simple linear regression on the <code>Auto</code> data set.</p>
<p><strong>8 a)</strong> Use the <code>lm()</code> function to perform a simple linear regression with <code>mpg</code> as the response and <code>horsepower</code> as the predictor. Use the <code>summary()</code> function to print the results.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ISLR2"</span>)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"Auto"</span>)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform linear regression</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>lmObj_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data=</span>Auto)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Use summary function to print the results</span></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmObj_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower, data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5710  -3.2592  -0.3435   2.7630  16.9240 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***
horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.906 on 390 degrees of freedom
Multiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 
F-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Comment on the output. For example:</p>
<p><strong>i)</strong> Is there a relationship between the predictor and the response?</p>
<p><strong>Answer:</strong></p>
<p>Yes, there is. The predictor horsepower has a statistically significant (<span class="math inline">\(p&lt;0.001\)</span>) linear relationship with the response.</p>
<p><strong>ii)</strong> How strong is the relationship between the predictor and the response?</p>
<p><strong>Answer:</strong></p>
<p>Statistical significance does not necessarily mean a practically strong or important relationship.</p>
<p>To quantify the strength of the relationship between the predictor and the response, we can look at the following quantities:</p>
<ul>
<li>Residual Standard Error (RSE) (estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>) in comparison to the RSE of the trivial linear regression model with only an intercept.</li>
<li>The <span class="math inline">\(R^2\)</span> Statistic (the proportion of variance explained by the model)</li>
<li>The <span class="math inline">\(F\)</span>-Statistic</li>
</ul>
<p>The Residual Standard Error (RSE) of the regression model with <code>intercept</code> and <code>horsepower</code> as predictors is given by:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="do">## RSE of lm(mpg ~ horsepower):</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>RSS <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(lmObj_1)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>n   <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">resid</span>(lmObj_1))</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>RSE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(RSS<span class="sc">/</span>(n<span class="dv">-2</span>))</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(RSE, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.906</code></pre>
</div>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Alternatively: </span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(lmObj_1)<span class="sc">$</span>sigma, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.906</code></pre>
</div>
</div>
<p>This RSE value is considerable smaller than the RSE of a model with only an intercept:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>lmObj_onlyIntercept <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="sc">+</span><span class="dv">1</span>, <span class="at">data =</span> Auto)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>RSS_onlyIntercept   <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(lmObj_onlyIntercept)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>n                   <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">resid</span>(lmObj_onlyIntercept))</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>RSE_onlyIntercept   <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(RSS_onlyIntercept<span class="sc">/</span>(n<span class="dv">-1</span>))</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(RSE_onlyIntercept, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7.805</code></pre>
</div>
</div>
<p>Thus, the larger model with <code>horsepower</code> included explains more of the variances in the response variable <code>mpg</code>. Including <code>horsepower</code> as a predictor reduces the RSE by <code>((RSE_onlyIntercept - RSE)/RSE_onlyIntercept)*100</code> %; i.e.&nbsp;by 37.15%.</p>
<p>The <span class="math inline">\(R^2\)</span> value:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(lmObj_1)<span class="sc">$</span>r.squared, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.61</code></pre>
</div>
</div>
<p>shows that <span class="math inline">\(60\%\)</span> of variability in <span class="math inline">\(Y\)</span> can be explained using an intercept and <code>horsepower</code> as predictors.</p>
<p>The value of the <span class="math inline">\(F\)</span> statistic ::: {.cell}</p>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(lmObj_1)<span class="sc">$</span>fstatistic, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> value  numdf  dendf 
599.72   1.00 390.00 </code></pre>
</div>
<p>::: is much larger than <span class="math inline">\(1\)</span> which means that the linear regression model with intercept and <code>horsepower</code> fits the data significantly better than the trivial regression model with only an intercept.</p>
<p><strong>iii)</strong> Is the relationship between the predictor and the response positive or negative?</p>
<p><strong>Answer:</strong></p>
<p>The relationship is negative, as we can see from the parameter estimate for <code>horsepower</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmObj_1)[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>horsepower 
-0.1578447 </code></pre>
</div>
</div>
<p><strong>iv)</strong> What is the predicted <code>mpg</code> associated with a <code>horsepower</code> of <span class="math inline">\(98\)</span>? What are the associated <span class="math inline">\(95\%\)</span> confidence and prediction intervals?</p>
<p><strong>Answer:</strong></p>
<p>The predicted value plus confidence interval:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Horsepower of 98</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>new_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">horsepower =</span> <span class="dv">98</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence interval </span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(<span class="at">object =</span> lmObj_1, <span class="at">newdata =</span> new_df, <span class="at">interval =</span> <span class="st">"confidence"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 24.46708 23.97308 24.96108</code></pre>
</div>
</div>
<p>The predicted value plus prediction interval: ::: {.cell}</p>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Horsepower of 98</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>new_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">horsepower =</span> <span class="dv">98</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co"># prediction interval</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(<span class="at">object =</span> lmObj_1, <span class="at">newdata =</span> new_df, <span class="at">interval =</span> <span class="st">"prediction"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit     lwr      upr
1 24.46708 14.8094 34.12476</code></pre>
</div>
<p>:::</p>
<p><strong>8 b)</strong> Plot the response and the predictor. Use the <code>abline()</code> function to display the least squares regression line.</p>
<p><strong>Answer:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> Auto<span class="sc">$</span>horsepower, <span class="at">y =</span> Auto<span class="sc">$</span>mpg, <span class="at">ylab =</span> <span class="st">"MPG"</span>, <span class="at">xlab =</span> <span class="st">"Horsepower"</span>)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lmObj_1, <span class="at">col=</span><span class="st">"blue"</span>)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, </span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"(y,x)"</span>, <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"("</span>,<span class="fu">hat</span>(y),<span class="st">",x)"</span>))), </span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="cn">NA</span>), <span class="at">lty=</span><span class="fu">c</span>(<span class="cn">NA</span>,<span class="dv">1</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>8 c)</strong> Use the <code>plot()</code> function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.</p>
<p><strong>Answer:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmObj_1, <span class="at">col=</span><span class="st">'blue'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Looking at the smoothing line of the residuals (<span class="math inline">\(e_i=y_i−\hat{y}_i\)</span>) vs.&nbsp;the fitted values (<span class="math inline">\(\hat{y}_i\)</span>), there is a strong pattern in the residuals, indicating non-linearity. You can see evidence of this also in the scatter plot in the answer for question 8 b).</p>
<p>There also appears to be non-constant variance in the error terms (heteroscedasticity), but this may be corrected to an extent when trying a quadratic fit. If not, transformations such as <span class="math inline">\(log(y)\)</span> or <span class="math inline">\(\sqrt{y}\)</span> can shrink larger responses by a greater amount and reduce this issue.</p>
<p>There are some observations with large standardized residuals &amp; high leverage (hence, high Cook’s Distance) that we need to review.</p>
</section>
<section id="exercise-9" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-9">Exercise 9</h3>
<p>This question involves the use of multiple linear regression on the <code>Auto</code> data set.</p>
<p><strong>9 a)</strong> Produce a scatterplot matrix which includes all of the variables in the data set.</p>
<p><strong>Answer:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ISLR2"</span>)</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"Auto"</span>)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce scatterplot matrix</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(Auto)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>9 b)</strong> Compute the matrix of correlations between the variables using the function <code>cor()</code>. You will need to exclude the <code>name</code> variable, which is qualitative.</p>
<p><strong>Answer:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(<span class="fu">subset</span>(Auto, <span class="at">select =</span> <span class="sc">-</span>name)), <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              mpg cylinders displacement horsepower weight acceleration year
mpg           1.0      -0.8         -0.8       -0.8   -0.8          0.4  0.6
cylinders    -0.8       1.0          1.0        0.8    0.9         -0.5 -0.3
displacement -0.8       1.0          1.0        0.9    0.9         -0.5 -0.4
horsepower   -0.8       0.8          0.9        1.0    0.9         -0.7 -0.4
weight       -0.8       0.9          0.9        0.9    1.0         -0.4 -0.3
acceleration  0.4      -0.5         -0.5       -0.7   -0.4          1.0  0.3
year          0.6      -0.3         -0.4       -0.4   -0.3          0.3  1.0
origin        0.6      -0.6         -0.6       -0.5   -0.6          0.2  0.2
             origin
mpg             0.6
cylinders      -0.6
displacement   -0.6
horsepower     -0.5
weight         -0.6
acceleration    0.2
year            0.2
origin          1.0</code></pre>
</div>
</div>
<p><strong>9 c)</strong> Use the <code>lm()</code> function to perform a multiple linear regression with <code>mpg</code> as the response and all other variables except <code>name</code> as the predictors. Use the <code>summary()</code> function to print the results. Comment on the output by answering the below questions 9 c i) to 9 c iii).</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-scrolled="true">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform multiplie linear regression</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>fit.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span>name, <span class="at">data=</span>Auto)</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ . - name, data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.5903 -2.1565 -0.1169  1.8690 13.0604 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***
cylinders     -0.493376   0.323282  -1.526  0.12780    
displacement   0.019896   0.007515   2.647  0.00844 ** 
horsepower    -0.016951   0.013787  -1.230  0.21963    
weight        -0.006474   0.000652  -9.929  &lt; 2e-16 ***
acceleration   0.080576   0.098845   0.815  0.41548    
year           0.750773   0.050973  14.729  &lt; 2e-16 ***
origin         1.426141   0.278136   5.127 4.67e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.328 on 384 degrees of freedom
Multiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 
F-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><strong>9 c i)</strong> Is there a relationship between the predictors and the response?</p>
<p><strong>Answer:</strong></p>
<p>Yes, there is a relationship between the predictors and the response. By testing the null hypothesis of whether all (except intercept) the regression coefficients are zero (i.e.&nbsp;H<span class="math inline">\(_0\)</span>: <span class="math inline">\(\beta_1=\dots=\beta_7=0\)</span>), we can see that the <span class="math inline">\(F\)</span>-statistic is big and its <span class="math inline">\(p\)</span>-value is close to zero, indicating evidence against the null hypothesis.</p>
<p><strong>9 c ii)</strong> Which predictors appear to have a statistically significant relationship to the response?</p>
<p><strong>Answer:</strong></p>
<p>Looking at the <span class="math inline">\(p\)</span>-values associated with each predictor’s <span class="math inline">\(t\)</span>-statistic, we see that <code>displacement</code>, <code>weight</code>, <code>year</code>, and <code>origin</code> have a statistically significant relationship, while <code>cylinders</code>, <code>horsepower</code>, and <code>acceleration</code> do not.</p>
<p><strong>Caution:</strong> This consideration neglects issues due to multiple testing. When testing at the significance level <span class="math inline">\(\alpha=0.05\)</span>, then each single test has a type I error (false H<span class="math inline">\(_0\)</span> rejections) rate of up to <span class="math inline">\(5\%\)</span>. These type I error rates accumulate since we consider seven hypothesis tests simultaneously, and thus the probability of seeing one type I error among the seven tests is up to <span class="math inline">\(7\cdot 5\%=35\%\)</span>. So is quite likely to see one type I error.</p>
<p><strong>Bonferroni correction for multiple testing:</strong> To determine if any of the seven predictors is statistically significant, the corresponding <span class="math inline">\(p\)</span>-value must be smaller than <span class="math inline">\(\alpha/7\)</span>. For instance, with <span class="math inline">\(\alpha/7=0.05/7\approx 0.007\)</span>, only <code>weight</code>, <code>year</code>, and <code>origin</code> have a statistically significant relationships to the response.</p>
<p><strong>9 c iii)</strong> What does the coefficient for the <code>year</code> variable suggest?</p>
<p><strong>Answer:</strong></p>
<p>The regression coefficient for <code>year</code> suggests that, on average, one <code>year</code> later year-of-construction is associated with an increased <code>mpg</code> by <span class="math inline">\(0.75\)</span>, when holding every other predictor value constant.</p>
<p><strong>9 d)</strong> Use the <code>plot()</code> function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?</p>
<p><strong>Answer:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">1</span>))</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<ul>
<li><p>The “Residuals vs Fitted” plot (1st plot) shows some systematic deviations of the residuals from <span class="math inline">\(0\)</span>. The reason is that we are imposing a straight “line” (better hyper plane) fit for the conditional mean function <span class="math inline">\(E[Y|X]=f(X)\)</span> which appears non-linear here. This results in a systematic underestimation of the true conditional mean function for large and small fitted values <span class="math inline">\(\hat{y}=\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p\)</span>.</p></li>
<li><p>The “Normal Q-Q” plot (2nd plot) suggests non-normally distributed residuals–particularly the upper tail deviates from that of a normal distribution.</p></li>
<li><p>The “Residuals vs Leverage” plot (3rd plot) shows that there are some potential outliers that we can see when: standardized residuals are below <span class="math inline">\(-2\)</span> or above <span class="math inline">\(+2\)</span>. Moreover, the plot shows also potentially problematic “high-leverage” points with leverage values heavily exceeding the rule-of-thumb threshold <span class="math inline">\((p+1)/n=8/392=0.02\)</span>. All points with simultaneously high-leverages and large absolute standardized residuals should be handled with care since these may distort the estimation.</p></li>
<li><p>The “Scale-Location” plot (4th plot) shows is rather inconclusive about heteroscedasticity. However the “Residuals vs Fitted” plot (1st plot)shows some clear sign of heteroscedastic residuals.</p></li>
</ul>
<p><strong>9 e)</strong> Use the <code>*</code> and <code>:</code> symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?</p>
<p><strong>Answer:</strong></p>
<p>Violating the hierarchy principle:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>fit.lm0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower<span class="sc">+</span>cylinders<span class="sc">+</span>year<span class="sc">+</span>weight<span class="sc">:</span>displacement, </span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>Auto)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.lm0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower + cylinders + year + weight:displacement, 
    data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.1046 -2.8861 -0.2415  2.3967 15.3221 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         -1.343e+01  5.043e+00  -2.663  0.00807 ** 
horsepower          -3.914e-02  1.278e-02  -3.063  0.00234 ** 
cylinders           -1.358e+00  3.233e-01  -4.201 3.31e-05 ***
year                 6.661e-01  6.019e-02  11.067  &lt; 2e-16 ***
weight:displacement -3.354e-06  1.352e-06  -2.480  0.01355 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.985 on 387 degrees of freedom
Multiple R-squared:  0.7419,    Adjusted R-squared:  0.7393 
F-statistic: 278.2 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Following the hierarchical principle: ::: {.cell}</p>
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>fit.lm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span>horsepower<span class="sc">+</span>cylinders<span class="sc">+</span>year<span class="sc">+</span>weight<span class="sc">*</span>displacement, </span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>Auto)</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower + cylinders + year + weight * displacement, 
    data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.7530 -1.8228 -0.0602  1.5780 12.6133 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         -2.210e+00  3.819e+00  -0.579  0.56316    
horsepower          -3.396e-02  9.560e-03  -3.552  0.00043 ***
cylinders            2.072e-01  2.914e-01   0.711  0.47756    
year                 7.858e-01  4.555e-02  17.250  &lt; 2e-16 ***
weight              -1.084e-02  6.346e-04 -17.076  &lt; 2e-16 ***
displacement        -7.947e-02  9.905e-03  -8.023 1.26e-14 ***
weight:displacement  2.431e-05  2.141e-06  11.355  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.976 on 385 degrees of freedom
Multiple R-squared:  0.8568,    Adjusted R-squared:  0.8546 
F-statistic: 384.1 on 6 and 385 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<p>:::</p>
<p>Note that there is a difference between using <code>A:B</code> and <code>A*B</code> when running a regression. While the first includes only the interaction term between the variable <code>A</code> and <code>B</code>, the second one also includes the stand-alone variables <code>A</code> and <code>B</code>.</p>
<p>Generally, you should follow the hierarchical principle for interaction effects: If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
<p><strong>9 f)</strong></p>
<p>Try a few different transformations of the variables, such as <span class="math inline">\(\log(X)\)</span>, <span class="math inline">\(\sqrt{X}\)</span>, <span class="math inline">\(X^2\)</span>. Comment on your findings.</p>
<p><strong>Answer:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>fit.lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span><span class="fu">log</span>(weight)<span class="sc">+</span><span class="fu">sqrt</span>(horsepower)<span class="sc">+</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>                acceleration<span class="sc">+</span><span class="fu">I</span>(acceleration<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>Auto)</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.lm2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ log(weight) + sqrt(horsepower) + acceleration + 
    I(acceleration^2), data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.2932  -2.5082  -0.2237   2.0237  15.7650 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       178.30303   10.80451  16.503  &lt; 2e-16 ***
log(weight)       -14.74259    1.73994  -8.473 5.06e-16 ***
sqrt(horsepower)   -1.85192    0.36005  -5.144 4.29e-07 ***
acceleration       -2.19890    0.63903  -3.441 0.000643 ***
I(acceleration^2)   0.06139    0.01857   3.305 0.001037 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.99 on 387 degrees of freedom
Multiple R-squared:  0.7414,    Adjusted R-squared:  0.7387 
F-statistic: 277.3 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">1</span>))</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.lm2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>This try suffers basically from the same issues as the model considered in 9 d)</p>
<p>Let’s consider again the model with all predictors (except <code>name</code>), but with transforming the outcome variable <code>mpg</code> by a <span class="math inline">\(\log\)</span>-transformation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>fit.lm3 <span class="ot">&lt;-</span><span class="fu">lm</span>(<span class="fu">log</span>(mpg)<span class="sc">~</span> . <span class="sc">-</span>name, <span class="at">data=</span>Auto)</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.lm3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(mpg) ~ . - name, data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40955 -0.06533  0.00079  0.06785  0.33925 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.751e+00  1.662e-01  10.533  &lt; 2e-16 ***
cylinders    -2.795e-02  1.157e-02  -2.415  0.01619 *  
displacement  6.362e-04  2.690e-04   2.365  0.01852 *  
horsepower   -1.475e-03  4.935e-04  -2.989  0.00298 ** 
weight       -2.551e-04  2.334e-05 -10.931  &lt; 2e-16 ***
acceleration -1.348e-03  3.538e-03  -0.381  0.70339    
year          2.958e-02  1.824e-03  16.211  &lt; 2e-16 ***
origin        4.071e-02  9.955e-03   4.089 5.28e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1191 on 384 degrees of freedom
Multiple R-squared:  0.8795,    Adjusted R-squared:  0.8773 
F-statistic: 400.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">1</span>))</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.lm3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_LinearRegression_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>This model specification is much better!</p>
<ul>
<li>No clear issues of systematic under/over estimations for given fitted values.</li>
<li>No clear issues of heteroscedastic residuals.</li>
<li>Normality assumption may be wrong, but this isn’t problematic since we have a large dataset, such that a central limit theorem will make the estimators asymptotically normal distributed.</li>
<li>One large leverage point which, however, has a small residual.</li>
</ul>


</section>
</section>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>The assumption <span class="math inline">\(f(X) = \beta_0 + \beta_1 X\)</span> is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch2_StatLearning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch4_Classification.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>